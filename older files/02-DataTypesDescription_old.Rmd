# Back to the basics

## What can statistics tell us?

Almost all statistical analysis boils down to answering one of two questions:

	1.	Do these groups differ?
	2.	Is there a relationship between these variables?
	
These seem like simple questions — in theory you might just “look at the data” to answer the questions — so **why do we need statistics**?

The short answer is: **error** and **sampling**.

Whenever we collect data, we introduce **error**:

– our instruments have limits
– humans make mistakes
– environmental conditions vary
– biological systems are inherently noisy

And we are always measuring a subset of the **true population**. Even when we try to measure every individual (e.g., rare plant censuses), some individuals may be dormant, hidden, or unreachable.

If we were omniscient, we would simply compute the true population parameters (mean, variance, etc.), compare them directly, and skip statistics entirely. But in the real world, we estimate these values from samples and quantify the uncertainty around them.

Statistics is practical: it asks, **what can we say given imperfect measurements, small samples, and noisy biological systems?**

## The Frequentist Framework

The statistics we use most commonly in ecology — t-tests, ANOVA, linear models, GLMs, mixed models — are all built within the *frequentist* or *parametric* framework.

**Frequentist statistics assumes:**

**1.	There is a true but unknown parameter**
The population mean, variance, slope, etc. are fixed constants — but we don’t know their values.
**2.	Your sample is one of many possible samples you could have taken**
If you collected data again under the same conditions, you’d get a different sample and different estimates.
**3.	Variation in your estimates comes from sampling error**
The parameter does not vary — you vary because you sampled imperfectly.
Uncertainty is therefore in the estimator, not the parameter.
**4.	Probability statements describe long-run frequencies of repeated sampling**
For example:

A 95% confidence interval contains the true parameter in 95% of hypothetical repeated samples.
It does not mean “there is a 95% chance the true mean is in this interval.”

**What does “parametric” mean?**

Parametric tests assume the data follow a particular **probability distribution** (e.g., normal, Poisson, binomial).
These assumptions influence:

- which model we choose
- how we estimate uncertainty
- what kinds of inferences we can make

If you understand **distributions**, you understand **why models behave the way they do**.

## Bayesian statistics

The main alternative statistical framework to frequentist or parameteric statistics is *Bayesian* statistics. Bayesian statistics is based on Bayes’ Theorem, which provides a mathematical way to update our beliefs in light of new data. In this framework, we start with a prior distribution representing what we believe about a parameter before collecting data. We then combine this prior with the likelihood (the information contained in the data) to obtain a posterior distribution, which reflects our updated beliefs after seeing the evidence. This allows us to make intuitive probability statements about parameters—such as the probability that a parameter lies within a certain range—and provides a flexible foundation for modeling uncertainty, especially in hierarchical and ecological systems.

We (and most other ecologist) will learn frequentist methods first, because:

- Most classical ecological statistics (t-tests, ANOVA, GLMs) use the frequentist framework.
- Frequentist thinking aligns with standard experimental design.
- Historically, Bayesian methods were computationally difficult.
- Most journals still expect p-values and confidence intervals.

But: Bayesian methods are increasingly common in population models, species distribution models, hierarchical models, and complex ecological forecasting.

## Data Types and Why They Matter

Before we analyze or visualize anything, we need to understand **data types**, because the type of **data determines the probability distribution**, which **determines the statistical test**.

**Data type → Distribution → Model choice**

*Categorical variables* are non-numeric variables. 
**Examples**: Pet type (dog, cat, fish, bird), Size (small, medium, large), Car type (sedan, SUV), Present/Absent

*Numerical variables* are variables that are numbers, and occur in two forms:
*Discrete = Counts of things (no decimal points/fractions)
Data are discrete when it does not make sense to have a partial number of the variable. For instance, if counting the number of insects in a pond, it does not make sense to count a half a species.
**Examples**: Number of people in a building, number of trees in a plot, number of bugs in a pond 

*Continuous = Numerical data that can occur at any value. 
These are variables that can occur in any quantity. If you can have a fraction of this variable, it is continuous.
**Examples** = Height, Weight, Length

*Ordinal variables* (sometimes referred to as ranked) can be categorical or numerical, but the order matters.
**Examples** = Grades (A, B, C, D, E), Likert scale variables (Strongly disagree, Agree, Strongly Agree), Class rank (1, 2, 3, 4, 5)

## Linking Data Types to Distributions

In statistics, a distribution describes how likely different values are to occur. We choose a distribution, because different types of ecological data arise from different underlying processes.

Here, is a reference table of common statistical distribution and the associated statistical test:

<div style="overflow-x: auto;">

| **Data Type**                               | **Distribution (GLM Family)**                | **Common Link Function**                  | **Example Ecological Question**                                                | **Typical Tests / Models**                                 |
|---------------------------------------------|----------------------------------------------|--------------------------------------------|--------------------------------------------------------------------------------|-------------------------------------------------------------|
| **Continuous**                               | Normal (Gaussian)                            | Identity                                   | Do burned plots have taller seedlings than unburned plots?                    | t-test, ANOVA, Linear Regression, Gaussian GLM, LMM         |
| **Continuous (positive-only, skewed)**       | Gamma                                         | Log                                        | Does fertilizer increase biomass (always > 0)?                                 | Gamma GLM, Gamma GLMM                                       |
| **Continuous (bounded 0–1)**                 | Beta (for proportions not from counts)        | Logit / log–log                            | What proportion of cover is bare soil?                                         | Beta regression, Beta GLMM                                  |
| **Counts (integers ≥ 0)**                    | Poisson                                       | Log                                        | Does precipitation influence the number of pitfall-trapped insects?            | Poisson GLM, Poisson GLMM                                   |
| **Counts with overdispersion**               | Negative Binomial                             | Log                                        | Does nutrient addition affect flower counts when variance > mean?              | Negative Binomial GLM, NB GLMM                              |
| **Counts with many zeros**                   | Zero-inflated Poisson / Zero-inflated NB      | Log                                        | Do invasive grasses produce many zero seedling counts?                         | ZIP / ZINB models, hurdle models                            |
| **Binary outcomes (0/1)**                    | Binomial (Bernoulli)                          | Logit / probit                              | Does shade increase the probability that a seedling survives?                  | Logistic regression, Binomial GLM, Binomial GLMM            |
| **Proportions from counts (successes/n)**    | Binomial                                       | Logit                                       | What proportion of seeds germinate in each treatment?                          | Binomial GLM, Logistic regression                            |
| **Categorical (unordered)**                  | Multinomial                                    | Logit                                       | Do grazing treatments differ in vegetation type frequencies?                    | Multinomial logistic regression, Chi-square                 |
| **Ordinal (ordered categories)**             | Ordinal logistic (cumulative link)             | Logit / probit / complementary log–log      | Does grazing intensity affect seedling vigor ranks?                             | Proportional odds models (CLM/CLMM)                         |
| **Time-to-event data**                       | Exponential / Weibull / Survival models        | Log (depends on model)                      | How long do seedlings survive under drought vs irrigation?                     | Survival analysis, Cox regression                           |
| **Nonlinear continuous response**            | Non-normal, unknown                            | Identity or specialized                     | Is the relationship between age and growth curved?                              | GAM, GAMM                                                   |

</div>                                    |

We will apply statistics later in later chapters, but let's check out some common statistical distributions!

## Overview of Common Statistical Distributions

### The Normal Distribution

This is the classic bell-shaped distribution! The Normal distribution arises from the **Central Limit Theorem**, which states that when many small, independent factors add together (growth, genetics, microclimate, measurement error), their sum tends to be normally distributed.

**Ecological examples:**

- Height of Asclepias seedlings
- Tree diameter (DBH)
- Leaf nitrogen content
- Continuous traits affected by many genes

Check out the distribution:
```{r}
set.seed(1)
heights <- rnorm(1000, mean = 10, sd = 2)

hist(heights, breaks = 20, col = "lightblue",
     main = "Simulated Normal Distribution",
     xlab = "Height (cm)")
```

### The Binomial Distribution: Outcomes are “success” or “failure”

The binomial distribution arises when:

	1.	There are two possible outcomes (success/failure).
	2.	Each trial has the same probability of success.
	3.	You repeat the trial n times.

Example: flipping a biased coin 10 times.

Ecological examples:

- Seed germination (germinated / not)
- Survival (alive / dead)
- Flowering (reproductive / not)
- Presence/absence at survey points

Check out the distribution:
```{r}
set.seed(1)
germination <- rbinom(1000, size = 10, prob = 0.6)

hist(germination, breaks = 10, col = "lightgreen",
     main = "Simulated Binomial Distribution",
     xlab = "Number of seeds germinated (out of 10)")
```

### The Poisson Distribution: Counts from random events

The Poisson distribution arises when:

	1.	Events happen independently
	2.	With a constant average rate
	3.	Over a fixed time or space
	4.	And events can occur 0, 1, 2, 3… times

It often appears in ecology because many biological events behave like “random arrivals.”

Ecological examples:

- Number of flowers on a plant
- Number of birds detected per point survey
- Number of insects in a pitfall trap
- Seedling recruitment counts

In a true Poisson process: mean = variance
If variance >> mean → overdispersion → use Negative Binomial, not Poisson.

Check out the distribution:
```{r}
set.seed(1)
flower_counts <- rpois(1000, lambda = 4)

hist(flower_counts, breaks = 15, col = "salmon",
     main = "Simulated Poisson Distribution",
     xlab = "Number of flowers")
```

### The Negative Binomial Distribution: Overdispersed counts

Where it comes from

When data are “count-like” but more variable than a Poisson model allows, they are overdispersed. This often happens when:

- Individuals vary in productivity (e.g., some plants are “super reproducers”)
- There is clustering (patchy distributions)
- Rates vary in space/time

Ecological examples:

- Insect counts in patchy habitat
- Flower counts with many zeros
- Seed production with strong individual differences

Check out the distribution:
```{r}
set.seed(1)
nb_counts <- rnbinom(1000, size = 2, mu = 4)

hist(nb_counts, breaks = 15, col = "tan",
     main = "Simulated Negative Binomial Distribution",
     xlab = "Count")
```

### The Uniform Distribution: All values equally likely

This distribution arises when every value in a range has equal probability.

Relatively rare but useful for:

- Simulating randomness
- Creating null models
- Generating starting conditions for stochastic models

Check out the distribution:
```{r}
set.seed(1)
uniform_vals <- runif(1000, min = 0, max = 1)

hist(uniform_vals, breaks = 20, col = "gray",
     main = "Simulated Uniform Distribution",
     xlab = "Value")
```

## What is a model?

A model is a mathematical description of how we think a response variable changes with one or more predictors.
All the models we will use in this course take the general form:

Response = Deterministic pattern + Random variation

The “deterministic” part is what we explain (e.g., does grazing treatment affect plant height?).
The “random variation” comes from the distributions we just learned.

In later chapters, we will build statistical models!

## Key takeaways

- Different kinds of data follow different probability distributions.
- Those distributions determine which statistical models we use.
- You don’t need to memorize distributions — you just need to recognize the data type.
- Everything in this course builds on this foundation.








## Describing data

First, let's take a spin with data description. We are starting here to introduce a few concepts that will be important to understand, as we launch into statistical analysis. We will start by describing continuous data.

Let's use a simplified version of a dataset that I'm working with right now to look at the performance of several species of pollinator-friendly native species in agricultural gardens. Eventually, we'd like to develop seed to provide to restorationists for restoration of arid and semiarid grasslands. To do this, we need to understand how reliable these species are at establishing, producing seed, and attracting pollinators. Initially, we are conducting experiments with multiple populations of each species to determine how consistently plants grow, reproduce, and perform. Here, We will take a look at the initial heights of 1 population of one species, *Asclepias subverticulata*. 

When doing an actual research write-up, I ask myself 'What is the most important information for my audience to know about this dataset?' to guide what descriptions of the data to include. Here, we are just going to play around with numbers and R code! 

```{r}
#create vector of heights (cm) of one population of A. subverticulata
sedonapopulation <- c(3, 3, 3, 3, 7, 8, 9)
#take the mean
mean(sedonapopulation)
#calculate variance
var(sedonapopulation)
#calculate standard deviation
sd(sedonapopulation)
#calculate standard error
#base r doesn't have this function
#so we have to write our own
std_error <- function(x) sd(x)/sqrt(length(x))
std_error(sedonapopulation)
```

Most of the time when writing up results, you present a mean (sum of numbers divided by the number of observations), and an estimate of variation (a measure of how different the observations are). Here, we calculated three estimates variation, variance, standard deviation, and standard error.

Since you will occasionally need to include equations in your write-ups, let's get use to mathematical syntax, with these simple examples.

The formula for the sample mean is: $\mu = \frac{\Sigma x_i}{n}$;
where $\mu$ indicates the sample mean (sample = group of numbers we are looking at);
$\Sigma$ means to add what ever follows; 
$x_{i}$ is the value of one observation; (subscript i is often used to indicate that the action should be repeated for all values);
$n$ is the number of observations

*Why didn't we just use $\bar{x}$ to indicate the mean?* 
Because statisticians typically use $\bar{x}$ to indicate the true mean of the population, and $\mu$ to indicate the sample mean!

Just to show you, what the mean() function is doing, let's run:
```{r}
sum = 3+3+3+3+7+8+9 #add all the numbers in the sample
n = length(sedonapopulation) #or you can just calculate the number of height measurements
mean = sum/n; mean #divide sum by number
```

This formula is simple, but sometimes with more complex formulas, I will solve the equations by hand, to make sure that I understand what is happening!

The formula for variance is: $S^{2} = \frac{\Sigma(x_i - \mu)^{2}}{n - 1}$
where $S^{2}$ is the sample variance;
$\mu$ is the sample mean (remember from above);
$x_{i}$ is the value of one observation;
$n$ is the number of observations

In other words:
```{r}
#We determine how much each observation varies from the mean.
diffobs1 = mean - 3
diffobs2 = mean - 3
diffobs3 = mean - 3
diffobs4 = mean - 3
diffobs5 = mean - 7 
diffobs6 = mean - 8
diffobs7 = mean - 9 

#Then we square each of these. 
diffobj1_sq = diffobs1^2
diffobj2_sq = diffobs2^2
diffobj3_sq = diffobs3^2
diffobj4_sq = diffobs4^2
diffobj5_sq = diffobs5^2
diffobj6_sq = diffobs6^2
diffobj7_sq = diffobs7^2
```

*Why do we square the differences rather than just adding them up?* 
Because differences will be positive and negative. If we added them without squaring, sample differences would negate each other. We want an estimate of the absolute differences of samples from the mean.

```{r}
#Then we add the differences up.
sumofsquares = sum(diffobj1_sq, diffobj2_sq, diffobj3_sq, diffobj4_sq, diffobj5_sq, diffobj6_sq, diffobj7_sq)
#Divide the sum of squares by n - 1.
variance = sumofsquares/(n-1); variance 
```

*Why n - 1 instead of n?*

One reason is that, theoretically, because we are taking the mean of a sample, rather than all individuals, we underestimate the variance, so taking n-1 corrects that bias. Consider it a penalty for measuring a sample, not the entire population! Another practical reason is that dividing by n-1 makes the variance of a single sample undefined (unsolvable) rather than zero (solvable)

For standard deviation, we just take the square root of the variance, to remove the effect of squaring the differences when calculating the variance, and thus contextualizing our estimate of variation with regard to the mean. For example, the variance for the Sedona population is 7.48, larger than the sample mean of 5.12; while the standard deviation is 2.73, indicating that you would expect most observations to be 5.12 +/- 2.73 (we'll get to quantiles in a minute).

The formula for standard deviation is: $\sigma = \sqrt\frac{\Sigma(x_i - \mu)^{2}}{n - 1}$
where $\sigma$ is the sample variance;
$\mu$ is the sample mean;
$x_{i}$ is the value of one observation;
$n$ is the number of observations.

Finally, standard error and confidence intervals (we'll get to confidence intervals later) are the most common metrics of variance presented in journals. 

The formula for standard error is: $SE = \frac{\sigma}{\sqrt n}$ 
where $SE$ is standard error of the sample;
$\sigma$ is the standard deviation; and
$n$ is the number of samples.

*Why do we divide the standard deviation by the square root of the sample size to get standard error?*

While standard deviation measures the variation of the sample, standard error is meant to estimate the variation of the entire population of samples, if we could measure all individuals accurately. By dividing by the $\sqrt n$, the larger the sample size, the lower the error, because you have a more complete estimate of the true mean. In other words, standard deviation is just a measure of the variation of our sample, while standard error also incorporates information about our sampling process (how many individuals we have sampled). *Want to delve deep into standard error and deviation (me neither - ha)?: Google central limit theorem + standard error / standard deviation.* 

Means and variance measures are the most common way to describe quantitative data. However, several other metrics are useful for understanding the nature of your data and making decisions about analyses. A comprehensive understanding of your dataset includes describing these four features:

*Location (Mean, Median)
*Spread (Variability)
*Shape (Normal, skewed)
*Outliers

We've talked about means. The median is just the central number in the dataset, and helps you identify skewness.
```{r}
#an example of an unskewed population
sedona_unskewed <- c(1, 2, 3, 4, 5, 6, 7)
mean(sedona_unskewed)
median(sedona_unskewed)

#previous sedona population; skewed
sedonapopulation <- c(3, 3, 3, 3, 7, 8, 9)
mean(sedonapopulation)
median(sedonapopulation)
```
In an unskewed population, the mean will equal the median. Skew may not seem important, but it has statistical ramifications, AND it tells us something meaningful about the data. For instance, what if I said that mean price of a home in Flagstaff is 350K, but the median price of a home is 300K? We would know the that average house prices are driven up by a smaller number of expensive homes. 

We can quantify skew by comparing means and medians (mean > median = right-skewed; median > mean = left-skewed), but it is helpful to visualize the shape of data with a **histogram**. A **histogram** is a graph of the frequency of different measurements.

Let's add a few more observations to our Sedona populations (skewed and unskewed) and check out the look of the data!

```{r}
sedona_unskewed <- c(7, 2, 2, 3, 3, 3, 3, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 0.5)
mean(sedona_unskewed)
median(sedona_unskewed)
#I'm renaming sedonapopulation, sedona_skewed for this example
sedona_skewed <- c(3, 3, 3, 3, 7, 3, 4, 5, 6, 3, 3, 3, 4, 4, 6, 7, 8, 9, 3, 4, 5, 2)
mean(sedona_skewed)
median(sedona_skewed)
```

```{r, echo=FALSE}
hist(sedona_unskewed, main = "Mostly Unskewed", xlab = "Plant height (cm)", breaks=5)
```
In this relatively unskewed example, the tails are approximately even. This shape is also referred to as a normal or Gaussian distribution.
```{r, echo=FALSE}
h <- hist(sedona_unskewed, main = "Mostly Unskewed", xlab = "Plant height (cm)", breaks=5)

xfit <- seq(min(sedona_unskewed), max(sedona_unskewed), length = 40) 
yfit <- dnorm(xfit, mean = mean(sedona_unskewed), sd = sd(sedona_unskewed)) 
yfit <- yfit * diff(h$mids[1:2]) * length(sedona_unskewed) 

lines(xfit, yfit, col = "black", lwd = 2)
```
Here, we superimposed the bellshaped Normal or Gaussian distribution. 

```{r, echo=FALSE}
hist(sedona_skewed, main = "Skewed", xlab = "Plant height (cm)", breaks = 5)
```
In this example of skewed data, the tail tapers to the right, indicated that the data is skewed to the right.

In order to explain outliers, we need to look at quantiles! Quantiles are proportions of your data, in other words a way to break your data into chunks to understand spread. You can break your data into as many quantiles as you would like, but it is most common to break your data into 4 parts, also called quartiles. (If you break data into 5 parts, the components are called quintiles, 10 parts = deciles, 100 parts = percentiles). 

When you break data into quartiles, roughly 25 percent of the data occurs within each data chunk. The first chunk of the dataset contains 25% of the data (25th percentile; 25% of the data fall at or below this cut-off) is called the first quartile, the 50th percentile is called the sample median or the second quartile, the 75th percentile is called the third quartile. 

Box and whisker plots are commonly used to quickly examine quartiles. Let's check out our plant height data again, using a box and whisker plot.
```{r, echo=FALSE}
boxplot(sedona_skewed, main="Skewed", ylab="Plant height (cm)")
```
In the plot shown here, the box encapsulates the Interquartile Range (IQR); the center of the data ranging from the 25th percentile to the 75th. The black line in the middle of the box is the median (also called the 50th percentile, because it bisects the dataset; half of the data occur above the median and half below). The lines emerging from the box (whiskers) indicate the extent of the first and third quartiles, and usually corresponding with the minimum and maximum values of the dataset, unless there are **outliers**. An outlier is a datapoint that occurs outside of the 1st or 3rd quantile. Let's add one to our Sedona dataset, and see how it is represented on the box and whisker plot.

```{r}
#Let's add a plant height of 20.
sedona_skewed <- c(3, 3, 3, 3, 7, 3, 4, 5, 6, 3, 3, 3, 4, 4, 6, 7, 8, 9, 3, 4, 5, 2, 20)
boxplot(sedona_skewed, main="Skewed", ylab="Plant height (cm)")
```
The outlier appears as a dot on the box and whisker plot, and is the maximum value of the dataset. 

One other thing to note: Standard deviation also breaks data into meaningful segments, but is only used when data conform to a normal distribution; the mean +/- 1 SD accounts for 68% of the data, +/-2 SDs contains 95% of data, and +/- 3SD includes 99% of data. That said, I've never presented standard deviation in a manuscript; it is much more common to include standard error or confidence intervals (discussed later).

We've played around a lot with data, but what do you actually need to take away from this?:

*Data types (Categorical, Numerical discrete, Numerical continuous, Ordinal)
**Why?** We will select analyses based on data type.
*The two basic questions that most statistical analyses answer. 
**Why?** This will help you define what statistics can and can't do and bound our learning space!
*Ways to describe numerical continuous data (Location, Spread, Shape, Outliers).
**Why?** You will describe your results using these concepts in write-up AND these concepts will be important for certain analyses.
*Know how to calculate mean, median, and standard error.
**Why?** These are typical ways to describe data in results sections.
*Start to familiarize yourself with mathematical annotation.
**Why?** You may need to include equations in your methods section.
*Start to familiarize yourself with R code.
**Why?** Most researchers now use R to analyze, describe, and visualize their data.
*Be able to interpret a histogram and box-whisker plot.
**Why?** These are commonly used ways to visualize data.


