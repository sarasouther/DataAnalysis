# Describing Data

Before running any statistical test, we need to describe what we're working with. Descriptive statistics summarize the key features of a dataset—where the values cluster, how spread out they are, what shape the distribution takes, and whether any observations seem unusual. These summaries serve two purposes: they help us understand our data before analysis, and they communicate our findings to readers afterward.

When writing up results, I ask myself: *What is the most important information for my audience to know about this dataset?* That question guides which descriptive statistics to include. But before we can make those decisions, we need to understand what each measure tells us.

A comprehensive description of any dataset addresses four features:

1. **Location**: Where is the center of the data? (mean, median)
2. **Spread**: How variable are the observations? (variance, standard deviation, range, IQR)
3. **Shape**: Is the distribution symmetric or skewed? (histograms, comparing mean to median)
4. **Outliers**: Are there unusual observations? (boxplots, extreme values)

We'll work through each of these, building intuition for what the numbers mean and when to use which measure.

## The dataset: Milkweed seedling heights

Let's use data from a project I'm working on right now. We're studying the performance of several pollinator-friendly native species in agricultural gardens across Arizona. The goal is to develop seed sources for restoration of arid and semiarid grasslands. To do this, we need to understand how reliably these species establish, produce seed, and attract pollinators.

We're conducting experiments with multiple populations of each species to see how consistently plants grow and perform. Here, we'll look at the initial heights (in cm) of one population of *Asclepias subverticillata* (horsetail milkweed) from near Sedona.

```{r create-asclepias-data}
# Heights (cm) of A. subverticillata seedlings from the Sedona population
sedona <- c(3, 3, 3, 3, 7, 8, 9)
```

This is a small dataset, which makes it easy to see exactly what each calculation is doing. In practice, you'll work with much larger datasets, but the principles are the same.

## Location: Where is the center?

### The mean

The most common measure of central tendency is the **mean**—the sum of all values divided by the number of observations. In R:

```{r mean-basic}
mean(sedona)
```

The mean height is about 5.1 cm. But what is R actually calculating?

The formula for the sample mean is:

$$\bar{x} = \frac{\sum_{i=1}^{n} x_i}{n}$$

where:

- $\bar{x}$ ("x-bar") is the sample mean
- $\sum$ means "add up"
- $x_i$ is the value of observation $i$
- $n$ is the number of observations

Let's verify this by hand:

```{r mean-by-hand}
# Add all the values
total <- 3 + 3 + 3 + 3 + 7 + 8 + 9
total

# Count the observations
n <- length(sedona)
n

# Divide
sample_mean <- total / n
sample_mean
```

This matches `mean(sedona)`. Sometimes with more complex formulas, I'll work through the calculation by hand to make sure I understand what's happening.

**A note on notation:** You'll see different symbols used in different textbooks. The convention is:

- $\bar{x}$ = sample mean (what we calculate from our data)
- $\mu$ = population mean (the true mean of the entire population, which we usually don't know)

When we calculate `mean(sedona)`, we're estimating the population mean using our sample. The sample mean is our best guess at what the true population mean might be.

### The median

The **median** is the middle value when observations are arranged in order. Half the data fall above the median, and half below.

```{r median-basic}
median(sedona)
```

For our Sedona seedlings, the median height is 3 cm—quite different from the mean of 5.1 cm!

To find the median by hand:
1. Arrange values in order: 3, 3, 3, 3, 7, 8, 9
2. Find the middle value (for n=7, that's the 4th value): **3**

For an even number of observations, the median is the average of the two middle values.

### Mean vs. median: Which should you report?

The difference between mean and median tells us something important about our data. When the mean is much larger than the median (5.1 vs. 3), it suggests that a few high values are pulling the average up. This is called **right skew** or positive skew.

**Guidelines:**

| Situation | Report | Why |
|-----------|--------|-----|
| Symmetric distribution | Mean | Mean and median are similar; mean uses all data |
| Skewed distribution | Median | More representative of typical values |
| Comparing to other studies | Whatever they used | Enables comparison |
| Ecological interest in extremes | Both | Full picture of distribution |

Here's a real-world example: If I told you the mean home price in Flagstaff is $550,000, but the median is $425,000, what would you conclude? A few expensive homes are pulling the mean up. The median better represents what a typical home buyer would pay.

Similarly, if we're interested in "typical" seedling size, the median of 3 cm might be more informative than the mean of 5.1 cm. But if we want to estimate total biomass (where large individuals contribute disproportionately), the mean matters more.

### The mode

For categorical data, we often report the **mode**—the most frequently occurring value. For continuous data, the mode is less useful because each value might occur only once.

```{r mode-demo}
# For our seedling data, we can find the mode manually:
table(sedona)
```

The mode is 3 cm, which appears four times. When the mode, median, and mean all differ substantially (mode=3, median=3, mean=5.1), the distribution is definitely skewed.

## Spread: How variable are the observations?

Knowing the center isn't enough. Two datasets can have the same mean but very different amounts of variation. Measures of spread tell us how much individual observations differ from one another.

### Range

The simplest measure of spread is the **range**—the difference between the largest and smallest values.

```{r range-calc}
# The range
max(sedona) - min(sedona)

# Or see both endpoints
range(sedona)
```

The range of our seedling heights is 6 cm (from 3 to 9 cm). 

The range is easy to understand but has a major limitation: it depends entirely on the two most extreme values. A single outlier can dramatically change the range, making it sensitive to data entry errors or unusual observations.

### Variance

**Variance** measures the average squared deviation from the mean. It captures how spread out the data are around their center.

The formula for sample variance is:

$$s^2 = \frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n - 1}$$

where:

- $s^2$ is the sample variance
- $x_i$ is each observation
- $\bar{x}$ is the sample mean
- $n$ is the number of observations

Let's calculate this step by step:

```{r variance-by-hand}
# Step 1: Calculate the mean
xbar <- mean(sedona)
xbar

# Step 2: Calculate each deviation from the mean
deviations <- sedona - xbar
deviations

# Step 3: Square each deviation
squared_devs <- deviations^2
squared_devs

# Step 4: Sum the squared deviations
sum_sq <- sum(squared_devs)
sum_sq

# Step 5: Divide by n-1
variance <- sum_sq / (n - 1)
variance

# Verify with R's function
var(sedona)
```

**Why do we square the deviations?**

If we just added up the deviations, the positives and negatives would cancel out (the sum would always be zero—try it!). Squaring makes all values positive, so we can see the total magnitude of variation.

**Why divide by n-1 instead of n?**

This is called **Bessel's correction**. When we estimate variance from a sample, we tend to underestimate the true population variance. Dividing by n-1 instead of n corrects this bias. Think of it as a penalty for using a sample instead of measuring the entire population.

Another way to think about it: we used up one "degree of freedom" when we estimated the mean. Only n-1 of our observations are truly free to vary; once you know the mean and n-1 values, the last value is determined.

### Standard deviation

The **standard deviation** is simply the square root of the variance:

$$s = \sqrt{\frac{\sum_{i=1}^{n} (x_i - \bar{x})^2}{n - 1}}$$

```{r sd-calc}
# Square root of variance
sqrt(variance)

# Or use R's function
sd(sedona)
```

**Why take the square root?**

Variance is in squared units (cm²), which is hard to interpret. Standard deviation puts us back in the original units (cm), making it easier to relate to the data.

For our seedlings, the standard deviation is about 2.7 cm. This means observations typically deviate from the mean by roughly 2.7 cm.

**The 68-95-99.7 rule:** For normally distributed data, approximately:
- 68% of observations fall within 1 SD of the mean
- 95% fall within 2 SDs
- 99.7% fall within 3 SDs

So if seedling heights were normally distributed with mean 5.1 cm and SD 2.7 cm, we'd expect about 68% of seedlings to be between 2.4 and 7.8 cm tall.

### Standard error

The **standard error** (SE) is different from standard deviation—it measures uncertainty in the *mean itself*, not variation in individual observations.

$$SE = \frac{s}{\sqrt{n}}$$

```{r se-calc}
# R doesn't have a built-in SE function, so we write our own
std_error <- function(x) sd(x) / sqrt(length(x))
std_error(sedona)
```

**What's the difference between SD and SE?**

- **Standard deviation** describes how variable the data are. It's a property of the sample.
- **Standard error** describes how precisely we've estimated the mean. It decreases with larger samples.

If you measured 7 seedlings and got SD = 2.7 cm, measuring 700 seedlings would give you roughly the same SD (the population is still variable). But your SE would be much smaller (you'd be more confident in your estimate of the mean).

**When to report which:**

| Situation | Report | Why |
|-----------|--------|-----|
| Describing the sample | SD | Shows actual variation in your data |
| Comparing means across groups | SE or CI | Shows precision of estimates |
| Publication results | SE or 95% CI | Standard practice in ecology |

### Interquartile range (IQR)

The **interquartile range** is the difference between the 75th percentile (Q3) and 25th percentile (Q1). It captures the middle 50% of the data.

```{r iqr-calc}
# Get quartiles
quantile(sedona)

# IQR specifically
IQR(sedona)
```

For our seedlings, Q1 = 3 and Q3 = 7.5, so the IQR is 4.5 cm. The middle half of seedlings range from 3 to 7.5 cm tall.

The IQR is useful because it's resistant to outliers—unlike range, which can change dramatically with a single extreme value.

### Summary table: Measures of spread

| Measure | Formula | Units | Sensitive to outliers? | Best for |
|---------|---------|-------|------------------------|----------|
| Range | max - min | Original | Very | Quick overview |
| Variance | $\frac{\sum(x_i - \bar{x})^2}{n-1}$ | Squared | Yes | Calculations, ANOVA |
| SD | $\sqrt{\text{variance}}$ | Original | Yes | Describing variation |
| SE | $\frac{s}{\sqrt{n}}$ | Original | Yes | Precision of mean |
| IQR | Q3 - Q1 | Original | No | Skewed data, outliers present |

## Shape: Is the distribution symmetric?

The shape of a distribution tells us whether values are evenly spread around the center or bunched up on one side. This matters for choosing appropriate statistical methods—many tests assume symmetric (approximately normal) distributions.

### Comparing mean and median

A quick way to assess skewness:

- **Mean ≈ Median**: Distribution is roughly symmetric
- **Mean > Median**: Right-skewed (tail extends toward high values)
- **Mean < Median**: Left-skewed (tail extends toward low values)

```{r skew-comparison}
# Our Sedona data
mean(sedona)
median(sedona)

# Mean (5.1) > Median (3) suggests right skew
```

### Visualizing shape with histograms

A **histogram** shows the frequency of values in different ranges (bins). It's our primary tool for visualizing distribution shape.

```{r histogram-basic, fig.cap="Histogram of seedling heights showing the frequency of observations in different height ranges."}
# Let's expand our dataset to better show distributions
set.seed(123)
sedona_large <- c(3, 3, 3, 3, 7, 8, 9,  # original data
                  round(rnorm(30, mean = 4, sd = 2), 1))  # add more realistic variation
sedona_large <- sedona_large[sedona_large > 0]  # heights must be positive

hist(sedona_large, 
     breaks = 10,
     col = "lightblue",
     border = "white",
     main = "Distribution of Seedling Heights",
     xlab = "Height (cm)",
     ylab = "Number of seedlings")

# Add vertical lines for mean and median
abline(v = mean(sedona_large), col = "firebrick", lwd = 2, lty = 1)
abline(v = median(sedona_large), col = "forestgreen", lwd = 2, lty = 2)
legend("topright", 
       legend = c(paste("Mean =", round(mean(sedona_large), 1)),
                  paste("Median =", round(median(sedona_large), 1))),
       col = c("firebrick", "forestgreen"),
       lty = c(1, 2), lwd = 2)
```

**Reading a histogram:**

- **X-axis**: The variable being measured (height)
- **Y-axis**: Count or frequency of observations in each bin
- **Bars**: Each bar represents a range of values; taller bars mean more observations

**What to look for:**

- **Modes**: How many peaks? One (unimodal), two (bimodal), or more?
- **Symmetry**: Are the left and right sides roughly mirror images?
- **Skewness**: Does one tail extend further than the other?
- **Gaps**: Are there ranges with no observations?
- **Outliers**: Are there isolated bars far from the rest?

### The normal (Gaussian) distribution

Many statistical methods assume data follow a **normal distribution**—the familiar bell-shaped curve. Key features:

- Symmetric around the mean
- Mean = Median = Mode
- Most observations cluster near the center
- Tails extend equally in both directions

```{r normal-overlay, fig.cap="A symmetric distribution with a normal curve overlay. In a perfect normal distribution, the mean and median are identical.", echo=FALSE}
# Create approximately normal data
set.seed(42)
normal_data <- rnorm(200, mean = 5, sd = 1.5)

h <- hist(normal_data, breaks = 15, col = "lightblue", border = "white",
          main = "Approximately Normal Distribution",
          xlab = "Height (cm)", probability = TRUE)

# Overlay normal curve
xfit <- seq(min(normal_data), max(normal_data), length = 100)
yfit <- dnorm(xfit, mean = mean(normal_data), sd = sd(normal_data))
lines(xfit, yfit, col = "firebrick", lwd = 2)
```

### Skewed distributions

Real ecological data are often skewed. Right-skewed (positive skew) data are especially common for measurements that can't be negative:

- Body sizes
- Count data (number of offspring, flowers, seeds)
- Time to event (seed germination, time to flowering)
- Many physiological rates

```{r skewed-examples, fig.cap="Comparison of symmetric vs. right-skewed distributions. Note how the mean is pulled toward the tail in the skewed distribution.", echo=FALSE, fig.width=10, fig.height=4}
par(mfrow = c(1, 2))

# Symmetric
set.seed(42)
symmetric <- rnorm(200, mean = 10, sd = 2)
hist(symmetric, breaks = 15, col = "lightblue", border = "white",
     main = "Symmetric Distribution",
     xlab = "Value")
abline(v = mean(symmetric), col = "firebrick", lwd = 2)
abline(v = median(symmetric), col = "forestgreen", lwd = 2, lty = 2)

# Right-skewed
right_skew <- rgamma(200, shape = 2, rate = 0.5)
hist(right_skew, breaks = 15, col = "lightblue", border = "white",
     main = "Right-Skewed Distribution",
     xlab = "Value")
abline(v = mean(right_skew), col = "firebrick", lwd = 2)
abline(v = median(right_skew), col = "forestgreen", lwd = 2, lty = 2)

par(mfrow = c(1, 1))
```

In the right-skewed example, notice how the mean (red line) is pulled toward the tail, while the median (green dashed line) stays closer to where most values cluster.

## Outliers: Unusual observations

**Outliers** are observations that fall far from the rest of the data. They might represent:

- **Data entry errors**: A decimal point in the wrong place
- **Measurement problems**: Equipment malfunction, observer error
- **Real biological variation**: Genuinely unusual individuals
- **Different populations**: A sample that doesn't belong with the others

The key is to investigate outliers before deciding what to do with them.

### Detecting outliers with boxplots

**Boxplots** (box-and-whisker plots) display the five-number summary and flag potential outliers:

```{r boxplot-anatomy, fig.cap="Anatomy of a boxplot showing the median, interquartile range (IQR), whiskers, and outliers."}
# Add an outlier to our data
sedona_with_outlier <- c(sedona, 25)

boxplot(sedona_with_outlier,
        main = "Seedling Heights with Outlier",
        ylab = "Height (cm)",
        col = "lightblue")
```

**Reading a boxplot:**

| Component | What it shows |
|-----------|---------------|
| Bold line in box | Median (50th percentile) |
| Box | IQR: middle 50% of data (Q1 to Q3) |
| Whiskers | Extend to most extreme points within 1.5 × IQR |
| Points beyond whiskers | Potential outliers |

The 1.5 × IQR rule is a common convention for flagging outliers, but it's not absolute. In small samples, even normal data may have points beyond the whiskers.

### Boxplots for comparing groups

Boxplots are especially useful for comparing distributions across groups:

```{r boxplot-groups, fig.cap="Comparing seedling heights across populations. Boxplots make it easy to compare medians, spread, and presence of outliers."}
# Simulate data from three populations
set.seed(42)
flagstaff <- rnorm(30, mean = 6, sd = 1.5)
sedona_sim <- rnorm(30, mean = 4.5, sd = 2)
tucson <- rnorm(30, mean = 8, sd = 2.5)

# Combine into a data frame
populations <- data.frame(
  height = c(flagstaff, sedona_sim, tucson),
  population = rep(c("Flagstaff", "Sedona", "Tucson"), each = 30)
)

boxplot(height ~ population, data = populations,
        col = c("lightblue", "lightyellow", "lightgreen"),
        main = "Seedling Height by Population",
        xlab = "Population",
        ylab = "Height (cm)")
```

At a glance, we can see:
- Tucson seedlings are tallest (highest median)
- Sedona seedlings are most variable (largest box and whiskers)
- Flagstaff has the smallest spread
- No obvious outliers in any group

### What to do with outliers

Never automatically delete outliers! Instead:

1. **Investigate**: Check original datasheets. Is this a typo? Equipment error?
2. **Verify**: Could this be real biology? A 25 cm seedling might be a legacy plant, not a true seedling.
3. **Document**: Whatever you decide, record your reasoning in your code/notes.
4. **Consider impact**: Run your analysis with and without the outlier. Do conclusions change?

Options for handling outliers:

| Action | When appropriate |
|--------|------------------|
| Correct | Clear data entry error with recoverable true value |
| Remove | Confirmed error, or observation doesn't belong in sample |
| Keep | Real biological variation, even if unusual |
| Use robust methods | Analyze with median-based or rank-based tests |
| Transform | Log or other transform may reduce outlier influence |

## Describing categorical data

So far we've focused on continuous variables. Categorical data require different descriptions.

### Frequencies and proportions

For categorical data, we count how many observations fall in each category:

```{r categorical-describe}
# Burn severity categories for 50 plots
set.seed(42)
burn <- sample(c("low", "moderate", "high"), 50, replace = TRUE,
               prob = c(0.3, 0.4, 0.3))
burn <- factor(burn, levels = c("low", "moderate", "high"))

# Frequency table
table(burn)

# Proportions
prop.table(table(burn))

# As percentages
round(prop.table(table(burn)) * 100, 1)
```

**Reporting categorical data:**

- Report both counts and percentages: "15 plots (30%) had low burn severity"
- Include sample size so readers can judge reliability
- For binary outcomes, often report only one category: "Survival was 73% (n = 150)"

### Visualizing categorical data with bar plots

```{r barplot-categorical, fig.cap="Bar plot showing the distribution of burn severity across plots."}
barplot(table(burn),
        col = c("forestgreen", "orange", "firebrick"),
        main = "Distribution of Burn Severity",
        xlab = "Burn Severity",
        ylab = "Number of Plots")
```

For categorical data, bar plots show counts or proportions. Don't confuse these with histograms—bar plots have gaps between bars (discrete categories), while histograms have touching bars (continuous ranges).

## Confidence intervals

You'll often see results reported with **confidence intervals** (CIs) rather than standard errors. A 95% confidence interval provides a range of plausible values for the true population parameter.

### Calculating a 95% CI for the mean

For large samples (n > 30), the 95% CI is approximately:

$$\bar{x} \pm 1.96 \times SE$$

For smaller samples, we use the t-distribution:

$$\bar{x} \pm t_{\alpha/2, n-1} \times SE$$

```{r ci-calculation}
# Using our Sedona data
xbar <- mean(sedona)
se <- std_error(sedona)
n <- length(sedona)

# Get the t critical value for 95% CI with n-1 degrees of freedom
t_crit <- qt(0.975, df = n - 1)  # 0.975 because we want 2.5% in each tail
t_crit

# Calculate CI
lower <- xbar - t_crit * se
upper <- xbar + t_crit * se

cat("Mean:", round(xbar, 2), "cm\n")
cat("95% CI:", round(lower, 2), "to", round(upper, 2), "cm\n")
```

**Interpreting the CI:**

We're 95% confident that the true population mean falls between these bounds. If we repeated this study many times, 95% of the calculated CIs would contain the true mean.

**CI vs. SE:**

- SE is smaller (it's just one standard error)
- 95% CI ≈ mean ± 2 × SE (roughly)
- CI is more directly interpretable ("the true mean is probably in this range")

Most ecology journals prefer CIs over SE because they're easier to interpret and directly show uncertainty.

### Using t.test() for quick CIs

```{r ci-ttest}
t.test(sedona)$conf.int
```

This gives you the 95% CI directly, without manual calculation.

## Putting it all together: Complete description

Here's a template for fully describing a continuous variable:

```{r complete-description}
describe_continuous <- function(x, name = "Variable") {
  cat("Description of", name, "\n")
  cat("=====================================\n")
  cat("Sample size:", length(x[!is.na(x)]), "\n")
  cat("Missing values:", sum(is.na(x)), "\n\n")
  
  cat("Location:\n")
  cat("  Mean:", round(mean(x, na.rm = TRUE), 2), "\n")
  cat("  Median:", round(median(x, na.rm = TRUE), 2), "\n\n")
  
  cat("Spread:\n")
  cat("  SD:", round(sd(x, na.rm = TRUE), 2), "\n")
  cat("  SE:", round(sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))), 2), "\n")
  cat("  Range:", round(min(x, na.rm = TRUE), 2), "to", 
      round(max(x, na.rm = TRUE), 2), "\n")
  cat("  IQR:", round(IQR(x, na.rm = TRUE), 2), "\n\n")
  
  cat("Shape:\n")
  cat("  Mean > Median:", mean(x, na.rm = TRUE) > median(x, na.rm = TRUE), 
      "(suggests right skew if TRUE)\n")
}

describe_continuous(sedona, "Seedling Height (cm)")
```

## Reporting in publications

When writing results, be concise but complete. Here are examples:

**For a single group:**
> Seedling heights averaged 5.1 cm (SE = 1.0, n = 7) and ranged from 3 to 9 cm.

or

> Mean seedling height was 5.1 cm (95% CI: 2.6 to 7.6 cm, n = 7).

**For skewed data, report the median:**
> Median seedling height was 3 cm (IQR: 3–7.5 cm, n = 7).

**For comparing groups:**
> Seedlings from Tucson were taller (mean = 8.0 cm, SE = 0.5) than those from Sedona (mean = 4.5 cm, SE = 0.4) or Flagstaff (mean = 6.0 cm, SE = 0.3).

**For categorical data:**
> Among the 50 plots surveyed, 14 (28%) experienced low burn severity, 21 (42%) moderate, and 15 (30%) high.

Always include:
- The statistic (mean, median, proportion)
- A measure of variability (SE, SD, CI, or IQR)
- Sample size
- Units

## Key takeaways

| Concept | What it tells you |
|---------|-------------------|
| Mean | Center of data; sensitive to outliers |
| Median | Center of data; robust to outliers |
| SD | How variable individual observations are |
| SE | How precisely you've estimated the mean |
| 95% CI | Plausible range for the true population parameter |
| Range/IQR | Spread of data; IQR is robust to outliers |
| Histogram | Shape of distribution |
| Boxplot | Five-number summary; flags potential outliers |
| Mean vs. Median | Tells you about skewness |

**Decision guide:**

- **Symmetric data**: Report mean ± SE (or 95% CI)
- **Skewed data**: Report median (IQR) or median (range)
- **Categorical data**: Report counts and percentages
- **Outliers present**: Investigate first, then consider robust measures

## Looking ahead

In the next chapter, we'll explore data more thoroughly—checking for errors, visualizing relationships between variables, and preparing data for analysis. The descriptive statistics from this chapter will help you characterize what you find.

Later, when we get to statistical tests, you'll see that many test statistics (like the t-statistic) are built from these same building blocks: means, standard deviations, and sample sizes. Understanding descriptive statistics makes everything else easier.

## Assignment

Using data from your own research project (or a provided dataset):

**Part 1: Continuous variables**

For your main response variable:

1. Calculate and report: mean, median, SD, SE, range, and IQR
2. Create a histogram and describe the shape (symmetric? skewed? outliers?)
3. Create a boxplot and identify any potential outliers
4. Write a 2-3 sentence description suitable for a results section, choosing appropriate statistics based on the distribution shape

**Part 2: Categorical variables** (if applicable)

1. Create a frequency table showing counts and percentages
2. Create a bar plot
3. Write a 1-2 sentence description for a results section

**Part 3: Comparing groups** (if applicable)

1. Create side-by-side boxplots comparing your response variable across treatment groups or categories
2. Calculate descriptive statistics (mean, SE) for each group
3. Based on the boxplots, what differences (if any) do you observe?

**Part 4: Reflection**

In 2-3 sentences, explain which measure of center (mean or median) and which measure of spread (SD, SE, or IQR) you would report for your data, and why.


