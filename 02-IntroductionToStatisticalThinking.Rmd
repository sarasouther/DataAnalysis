# Back to the basics

## What can statistics tell us?

Almost all statistical analysis boils down to answering one of two questions:

	1.	Do these groups differ?
	2.	Is there a relationship between these variables?
	
These seem like simple questions — in theory you might just “look at the data” to answer the questions — so **why do we need statistics**?

The short answer is: **sampling**. We are always measuring a subset of the **true population**. Even when we try to measure every individual (e.g., rare plant censuses), some individuals may be dormant, hidden, or unreachable.

If we were omniscient, we would simply compute the true population parameters (mean, variance, etc.), compare them directly, and skip statistics entirely. But in the real world, we estimate these values from samples and quantify the uncertainty around them.

Statistics is practical: it asks, **what can we say given imperfect measurements, "small" samples (relative to the true population), and noisy biological systems?**

## The Frequentist Framework

The statistics we use most commonly in ecology — t-tests, ANOVA, linear models, GLMs, mixed models — are all built within the *frequentist* or *parametric* framework.

**Frequentist statistics assumes:**

**1.	There is a true but unknown parameter**
The population mean, variance, slope, etc. are fixed constants — but we don’t know their values.
**2.	Your sample is one of many possible samples you could have taken**
If you collected data again under the same conditions, you’d get a different sample and different estimates.
**3.	Variation in your estimates comes from sampling error**
The parameter does not vary — it varies because you sampled imperfectly.
Uncertainty is therefore in the estimator, not the parameter.
**4.	Probability statements describe long-run frequencies of repeated sampling**
In other words, frequentist statistics asks: “If we repeated this experiment many times, how often would our method work?” A 95% confidence interval means that 95% of intervals generated this way would contain the true parameter. Once an interval is calculated, the true value is either inside it or not—there is no probability attached to that specific interval.

**What does “parametric” mean?**

Parametric tests assume the data follow a particular **probability distribution** (e.g., normal, Poisson, binomial). These assumptions influence:

- which model we choose
- how we estimate uncertainty
- what kinds of inferences we can make

## Bayesian statistics

The main alternative statistical framework to frequentist or parameteric statistics is *Bayesian* statistics. Bayesian statistics is based on Bayes’ Theorem, which provides a mathematical way to update our beliefs in light of new data. In this framework, we start with a prior distribution representing what we believe about a parameter before collecting data. We then combine this prior with the likelihood (the information contained in the data) to obtain a posterior distribution, which reflects our updated beliefs after seeing the evidence. This allows us to make intuitive probability statements about parameters—such as the probability that a parameter lies within a certain range—and provides a flexible foundation for modeling uncertainty, especially in hierarchical and ecological systems.

We (and most other ecologist) will learn frequentist methods first, because:

- Most classical ecological statistics (t-tests, ANOVA, GLMs) use the frequentist framework.
- Frequentist thinking aligns with standard experimental design.
- Historically, Bayesian methods were computationally difficult.
- Most journals still expect p-values and confidence intervals.

### When might you choose Bayesian statistics?

Although we begin with frequentist methods for their simplicity and historical use in ecology, there are many situations where Bayesian approaches offer clear advantages. Ecologists often choose Bayesian statistics when:

- Data are sparse, noisy, or imbalanced. Bayesian priors help stabilize estimates when sample sizes are small or data contain many zeros (common in rare species monitoring or demographic studies).
- Hierarchical or multilevel structure is important. Many ecological datasets have natural grouping—plots within sites, individuals within populations, years within climate regimes.
- Bayesian methods handle hierarchical models gracefully and provide full uncertainty estimates for every level.You want to incorporate prior information.
- In wildlife ecology, population modeling, or long-term monitoring, previous studies often contain valuable information that can inform current estimates. Priors allow you to formally combine past research with new data.
- You care more about parameter uncertainty than about p-values. Bayesian posterior distributions provide intuitive quantities such as: “There is a 94% probability that survival increased after treatment.”
This is often more interpretable than a traditional p-value.
- Models are too complex for frequentist methods.Nonlinear state–space models, integrated population models, hierarchical occupancy models, and many SDM frameworks are more stable and easier to fit using Bayesian tools like Stan, JAGS, or NIMBLE.
- You need robust propagation of uncertainty. Bayesian methods naturally propagate uncertainty across multiple model components, which is critical for forecasting, resilience modeling, and structured decision making.

In short, Bayesian statistics shines when ecological questions involve complex models, low sample sizes, hierarchical structure, prior knowledge, or full uncertainty estimation.

## Fundaments of frequentist statistics

### Statistical error

Statistical error is the discrepancy between what we observe and the true value we seek to estimate. In statistics, error does not necessilarly mean a mistake—it refers to unavoidable variation introduced by sampling, measurement, and natural processes.

Whenever we collect data, we introduce **error**:

– our instruments have limits
– humans make mistakes
– environmental conditions vary
– biological systems are inherently noisy

Frequentist statistical approaches classify and handle error in different ways. 

#### Sampling error 

Sampling error (variation in estimates) arises because different random samples from the same population produce different estimates, even when the underlying population does not change.

Imagine:

- There is a true population mean (fixed but unknown).
- You take a random sample and calculate the sample mean.
- You repeat this process many times.

What happens?

Each sample produces a slightly different sample mean. That variability among sample means is sampling error.

#### Process error 

Process variability reflects true heterogeneity in the system itself:

- Individuals differ
- Environments differ
- Years differ

This variability exists even if you observed the entire population.

Key characteristics:

- Does not disappear with larger sample size
- Reflects ecological mechanisms

**Example**: Some trees grow faster than others because of microsite conditions. 

We are sometimes about to reduce process error by accounting for and measuring variables that drive process error, but often we can't, because the driver of this error is unknown, too complext, or difficult to measure.

#### Measurement error

Measurement error arises anytime imperfect humans with imperfect instruments (the situation all the time) measure a sample and can include error introduced by:

- Imprecise instruments
- Observer differences

#### Model misspecification

Model misspecification occurs when the statistical model does not correctly represent the true data-generating process, arising from: 

- Missing predictors: A relevant variable that influences the response is omitted from the model.
**Ecological example**: Modeling plant growth without including soil moisture.
- Wrong functional form (linear vs non-linear): The relationship between predictor and response is modeled incorrectly (e.g., linear when nonlinear).
**Ecological example**: Assuming linear temperature effects when responses are unimodal.
- Ignored interactions: The effect of one predictor depends on another, but this dependency is not modeled.
**Ecological example**: Fire effects differing by precipitation regime.

In theory, these are avoidable forms of error, but, particularly in the case of missing predictors, it is often extremely challenging to collect all the relevant predictors in a complex system. When planning an experiment, however, it is really important to think about the underlying processes in your system in order to capture the most important predictors of performance.

#### Systematic error

Systematic error refers to consistent, directional error that causes estimates to be biased in the same way across observations or samples. Unlike random error, systematic error does not average out with increased sample size.

Examples include:

- Biased sampling (e.g., sampling only accessible sites)
- Consistent measurement bias (e.g., miscalibrated instruments; measuring all plants in the ambient treatment with a device is calibrated such at it adds .1 cm to each measure) 
- Observer bias

**Why it matters**: Systematic error leads to biased estimates and misleading conclusions, even when statistical uncertainty appears small.

Because systematic error introduces bias, it should be minimized through design (mostly through randomization and careful consideration of experimental design) and modeling choices rather than absorbed into model error.

Sampling error, and all unavoidable types of process error, measurement error, model misspecification, are specified in the error component of a statistical **model**.

## What is a model?

A statistical model is a mathematical expression that describes how a response variable changes as a function of one or more predictors.

Since we are taking a frequentist approach, models we will use in this course follow the same structure:

Response = Deterministic component + Random variation

- The deterministic component represents the systematic pattern we want to explain (e.g., how plant height changes with grazing treatment or soil moisture).
- The random variation represents the noise or uncertainty that remains after accounting for the predictors, and it follows one of the probability distributions we learned earlier.

In later chapters, we will learn how to build these models and interpret what they tell us about ecological patterns and processes.

## Test your knowledge!

Answer the following questions:

**Part 1 — Why We Use Statistics**

1.	In your own words, why do we need statistics in ecology?
Name at least two sources of variation or uncertainty that make statistics necessary.

2.	Explain the difference between a population and a sample.
Why do we rarely observe entire populations in ecological studies?

3.	What are the two main questions most statistical analyses attempt to answer?

**Part 2 — Frequentist Thinking**

4.	Fill in the blank:
In the frequentist framework, parameters (such as the true mean height of a plant population) are considered _______ but _______.
5.	What does a 95% confidence interval mean in frequentist statistics?
Choose the correct statement:

a) There is a 95% probability that the true mean lies in this specific interval.
b) If we repeated our sampling many times, 95% of the intervals we calculate would contain the true mean.
c) The data contain 95% of the true values.
  
6.	In frequentist statistics, uncertainty comes from:

a) The parameter being uncertain
b) Variation in the estimator due to sampling
c) The prior distribution
(circle all that apply)

**Part 3 — Short Model Concept**

7.	Fill in the blanks:

A statistical model can be written as:
Response = ______ + ______

8.	In this expression, what does the “deterministic component” represent?

9.	What does the “random variation” represent?

**Reflection Question**

10.	Write 2–3 sentences about one concept from this chapter that felt intuitive and one that felt confusing.
What would help clarify the confusing part?

## Assignment

Decide your general approach to your statistical analyses. Will frequentists statistics generally work? Are there analysis for which you would like to use Bayesian statistics?