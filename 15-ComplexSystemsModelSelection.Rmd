# Describing ecological patterns

When working at landscape-scales, we often need to employ model selection techniques to identify important predictor variables or co-variates to explain response patterns. There are several discrete steps and important considerations when completing such analyses, but before that, let's talk a little about the goals of model selection.

## When to use model selection?

Put simply, model selection is used when you have multiple competing hypotheses or models and want to:

1. Identify the best-supported model give the data
2. Quantify the relative support for each model
3. Avoid overfitting by penalizing overly complex models

Now, competing hypotheses can mean many things. Typically, I have strong hypothesis related to a research question, but less understanding of what covariates could mediate, enhance, or otherwise alter the effect of the predictor variables on the response variable. However, you can be more strategic in your tests, exploring whether data conform better to particular hypotheses or others.

In this tutorial, we will examine a dataset related to understanding resilience to drought in the Southwest. From 2020-2021, the Southwestern US experienced an extreme drought event that resulted in widespread die-offs several woody species. To derive models that predict resilience or recovery in Emory oak woodlands, we collected demographic and community data for overstory and understory communities at 100 plots across Emory oak's range. We measured resilience or recovery in two ways. First, we calculated a metric of community divergence between over- and understory communities, and second, we retrieved remotely sensed data that estimates productivity on the landscape. 

Building on resilience theory, we hypothesized that stands that were better connected and more diverse would exhibit higher resilience or recovery. Additionally, those sites experiencing lower climatic stress, before, during and after the drought would perform better (i.e., seedling communities exhibit less divergence from the adult plant community, and productivity returns to or exceeds pre-drought levels). However, there are many, many ways to describe climate, and many climatic variables are correlated, and in complex systems, one pillar of resilience (connectivity, diversity) may drive recovery. For this reason, we applied standard model selection techniques to appropriately model resilience/recovery in this system.

The results of these models were used to identify stands for restoration treatment likely to be resilient to perturbations like drought.

## Step 1: Identifying predictor variables or co-variates of importance

In this example, we identified several predictor variables based on resilience theory. However, climatic covariates could also reveal patterns of stress experienced by oaks. Given the numerous ways that climatic data can be summarized, we first identified variables that could be important for our system. 

Semi-arid regions of the Southwest are water limited, so we wanted to include a variety of precipitation metrics. Since precipitation patterns are bimodal (we receive roughly half our precipitation in the winter, often as snow, and the other half during the summer monsoons), we wanted to include both annual summaries of climate as well as monsoon specific summaries (including winter precipitation would be a strategic idea as well). Additionally, temperatures influence drought stress as well as respiration rates, affecting tree performance. Maximum and minimum temperatures may affect distinct physiological processes. For instance, minimum temperatures occur primarily at night, and affect the energy balance of trees, since higher temperatures increase respiration rates during a time when trees are photosynthesizing. This can lead to a loss of energetic stores to allocate to growth, reproduction and tissue maintenance and repair. Maximum temperatures, mostly occurring during the day, affect the ability to photosynthesize and maintain water balance. To reduce complexity, we decided to summarize only mean temperatures, rather than mean minimum or maximum temperatures, since we felt temperature would be less important that precipitation in this system. 

When identifying covariates for your system, think through likely drivers of performance, which in addition to climate could include information on soils or disturbances, like grazing, fires, and human activity.

## Step 2: Removing colinear variables

When two or more predictor variables are highly collinear (i.e., strongly correlated with each other), they essentially carry the same information. This can confuse your model in several ways:

1.	Unstable estimates: The model struggles to decide how much each variable contributes, which can lead to inflated or inconsistent coefficient estimates.
2.	Reduced interpretability: It’s harder to understand the effect of each variable because their influence overlaps — they “share the credit.”
3.	Misleading significance tests: Collinearity can make some variables appear non-significant, even if they’re actually important, because their shared information is spread across multiple predictors.
4.	Poor model generalization: Models with collinear variables are more likely to overfit to your data and perform worse when applied to new situations.

By removing or combining collinear variables, you help the model focus on independent sources of information, making it more stable, interpretable, and accurate. So let's do that!

First, we will want to select a method to remove correlated variables. We have two major options - We can either automate variable removal or use expertise to select variables. 

### Automated Variable Removal

Using statistical metrics like correlation thresholds or Variance Inflation Factor (VIF) to reduce multicollinearity.

#### Merits:

	•	Objective & reproducible: Provides consistent results across datasets.
	•	Handles high-dimensional data: Especially useful when there are many predictor variables.
	•	Improves model stability: Reduces inflated standard errors and overfitting risk in regression models.
	•	Compatible with model selection workflows: Can be integrated with stepwise selection, LASSO, or random forest importance.
	
**Note**: I only completely automate variable selection when I'm batch processing loads of data and the objective of the analysis is to look broadly across numerous comparisons. As an example, I automate variable removal when I'm generating a large number of Species Distribution Models to explore richness landscape-level patterns.

#### Limitations:

	•	Ignores ecological meaning: May discard biologically relevant or interpretable variables.
	•	Over-reliance can lead to information loss: For example, two drought metrics may both be predictive, even if correlated, because they represent different climate processes (input vs. deficit).
	•	Thresholds are somewhat arbitrary: Why r > 0.7 or VIF > 10? Different fields use different cutoffs.

### Expert Knowledge-Driven Selection

Manually selecting variables based on ecological theory, prior studies, or local knowledge.

#### Merits:

	•	Maintains interpretability: You keep variables with clear ecological meaning.
	•	Aligns with study goals: You can prioritize variables that are hypothesized drivers of your response.
	•	Supports story-driven science: Especially useful when communicating with land managers, policymakers, or interdisciplinary teams.

#### Limitations:

	•	May retain collinearity: Which can destabilize models if unaddressed.
	•	Subjectivity: Different experts may choose different variables.
	•	Less scalable: Can be slow or inconsistent across many datasets or repeated analyses.

### Blended Strategy (Recommended)

Use expert knowledge to define the pool of candidate predictors, then apply statistical tools to reduce redundancy within that pool.

Workflow:

	1.	Start with expert-justified variables.
	2.	Check for high correlations or high VIFs.
	3.	If two variables are strongly collinear, decide which is more interpretable or central to your ecological question.
	4.	Optionally test models with and without each to evaluate performance/stability.

## Example: Reducing colinear climate variables

Let's remove colinear climate variables! First, we can use a fully automated method, then we will transition to the blended strategy. We can contrast these approaches with each other, and with a dataset we would have put together with no statistical information on covariance! We divided the climatic environment into three categories: 1) Pre-drought baseline conditions, 2) Climatic change rates, and 3) Post-drought recovery conditions. Sectioning different categories of variables that you suspect *a priori* influence the response variable is a strategy that allows you to capture different ecological processes, while reducing colinearity of predictors.

```{r}
library(car)
library(dplyr)
library(purrr)
library(tibble)


# # Load directly from Google Drive (readr-style)
# library(readr)
# 
# data <- read_csv("https://drive.google.com/uc?export=download&id=1Yap0vVnrf-5nJemsFQ8sSIH3RNLxU5WF")
# 
# # Select only numeric predictors and drop rows with missing values
# numeric_predictors <- data %>%
#   dplyr::select(where(is.numeric))
# 
# # Visualize
# library(corrplot)
# cor_matrix <- cor(numeric_predictors, use = "complete.obs")
# corrplot(cor_matrix, method = "color", type = "lower", tl.cex = 0.7)
# 
# climatechange_df <- read_csv("climate_change_allvariables.csv")
# 
# # Select only numeric predictors and drop rows with missing values
# numeric_predictors <- climatechange_df %>%
#   dplyr::select(where(is.numeric))
# 
# # Remove identifier column
# predictor_data <- resilience_clean %>% dplyr::select(-Plot_ID)
# 
# # Loop over each variable and calculate max VIF
# vif_results <- map_dfr(names(predictor_data), function(var) {
#   # Build formula: var ~ all other variables
#   formula <- as.formula(paste(var, "~ ."))
#   # Create modeling dataset: use current var as response, others as predictors
#   model_data <- predictor_data %>%
#     dplyr::select(-all_of(var)) %>%
#     mutate(response = predictor_data[[var]])
#   # Fit the model
#   model <- lm(response ~ ., data = model_data)
#   # Get VIFs
#   vif_vals <- vif(model)
#   tibble(response_var = var, max_vif = max(vif_vals, na.rm = TRUE))
# })
# 
# print(vif_results)

```

What happened here? 

Let's repeat the process.

```{r}

```

Repeat for the other suites of climatic variables: climate change and post-drought conditions. Then, merge all datasets, and calculate VIFs again. 

```{r}

```

That's it! We have our final set of climatic predictors! I repeated this process again for other types of variables, including diversity estimates and connectivity. As you can see, generating a biologically-grounded, statistically sound predictor dataset is an iterative process, but worth the effort!

## Step 3: Select the best model

Now that we have assembled a dataset of climate predictors, we will want to select the model that best explains variation in our response variable, in this case a measure of community dissimilarity (labeled 'bray_curtis' in the dataset). 

Model selection is the process of identifying the best model(s) from a set of candidates that explain or predict a response variable based on a set of predictors. Below are the major approaches, their definitions, benefits, drawbacks, and primary applications in ecology and related disciplines.

Model selection is a hotly debated topic in statistics, with disagreements about when and how it should be used. Some argue it’s essential for identifying the best-supported models, while others caution that it can be misused, leading to overfitting or misinterpretation if not grounded in strong theory and careful reasoning. 

As an ecologist working in complex systems, I typically choose to use some sort of model selection tool. 

**Here’s why**:

1.	**Complex systems often need simplification**: In ecology (and many other fields), there are often many plausible predictors. Model selection helps identify which ones meaningfully contribute to explaining patterns, rather than relying solely on intuition or arbitrary choices.
2.	**It guards against overfitting**: By comparing models using criteria like AIC, BIC, or cross-validation error, model selection balances fit with complexity, helping you avoid models that explain your current data perfectly but perform poorly on new data.
3.	**It enables hypothesis comparison**: You can formally compare models that reflect different ecological hypotheses — e.g., does drought or disturbance better explain recovery? — rather than testing each variable in isolation.

Let's explore several popular methods of model selection.

### Major Types of Model Selection: Overview, Benefits, Drawbacks, and Applications

#### Information-Theoretic Approaches (e.g., AIC, BIC)

What it is:

	•	Compares multiple models based on goodness of fit penalized by model complexity.
	•	AIC (Akaike Information Criterion) focuses on minimizing information loss.
	•	BIC (Bayesian Information Criterion) penalizes complexity more harshly and favors simpler models as sample size increases.

Benefits:

	•	Allows comparison of non-nested models.
	•	Emphasizes parsimony (balancing fit with simplicity).
	•	Useful in exploratory studies with many competing hypotheses.

Drawbacks:

	•	Can favor overly complex models if predictors are correlated or sample size is small (AIC).
	•	Results depend on the candidate model set — unconsidered models are ignored.

Applications:

	•	Ecology, epidemiology, evolutionary biology.
	•	Useful when comparing alternative ecological hypotheses or predictor sets.

#### Stepwise Regression (Forward, Backward, Bidirectional)

What it is:

	•	Automatically adds or removes predictors based on p-values or information criteria (e.g., AIC).
	•	Forward: starts with no predictors; adds variables one at a time.
	•	Backward: starts with all variables; removes one at a time.

Benefits:

	•	Easy to implement.
	•	Reduces large variable sets quickly.

Drawbacks:

	•	Ignores model uncertainty.
	•	Inflated Type I error rates.
	•	Final model can be unstable (small changes in data can lead to different models).

Applications:

	•	Often used in exploratory analyses.
	•	Sometimes used in automated workflows, though not recommended for inference.

#### All-Subsets Regression (aka Best Subsets Selection)

What it is:

	•	Evaluates all possible combinations of predictors and ranks models based on a selection criterion (AIC, BIC, R-squared, etc.).

Benefits:

	•	Thorough exploration of model space.
	•	Helps identify the best model given a criterion.

Drawbacks:

	•	Computationally intensive, especially with many predictors.
	•	High risk of overfitting without careful constraints.

Applications:

	•	Model exploration when the number of predictors is moderate (<20).
	•	Common in applied ecological modeling where model space is small and well-defined.

#### Cross-Validation (CV)

What it is:

	•	Divides the dataset into training and testing subsets repeatedly.
	•	Evaluates model performance based on predictive accuracy (e.g., RMSE, classification error).
	•	K-fold CV is common (e.g., 5- or 10-fold).

Benefits:

	•	Focuses on model predictive performance.
	•	Helps avoid overfitting.

Drawbacks:

	•	Does not provide information about explanatory power.
	•	More computationally demanding.

Applications:

	•	Machine learning, predictive modeling.
	•	Useful in ecological forecasting, SDMs, or models aimed at out-of-sample prediction.

#### Bayesian Model Selection

What it is:

	•	Uses Bayes factors or posterior probabilities to compare models.
	•	Accounts for model uncertainty explicitly.

Benefits:

	•	Incorporates prior knowledge.
	•	Quantifies model uncertainty.
	•	Can average over multiple models (Bayesian model averaging).

Drawbacks:

	•	Requires strong computational resources.
	•	Sensitive to choice of priors.

Applications:

	•	Hierarchical models, small-sample inference, decision analysis.
	•	Growing use in ecological and conservation decision-making.

#### Regularization Methods (e.g., Lasso, Ridge, Elastic Net)

What it is:

	•	Shrinks coefficients by adding a penalty term to the loss function.
	•	Lasso (L1) can set some coefficients to zero (variable selection).
	•	Ridge (L2) shrinks coefficients but keeps all predictors.

Benefits:

	•	Handles multicollinearity well.
	•	Useful in high-dimensional datasets.

Drawbacks:

	•	Less interpretable coefficients.
	•	Choice of tuning parameter (lambda) requires validation.

Applications:

	•	Genomics, remote sensing, high-dimensional ecological models.
	•	When predictor > sample size.

#### Summary of model selection techniques

For most applications, I use AIC-based approaches. They offer a nice balance between model fit and complexity, and you can average across top models to incorporate model uncertainty (explained more below). Here are a couple of exceptions:

•	If I had very high-dimensional data (e.g., genomics, remote sensing), I’d switch to LASSO (Least Absolute Shrinkage and Selection Operator). Regularization technique that performs variable selection and regularization simultaneously, so it is computationally efficient.
•	If the goal was out-of-sample prediction, I’d rely on cross-validation or machine learning methods with holdout sets. I use these techniques often when creating species distribution models and I'm trying to extrapolate suitable habitat across the landscape.
•	If I needed to quantify uncertainty explicitly (e.g., in decision analysis), I’d consider Bayesian model selection or Bayesian model averaging.

### Stepwise regression based on AIC (Akaike Information Criterion).

```{r}

library(MASS)

#full_model <- glm(response ~ ., data = data_reduced, family = binomial)
#stepwise_model <- stepAIC(full_model, direction = "both")
#summary(stepwise_model)

```

LASSO (Least Absolute Shrinkage and Selection Operator) Regularization technique that performs variable selection and regularization simultaneously.

Benefits:
Automatic Variable Selection: LASSO can automatically shrink some coefficients to zero, effectively performing variable selection and reducing model complexity.
Handles High Dimensionality: Particularly useful when the number of predictors is much larger than the number of observations.
Regularization: Helps prevent overfitting by adding a penalty to the magnitude of coefficients.
Interpretability: The resulting model is often simpler and easier to interpret, as it includes only a subset of the predictors.
Computational Efficiency: Computationally efficient and can be solved using convex optimization methods.
Drawbacks:
Bias: The shrinkage can introduce bias into the estimates of the coefficients.
Model Assumptions: Assumes linear relationships between predictors and the response variable.
Choice of Penalty Parameter: The performance depends on the choice of the penalty parameter (lambda), which requires cross-validation to tune.
Cannot Handle Multicollinearity Well: While LASSO can select variables, it might not perform well in the presence of high multicollinearity compared to other techniques like ridge regression.

```{r}

library(glmnet)

#x <- model.matrix(response ~ ., data_reduced)[, -1]
#y <- data_reduced$response

#lasso_model <- cv.glmnet(x, y, alpha = 1, family = "binomial")
#best_lambda <- lasso_model$lambda.min
#best_model <- glmnet(x, y, alpha = 1, family = "binomial", lambda = best_lambda)
#coef(best_model)

```

Best Subset Selection; Evaluate all possible subsets of predictors and select the best model based on a criterion like AIC, BIC, or adjusted R-squared. 

Benefits:
Exhaustive Search: Considers all possible combinations of predictors, ensuring the best possible model according to a specified criterion (e.g., AIC, BIC).
Model Performance: Often provides the best fit in terms of the criterion used for selection.
Flexibility: Can be used with any type of regression model and can include interactions and polynomial terms.
Drawbacks:
Computationally Intensive: Becomes impractical with a large number of predictors due to the exponential increase in the number of models to be evaluated.
Overfitting: Risk of overfitting, especially when the number of predictors is large relative to the number of observations.
Multicollinearity: Does not address multicollinearity among predictors, which can lead to unstable coefficient estimates.

```{r}
library(leaps)

#best_subset <- regsubsets(response ~ ., data = data_reduced, nbest = 1, really.big = TRUE)
#summary(best_subset)

```

Model Averaging; Perform model averaging to account for model uncertainty.

Benefits:
Accounts for Model Uncertainty: By averaging over multiple models, it incorporates model uncertainty into the final predictions.
Improved Predictive Performance: Often leads to better predictive performance by averaging out the errors of individual models.
Flexibility: Can be applied to various types of models and selection criteria.
Stability: Provides more stable estimates by considering multiple models rather than relying on a single best model.
Drawbacks:
Complexity: Results in a more complex model that can be harder to interpret compared to selecting a single best model.
Computationally Intensive: Requires fitting and averaging a large number of models, which can be computationally demanding.
Weight Selection: The choice of weights for averaging (e.g., based on AIC, BIC, or cross-validation errors) can influence the final model and may not always be straightforward.
Potential for Overfitting: If not carefully managed, averaging over too many models can still lead to overfitting, particularly if the individual models are not regularized.


```{r}

# library(MuMIn)
# options(na.action = "na.fail")
#global_model <- glm(response ~ ., data = data_reduced, family = binomial)
#dredged_models <- dredge(global_model)
#averaged_model <- model.avg(dredged_models, subset = delta < 2)
#summary(averaged_model)

```

# Plot establishment

One other thing to consider, is the placement of plots across the landscape. Though this is step one of the process, since many folks will have previously established plots, we will discuss here!

Spatially balanced random sampling offers several benefits, particularly in ecological and environmental studies. Here are some of the key advantages:

1. Improved Representativeness
Coverage: Ensures that the sample points are spread out evenly across the study area, which helps in capturing the spatial heterogeneity of the environment.
Reduction of Bias: Minimizes the chances of over-sampling or under-sampling specific areas, leading to more accurate and generalizable results.
2. Enhanced Statistical Efficiency
Reduced Variance: By evenly distributing sample points, spatially balanced sampling often reduces the variance of the estimates compared to simple random sampling.
Better Inference: Provides better estimates of population parameters and improves the precision of spatially explicit models.
3. Flexibility and Adaptability
Multiple Scales: Can be applied at various spatial scales, making it suitable for different types of studies ranging from local to regional levels.
Integration with GIS: Easily integrated with Geographic Information Systems (GIS) to facilitate sample design and data collection.
4. Cost-Effectiveness
Efficient Use of Resources: Reduces travel time and costs associated with fieldwork by ensuring that sample locations are optimally distributed.
Focused Sampling Effort: Enables targeted sampling in areas of interest while still maintaining a representative coverage.
5. Robustness to Spatial Autocorrelation
Handling Spatial Dependencies: Helps in accounting for spatial autocorrelation by ensuring that samples are not clustered, which can lead to more reliable statistical analyses.
Examples of Applications
Ecological Monitoring: Used to monitor biodiversity, habitat quality, and species distributions across large landscapes.
Environmental Assessment: Applied in studies assessing soil contamination, water quality, and air pollution to ensure comprehensive spatial coverage.
Resource Management: Useful in forestry, agriculture, and fisheries to assess the distribution and abundance of resources.
Key Methods and Tools
Spatially Balanced Sampling Algorithms: Such as the Generalized Random-Tessellation Stratified (GRTS) design.
R Packages: spsurvey package in R provides tools for implementing spatially balanced sampling designs.

```{r}

library(spsurvey)

# Define the study area and number of sample points
#study_area <- as.spatialPolygons(my_shapefile)
#n_samples <- 100

# Generate spatially balanced sample points
#sample_points <- grts(study_area, n = n_samples)

# Plot the sample points
#plot(study_area)
#points(sample_points, col = "red")

```

