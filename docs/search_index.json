[["index.html", "Structural Equation Models Chapter 1 Introduction 1.1 Student Learning Objectives 1.2 Downloads 1.3 Description 1.4 Grading 1.5 Overview 1.6 R 1.7 What can statistics tell us? 1.8 Sampling populations 1.9 Data types 1.10 Describing data 1.11 Prework 1.12 Randomness 1.13 The sample point method 1.14 Combinatorials 1.15 Union and intersection of events 1.16 Conditional probability 1.17 Multiplicative Law of Probability of Independent Events 1.18 Additive Law of Probability of Mutually Exclusive Events 1.19 General Additive Law of Probability 1.20 Key concepts 1.21 Practice problems 1.22 Basic hypothesis testing review 1.23 Tailed tests 1.24 Drawing conclusions from statistics 1.25 Reporting results 1.26 Review problems 1.27 A foray into statistics 1.28 Selecting statistical analyses", " Structural Equation Models Dr. Sara Souther 2025-05-31 Chapter 1 Introduction 1.1 Student Learning Objectives 1.2 Downloads 1.3 Description 1.4 Grading 1.5 Overview 1.6 R Add an intro or just use the index file? 1.7 What can statistics tell us? Welcome to our statistical exploration of the natural world! I want you to have an intuitive understanding of what we do when we conduct statistical analysis, understand how to select the appropriate statistical analysis and the assumptions of that analysis, and make a connection between running an analysis in a statistical software package and the statisticese of those unique individuals that we call statisticians. I think this will give you the confidence to tackle any analytical situation! First off, we are not statisticians - whew! We are ecologists and social scientists. This means that we do not need to understand theoretical mathematical frameworks. We need to APPLY statistics appropriately. There will be limits to our mathematical understanding of statistics, and this is OKAY! Think of all of the knowledge about the natural world and conducting field work that we possess that statisticians don’t! I’m happy to dive down math rabbit holes with you, BUT we don’t need to and CAN’T know everything. Almost all statistical analysis boils down to answering 1 of 2 questions: Do these groups differ? Is there a relationship between these variables? These seem like relatively simple questions to answer, perhaps just by looking at our data, so Why do we need statistics? The short answer is: error and sampling! Whenever we collect data, we introduce error; our instruments are imprecise and do not capture an exact measure of whatever you are measuring (e.g., height, weight), and humans make mistakes during measurement collection. Secondly, we are always measuring a sub-sample of the true population (true population meaning all representatives of whatever you are trying to measure; this can be grass, marbles, or the tibia of humans). Not only is in intractable in most cases to measure all individuals of whatever you are interested in, even when it is possible to attempt to measure all individuals (like in the case of rare plant work), statistics acknowledges that it is still unlikely that we are able to completely measure all individuals in your focal population, since individuals may be dormant or challenging to locate. If we could measure all individuals of our population of interest with perfect accurately, we could calculate population parameters, or quantities describing populations like averages and variation, rather than estimating these metrics, and in many cases just compare them. In this way, statistics is inherently practical, and asks, what can we say about whatever we are looking at, given our numerous flaws! 1.8 Sampling populations After a few classes, we will explore sampling methodology in greater depth in order to design appropriate experiments that test a statistical hypothesis. Let’s quickly talk about sampling now so that we have a shared understanding and vocabulary to build on - after all, statistics really centers around estimating characteristics of a true population from a sample. The really, truly amazing thing is that by properly applying statistics, we can learn practically anything about almost any population using samples! In statistics, a population refers to the all units of the thing that you are interested (i.e., all suriname frogs, all grains of sand, all aspen leaves from a genotype found in southern Arizona). Note: Population in statistics differs from the term population in population ecology, where a population refers to a group of individuals in a particular area that interbreed. A sample is a subset of the population that we measure to infer something about the population. Statistical analyses depend on a random sample or must account for non-randomness within the sample. Just imagine, for instance, that you were interested in whether coat color in cats differed between house cats and feral cats. To select the house cat sample, you randomly select house numbers, visit the house and record coat color, thus collecting a random sample. However, to survey feral cats, you go to several cat colonies at night and record the first cat that you see, which are always white or tan. The sample of feral cats introduces bias, and causes you to overestimate the number of light colored feral cats, and underestimate dark feral cats. We can conduct statistical analysis until the cats come home (ha!), but if your sample is biased, our results will always be meaningless. In the cat example, it was pretty obvious that the researcher was introducing bias, BUT it is REALLY easy to introduce bias in ecological and social research on accident! Imagine that you looking at fire effects on vegetative communities in the Sonoran. In high severity burn areas, there are thickets of cat’s claw (a pokey plant). Without proper field sampling protocols, it is very tempting to avoid establishing plots in the cat claw thickets, thus not capturing true differences in vegetation along burn severity gradients. We can go over field sampling methodology later, but let’s talk about several types of appropriate sampling strategies. The key is that we want our sample to be representative of the true population, so that estimates of values (i.e., means, variance) from the sample represent true population parameters. Simple Random Sampling is when every sample has an equal likelihood of being selected. We can quickly and easily generate such a sample in R, using the sample function. sample(1:100, 10, replace=FALSE) ## [1] 77 44 54 85 25 29 20 93 13 100 #1:10000 = numbers to chose among #number of random numbers you wish to generate #to replace or not (in other words do you wish for the same number to be selected multiple times) Random Cluster Sampling randomly select groups (aka clusters) within a population. This sampling design is used commonly in ecology, when we select random locations for plots, then measure all individuals within those plots. If for instance, we are interested in Ponderosa Pine growth rates on the Coconino National Forest, we would randomly assign points across Pondo habitat on the Coconino. At each point, we would set up a plot in which we measure Ponderosa Pines within an 11.71m radius plot. Why wouldn’t we just go out to a point and measure 1 tree to create a totally random sample? The plots are randomly assigned (yay!), but the trees within the plots are not independent. In other words, we might expect measures of trees within plot A to be more similar to each other than they are to trees within plot B, due to differences in microsite characteristics, genetic similarity among co-occurring trees, or site history (logging, fire). Luckily, we can account for this non-independence, as long as the plots are random! Stratified Sampling draws samples using proportionality based on homogeneous groupings known as strata. This type of sampling is frequently used in ecology to account for landscape differences in key factors. For instance, say you asked to classify vegetation types for a Nature preserve in southern Arizona. The reserve has a large riparian area (25% of the property) with completely different vegetation from the upland area (75%). Random sampling might, by chance, under or over represent one of these two areas. To create a stratified sampling design, you would ensure proportional representation of both areas by randomly placing 25% of the sample points within the riparian area, and 75% of the sample points within the upland area. Sample size More is better. However, practically we are often limited by time and money! Statistical analysis is only one part of presenting your research results. Generally, a results section in a manuscript includes: statistical results, data description (e.g., describing means, ranges, maxima, minima of groups of interest), and data visualization (i.e., creating beautiful figures). For each analysis that we cover, we will talk about how to present statistical results, describe data, and create appropriate supporting figures. 1.9 Data types Before we start learning to present research results (analysis, description, visualization), let’s talk about data! Data comes in several varieties, and the variety dictates which statistical analysis we choose! Categorical variables are non-numeric variables. Examples: Pet type (dog, cat, fish, bird), Size (small, medium, large), Car type (sedan, SUV), Present/Absent Numerical variables are variables that are numbers, and occur in two forms: *Discrete = Counts of things (no decimal points/fractions) Data are discrete when it does not make sense to have a partial number of the variable. For instance, if counting the number of insects in a pond, it does not make sense to count a half a species. Examples: Number of people in a building, number of trees in a plot, number of bugs in a pond *Continuous = Numerical data that can occur at any value. These are variables that can occur in any quantity. If you can have a fraction of this variable, it is continuous. Examples = Height, Weight, Length Ordinal variables (sometimes referred to as ranked) can be categorical or numerical, but the order matters. Examples = Grades (A, B, C, D, E), Likert scale variables (Strongly disagree, Agree, Strongly Agree), Class rank (1, 2, 3, 4, 5) 1.10 Describing data First, let’s take a spin with data description. We are starting here to introduce a few concepts that will be important to understand, as we launch into statistical analysis. We will start by describing continuous data. Let’s use a simplified version of a dataset that I’m working with right now to look at the performance of several species of pollinator-friendly native species in agricultural gardens. Eventually, we’d like to develop seed to provide to restorationists for restoration of arid and semiarid grasslands. To do this, we need to understand how reliable these species are at establishing, producing seed, and attracting pollinators. Initially, we are conducting experiments with multiple populations of each species to determine how consistently plants grow, reproduce, and perform. Here, We will take a look at the initial heights of 1 population of one species, Asclepias subverticulata. When doing an actual research write-up, I ask myself ‘What is the most important information for my audience to know about this dataset?’ to guide what descriptions of the data to include. Here, we are just going to play around with numbers and R code! #create vector of heights (cm) of one population of A. subverticulata sedonapopulation &lt;- c(3, 3, 3, 3, 7, 8, 9) #take the mean mean(sedonapopulation) ## [1] 5.142857 #calculate variance var(sedonapopulation) ## [1] 7.47619 #calculate standard deviation sd(sedonapopulation) ## [1] 2.734262 #calculate standard error #base r doesn&#39;t have this function #so we have to write our own std_error &lt;- function(x) sd(x)/sqrt(length(x)) std_error(sedonapopulation) ## [1] 1.033454 Most of the time when writing up results, you present a mean (sum of numbers divided by the number of observations), and an estimate of variation (a measure of how different the observations are). Here, we calculated three estimates variation, variance, standard deviation, and standard error. Since you will occasionally need to include equations in your write-ups, let’s get use to mathematical syntax, with these simple examples. The formula for the sample mean is: \\(\\mu = \\frac{\\Sigma x_i}{n}\\); where \\(\\mu\\) indicates the sample mean (sample = group of numbers we are looking at); \\(\\Sigma\\) means to add what ever follows; \\(x_{i}\\) is the value of one observation; (subscript i is often used to indicate that the action should be repeated for all values); \\(n\\) is the number of observations Why didn’t we just use \\(\\bar{x}\\) to indicate the mean? Because statisticians typically use \\(\\bar{x}\\) to indicate the true mean of the population, and \\(\\mu\\) to indicate the sample mean! Just to show you, what the mean() function is doing, let’s run: sum = 3+3+3+3+7+8+9 #add all the numbers in the sample n = length(sedonapopulation) #or you can just calculate the number of height measurements mean = sum/n; mean #divide sum by number ## [1] 5.142857 This formula is simple, but sometimes with more complex formulas, I will solve the equations by hand, to make sure that I understand what is happening! The formula for variance is: \\(S^{2} = \\frac{\\Sigma(x_i - \\mu)^{2}}{n - 1}\\) where \\(S^{2}\\) is the sample variance; \\(\\mu\\) is the sample mean (remember from above); \\(x_{i}\\) is the value of one observation; \\(n\\) is the number of observations In other words: #We determine how much each observation varies from the mean. diffobs1 = mean - 3 diffobs2 = mean - 3 diffobs3 = mean - 3 diffobs4 = mean - 3 diffobs5 = mean - 7 diffobs6 = mean - 8 diffobs7 = mean - 9 #Then we square each of these. diffobj1_sq = diffobs1^2 diffobj2_sq = diffobs2^2 diffobj3_sq = diffobs3^2 diffobj4_sq = diffobs4^2 diffobj5_sq = diffobs5^2 diffobj6_sq = diffobs6^2 diffobj7_sq = diffobs7^2 Why do we square the differences rather than just adding them up? Because differences will be positive and negative. If we added them without squaring, sample differences would negate each other. We want an estimate of the absolute differences of samples from the mean. #Then we add the differences up. sumofsquares = sum(diffobj1_sq, diffobj2_sq, diffobj3_sq, diffobj4_sq, diffobj5_sq, diffobj6_sq, diffobj7_sq) #Divide the sum of squares by n - 1. variance = sumofsquares/(n-1); variance ## [1] 7.47619 Why n - 1 instead of n? One reason is that, theoretically, because we are taking the mean of a sample, rather than all individuals, we underestimate the variance, so taking n-1 corrects that bias. Consider it a penalty for measuring a sample, not the entire population! Another practical reason is that dividing by n-1 makes the variance of a single sample undefined (unsolvable) rather than zero (solvable) For standard deviation, we just take the square root of the variance, to remove the effect of squaring the differences when calculating the variance, and thus contextualizing our estimate of variation with regard to the mean. For example, the variance for the Sedona population is 7.48, larger than the sample mean of 5.12; while the standard deviation is 2.73, indicating that you would expect most observations to be 5.12 +/- 2.73 (we’ll get to quantiles in a minute). The formula for standard deviation is: \\(\\sigma = \\sqrt\\frac{\\Sigma(x_i - \\mu)^{2}}{n - 1}\\) where \\(\\sigma\\) is the sample variance; \\(\\mu\\) is the sample mean; \\(x_{i}\\) is the value of one observation; \\(n\\) is the number of observations. Finally, standard error and confidence intervals (we’ll get to confidence intervals later) are the most common metrics of variance presented in journals. The formula for standard error is: \\(SE = \\frac{\\sigma}{\\sqrt n}\\) where \\(SE\\) is standard error of the sample; \\(\\sigma\\) is the standard deviation; and \\(n\\) is the number of samples. Why do we divide the standard deviation by the square root of the sample size to get standard error? While standard deviation measures the variation of the sample, standard error is meant to estimate the variation of the entire population of samples, if we could measure all individuals accurately. By dividing by the \\(\\sqrt n\\), the larger the sample size, the lower the error, because you have a more complete estimate of the true mean. In other words, standard deviation is just a measure of the variation of our sample, while standard error also incorporates information about our sampling process (how many individuals we have sampled). Want to delve deep into standard error and deviation (me neither - ha)?: Google central limit theorem + standard error / standard deviation. Means and variance measures are the most common way to describe quantitative data. However, several other metrics are useful for understanding the nature of your data and making decisions about analyses. A comprehensive understanding of your dataset includes describing these four features: Location (Mean, Median) Spread (Variability) Shape (Normal, skewed) Outliers We’ve talked about means. The median is just the central number in the dataset, and helps you identify skewness. #an example of an unskewed population sedona_unskewed &lt;- c(1, 2, 3, 4, 5, 6, 7) mean(sedona_unskewed) ## [1] 4 median(sedona_unskewed) ## [1] 4 #previous sedona population; skewed sedonapopulation &lt;- c(3, 3, 3, 3, 7, 8, 9) mean(sedonapopulation) ## [1] 5.142857 median(sedonapopulation) ## [1] 3 In an unskewed population, the mean will equal the median. Skew may not seem important, but it has statistical ramifications, AND it tells us something meaningful about the data. For instance, what if I said that mean price of a home in Flagstaff is 350K, but the median price of a home is 300K? We would know the that average house prices are driven up by a smaller number of expensive homes. We can quantify skew by comparing means and medians (mean &gt; median = right-skewed; median &gt; mean = left-skewed), but it is helpful to visualize the shape of data with a histogram. A histogram is a graph of the frequency of different measurements. Let’s add a few more observations to our Sedona populations (skewed and unskewed) and check out the look of the data! sedona_unskewed &lt;- c(7, 2, 2, 3, 3, 3, 3, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 0.5) mean(sedona_unskewed) ## [1] 3.975 median(sedona_unskewed) ## [1] 4 #I&#39;m renaming sedonapopulation, sedona_skewed for this example sedona_skewed &lt;- c(3, 3, 3, 3, 7, 3, 4, 5, 6, 3, 3, 3, 4, 4, 6, 7, 8, 9, 3, 4, 5, 2) mean(sedona_skewed) ## [1] 4.454545 median(sedona_skewed) ## [1] 4 In this relatively unskewed example, the tails are approximately even. This shape is also referred to as a normal or Gaussian distribution. Here, we superimposed the bellshaped Normal or Gaussian distribution. In this example of skewed data, the tail tapers to the right, indicated that the data is skewed to the right. In order to explain outliers, we need to look at quantiles! Quantiles are proportions of your data, in other words a way to break your data into chunks to understand spread. You can break your data into as many quantiles as you would like, but it is most common to break your data into 4 parts, also called quartiles. (If you break data into 5 parts, the components are called quintiles, 10 parts = deciles, 100 parts = percentiles). When you break data into quartiles, roughly 25 percent of the data occurs within each data chunk. The first chunk of the dataset contains 25% of the data (25th percentile; 25% of the data fall at or below this cut-off) is called the first quartile, the 50th percentile is called the sample median or the second quartile, the 75th percentile is called the third quartile. Box and whisker plots are commonly used to quickly examine quartiles. Let’s check out our plant height data again, using a box and whisker plot. In the plot shown here, the box encapsulates the Interquartile Range (IQR); the center of the data ranging from the 25th percentile to the 75th. The black line in the middle of the box is the median (also called the 50th percentile, because it bisects the dataset; half of the data occur above the median and half below). The lines emerging from the box (whiskers) indicate the extent of the first and third quartiles, and usually corresponding with the minimum and maximum values of the dataset, unless there are outliers. An outlier is a datapoint that occurs outside of the 1st or 3rd quantile. Let’s add one to our Sedona dataset, and see how it is represented on the box and whisker plot. #Let&#39;s add a plant height of 20. sedona_skewed &lt;- c(3, 3, 3, 3, 7, 3, 4, 5, 6, 3, 3, 3, 4, 4, 6, 7, 8, 9, 3, 4, 5, 2, 20) boxplot(sedona_skewed, main=&quot;Skewed&quot;, ylab=&quot;Plant height (cm)&quot;) The outlier appears as a dot on the box and whisker plot, and is the maximum value of the dataset. One other thing to note: Standard deviation also breaks data into meaningful segments, but is only used when data conform to a normal distribution; the mean +/- 1 SD accounts for 68% of the data, +/-2 SDs contains 95% of data, and +/- 3SD includes 99% of data. That said, I’ve never presented standard deviation in a manuscript; it is much more common to include standard error or confidence intervals (discussed later). We’ve played around a lot with data, but what do you actually need to take away from this? Data types (Categorical, Numerical discrete, Numerical continuous, Ordinal) Why? We will select analyses based on data type. The two basic questions that most statistical analyses answer. Why? This will help you define what statistics can and can’t do and bound our learning space! Ways to describe numerical continuous data (Location, Spread, Shape, Outliers). Why? You will describe your results using these concepts in write-up AND these concepts will be important for certain analyses. Know how to calculate mean, median, and standard error. Why? These are typical ways to describe data in results sections. Start to familiarize yourself with mathematical annotation. Why? You may need to include equations in your methods section. Start to familiarize yourself with R code. Why? Most researchers now use R to analyze, describe, and visualize their data. *Be able to interpret a histogram and box-whisker plot. Why? These are commonly used ways to visualize data. 1.11 Prework Listen to the podcast on stochasticity Reads on randomness and determinism: https://towardsdatascience.com/when-science-and-philosophy-meet-randomness-determinism-and-chaos-abdb825c3114 *Load packages and datasets 1.12 Randomness Randomness is statistically a simple concept with incredible repercussions for understanding natural systems, human decisions, and, well, life! One of the most interesting panels that I attended as a graduate student was a debate between scientific and religious philosophers over the meaning of randomness. There were a range of viewpoints; the theologian believed that randomness was where the divine could work; at the other extreme, the science philosopher believed that there is no such thing as random, only that we haven’t developed the capacity to measure what we perceive as random, and that everything we experience is a deterministic outcome of physics manifest in the world around us. As I write this lesson, I’m sitting on the couch with my younger child watching Jurassic Park. Ian Malcolm has just introduced himself as a ‘chaotician’ (he he). Chaos theory is one way to explain what appears to us as ‘randomness’, because small initial differences in conditions of complex systems leads to extremely different end states, even when the same deterministic rules apply to that system. The ‘butterfly effect’ (a butterfly flaps its wings in Asia causes a tornado in Texas) is often used to describe this core concept of chaos (the example originally created by Edward Norton Lorenz used a seagull instead of a butterfly). Chaos theory is incorporated into modeling simulations (particularly in population biology). In statistics, we have specific definition of randomness: The case when each item in a set has an equal probably of being selected. Statistics also acknowledges unexplained variation in nature, but doesn’t distinguish between true randomness or the inability to perfectly measure all variables explaining an observed phenomenon. Instead, all unpredictability is lumped together. The difference between the true value of an estimate (e.g., mean hieght of giraffes, slope describing the relationship between snail shell length and speed) and the observed estimate is called ‘error’. We use statistical models to parse ‘error’ (unexplained variation in the response variable) from treatment effects (variation in the response variable explained by the explanatory variables). [Add info on random effects here or introduce later?] Statistical error can arise for several reasons: random variation inherent to natural systems, mistakes during measurement or data collection, inaccuracy of measuring devices, omitted variables. As long as this error isn’t ‘systematic’, we accept that it is there, complete statistical analysis and move-on. However, systematic error, whenever you are directionally biasing your sample, or biasing your sampling in the same way, is something to be avoided when designing your experiment and collecting data. An example of a systematic measure would be if you are trying to estimate the mean weight of blue morpho butterflies in southern Brazil, and you never zero your balance. The true mean weight is lower than the mean weight that you calculate. Since randomness is inherent, we need to calculate the probability that we Our ability to say that an observed effect is statistically significant is based on probability: the proportion of times an event occurs in repeated trials or more generally , the quantitative measure of one’s belief in the occurrence of a future event. In other words, when conducting a statistical analysis, we need to be able to determine the likelihood that two or more groups differ by chance, or that an observed a relationship occurred by chance. Remember that probabilities vary between 0 and 1, with 0 indicating no chance of an event occurring and 1 indicating a 100% chance of an event occurring. Because statistics relies so heavily on probability, let’s review some basic concepts. Prepare yourself for lots of focus on ‘true coins’ (coins that have the equal likelihood have being heads or tails) and dice! Rolling dice and flipping coins are random events, and we will use them to look at predictability! [When to introduce Bayesian vs Frequentist concepts?] 1.13 The sample point method The sample point method is one method of determining the likelihood of an event, and entails the following steps: Define the sample space, S, by listing all possible outcomes of the ‘experiment’ or ‘trial’ you are conducting. Assign probabilities to all sample points, Pi, such that the probability of all events within the experiment tally to 1. *Sum all sample points that constitute the outcome you are interested in to find the probability of that outcome. Let’s practice this concept with an example. Let’s calculate the probability of getting two heads in three tosses of a fair coin. Table 1.1: Sample Space Outcome Toss1 Toss2 Toss3 Abbreviated Probability 1 Heads Heads Heads HHH 0.125 2 Heads Heads Tails HHT 0.125 3 Heads Tails Heads HTH 0.125 4 Tails Heads Heads THH 0.125 5 Tails Tails Heads TTH 0.125 6 Tails Heads Tails THT 0.125 7 Heads Tails Tails HTT 0.125 8 Tails Tails Tails TTT 0.125 The first step is to define the sample space for this experiment, by showing all possible outcomes of tossing a coin 3 times, as we have done in the table, called ‘Sample Space’. Since this is a fair or balanced coin, all of the outcomes are equally likely, so we assign them a probability of 1/8. Then, we can simply sum the runs that meet our criterion of 2 heads (HHT, HTH, THH): 1/8 + 1/8 + 1/8 = 0.375. The equation of an event occurring (A) is: \\(P(A) =\\frac{n_a}{N}\\) where P(A) is the probability of event A equals the number of points constituting event A (\\(n_a\\)) divided by the total number of sample points (N). Applying this equation to the above example, P(A) = \\(\\frac{3}{8} = 0.375\\) 1.14 Combinatorials Because defining the sample space can be cumbersome for larger sample spaces, we can use combinatorial math, specifically the mn rule, to calculate sample space! The mn rule simply states that if one group contains m elements and another group contains n elements, you can for m x n pairs containing an element from each group. For instance, if we were determining the sample space for outcomes of rolling 2, 6-sided dice, we could multiply 6 x 6 (or \\(6^2\\)). Using the equation for calculating the probability of an event, what is the probability of rolling double 6s? answer = 1/36; answer ## [1] 0.02777778 Let’s try another example with even larger sample space: Calculate the probability of each person in a class of 14 students has a different birthday. In this case, a sample point consists of 14 dates, since there are 14 different students. Assuming that each student has the same probability of being born on any one of 365 days (over course in the real world the likelihood of being born on particular dates differs). The total number of sample points is \\(N = 365^{14}\\) (i.e., there are 365 possible birthdays for Student 1, 365 possible birthdays for student 2, yielding 716835279219613000000000000000000000 possible combinations of b-days). To calculate the probability that each student has a different birthday, you would apply the same equation as above, but calculate the number of points to satisfy our event, keeping in mind that there are 365 birthdays possible for Student 1, 264 possible birthdays for Student 2, 263 possible birthdays for Student 3, and so on, as: P(A) = \\(\\frac{n_a}{N} = \\frac{365 \\times 364 \\times 363 \\dots \\times 352}{365^{14}}=0.7769\\) Sample points can be represented as sequences of numbers or symbols. An ordered arrangement of of distinct objects is called a permutation. We can calculate the sample space as total number of distinct ways of arranging these symbols or numbers in a sequence, using the following equation: \\(P_r^n = n(n-1)(n-2)\\dots(n-r+1) = \\frac{n!}{(n-r)!}\\) where \\(n! = n \\times (n-1) \\times (n-2) \\times\\dots\\times2\\times1\\), where the total number of ordering \\(n\\) objects taken \\(r\\) at a time. Quick factorial review and calculating in r #factorial of 5 or 5! factorial &lt;- 5*4*3*2*1; factorial ## [1] 120 factorialR &lt;- factorial(5); factorialR ## [1] 120 #remember that factorial(0) ## [1] 1 Let’s apply this equation: How many trinucleotide sequences can be formed without repeating a nucleotide? To put this another way, we ar interested in the number of ways of ordering \\(n = 4\\) elements (A, T, C, and G) taken 3 at a time (trinucleotide = 3; \\(r = 3\\)). permutations = (factorial(4))/factorial(4-3); permutations ## [1] 24 When the sequence is not important, we use a different formula. For unordered sets of r elements chosen without replacement from n available elements are called combinations, with total number of combinations calculated as: \\(C^n_r = \\frac{n!}{r!(n-r)!}\\) What are the number of combinations of 2 colors of m&amp;ms that we can select out of the 5 total colors? combinations = (factorial(5))/(factorial(2)*factorial(5-2)); combinations ## [1] 10 If it’s helpful, you can test the answer by hand, using these colors: tan, brown, orange, red, and green. Again, the basic concepts of the sample-point method, are to define sample space, and either assign probabilities to all sample points, then sum the probabilities OR calculate \\(P(A) =\\frac{n_a}{N}\\) where P(A) is the probability of event A equals the number of points constituting event A (\\(n_a\\)) divided by the total number of sample points (N). Note: these are the same equation, they only differ in terms of when you calculate the probability (of the points individually, or at the end after you define the sample space / and event number). When sample space is large, it is easier to use combinatorial math to calculate the points and the sample space. Let’s apply this understanding of probability to calculate the likelihood of two events occurring. 1.15 Union and intersection of events You might be gleaning that defining the appropriate sample space is critically important to correctly calculate probabilities of particular events occurring. Also important is the understanding relationship between events of interest for which we calculate probabilities. These concepts are also important for understanding Boolean operators, computer coding concepts, and modeling. Note: Boolean operators form the basis of mathematical sets and database logic. They connect your search words together to either narrow or broaden your set of results. The three basic boolean operators are: AND, OR, and NOT. (I copied and pasted this from the internet, should rewrite a bit - this is just taking a while) Union of events Union of events: What is the likelihood of both events A and B (this can be written as \\(A \\cup B\\))? contains all sampling points for A or B. Example calculating probability of union event: What is the likelihood of rolling an odd number (Event A) on a 6-side fair die OR that is less than 4 (Event B)? Event A = 1, 3, 5 Event B = 1, 2, 3 #The sample space is all possible rolls samplespace &lt;- c(1, 2, 3, 4, 5, 6); samplespace ## [1] 1 2 3 4 5 6 #We will use Boolean operators in R. They will return TRUE / FALSE statements. #Below we tell R to look for odd numbers OR (indicated by line) #numbers less than 4. unionevent &lt;- (samplespace%%2==1) | (samplespace &lt; 4); unionevent ## [1] TRUE TRUE TRUE FALSE TRUE FALSE #If the conditions are satisfied, TRUE will be returned. #If conditions are not met, FALSE will be returned. #Now, let&#39;s calculate the probability using techniques that you #have already seen above. probunion &lt;- 4/6; probunion ## [1] 0.6666667 Intersection of events Intersection of events: What is the likelihood of both events A and B (this can be written as \\(A \\cap B\\))? contains all sampling points for A and B. Example calculating probability of an intersection event: What is the likelihood of rolling an odd number (Event A) on a 6-side fair die AND that is less than 4 (Event B)? Event A = 1, 3, 5 Event B = 1, 2, 3 #The sample space is all possible rolls samplespace &lt;- c(1, 2, 3, 4, 5, 6); samplespace ## [1] 1 2 3 4 5 6 #We will use Boolean operators in R. They will return TRUE / FALSE statements. #Below we tell R to look for odd numbers OR (indicated by line) #numbers less than 4. intersectionevent &lt;- (samplespace%%2==1) &amp; (samplespace &lt; 4); intersectionevent ## [1] TRUE FALSE TRUE FALSE FALSE FALSE #If the conditions are satisfied, TRUE will be returned. #If conditions are not met, FALSE will be returned. #Now, let&#39;s calculate the probability using techniques that you #have already seen above. probintersection &lt;- 2/6; probintersection ## [1] 0.3333333 #Just for fun #The other common Boolean operator used in computer code is &#39;not&#39; #We want to roll any number except 2 nottwo &lt;- samplespace != 2; nottwo ## [1] TRUE FALSE TRUE TRUE TRUE TRUE probNOTtwo &lt;- 5/6; probNOTtwo ## [1] 0.8333333 1.16 Conditional probability Conditional probability indicates a situation when the probability of one event depends on the occurrence of another event. Conditional probability (written as P(A|B), and read as the probability of event A given B) is calculated with the following equation: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\) where \\(A \\cap B\\) indicates the probability of both event A and event B occurring. Example: I roll a die and ask you to guess the number. I want to increase your odds of guessing the correct number, so I tell you that the number is odd. You are going to guess the number 3 - What are your odd of guessing the correct number (odds that the answer is 3; event A) given that the result is an odd number (the answer is an odd number; event B)? To break this down, the probability of both A and B being true (\\(A \\cap B\\)) is 1/6. The likelihood of the roll being a 3 is the limiting factor in essence, so an ‘and’ probability is constrained by the least likely event. Then, we divide 1/6 by the probability of the roll being odd, which is 1/2 since there are 3 odd and 3 even numbers. conditional &lt;- (1/6)/(1/2); conditional ## [1] 0.3333333 #the likelihood of guessing the correct number, if the number is odd is 1/3. 1.17 Multiplicative Law of Probability of Independent Events For independent events, in other words, two events in which the occurrence of one event doesn’t depend on the occurrence of the other event, the probability of BOTH events occurring is the found by multiplying the probability of each event A and B happening: \\(P(A \\cap B)= P(A) \\times P(B)\\) where \\(A \\cap B\\) indicates the probability of both event A and event B occurring. Example: What is the probability of getting heads in two consecutive coin tosses? heads &lt;- (1/2)*(1/2); heads ## [1] 0.25 1.18 Additive Law of Probability of Mutually Exclusive Events For two events that are mutually exclusive (if event A occurs, then event B cannot occur), the probability of one of two mutually exclusive events happening is found by adding their individuals probabilities. \\(P(A \\cup B)= P(A) + P(B)\\) where \\(A \\cap B\\) indicates the probability of both event A and event B occurring. Example: What is the probability of rolling a 5 and a 6 on a fair die? That’s right! It’s zero, because the definition of mutual exclusivity indicates that if one event occurs the other can’t. Real example: What is the probability of rolling a 5 or a 6 on a fair die? fivesix &lt;- (1/6)+(1/6); fivesix ## [1] 0.3333333 1.19 General Additive Law of Probability When events are not mutually exclusive, we apply the general additive law of probability, which is written as: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) where \\(P(A \\cap B)\\) is the intersection of event A and event B. Why do we remove the intersection event? We essentially don’t want to double count the intersecting area of Event A and Event B. You’ve already completed one of these problems, we just hadn’t gone over intersection events. Let’s apply the general additive law of probability to the previous problem: What is the likelihood of rolling an odd number (Event A) on a 6-side fair die OR that is less than 4 (Event B)? Event A = 1, 3, 5 Event B = 1, 2, 3 Original calculation: #The sample space is all possible rolls samplespace &lt;- c(1, 2, 3, 4, 5, 6); samplespace ## [1] 1 2 3 4 5 6 #We will use Boolean operators in R. They will return TRUE / FALSE statements. #Below we tell R to look for odd numbers OR (indicated by line) #numbers less than 4. unionevent &lt;- (samplespace%%2==1) | (samplespace &lt; 4); unionevent ## [1] TRUE TRUE TRUE FALSE TRUE FALSE #If the conditions are satisfied, TRUE will be returned. #If conditions are not met, FALSE will be returned. #Now, let&#39;s calculate the probability using techniques that you #have already seen above. probunion &lt;- 4/6; probunion ## [1] 0.6666667 Calculation using the equation: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) probA &lt;- 3/6 probB &lt;- 3/6 intersection &lt;- 1/3 finalprob &lt;- probA + probB - intersection; finalprob ## [1] 0.6666667 1.20 Key concepts Randomness is inherent in natural systems. Whether this randomness is truly random or simply appears random due to the complexity of natural systems, we treat it the same in statistics. We want to avoid any form of systematic bias in experimental design. Randomness and other factors are lumped together in what we call statistical error; the difference between a true estimate of a parameter and what we estimate in our sample. Probability is the proportion of times an event occurs in repeated trials, and varies between 0 and 1. We practiced a bunch of different ways of calculating probability, for the long-term, what you really need to know is that to calculate probabilities, you need to understand your sample space and the likelihood and relationship of events of interest. Two great rules of probability are: Additive Law of Probability &amp; the Multiplicative Law of Probability *Union events and Intersection events 1.21 Practice problems We will highlight the basic components of a statistical test with a very simple statistical ‘tailed’ test. For this example, we will: State a hypothesis Calculate a test statistic Determine the p-value Interpret results 1.22 Basic hypothesis testing review We are all familiar with the basic scientific process: A researcher makes observations about the natural world, then generates a hypothesis to test a particular explanation of an observed phenomenon within the system, rejects or fails to reject the hypothesis, and reports these findings, growing the understanding of the system. In most day-to-day research, researchers perform the scientific process intuitively, rather than explicitly. For instance, I have never included hypotheses in a manuscript. This isn’t to say you can’t include hypotheses, just that most journals don’t require writing your methods this way, and often space constraints and readability considerations lead authors to avoid including specific hypotheses in write-ups. Frequently, I will include hypotheses in a research proposal to help illustrate key aspects of the system and demonstrate that the statistical tests are testable, but not always! In other words, hypothesis testing is the foundation of scientific inquiry, but easily overlooked, since it often isn’t necessary to explicitly state hypotheses to create scientific products. Statistical hypothesis testing answers very simple questions, while scientists work in very complex knowledge environments. For beginning researchers, it is important to understand what statistics can tell us, and how this builds information to address amazing and very interesting research questions. Hypotheses allow us to articulate exactly what we are testing in statistics and to organize thoughts and analyses. In Week 1, we discussed that most statistical tests are designed to answer one of two questions: 1) Are there differences between these groups? and 2) are there relationships between these variables? Let’s connect these concepts directly to hypothesis testing. The null hypothesis (\\(H_0\\)) is a statement about a population parameter that would be interesting to reject. The null hypothesis typically asserts that there is no effect or relationship or that results will not deviate from established knowledge. For instance: The mean height of giraffes in captivity and in the wild do NOT differ. The incidence of toenail fungus is the SAME in the control group and the group given anti-fungal medicine. There is NO relationship between sea grass height and the number of sea snails. The mean human body temperature is 98.6 degrees Fahrenheit. Null hypotheses are paired with alternative hypotheses (\\(H_A\\)) that represent ALL other possibilities other than that stated in the null hypothesis. For instance: The mean height of giraffes in captivity and in the wild differs. The incidence of toenail fungus is different in the control group and the group given anti-fungal medicine. There is a relationship between sea grass height and the number of sea snails. The mean human body temperature is not 98.6 degrees Fahrenheit. Note that the null hypothesis is very specific, while the alternative hypothesis is general. Statistical tests are designed to either reject or fail to reject the null hypothesis. 1.23 Tailed tests Tailed tests are very simple tests for comparing means or proportions, and are great for illustrating the basic components of a frequentist statistical analysis. Let’s walk through an example! Handedness is common in humans. Around 90% of humans preferentially use their right hand. You’ve been watching your cat, Geraldo, play with his toy mouse, and you notice that he preferentially uses his right paw to bat the mouse around. You start to wonder if cats display handedness like humans! You run around visiting cats and observing their paw usage and determine that 14 cats of the 18 you observe appear to be right-pawed, while 4 preferentially use their left paw. Is this enough evidence to suggest that cats display ‘handedness’ or did this pattern just emerge by chance? Generate hypotheses What is the null hypothesis in this case? Remember it must be specific so that we can either reject or fail to reject the null! \\(H_0\\): Left and right-pawed cats are equally frequent in the population (i.e., Cats are not right or left-pawed). Note that this null hypothesis is very specific. If we would describe this mathematically, we would say that we expect half the cats (9 / 18) to use their right paws and half to use their left paws. If we express this as a proportion, (\\(p\\)), we are testing whether \\(p\\) = 0.5. Very specific. What is the alternative hypothesis? \\(H_A\\): Left and right-pawed cats are not equally frequent in the population. Note that the alternative hypothesis is very broad and encompasses all other possibilities. Because, in theory, we could observe proportions below 0.5 (1/18 cats are right-handed) or above 0.5 (16/19 cats are right-handed), we refer to this as two-tailed. In a two-tailed test, the alternative hypothesis includes parameters on both sides of the value specified by the null hypothesis. Before we move on, can you think of an example of a 1-tailed test? Examples: Did you score better than the class average? Is the time to ARD slower when you avoid driving through campus? Note that we are only interested in values in one direction. In the first example, we are interested in whether your score is &gt;80% (class average). In the second, we are interested in whether your drive time is &lt;10 minutes (time to ARD driving through campus from your house). How could you phrase the first example to be a 2-sided test? Answer: Was my score different than the class average? In this case, your score could be higher or lower. Calculate a test statistic We generated our hypotheses. Let’s calculate a test statistic. What is a test statistic? A test statistic is a quantity calculated from that data used in statistical analysis to evaluate the null hypothesis. For this simple test, our test statistic will be 14, since this is the number of right-handed cats we observed. We want to ask whether observing 14 out of 18 cats using their right paw is truly different from the null (9 out of 18 cats using their right paw), or did this pattern occur by chance in our sample? Determine the p-value Frequentists statistics is based on the concept of statistical distributions. If we run many trials, we can determine the likelihood of certain events occurring. We refer to the patterns of occurrence of trials as frequency distributions. Let’s illustrate using the data above. A cat can either be right-handed or left-handed (in this case there are no ambidextrous cats). To determine the likelihood that our pattern arose by chance, we conduct numerous trials like a coin toss. We would randomly flip a coin 18 times and record the outcome of each trial; heads being right-pawed, and tails being left-pawed. Let’s do that now: Each student take a coin. Flip the coin 18 times and record number of heads. *Divide by total number of trials (in this case students) to derive relative frequency for each event. We probably generated something that looks like the figure below. This is referred to a probability distribution and expresses the relative frequency of particular events occurring. Frequentist statistics derives its name from this probability distribution. paws &lt;- 0:18 plot(paws, dbinom(paws, 18, 0.5), type=&#39;h&#39;, ylab=&quot;Probability&quot;, xlab=&quot;Number Right-pawed Cats&quot;) Here is a table of those probabilities: probability &lt;- dbinom(paws, 18, 0.5) N &lt;- 0:18 pawtable &lt;- cbind(N, probability) pawtableF &lt;- as.data.frame(pawtable); pawtableF ## N probability ## 1 0 3.814697e-06 ## 2 1 6.866455e-05 ## 3 2 5.836487e-04 ## 4 3 3.112793e-03 ## 5 4 1.167297e-02 ## 6 5 3.268433e-02 ## 7 6 7.081604e-02 ## 8 7 1.213989e-01 ## 9 8 1.669235e-01 ## 10 9 1.854706e-01 ## 11 10 1.669235e-01 ## 12 11 1.213989e-01 ## 13 12 7.081604e-02 ## 14 13 3.268433e-02 ## 15 14 1.167297e-02 ## 16 15 3.112793e-03 ## 17 16 5.836487e-04 ## 18 17 6.866455e-05 ## 19 18 3.814697e-06 Take a look at the chart. What is the probability of observing right pawedness in 14 out of 18 cats? Is this difference from the null (9 cats are right-handed, no better than random) big enough to reject the null? To determine this we calculate a p-value. A p-value is the probability of obtaining the data that we observe if the null hypothesis were true. In this case, we will generate a p-value for a two-tailed test. To do this, we will add the probabilities of observing 14 right pawed cats or more by chance AND for the possibility of observing 4 or fewer right paws, which would indicate left pawedness. P-value = Pr[14] + Pr[15] + Pr[16] + Pr[17] + Pr[18] + Pr[4] + Pr[3] + Pr[2] + Pr[1] + Pr[0] Recall that we can add these probabilities up, since, in our example, we say that being right or left pawed are mutually exclusive events. pvalue &lt;- (0.0117 + 0.0031 + 0.0006 + 0.00007 + 0.000004)*2; pvalue ## [1] 0.030948 We generate a P-value of 0.031. In most sciences, we have agreed on a threshold of 0.05 for establishing statistical significance. If p-values are less than or equal to 0.05, we reject the null hypothesis. If larger, we fail to reject. The significance level, \\(\\alpha\\), is a probability used as a criterion for rejecting the null hypothesis. This significance level is important. In the sciences, we would rather err on the side of not identifying a pattern, rather than saying there is a pattern, when there it doesn’t actually exist. This concept is included in the in the discussion of *statistical errors**. A Type I error is when you reject a true null hypothesis. You are saying there is a difference, when actually, if could perfectly measure your focal population, there is no difference. By establishing a significance level (\\(\\alpha\\)) of 0.05, we are saying that we are willing to accept that 5% of the time, we will say there is an effect when there isn’t one. A Type II error is failing to find a pattern or a difference when there actually is one. If you reduce your \\(\\alpha\\) to reduce your likelihood of making a Type I, you increase the likelihood of committing a Type II error. The probability of committing a Type II error is more challenging to quantify and is related to the concept of statistical power. A study with high power has a low likelihood of committing a Type II error. Statistical power depends on several things, including sample size, the magnitude of the effect of the treatment, and variation within the sample. A study with a LARGE sample size, a BIG treatment effect, and SMALL variation within samples will have high statistical power. We will talk about calculating statistical power later. 1.24 Drawing conclusions from statistics In this case, we reject the null hypothesis, and state that our results support the alternative hypothesis that there is handedness in cats. Note that we ‘support’ the alternative hypothesis, rather than saying that there is pawedness in cats or accepting the alternative hypothesis. Statements about statistics are phrased to reflect that we are dealing in probabilities, and there is always a chance that our findings are incorrect. Additionally, statistical tests specifically test the null hypothesis, not the alternative (for which there are often many possibilities). What would we say if we didn’t reject the null? We would state that we failed to reject the null hypothesis. Failing to reject the null indicates that our sample did not provide sufficient evidence to conclude that the effect exists, but lack of evidence doesn’t prove that the effect does not exist. For this reason, we never accept the null. 1.25 Reporting results When we report findings, we will provide: 1) A statement of findings 2) The test statistic 3) The P-value 4) A description of differences, if differences exist 5) A visualization of differences, if differences exist Here, let’s focus on reporting information 1 - 3. We might say something like: Cats displayed higher levels of handedness than expected by chance (t = 14, p = 0.031). 1.26 Review problems 1.27 A foray into statistics Last week we talked about data types and describing data. This week we are applying concepts that we learned last week to select (and run) a statistical analysis. When conducting an analysis, a dataset will contain one or more response variables (aka y-variables, dependent variables) and one or more explanatory variables (aka x-variables, independent variables). The data types of the explanatory and response variables determine which statistical test we use. Brief review: Explanatory variables are used to explain variation in response variables. For instance, imagine that we want to identify areas that support high numbers of plants from the genus Mitella. We hypothesize that Mitella occurrence is positively related to water supply (i.e., the number of Mitella plants goes up as water availability increases). In this case, my response variable would be number of plants in genus Mitella, and my explanatory variable would be some measure of water supply. 1.28 Selecting statistical analyses Below, you will see a very simple decision matrix for selecting statistical analyses. We will build on this basic matrix and move into alternative ways of conducting the analyses indicated within the matrix throughout the semester. However, this basic structure gives us a great launch point for structuring the next discussions. Also, the statistical analyses presented below are super common in social and ecological sciences! When both the response and explanatory variables are categorical, we use what is called a G-test! "],["structural-equation-models.html", "Chapter 2 Structural equation models 2.1 What are structural equation models? 2.2 Anatomy of a Structural Equation Model 2.3 Building Multivariate Models 2.4 From Concept to Model 2.5 Common Structures in Causal Diagrams 2.6 Chapter 5: Covariance-Based Estimation in SEM 2.7 Getting Started with lavaan – Model Specification, Estimation, and Interpretation 2.8 Part 1: What is Piecewise SEM? 2.9 Overview", " Chapter 2 Structural equation models 2.1 What are structural equation models? Structural Equation Modeling (SEM) is a statistical technique that enables the modeling of complex causal relationships among variables. It extends regression by allowing: Simultaneous estimation of multiple equations Inclusion of both observed and latent (unmeasured) variables Specification of indirect effects and feedback loops Assessment of model fit against observed data SEM is often visualized as a path diagram, where arrows represent hypothesized relationships between variables. 2.1.1 When Should You Use SEM? Use SEM when: You have multiple dependent variables that may influence one another You want to estimate direct and indirect effects between variables You need to account for measurement error in survey or ecological constructs You are testing a theory or conceptual model with multiple pathways Your system includes latent constructs like “habitat quality” or “disturbance pressure” inferred from several indicators SEM is ideal when a single regression model is too simplistic to capture the interdependent relationships in your system. 2.1.2 How is SEM Different from Multiple Regression? Feature Multiple Regression SEM Number of equations One Many simultaneously Latent variables ❌ Not allowed ✅ Allowed Indirect effects ❌ Manual calculation ✅ Modeled explicitly Measurement error (in predictors) ❌ Ignored ✅ Can be modeled Model fit assessment R², AIC, etc. Chi-square test, RMSEA, CFI, TLI, etc. In multiple regression, you can only estimate one equation at a time, with no ability to model feedback loops or measurement error. In contrast, SEM treats your whole system as a network, testing how well your full model fits the data. 2.1.3 Summary SEM is a flexible and powerful framework for: Exploring and testing theoretical models Modeling complex causal chains Incorporating unobservable constructs It provides more rigorous insights into systems where variables are interdependent, influenced by hidden factors, and subject to error. 2.2 Anatomy of a Structural Equation Model Structural Equation Modeling (SEM) provides a flexible framework for representing complex causal relationships between variables. In this chapter, we break down the structure of an SEM into its key components. 2.2.1 Types of Variables SEM uses two main types of variables: Observed variables: Directly measured values (e.g., soil nitrogen, species richness). Latent variables: Not directly observed, but inferred from multiple observed indicators (e.g., a latent “Disturbance Pressure” factor inferred from road density, fire frequency, and grazing). Each variable also plays one of two roles in the model: Variable Role Description Exogenous Predictor variables not explained by any other variables in the model. Endogenous Response variables influenced by one or more other variables in the model. Note: Endogenous variables can act as both outcomes and predictors of other endogenous variables. 2.2.2 Path Diagrams Path diagrams are a central feature of SEM. They visually represent the hypothesized relationships between variables using arrows: Single-headed arrows (→) represent causal/predictive paths Double-headed arrows (↔︎) represent covariances (associations not assumed to be causal) 2.3 Building Multivariate Models 2.3.1 What is Causality? Causality refers to understanding the directional effect one variable has on another. In SEM, causal relationships are explicitly modeled, unlike in multiple regression where they may be implied but not specified. Judea Pearl’s Ladder of Causation is a framework for understanding levels of causal reasoning: 1. Observation — associational reasoning (“what is”) 2. Intervention — action-based reasoning (“what if I do”) 3. Counterfactuals — imagining alternate realities (“what if I had done”) We ascend the ladder by incorporating more assumptions and a model of the system. 2.3.2 Why move beyond Multiple Regression? Multiple regression models: • Estimate associations while “controlling for” other variables • Assume independence among predictors • Do not explicitly encode causal structure Causal models (via SEM): • Represent direction and strength of causal pathways • Account for mediators, confounders, and latent variables • Enable testing of hypotheses about mechanisms 2.3.3 Meta-Modeling Your System Before diving into data, build a conceptual model: 1. Define your research Purpose: • Discovery: Are you exploring patterns or relationships for the first time? • Hypothesis Testing: Are you evaluating a specific theoretical prediction? • Prediction: Are you aiming to forecast outcomes under new conditions? 🔍 Tip: Discovery-based models might be more flexible or exploratory, while hypothesis testing demands tighter causal logic and pre-specified relationships. 2. Identify the Focus: What role do your variables play in the system?: • Drivers: Variables that initiate causal change (e.g., treatments, environmental context). • Responses: Outcome variables (e.g., pollinator abundance). • Mediators: Variables that transmit effects from causes to outcomes (e.g., plant cover). • Theory Testing: Are you evaluating a known causal pathway or testing a novel ecological hypothesis? 🧠 Make sure the pieces of your model are causal! Avoid throwing in all your variables just because you measured them — each should have a theorized role. 3. Determine Span of Inference: Are your findings meant to be context-specific or to inform broader generalizations?: • Local/System-Specific: Targeted insights for a particular site or management action. • Generalizable Process: Testing broad ecological principles or multi-site patterns. 📏 This choice influences how you frame your model structure and what covariates (like “site”) you might treat as fixed or random. 2.4 From Concept to Model Once your meta-model is constructed: • Reify with data availability in mind • Ensure your DAG closes appropriate backdoors • Align structure with your research purpose “Make sure the pieces of your model are causal!” 2.5 Common Structures in Causal Diagrams In structural equation modeling (SEM) and causal inference, understanding the basic building blocks of causal diagrams is crucial. These structures form the foundation for determining what to control for and what to avoid controlling for. Below are the most common structures you’ll encounter in Directed Acyclic Graphs (DAGs), with brief explanations and examples. 2.5.1 Chains (Mediation Paths) Structure: Cause → Mediator → Effect Interpretation: • The mediator is an intermediate variable through which the cause affects the effect. • If you control for the mediator, you block the indirect path, potentially underestimating the total effect of the cause. Example: Fire frequency → Canopy cover → Soil moisture • If you control for canopy cover, you might miss the full effect that fire frequency has on soil moisture via changes in canopy cover. Visual: library(dagitty) library(ggdag) dag &lt;- dagify( SoilMoisture ~ CanopyCover, CanopyCover ~ Fire, labels = c(Fire = &quot;Fire Frequency&quot;, CanopyCover = &quot;Canopy Cover&quot;, SoilMoisture = &quot;Soil Moisture&quot;), exposure = &quot;Fire&quot;, outcome = &quot;SoilMoisture&quot; ) ggdag(dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() 2.5.2 Colliders Structure: Cause1 → Collider ← Cause2 Interpretation: • A collider is a variable that is influenced by two (or more) variables. • Controlling for the collider opens a path between the two causes, creating spurious associations where none may exist. Example: Soil nitrogen → Plant growth ← Pathogen load • Both soil nitrogen and pathogen load affect plant growth. If you control for plant growth (the collider), it might look like soil nitrogen and pathogens are correlated—even if they’re not. Visual: dag &lt;- dagify( PlantGrowth ~ SoilNitrogen + PathogenLoad, labels = c(SoilNitrogen = &quot;Soil N&quot;, PathogenLoad = &quot;Pathogens&quot;, PlantGrowth = &quot;Plant Growth&quot;), exposure = &quot;SoilNitrogen&quot;, outcome = &quot;PathogenLoad&quot; ) ggdag(dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() 2.5.3 Forks (Confounding Paths) Structure: CommonCause → Cause, CommonCause → Effect Interpretation: • This is a confounding structure, where the common cause explains the observed correlation between two variables. • Controlling for the common cause blocks the backdoor path, helping isolate the causal effect. Example: Soil type → Invasive species cover Soil type → Native species richness • If you don’t control for soil type, you may falsely conclude that invasive cover affects native richness, when both are simply driven by the soil. Visual: dag &lt;- dagify( InvasiveCover ~ SoilType, NativeRichness ~ SoilType, labels = c(SoilType = &quot;Soil Type&quot;, InvasiveCover = &quot;Invasive Cover&quot;, NativeRichness = &quot;Native Richness&quot;), exposure = &quot;InvasiveCover&quot;, outcome = &quot;NativeRichness&quot; ) ggdag(dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() 2.5.4 Descendants of Colliders Key Point: • Controlling for descendants of colliders can also open spurious paths. Example: Imagine you control for a variable like Plant biomass, which is a descendant of a collider (e.g., Herbivory ← Plant defense → Biomass). This can reintroduce bias just like controlling for the collider itself. 2.5.5 Summary of strcutures Structure Do You Control For It? Why? Chain (mediator) ❌ (if estimating total effect) Controls block indirect paths Collider ❌ Conditioning opens spurious paths Fork (common cause) ✅ Controls block confounding Descendant of Collider ❌ Opens collider paths indirectly Understanding these structures helps you build better SEMs, choose correct adjustment sets, and avoid introducing bias into your models. 2.5.6 Identifying Causality You don’t need to know all mechanisms to make causal claims. But you do need to: • Map the system (e.g., using DAGs) • Identify and control for confounders • Avoid controlling for mediators 2.5.7 Backdoor Criterion To estimate a causal effect of X → Y, block all backdoor paths (those that flow into X) by conditioning on appropriate variables not affected by X. 2.5.8 Frontdoor Criterion Useful when you can’t block all backdoor paths. Identify a mediator that: • Is affected by X • Affects Y • Is not affected by any confounders of X and Y 2.5.9 Build your own DAG: SEM Case Study on Pollinator Impacts of Vegetation Management This research project explores how different Integrated Vegetation Management (IVM) treatments conducted by Arizona Public Service (APS) on powerline rights-of-way (ROWs) affect pollinator communities, using Structural Equation Modeling (SEM) to untangle direct and indirect effects. 2.5.9.1 Study Context APS manages vegetation beneath powerlines for safety and access, using a mix of: • Mechanical removal • Herbicide application • Combined Mechanical + Herbicide • Untreated (Control) plots Understanding how these treatments influence floral resources is key to predicting changes in pollinator abundance and diversity. 2.5.9.2 Data Collection At 3 sites, treatments were applied in a randomized block design across plots categorized as: • Control • Herbicide • Mechanical • Mechanical + Herbicide Researchers want to test whether management effects on pollinators are mediated through floral resources, or whether there are residual (direct) effects of treatment type beyond vegetation structure. 2.5.9.3 Key Variables and Their Roles Exogenous (Independent) Variables: • Treatment (main predictor—e.g., herbicide, mowing) • Soil substrate (moderator/stratifier of treatment effects) • Cattle presence (a confounder—not caused by treatment) Plant Community Mediators: • Plant richness • Plant cover • Plant height • Ceanothus presence/abundance • Woody debris Pollinator Response Variables: • Pollinator abundance • Pollinator richness library(dagitty) library(ggdag) library(tidyverse) # Define the DAG ivm_dag &lt;- dagitty(&quot; dag { Treatment Soil_Substrate Cattle Plant_Richness Plant_Cover Plant_Height Ceanothus Woody_Debris Pollinator_Richness Pollinator_Abundance Treatment -&gt; Plant_Richness Treatment -&gt; Plant_Cover Treatment -&gt; Plant_Height Treatment -&gt; Ceanothus Treatment -&gt; Woody_Debris Soil_Substrate -&gt; Treatment Soil_Substrate -&gt; Plant_Richness Soil_Substrate -&gt; Plant_Cover Soil_Substrate -&gt; Woody_Debris Cattle -&gt; Plant_Richness Cattle -&gt; Plant_Cover Cattle -&gt; Pollinator_Abundance Cattle -&gt; Pollinator_Richness Plant_Richness -&gt; Pollinator_Richness Plant_Richness -&gt; Pollinator_Abundance Plant_Cover -&gt; Pollinator_Richness Plant_Cover -&gt; Pollinator_Abundance Plant_Height -&gt; Pollinator_Richness Plant_Height -&gt; Pollinator_Abundance Ceanothus -&gt; Pollinator_Richness Ceanothus -&gt; Pollinator_Abundance Woody_Debris -&gt; Pollinator_Richness Woody_Debris -&gt; Pollinator_Abundance } &quot;) node_roles &lt;- tibble( name = c( &quot;Treatment&quot;, &quot;Soil_Substrate&quot;, &quot;Cattle&quot;, &quot;Plant_Richness&quot;, &quot;Plant_Cover&quot;, &quot;Plant_Height&quot;, &quot;Ceanothus&quot;, &quot;Woody_Debris&quot;, &quot;Pollinator_Richness&quot;, &quot;Pollinator_Abundance&quot; ), role = c( &quot;Treatment&quot;, &quot;Context&quot;, &quot;Context&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Pollinator&quot;, &quot;Pollinator&quot; ) ) # Get layout and merge roles dag_df &lt;- tidy_dagitty(ivm_dag, layout = &quot;nicely&quot;) %&gt;% left_join(node_roles, by = &quot;name&quot;) # Plot using ggplot2 with corrected edge structure ggplot() + geom_segment( data = dag_df %&gt;% filter(!is.na(xend)), aes(x = x, y = y, xend = xend, yend = yend), arrow = arrow(length = unit(0.02, &quot;npc&quot;)), color = &quot;grey40&quot; ) + geom_point( data = dag_df %&gt;% filter(!is.na(x)), aes(x = x, y = y, color = role), size = 8, alpha = 0.85 ) + geom_text( data = dag_df %&gt;% filter(!is.na(x)), aes(x = x, y = y, label = name), color = &quot;black&quot;, size = 4 ) + scale_color_manual(values = c( &quot;Treatment&quot; = &quot;#FB7E21FF&quot;, &quot;Context&quot; = &quot;#A91601FF&quot;, &quot;Plant&quot; = &quot;#18DDC2FF&quot;, &quot;Pollinator&quot; = &quot;#00468BFF&quot; )) + labs( title = &quot;Hypothesized DAG for Pollinator Response to IVM Treatment&quot;, color = &quot;Variable Type&quot; ) + theme_void() 2.5.10 Adjustment Sets An adjustment set is a group of variables you control for (include as covariates) in order to estimate the causal effect of one variable (say, Treatment) on another (say, Pollinator_Richness) without bias. In short, it blocks backdoor paths — those sneaky alternative routes through which spurious associations can travel. To find variables to condition on: adjustmentSets(ivm_dag, exposure = &quot;Treatment&quot;, outcome = &quot;Pollinator_Richness&quot;) ## { Soil_Substrate } adjustmentSets(ivm_dag, exposure = &quot;Treatment&quot;, outcome = &quot;Pollinator_Abundance&quot;) ## { Soil_Substrate } Interpretation: To estimate the total causal effect of Treatment on Pollinator_Richness, you only need to adjust for Soil_Substrate. This means: • Soil_Substrate is a backdoor variable — it opens a non-causal path because it influences both Treatment and plant variables that, in turn, influence pollinators. • Controlling for Soil_Substrate blocks that spurious path and gives you an unbiased estimate of the effect of Treatment. Do not control for: • Mediators, like Plant_Richness, Plant_Cover, or Woody_Debris, if you want the total effect of treatment. • Colliders, such as anything caused by both Treatment and another variable (you don’t have a clear collider in this DAG, but good to keep in mind). Example in a model: • model &lt;- lm(Pollinator_Richness ~ Treatment + Soil_Substrate, data = your_data) GREG HERE YOU NEED TO CREATE YOUR FINAL DATASET BY SUMMARIZING THE VEG DATA AND ADDING YOUR POLLINATOR DATA PER PLOT. THEN WE CAN ADJUST THE CODE BELOW TO TEST WHETHER YOU REPLICATION / DATA ARE OK AND THEN MOVE ONTO NEXT STEPS 2.6 Chapter 5: Covariance-Based Estimation in SEM library(lavaan) library(mvtnorm) library(mvnormtest) library(psych) library(Matrix) 2.6.1 What is Covariance-Based SEM? Structural Equation Modeling (SEM) using covariance-based maximum likelihood estimation fits parameters so that the model-implied covariance matrix matches the observed covariance matrix as closely as possible. Unlike individual regression models: SEM accounts for how estimation of one parameter affects others. SEM allows for modeling feedbacks and latent variables. SEM is based on maximum likelihood estimation (MLE). 2.6.2 Maximum Likelihood Estimation In SEM, MLE identifies parameters that maximize the likelihood of observing the data, given the model. This involves: Exploring the parameter space iteratively. Estimating model fit using a fitting function like ML. Computationally intensive with more parameters. 2.6.3 Assumptions Behind ML Estimation Multivariate normality (use mvnormtest::mshapiro.test()) # Simulate multivariate normal data set.seed(123) data &lt;- data.frame( x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100) ) # Use mshapiro.test from mvnormtest mshapiro.test(t(data)) # Note the transpose! ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.97635, p-value = 0.0689 No severe skew or missing data No redundant variables (covariance matrix must be positive definite) # Check skew and kurtosis psych::describe(data) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## x1 1 100 0.09 0.91 0.06 0.08 0.89 -2.31 2.19 4.50 0.06 -0.22 0.09 ## x2 2 100 -0.11 0.97 -0.23 -0.17 0.97 -2.05 3.24 5.29 0.63 0.58 0.10 ## x3 3 100 0.12 0.95 0.04 0.09 0.91 -1.76 2.29 4.05 0.32 -0.59 0.09 # Check for missing data summary(is.na(data)) # Should be all FALSE ## x1 x2 x3 ## Mode :logical Mode :logical Mode :logical ## FALSE:100 FALSE:100 FALSE:100 Sufficient sample size relative to parameters # Count parameters in a simple lavaan model model &lt;- &#39; y1 ~ x1 + x2 y2 ~ y1 &#39; # Simulate data data &lt;- data.frame(x1 = rnorm(100), x2 = rnorm(100), y1 = rnorm(100), y2 = rnorm(100)) # Estimate number of parameters fit &lt;- sem(model, data = data) summary(fit) ## lavaan 0.6-19 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 0.497 ## Degrees of freedom 2 ## P-value (Chi-square) 0.780 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y1 ~ ## x1 -0.078 0.088 -0.882 0.378 ## x2 0.194 0.092 2.103 0.036 ## y2 ~ ## y1 -0.071 0.109 -0.648 0.517 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 0.829 0.117 7.071 0.000 ## .y2 1.042 0.147 7.071 0.000 # Count parameters n_params &lt;- lavInspect(fit, &quot;npar&quot;) n_obs &lt;- nrow(data) # Portnoy’s Rule portnoy_value &lt;- n_params^(3/2) / n_obs portnoy_value # Should be close to 0 ## [1] 0.1118034 2.6.4 Identifiability Before fitting a SEM, you must ensure it is identified — meaning you have enough unique information to estimate all model parameters. 2.6.4.1 Key Rules T-rule: Number of parameters ≤ unique entries in covariance matrix. # Number of unique observed covariances p &lt;- ncol(data) n_cov &lt;- p * (p + 1) / 2 # Compare to number of free parameters n_cov ## [1] 10 lavInspect(fit, &quot;npar&quot;) # Should be ≤ n_cov ## [1] 5 Order condition: For each endogenous variable, incoming paths ≤ connected variables. There’s no direct test — but: • Draw your DAG. • Count how many arrows go into each endogenous variable. • Make sure that number ≤ number of variables related to it. Rank condition: Variables in feedback loops must be influenced by different causes. • For feedback loops: ensure that each variable has at least one unique exogenous predictor. • Use DAG tools (ggdag or dagitty) to visualize and confirm. If these rules are violated, the model is underidentified and cannot be estimated. 2.6.5 Degrees of Freedom DF = number of observed variances/covariances – number of estimated parameters Just-identified models (DF = 0): Can’t test fit Overidentified models (DF &gt; 0): Preferred — allows model fit evaluation library(lavaan) # define model as a lavaan-style string model &lt;- &#39; cover ~ age + elev firesev ~ age + cover &#39; fit &lt;- sem(model, data = keeley) fitMeasures(fit, c(&quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;cfi&quot;, &quot;rmsea&quot;)) ## chisq df pvalue cfi rmsea ## 0.900 1.000 0.343 1.000 0.000 2.6.6 Sample Size Considerations A model’s complexity is limited by your sample size: Rule of thumb: ≥ 5 observations per estimated parameter Preferably: ≥ 20 observations per parameter Use Portnoy’s rule: ρ3/2 / n → 0 Account for: Exogenous variable variances/covariances (often directly estimated from data) Endogenous variable error variances (often derived, not estimated directly) 2.6.7 Summary Covariance-based SEM is powerful but demands careful model design: Check identifiability before estimation Be aware of sample size constraints Use model diagnostics to evaluate fit after estimation 2.6.8 Chapter 6: Structural Equation Modeling in R with lavaan 2.7 Getting Started with lavaan – Model Specification, Estimation, and Interpretation Setting Up Before you begin, make sure the lavaan and lavaanPlot packages are installed: What is lavaan? • lavaan stands for Latent Variable Analysis. • Developed by Yves Rosseel (2010). • Syntax is similar to regression in R using formulas. • It supports latent and observed variables, covariance-based SEM, mediation, path analysis, and more. Example: Post-Fire Plant Recovery We’ll analyze a dataset from Keeley et al. (2006) studying how stand age, fire severity, and other factors affect plant cover. Step 1: Start Simple – A Regression as SEM # Load dataset # Example: keeley &lt;- read.csv(&quot;path/to/your/keeley_data.csv&quot;) # Fit SEM #model1 &lt;- &#39;cover ~ age&#39; #fit1 &lt;- sem(model1, data = keeley) #summary(fit1, standardized = TRUE, rsquare = TRUE) Intercepts and Mean Structures To explicitly estimate intercepts: #fit1_mean &lt;- sem(model1, data = keeley, meanstructure = TRUE) #summary(fit1_mean) Viewing the Model #lavaanPlot(model = fit1, coefs = TRUE, stand = TRUE) Standardized Estimates standardizedSolution(fit1) This gives standardized coefficients and helps compare the relative strength of predictors. Mediation Example: Indirect Effects #model2 &lt;- &#39; # firesev ~ age # cover ~ firesev + age #&#39; #fit2 &lt;- sem(model2, data = keeley) #summary(fit2, standardized = TRUE, rsquare = TRUE) Direct, Indirect, and Total Effects #model3 &lt;- &#39; # firesev ~ af*age # cover ~ fc*firesev + ac*age # Derived # indirect := af * fc # total := ac + (af * fc) #&#39; #fit3 &lt;- sem(model3, data = keeley) #standardizedSolution(fit3) Warnings: Variance Scaling #varTable(fit3) If you get a warning about variances differing by orders of magnitude, consider rescaling your variables or using standardized solutions. Visualizing Complex Models #lavaanPlot( # model = fit3, # coefs = TRUE, # stand = TRUE, # sig = 0.05, # graph_options = list(layout = &quot;circo&quot;) #) Final Exercise Prompt Try fitting the following model: #model_final &lt;- &#39; # rich ~ distance + abiotic + hetero # hetero ~ distance # abiotic ~ distance # abiotic ~~ hetero #&#39; #fit_final &lt;- sem(model_final, data = keeley) #summary(fit_final, standardized = TRUE) 2.7.1 Chapter 7:Assessing Fit and Normality library(lavaan) library(mvnormtest) library(MVN) Overview In this tutorial, we learn how to evaluate model fit and test for normality, key assumptions in covariance-based SEM. We’ll cover: • Standard fit indices (e.g., RMSEA, CFI) • Residuals and modification indices • Normality diagnostics • Remedies for assumption violations Example: Fully Mediated SEM #model_full &lt;- &#39; # firesev ~ age # cover ~ firesev #&#39; #fit_full &lt;- sem(model_full, data = keeley, meanstructure = TRUE) #summary(fit_full, fit.measures = TRUE) Interpretation: • Chi-square (p &gt; 0.05) → Model is not significantly different from the observed data. • Check additional fit indices: RMSEA, CFI, SRMR, AIC, BIC. Fit Indices (Kline 2023 Recommendations) Fit Measure Interpretation Chi-square test Prefer p &gt; 0.05 RMSEA 90% CI lower bound &lt; 0.05 CFI &gt; 0.90 SRMR &lt; 0.10 Diagnosing Misfit with Residuals #residuals(fit_full, type = &quot;cor&quot;) # residual correlations #modificationIndices(fit_full, standardized = FALSE, sort. = TRUE) • Large residuals or modification indices &gt; 3.84 suggest misfit. • Inspect residual correlation between rich and distance. Testing Normality of Residuals library(MVN) # Step 1: Get residuals from lavaan model #resids &lt;- lavPredict(fit_full, type = &quot;ov&quot;) # residuals for observed variables # Optional: check univariate normality visually #apply(resids[, 1:2], 2, function(x) { # qqnorm(x); qqline(x) #}) # Step 2: Multivariate Shapiro-Wilk (for n ≤ 50) #mshapiro.test(t(resids[, 1:2])) # Step 3: Mardia&#39;s test for multivariate normality #MVN::mvn(data = resids[, 1:2], mvn_test = &quot;mardia&quot;) # Step 1: Get residuals from lavaan model #tryCatch({ # resids &lt;- lavPredict(fit_full, type = &quot;ov&quot;) # Make sure residuals are numeric and have no missing values # if (!is.null(resids) &amp;&amp; is.matrix(resids) &amp;&amp; all(is.finite(resids[, 1:2]))) { # Step 2: Multivariate Shapiro-Wilk (for n ≤ 50) # print(mshapiro.test(t(resids[, 1:2]))) # Step 3: Mardia&#39;s test # print(MVN::mvn(data = resids[, 1:2], mvn_test = &quot;mardia&quot;)) # } else { # message(&quot;Residuals are missing, not numeric, or contain NA/Inf values.&quot;) # } #}, error = function(e) { # message(&quot;MVN test failed: &quot;, conditionMessage(e)) #}) ⸻ If Assumptions Are Violated… Option 1: Satorra-Bentler Correction #fit_sb &lt;- sem(model_full, data = keeley, test = &quot;Satorra.Bentler&quot;) #summary(fit_sb) Option 2: Bollen-Stine Bootstrap #fit_bs &lt;- sem(model_full, data = keeley, test = &quot;bollen.stine&quot;, se = &quot;boot&quot;, bootstrap = 1000) #summary(fit_bs) Summary: • Use fit indices and residuals to assess model performance. • Check assumptions of normality; consider corrections if violated. • Explore modification indices to identify potential improvements. 2.7.2 Chapter 8: Comparing Models and Testing Mediation This section introduces two major approaches for comparing SEM models: • Likelihood Ratio Tests (LRTs) for nested models. • Information Criteria (e.g., AIC, AICc) for both nested and non-nested models. We also explore mediation, which refers to how a relationship between two variables is explained by one or more intervening variables. # Load required libraries library(lavaan) library(AICcmodavg) # Fully Mediated Model fullMedModel &lt;- &#39; firesev ~ age cover ~ firesev &#39; fullMedSEM &lt;- sem(fullMedModel, data = keeley) # Partially Mediated Model partialMedModel &lt;- &#39; firesev ~ age cover ~ firesev + age &#39; partialMedSEM &lt;- sem(partialMedModel, data = keeley) # Likelihood Ratio Test (nested models) anova(partialMedSEM, fullMedSEM) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## partialMedSEM 0 359.4 371.9 0.0000 ## fullMedSEM 1 360.7 370.7 3.2974 3.2974 0.15977 1 0.06939 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Interpretation: A non-significant LRT suggests that the simpler (fully mediated) model fits the data about as well as the more complex model. AIC-Based Model Comparison # AICc model comparison aictab( cand.set = list(fullMedSEM, partialMedSEM), modnames = c(&quot;Full&quot;, &quot;Partial&quot;) ) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## Partial 5 360.11 0.00 0.63 0.63 -174.70 ## Full 4 361.17 1.05 0.37 1.00 -176.35 Interpretation: Models within 2 ΔAICc units are considered roughly equivalent. Higher AIC weight (AICcWt) indicates stronger support for that model. Additional Example: Distance and Species Richness Key Takeaways: • Use LRT for nested models; lower chi-square and higher p-value = simpler model may suffice. • Use AICc for broader comparisons; lower AICc and higher weight = better. • Mediation is central to SEM and can be tested with both approaches. • Fully vs. Partially Mediated models differ by whether direct paths bypass mediators. 2.7.3 Chapter 8: Latent Variables as Drivers What is a Latent Variable? Latent variables are unobserved constructs that we infer from multiple observed indicators. They represent abstract concepts like intelligence, disturbance, or biodiversity. • In SEM, latent variables are drawn as circles. • Observed indicators are squares or rectangles. • Latent variables are typically estimated through Confirmatory Factor Analysis (CFA). Confirmatory Factor Analysis (CFA) CFA allows you to test whether certain observed variables co-vary in ways consistent with an underlying theoretical construct. Example: Aposematism in Poison Frogs # Sample covariance matrix from Santos &amp; Cannatella (2011) # santosCov &lt;- read.table(&quot;https://raw.githubusercontent.com/username/santosCov.txt&quot;, na.strings = #&quot;.&quot;) #santosCov &lt;- as.matrix(santosCov) # Create covariance matrix manually (example values) santosCov &lt;- matrix(c( 1.00, 0.45, 0.38, 0.45, 1.00, 0.50, 0.38, 0.50, 1.00 ), nrow = 3, byrow = TRUE) # Add row and column names (must match your CFA model exactly) colnames(santosCov) &lt;- rownames(santosCov) &lt;- c(&quot;Alkaloid.quantity&quot;, &quot;Alkaloid.diversity&quot;, &quot;Conspicuous.coloration&quot;) # Specify CFA model santosCFA1 &lt;- &#39; Aposematism =~ Alkaloid.quantity + Alkaloid.diversity + Conspicuous.coloration &#39; # Fit the model santosFit1 &lt;- sem(santosCFA1, sample.cov = santosCov, sample.nobs = 21) summary(santosFit1, standardized = TRUE) ## lavaan 0.6-19 ended normally after 23 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 21 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Aposematism =~ ## Alkaloid.qntty 1.000 0.571 0.585 ## Alkalod.dvrsty 1.316 0.714 1.842 0.065 0.751 0.769 ## Conspics.clrtn 1.111 0.572 1.943 0.052 0.634 0.650 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Alkaloid.qntty 0.627 0.249 2.514 0.012 0.627 0.658 ## .Alkalod.dvrsty 0.388 0.298 1.305 0.192 0.388 0.408 ## .Conspics.clrtn 0.550 0.258 2.133 0.033 0.550 0.578 ## Aposematism 0.326 0.272 1.199 0.230 1.000 1.000 Why Use Latent Variables? • Increase accuracy by pooling information across multiple imperfect indicators. • Reduce measurement error. • Enable modeling of unobservable constructs. Identification Rules (How to Know Your Model Can Be Estimated): 1. T-Rule: Number of estimated parameters ≤ number of unique elements in the covariance matrix. 2. Three-indicator rule: Each latent variable has ≥ 3 uncorrelated indicators → SUFFICIENT. 3. Two-indicator rule: Works for multiple latent variables if indicators don’t share variance → SUFFICIENT. 4. Fixing scale: • Set variance of latent = 1.0, or • Set one loading to 1.0 to put latent on that indicator’s scale. Models with 2 indicators and shared error may be underidentified. Fit a Second Latent Variable: Body Size #santosSize &lt;- &#39; # Size =~ Log.Mass + Log.RMR + Log.Scope #&#39; #santosSizeFit &lt;- sem(santosSize, sample.cov = santosCov, sample.nobs = 21) #summary(santosSizeFit, standardized = TRUE) Combine Latent Variables You can model multiple latent variables and test how they relate to each other. #santosCFA2 &lt;- &#39; # Aposematism =~ Alkaloid.quantity + Alkaloid.diversity + Conspicuous.coloration + #Ant.Mite.Specialization + log.Prey # Scale =~ Log.Mass + Log.RMR + Log.Scope + Conspicuous.coloration #&#39; #santosFit2 &lt;- sem(santosCFA2, sample.cov = santosCov, sample.nobs = 21) #summary(santosFit2, standardized = TRUE) Summary: • Latent variables allow you to estimate unobservable concepts. • CFA is the method used to define latent variables in SEM. • Identification is critical—use the rules to check if your model can be estimated. • Measurement error is reduced by leveraging multiple indicators. 2.7.4 Chapter 10: Latent Responses and Measurement Error library(lavaan) library(semPlot) Overview In this section, we explore how latent variables can be modeled as responses and how to account for measurement error in observed indicators. Latent variables are constructs that cannot be measured directly (e.g., biodiversity, ecosystem health, intelligence), but are inferred from multiple observed indicators. Key Concepts Latent Variables as Responses Latent variables can be endogenous (influenced by other variables in the model). For instance: • A latent construct such as “Habitat Quality” could be influenced by soil moisture, disturbance, and vegetation cover. • Each latent construct is defined by observed indicators, which are imperfect and contain measurement error. Measurement Error SEM is powerful because it separates true score variance from error variance. Each observed variable has two components: • The true score linked to the latent construct. • The error term (random noise or instrument error). Accounting for measurement error prevents biased parameter estimates and inflated correlations. Example: Latent Response Model with Measurement Error We model a latent response Performance, influenced by an observed predictor Treatment, and measured via three indicators: perf1, perf2, perf3. # Define the SEM model &lt;- &#39; # Measurement model Performance =~ perf1 + perf2 + perf3 # Structural model Performance ~ Treatment &#39; # Simulate data set.seed(123) n &lt;- 200 Treatment &lt;- rnorm(n) perf1 &lt;- 0.6*Treatment + rnorm(n, sd = 1) perf2 &lt;- 0.6*Treatment + rnorm(n, sd = 1) perf3 &lt;- 0.6*Treatment + rnorm(n, sd = 1) data &lt;- data.frame(Treatment, perf1, perf2, perf3) # Fit the SEM fit &lt;- sem(model, data = data) summary(fit, standardized = TRUE) ## lavaan 0.6-19 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 1.871 ## Degrees of freedom 2 ## P-value (Chi-square) 0.392 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Performance =~ ## perf1 1.000 0.552 0.489 ## perf2 0.995 0.180 5.522 0.000 0.549 0.499 ## perf3 0.968 0.183 5.288 0.000 0.535 0.467 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Performance ~ ## Treatment 0.570 0.075 7.642 0.000 1.032 0.971 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .perf1 0.969 0.106 9.135 0.000 0.969 0.761 ## .perf2 0.910 0.100 9.058 0.000 0.910 0.751 ## .perf3 1.024 0.110 9.289 0.000 1.024 0.782 ## .Performance 0.017 0.042 0.410 0.682 0.056 0.056 Visualizing the SEM semPaths(fit, &quot;std&quot;, layout = &quot;tree&quot;, whatLabels = &quot;std&quot;) Why Model Latent Responses? • More reliable constructs by combining multiple indicators. • Reduces noise from any single observed variable. • Better reflects theoretical constructs (e.g., stress, biodiversity). Key Assumptions • Indicators are unidimensional (reflect one latent factor). • Measurement errors are uncorrelated. • Sufficient variation and correlation among indicators. Takeaway Modeling latent responses allows you to capture complex, unobserved constructs while accounting for error in measurements. SEM enables estimation of both the relationships among constructs and their measurement structure. 2.7.5 Chapter 11: Local Estimation and D-Separation Overview This chapter introduces local (piecewise) SEM — an approach that decomposes a full SEM into a series of local linear models. It contrasts with global covariance-based SEM in both assumptions and evaluation. Why Use Piecewise SEM? Covariance SEM Piecewise SEM Requires multivariate normality Robust to non-normal data Models all paths simultaneously Fits multiple linear models separately Fit assessed globally (χ², RMSEA) Fit assessed via D-separation tests Difficult with small N More flexible with limited sample size Example: Breaking SEM Into Local Regressions library(piecewiseSEM) # Now check if the functions are available ls(&quot;package:piecewiseSEM&quot;) ## [1] &quot;%~~%&quot; &quot;AIC_psem&quot; &quot;as.psem&quot; &quot;basisSet&quot; &quot;cerror&quot; &quot;coefs&quot; ## [7] &quot;dSep&quot; &quot;evaluateClasses&quot; &quot;fisherC&quot; &quot;getCoefficients&quot; &quot;getDAG&quot; &quot;getSortedPsem&quot; ## [13] &quot;keeley&quot; &quot;LLchisq&quot; &quot;meadows&quot; &quot;multigroup&quot; &quot;partialCorr&quot; &quot;partialResid&quot; ## [19] &quot;psem&quot; &quot;rsquared&quot; &quot;shipley&quot; &quot;stdCoefs&quot; &quot;unstdCoefs&quot; # Simulate data set.seed(123) n &lt;- 100 x &lt;- rnorm(n) y2 &lt;- 0.5 * x + rnorm(n) y1 &lt;- 0.6 * x + 0.4 * y2 + rnorm(n) example_data &lt;- data.frame(x, y1, y2) # Fit piecewise SEM mod_list &lt;- psem( lm(y2 ~ x, data = example_data), lm(y1 ~ x + y2, data = example_data) ) # Test model fit using Fisher&#39;s C fisherC(mod_list) ## Fisher.C df P.Value ## 1 NA 0 NA # Extract standardized coefficients stdCoefs(mod_list) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate sig ## 1 y2 x 0.4475284 0.10687862 98 4.187258 6.172903e-05 0.3895619 *** ## 2 y1 x 0.4549228 0.11372501 97 4.000200 1.236720e-04 0.3509054 *** ## 3 y1 y2 0.4238113 0.09899469 97 4.281152 4.371472e-05 0.3755510 *** dag &lt;- getDAG(mod_list) plot(dag) Directed Separation (D-sep) D-sep tests whether missing paths in your model are truly unnecessary. It evaluates conditional independence assumptions implied by the model. Understanding D-separation • Two variables are D-separated if they are conditionally independent, given a set of other variables. • D-sep claims are testable and form the basis set of the model. # Define DAG g &lt;- dagitty(&quot;dag { x -&gt; y2 -&gt; y1 x -&gt; y1 }&quot;) # View conditional independencies implied by the DAG impliedConditionalIndependencies(g) Model Fit via Fisher’s C library(piecewiseSEM) # Simulate example data set.seed(123) n &lt;- 100 x &lt;- rnorm(n) y2 &lt;- 0.5 * x + rnorm(n) y1 &lt;- 0.6 * x + 0.4 * y2 + rnorm(n) example_data &lt;- data.frame(x, y1, y2) # Fit piecewise SEM mod_list &lt;- psem( lm(y2 ~ x, data = example_data), lm(y1 ~ x + y2, data = example_data) ) # ✅ Get model fit statistics (replaces sem.fit) fisherC(mod_list) ## Fisher.C df P.Value ## 1 NA 0 NA # ✅ Get standardized path coefficients (replaces sem.coefs) stdCoefs(mod_list) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate sig ## 1 y2 x 0.4475284 0.10687862 98 4.187258 6.172903e-05 0.3895619 *** ## 2 y1 x 0.4549228 0.11372501 97 4.000200 1.236720e-04 0.3509054 *** ## 3 y1 y2 0.4238113 0.09899469 97 4.281152 4.371472e-05 0.3755510 *** # ✅ Optional: View DAG plot(getDAG(mod_list)) Interpretation: • p &gt; 0.05 → model is not rejected, D-sep claims hold. • p &lt; 0.05 → model is rejected, missing paths may be important. Summary: • Piecewise SEM is ideal when: • Data violate global SEM assumptions. • Sample size is too small for global SEM. • You want to understand model fit using D-sep logic. • Use dagitty to derive implied independencies. • Use sem.fit() and sem.coefs() for piecewise evaluation. 2.7.6 Chapter 12: Introduction to Piecewise SEM in R Learning Objectives By the end of this tutorial, you will be able to: • Understand how piecewise SEM differs from traditional covariance-based SEM. • Fit a multi-equation SEM using the psem() function. • Evaluate model fit using d-separation tests and Fisher’s C statistic. • Extract path coefficients and R² values. • Visualize model structure and relationships using visreg, DiagrammeR, or plot(). 2.8 Part 1: What is Piecewise SEM? Piecewise SEM uses a local estimation approach: each equation is estimated separately using standard regression (e.g., lm, lmer). This provides greater flexibility, such as: • Including non-normal or mixed-effect models. • Dealing with small sample sizes or nested structures. • Assessing model fit through d-sep tests (Shipley’s test of directed separation). Limitations: • Can’t handle latent variables. • Not suitable for cyclical feedback loops. • Difficult to interpret in overidentified models with correlated errors. Part 2: Load Packages and Data # Load libraries library(piecewiseSEM) library(visreg) library(DiagrammeR) # Load sample data (or use your own) data(keeley) # from piecewiseSEM Part 3: Specify and Fit Your Model # Fit individual models mod1 &lt;- lm(abiotic ~ distance, data = keeley) mod2 &lt;- lm(hetero ~ distance, data = keeley) mod3 &lt;- lm(rich ~ abiotic + hetero, data = keeley) # Combine into a psem object keeley_sem &lt;- psem(mod1, mod2, mod3) Evaluate Model Fit # Directed separation test #dSep(keeley_sem) # Fisher&#39;s C statistic #fisherC(keeley_sem) Interpret: • A non-significant p-value (&gt; 0.05) = model fits. • Significant missing paths → consider adding them and reassessing fit. Part 5: Summarize Coefficients and R² # Coefficients coefs(keeley_sem) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 abiotic distance 0.3998 0.0823 88 4.8562 0e+00 0.4597 *** ## 2 hetero distance 0.0045 0.0013 88 3.4593 8e-04 0.3460 *** ## 3 rich abiotic 0.8136 0.1746 87 4.6586 0e+00 0.4136 *** ## 4 rich hetero 45.0702 11.6797 87 3.8589 2e-04 0.3426 *** # R-squared values rsquared(keeley_sem) ## Response family link method R.squared ## 1 abiotic gaussian identity none 0.2113455 ## 2 hetero gaussian identity none 0.1197074 ## 3 rich gaussian identity none 0.3667967 Part 6: Plot Your Model Option A: Quick Base R Plot keeley_sem &lt;- psem( lm(firesev ~ age + cover, data = keeley), lm(cover ~ age + elev + firesev, data = keeley), data = keeley ) plot(keeley_sem) #Option B: Refined Graph with DiagrammeR # Optional customization plot(keeley_sem, node_attrs = list( x = c(2.5, 2.5, 4, 1), y = c(3, 1, 2, 2), shape = &quot;rectangle&quot;, fillcolor = &quot;white&quot; )) Part 7: Mediation Example mod_firesev &lt;- lm(firesev ~ age, data = keeley) mod_cover &lt;- lm(cover ~ firesev, data = keeley) firesev_model &lt;- psem(mod_firesev, mod_cover) summary(firesev_model) ## | | | 0% | |=========================================================================================================| 100% ## ## Structural Equation Model of firesev_model ## ## Call: ## firesev ~ age ## cover ~ firesev ## ## AIC ## 364.696 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## cover ~ age + ... coef 87 -1.8018 0.075 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 3.297 with P-value = 0.069 and on 1 degrees of freedom ## Fisher&#39;s C = 5.18 with P-value = 0.075 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## firesev age 0.0597 0.0125 88 4.7781 0 0.4539 *** ## cover firesev -0.0839 0.0184 88 -4.5594 0 -0.4371 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## firesev none 0.21 ## cover none 0.19 dSep(firesev_model) ## | | | 0% | |=========================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 cover ~ age + ... coef 87 -1.80184 0.07503437 rsquared(firesev_model) ## Response family link method R.squared ## 1 firesev gaussian identity none 0.2059938 ## 2 cover gaussian identity none 0.1910867 Bonus: Visualize with Covariates Held Constant # Visualize fire severity&#39;s effect on cover visreg(firesev_model[[2]], xvar = &quot;firesev&quot;) Wrap-Up Piecewise SEM offers flexibility and clarity for causal inference in ecology. By evaluating each path independently while testing the full model’s coherence through d-sep and Fisher’s C, you gain both transparency and statistical rigor. 2.8.1 Chapter 13: Nonlinearity and Interaction in SEM Overview In this tutorial, we’ll cover: • Nonlinearities (e.g., polynomial terms) • Centering variables to reduce multicollinearity • Interaction terms • Implementing these in SEM frameworks Example 1: Nonlinear Effects (Cardinale et al. 2009) Load and Prepare Data # Download data url &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1oHBul4_JcqlPFZgYsH3WOIZJRQRw1O4F&quot; cardinale &lt;- read.csv(url) # Check it loaded head(cardinale) ## Stream Well N Chl SR SA ## 1 Adobe Creek 1 0e+00 0.0094 105 26 ## 2 Adobe Creek 5 1e-06 0.0100 105 24 ## 3 Adobe Creek 3 1e-04 0.0613 105 20 ## 4 Adobe Creek 4 1e-02 0.1003 105 23 ## 5 Adobe Creek 2 1e+00 0.2000 105 20 ## 6 Adobe Creek 1 0e+00 0.0377 105 21 # Log-transform variables cardinale$logN &lt;- log10(cardinale$N + 1e-6) cardinale$logN2 &lt;- cardinale$logN^2 cardinale$logChl &lt;- log10(cardinale$Chl) #Fit SEM with piecewiseSEM model1 &lt;- psem( lm(SA ~ logN + logN2 + SR, data = cardinale), lm(logChl ~ SA + logN + logN2, data = cardinale), logN %~~% logN2, data = cardinale ) summary(model1) ## | | | 0% | |=========================================================================================================| 100% ## ## Structural Equation Model of model1 ## ## Call: ## SA ~ logN + logN2 + SR ## logChl ~ SA + logN + logN2 ## logN ~~ logN2 ## ## AIC ## 1192.444 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## logChl ~ SR + ... coef 122 0.6639 0.508 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.458 with P-value = 0.499 and on 1 degrees of freedom ## Fisher&#39;s C = 1.355 with P-value = 0.508 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## SA logN -2.9944 1.5375 123 -1.9476 0.0537 -0.5044 ## SA logN2 -0.4742 0.2424 123 -1.9568 0.0526 -0.5067 ## SA SR 0.3838 0.0359 123 10.6844 0.0000 0.6893 *** ## logChl SA 0.0201 0.004 123 5.0327 0.0000 0.3946 *** ## logChl logN 0.1168 0.0953 123 1.2258 0.2226 0.3858 ## logChl logN2 0.0032 0.015 123 0.2108 0.8334 0.0664 ## ~~logN ~~logN2 -0.9685 - 125 -43.4652 0.0000 -0.9685 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## SA none 0.49 ## logChl none 0.25 Reducing Collinearity via Centering # Center predictors cardinale$logN.cen &lt;- scale(cardinale$logN, scale = FALSE) cardinale$logN2.cen &lt;- cardinale$logN.cen^2 # Check correlation cor(cardinale$logN.cen, cardinale$logN2.cen) ## [,1] ## [1,] 0.5126311 Refit Model with Centered Predictors model2 &lt;- psem( lm(SA ~ logN.cen + logN2.cen + SR, data = cardinale), lm(logChl ~ SA + logN.cen + logN2.cen, data = cardinale), logN.cen %~~% logN2.cen, data = cardinale ) summary(model2) ## | | | 0% | |=========================================================================================================| 100% ## ## Structural Equation Model of model2 ## ## Call: ## SA ~ logN.cen + logN2.cen + SR ## logChl ~ SA + logN.cen + logN2.cen ## logN.cen ~~ logN2.cen ## ## AIC ## 1192.444 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## logChl ~ SR + ... coef 122 0.6639 0.508 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.458 with P-value = 0.499 and on 1 degrees of freedom ## Fisher&#39;s C = 1.355 with P-value = 0.508 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## SA logN.cen 0.3668 0.446 123 0.8223 0.4125 0.0618 ## SA logN2.cen -0.4742 0.2424 123 -1.9568 0.0526 -0.1470 ## SA SR 0.3838 0.0359 123 10.6844 0.0000 0.6893 *** ## logChl SA 0.0201 0.004 123 5.0327 0.0000 0.3946 *** ## logChl logN.cen 0.0944 0.0275 123 3.4320 0.0008 0.3116 *** ## logChl logN2.cen 0.0032 0.015 123 0.2108 0.8334 0.0193 ## ~~logN.cen ~~logN2.cen 0.5126 - 125 6.6752 0.0000 0.5126 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## SA none 0.49 ## logChl none 0.25 Try lavaan for the Same Model Example 2: Interaction Effects (Keeley et al.) Center and Create Interaction url2 &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1YTsFP1T__Hn13hTvj9TVOK-wbGDxLd01&quot; # Try to read the CSV directly keeley &lt;- read.csv(url2) keeley$age_cent &lt;- scale(keeley$age, scale = FALSE) keeley$fire_cent &lt;- scale(keeley$firesev, scale = FALSE) keeley$int_term &lt;- keeley$age_cent * keeley$fire_cent Fit SEM with Interaction in piecewiseSEM keeley_int &lt;- psem( lm(cover ~ age_cent * fire_cent, data = keeley), lm(fire_cent ~ age_cent, data = keeley), data = keeley ) summary(keeley_int) ## ## Structural Equation Model of keeley_int ## ## Call: ## cover ~ age_cent * fire_cent ## fire_cent ~ age_cent ## ## AIC ## 362.993 ## ## --- ## Tests of directed separation: ## ## No independence claims present. Tests of directed separation not possible. ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0 with P-value = 1 and on 0 degrees of freedom ## Fisher&#39;s C = NA with P-value = NA and on 0 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## cover age_cent -0.0050 0.0027 86 -1.8810 0.0634 -0.1985 ## cover fire_cent -0.0684 0.0203 86 -3.3752 0.0011 -0.3561 ** ## cover age_cent:fire_cent -0.0021 0.0014 86 -1.5263 0.1306 -0.1438 ## fire_cent age_cent 0.0597 0.0125 88 4.7781 0.0000 0.4539 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## cover none 0.24 ## fire_cent none 0.21 Optional: Fit Interaction SEM with lavaan Final Notes: • Polynomial terms allow us to model curvature. • Centering reduces collinearity and changes interpretation. • Interaction terms help model conditional effects. • Both lavaan and piecewiseSEM support these techniques. 2.8.2 Chapter 14: GLMs with SEM using PiecewiseSEM This section integrates Generalized Linear Models (GLMs) into Structural Equation Modeling, especially using piecewiseSEM. It also introduces important adjustments for working with non-normal data, and compares latent theoretic (LT) and observed error (OE) approaches for standardizing coefficients. 2.9 Overview In this tutorial, we’ll: • Learn how to incorporate GLMs into SEM using piecewiseSEM • Understand how to deal with non-normality and directed separation warnings • Compute standardized coefficients using both Latent Theoretic (LT) and Observed Error (OE) approaches Data from Anderson et al. 2010 Example # Simulated structure to mimic Anderson et al. set.seed(42) anderson &lt;- data.frame( biomass.kg = rnorm(100, mean = 10, sd = 2), leafN = rnorm(100, mean = 3, sd = 0.5), landscape = sample(0:1, 100, replace = TRUE), hotspotYN = rbinom(100, 1, 0.4) ) Model: Using GLM in psem library(piecewiseSEM) anderson.sem &lt;- psem( lm(leafN ~ biomass.kg, data = anderson), glm(hotspotYN ~ leafN + biomass.kg + landscape, family = &quot;binomial&quot;, data = anderson) ) summary(anderson.sem) ## | | | 0% | |=========================================================================================================| 100% ## ## Structural Equation Model of anderson.sem ## ## Call: ## leafN ~ biomass.kg ## hotspotYN ~ leafN + biomass.kg + landscape ## ## AIC ## 270.381 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## leafN ~ landscape + ... coef 97 1.1151 0.2676 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 1.274 with P-value = 0.259 and on 1 degrees of freedom ## Fisher&#39;s C = 2.637 with P-value = 0.268 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## leafN biomass.kg 0.0068 0.0219 98 0.3098 0.7574 0.0313 ## hotspotYN leafN -0.1212 0.4652 96 -0.2604 0.7945 -0.0301 ## hotspotYN biomass.kg 0.0374 0.1005 96 0.3720 0.7099 0.0428 ## hotspotYN landscape -0.1553 0.4181 96 -0.3715 0.7103 -0.0427 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## leafN none 0 ## hotspotYN nagelkerke 0 Directed Separation &amp; Non-Normality # Add the &#39;conserve = TRUE&#39; argument to be conservative in tests summary(anderson.sem, conserve = TRUE) ## | | | 0% | |=========================================================================================================| 100% ## ## Structural Equation Model of anderson.sem ## ## Call: ## leafN ~ biomass.kg ## hotspotYN ~ leafN + biomass.kg + landscape ## ## AIC ## 270.381 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## leafN ~ landscape + ... coef 97 1.1151 0.2676 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 1.274 with P-value = 0.259 and on 1 degrees of freedom ## Fisher&#39;s C = 2.637 with P-value = 0.268 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## leafN biomass.kg 0.0068 0.0219 98 0.3098 0.7574 0.0313 ## hotspotYN leafN -0.1212 0.4652 96 -0.2604 0.7945 -0.0301 ## hotspotYN biomass.kg 0.0374 0.1005 96 0.3720 0.7099 0.0428 ## hotspotYN landscape -0.1553 0.4181 96 -0.3715 0.7103 -0.0427 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## leafN none 0 ## hotspotYN nagelkerke 0 If the model includes non-normal endogenous variables (e.g., binary hotspotYN), the direction of independence tests matters. Use: dSep(anderson.sem, direction = c(&quot;hotspotYN &lt;- leafN&quot;)) ## | | | 0% | |=========================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 leafN ~ landscape + ... coef 97 1.115091 0.2675663 Or specify a correlated error structure: anderson.sem2 &lt;- update(anderson.sem, hotspotYN %~~% leafN) dSep(anderson.sem2) ## | | | 0% | |=========================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 leafN ~ landscape + ... coef 97 1.115091 0.2675663 Standardizing Coefficients Latent Theoretic (LT) Approach anderson.glm &lt;- anderson.sem[[2]] Betas &lt;- coefs(anderson.sem)[2:4, 3] # GLM coefficients preds &lt;- predict(anderson.glm, type = &quot;link&quot;) sd.y.LT &lt;- sqrt(var(preds) + pi^2/3) sd.x &lt;- sapply(anderson[, c(&quot;leafN&quot;, &quot;biomass.kg&quot;, &quot;landscape&quot;)], sd) Betas.LT &lt;- Betas * sd.x / sd.y.LT Betas.LT ## leafN biomass.kg landscape ## -0.03014129 0.04284880 -0.04271486 Observed Error (OE) Approach preds_response &lt;- predict(anderson.glm, type = &quot;response&quot;) R &lt;- cor(anderson$hotspotYN, preds_response) sd.y.OE &lt;- sqrt(var(preds_response)) / R Betas.OE &lt;- Betas * sd.x / sd.y.OE Betas.OE ## leafN biomass.kg landscape ## -0.1089265 0.1548496 -0.1543656 Compare LT vs OE Approaches # Indirect effect: leafN → hotspotYN (through biomass.kg) Beta.leafN &lt;- coefs(anderson.sem)$Std.Estimate[1] indirect_LT &lt;- Beta.leafN * Betas.LT[1] indirect_OE &lt;- Beta.leafN * Betas.OE[1] c(LT = indirect_LT, OE = indirect_OE) ## LT.leafN OE.leafN ## -0.0009434225 -0.0034093982 Summary: • Use glm() in psem() for binary or count outcomes. • Add conserve = TRUE for directed separation when variables are non-normal. • Use both LT and OE standardization to interpret effect sizes. 2.9.1 Chapter 15: Categorical Predictors &amp; Multigroup SEM library(lme4) library(piecewiseSEM) library(emmeans) library(lavaan) Overview In this tutorial, we explore: • Using categorical predictors in SEM. • Accounting for random effects (e.g., genotype). • Conducting multigroup SEM across different contexts or study sites. Categorical Predictors and Random Effects Example: Bowen et al. (2017) tested whether Phragmites genotype affects soil microbes and productivity. library(multcompView) # Simulated structure: Genotype nested within Phragmites status (e.g., native, invasive) set.seed(1) n &lt;- 90 bowen &lt;- data.frame( status = factor(rep(c(&quot;native&quot;, &quot;invasive&quot;, &quot;introduced&quot;), each = 30)), Genotype = rep(paste0(&quot;G&quot;, 1:9), each = 10), observed_otus = rnorm(n, mean = 2500, sd = 100), RNA.DNA = rnorm(n, 0.7, 0.05), below.C = rnorm(n, 43, 1), abovebiomass_g = rnorm(n, 2, 0.5) ) # Mixed models for each component div_mod &lt;- lmer(observed_otus ~ status + (1 | Genotype), data = bowen) ## boundary (singular) fit: see help(&#39;isSingular&#39;) activity_mod &lt;- lmer(RNA.DNA ~ status + observed_otus + (1 | Genotype), data = bowen) carbon_mod &lt;- lmer(below.C ~ observed_otus + status + (1 | Genotype), data = bowen) ## boundary (singular) fit: see help(&#39;isSingular&#39;) biomass_mod &lt;- lmer(abovebiomass_g ~ RNA.DNA + observed_otus + below.C + status + (1 | Genotype), data = bowen) ## Warning: Some predictor variables are on very different scales: consider rescaling ## boundary (singular) fit: see help(&#39;isSingular&#39;) # Build piecewise SEM bowen_mod &lt;- psem(div_mod, activity_mod, carbon_mod, biomass_mod, data = bowen) summary(bowen_mod) ## | | | 0% ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## | |=========================================================================================================| 100% ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning: Categorical or non-linear variables detected. Please refer to documentation for interpretation of ## Estimates! ## ## Structural Equation Model of bowen_mod ## ## Call: ## observed_otus ~ status ## RNA.DNA ~ status + observed_otus ## below.C ~ observed_otus + status ## abovebiomass_g ~ RNA.DNA + observed_otus + below.C + status ## ## AIC ## 1251.338 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## below.C ~ RNA.DNA + ... coef 83.905 0.018 0.8937 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 3.513 with P-value = 0.061 and on 1 degrees of freedom ## Fisher&#39;s C = 0.225 with P-value = 0.894 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## observed_otus status - - 2.0000 0.0237 0.9766 - ## observed_otus status = native 2508.2458 16.3598 6.0000 153.3172 0.0000 - *** ## observed_otus status = introduced 2511.0278 16.3598 6.0000 153.4872 0.0000 - *** ## observed_otus status = invasive 2513.2775 16.3598 6.0000 153.6247 0.0000 - *** ## RNA.DNA observed_otus -0.0062 0.0147 84.1918 0.1100 0.7409 - ## RNA.DNA status - - 2.0000 2.0441 0.2111 - ## RNA.DNA status = invasive 0.6835 0.0104 5.9386 65.7032 0.0000 - *** ## RNA.DNA status = native 0.7056 0.0104 5.9389 67.8238 0.0000 - *** ## RNA.DNA status = introduced 0.7119 0.0104 5.9365 68.4304 0.0000 - *** ## below.C observed_otus -4e-04 0.0012 85.2844 0.1004 0.7521 - ## below.C status - - 2.0000 0.7820 0.4997 - ## below.C status = invasive 42.763 0.1857 5.9128 230.2879 0.0000 - *** ## below.C status = introduced 43.0245 0.1857 5.9100 231.7263 0.0000 - *** ## below.C status = native 43.0658 0.1857 5.9132 231.9142 0.0000 - *** ## abovebiomass_g RNA.DNA 0.1948 1.1068 82.9464 0.0290 0.8651 - ## abovebiomass_g observed_otus 2e-04 6e-04 83.1788 0.1687 0.6824 - ## abovebiomass_g below.C -0.1292 0.0523 83.1665 5.9076 0.0172 - * ## abovebiomass_g status - - 2.0000 0.2393 0.7943 - ## abovebiomass_g status = introduced 2.0074 0.0911 5.8667 22.0333 0.0000 - *** ## abovebiomass_g status = native 2.0829 0.0905 5.7339 23.0193 0.0000 - *** ## abovebiomass_g status = invasive 2.0871 0.0927 6.2237 22.5236 0.0000 - *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method Marginal Conditional ## observed_otus none 0.00 0.00 ## RNA.DNA none 0.06 0.10 ## below.C none 0.02 0.02 ## abovebiomass_g none 0.07 0.07 Visualizing Categorical Effects Estimated Marginal Means by status: lapply(bowen_mod[-length(bowen_mod)], emmeans, specs = ~status) ## [[1]] ## status emmean SE df lower.CL upper.CL ## introduced 2511 16.4 6 2471 2551 ## invasive 2513 16.4 6 2473 2553 ## native 2508 16.4 6 2468 2548 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## [[2]] ## status emmean SE df lower.CL upper.CL ## introduced 0.712 0.0104 5.94 0.686 0.737 ## invasive 0.684 0.0104 5.94 0.658 0.709 ## native 0.706 0.0104 5.94 0.680 0.731 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## [[3]] ## status emmean SE df lower.CL upper.CL ## introduced 43.0 0.186 5.91 42.6 43.5 ## invasive 42.8 0.186 5.91 42.3 43.2 ## native 43.1 0.186 5.91 42.6 43.5 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## [[4]] ## status emmean SE df lower.CL upper.CL ## introduced 2.01 0.0911 5.87 1.78 2.23 ## invasive 2.09 0.0927 6.22 1.86 2.31 ## native 2.08 0.0905 5.73 1.86 2.31 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 # Post-hoc Tests (Tukey) generic_tukey &lt;- function(x) emmeans(x, list(pairwise ~ status)) lapply(bowen_mod[-length(bowen_mod)], generic_tukey) ## [[1]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 2511 16.4 6 2471 2551 ## invasive 2513 16.4 6 2473 2553 ## native 2508 16.4 6 2468 2548 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive -2.25 23.1 6 -0.097 0.9948 ## introduced - native 2.78 23.1 6 0.120 0.9921 ## invasive - native 5.03 23.1 6 0.217 0.9744 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates ## ## ## [[2]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 0.712 0.0104 5.94 0.686 0.737 ## invasive 0.684 0.0104 5.94 0.658 0.709 ## native 0.706 0.0104 5.94 0.680 0.731 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive 0.02831 0.0147 5.94 1.924 0.2130 ## introduced - native 0.00624 0.0147 5.94 0.424 0.9072 ## invasive - native -0.02207 0.0147 5.94 -1.500 0.3560 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates ## ## ## [[3]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 43.0 0.186 5.91 42.6 43.5 ## invasive 42.8 0.186 5.91 42.3 43.2 ## native 43.1 0.186 5.91 42.6 43.5 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive 0.2615 0.263 5.91 0.996 0.6061 ## introduced - native -0.0413 0.263 5.91 -0.157 0.9865 ## invasive - native -0.3029 0.263 5.92 -1.153 0.5204 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates ## ## ## [[4]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 2.01 0.0911 5.87 1.78 2.23 ## invasive 2.09 0.0927 6.22 1.86 2.31 ## native 2.08 0.0905 5.73 1.86 2.31 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive -0.07972 0.132 6.41 -0.603 0.8234 ## introduced - native -0.07557 0.128 5.67 -0.592 0.8295 ## invasive - native 0.00415 0.131 6.21 0.032 0.9994 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates Multigroup SEM in lavaan Fit the same SEM model to different groups and test whether path coefficients differ. # Create simulated dataset group_df &lt;- data.frame( site = rep(c(&quot;A&quot;, &quot;B&quot;), each = 50), x = rnorm(100), m = rnorm(100), y = rnorm(100) ) # Define a simple SEM model sem_model &lt;- &#39; m ~ a*x y ~ b*m + c*x &#39; # Fit multi-group SEM fit_multi &lt;- lavaan::sem(sem_model, data = group_df, group = &quot;site&quot;) ## Warning: lavaan-&gt;lavParTable(): ## using a single label per parameter in a multiple group setting implies imposing equality constraints across ## all the groups; If this is not intended, either remove the label(s), or use a vector of labels (one for each ## group); See the Multiple groups section in the man page of model.syntax. # View summary summary(fit_multi, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-19 ended normally after 12 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## Number of equality constraints 3 ## ## Number of observations per group: ## A 50 ## B 50 ## ## Model Test User Model: ## ## Test statistic 0.790 ## Degrees of freedom 3 ## P-value (Chi-square) 0.852 ## Test statistic for each group: ## A 0.498 ## B 0.292 ## ## Model Test Baseline Model: ## ## Test statistic 2.093 ## Degrees of freedom 6 ## P-value 0.911 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) -0.131 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -293.784 ## Loglikelihood unrestricted model (H1) -293.389 ## ## Akaike (AIC) 609.568 ## Bayesian (BIC) 638.225 ## Sample-size adjusted Bayesian (SABIC) 603.484 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.130 ## P-value H_0: RMSEA &lt;= 0.050 0.874 ## P-value H_0: RMSEA &gt;= 0.080 0.098 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.029 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [A]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) -0.011 0.099 -0.116 0.908 -0.011 -0.009 ## y ~ ## m (b) -0.105 0.095 -1.106 0.269 -0.105 -0.128 ## x (c) 0.031 0.101 0.310 0.757 0.031 0.031 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.144 0.170 0.844 0.398 0.144 0.119 ## .y 0.076 0.139 0.544 0.586 0.076 0.077 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 1.445 0.289 5.000 0.000 1.445 1.000 ## .y 0.960 0.192 5.000 0.000 0.960 0.983 ## ## ## Group 2 [B]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) -0.011 0.099 -0.116 0.908 -0.011 -0.014 ## y ~ ## m (b) -0.105 0.095 -1.106 0.269 -0.105 -0.088 ## x (c) 0.031 0.101 0.310 0.757 0.031 0.031 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m -0.116 0.134 -0.860 0.390 -0.116 -0.124 ## .y -0.240 0.160 -1.506 0.132 -0.240 -0.215 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.872 0.174 5.000 0.000 0.872 1.000 ## .y 1.234 0.247 5.000 0.000 1.234 0.991 Constraining Parameters Across Groups You can test whether coefficients differ across groups: A significant p-value means the unconstrained model fits better — i.e., group differences do matter. Summary: • Use lme4 and piecewiseSEM for models with random effects. • lavaan enables multigroup comparisons to test generality across systems. • Post-hoc tools like emmeans help interpret categorical predictors. 2.9.2 Chapter 16: Multigroup SEM in R library(lavaan) What is Multigroup SEM? Multigroup SEM allows you to: • Test whether path coefficients differ between groups. • Assess whether a model holds equally well across groups. • Investigate measurement invariance (i.e., whether constructs are perceived similarly). Example Model Setup We’ll build a simple mediation model where group is a binary factor (“A” vs “B”). # Simulate data set.seed(123) n &lt;- 100 group &lt;- rep(c(&quot;A&quot;, &quot;B&quot;), each = n) x &lt;- rnorm(2 * n) m &lt;- 0.5 * x + rnorm(2 * n) y &lt;- 0.6 * m + 0.3 * x + rnorm(2 * n) data &lt;- data.frame(group, x, m, y) Define the SEM model &lt;- &#39; m ~ a*x y ~ b*m + c*x &#39; Fit Multigroup SEM fit_multi &lt;- sem(model, data = data, group = &quot;group&quot;) ## Warning: lavaan-&gt;lavParTable(): ## using a single label per parameter in a multiple group setting implies imposing equality constraints across ## all the groups; If this is not intended, either remove the label(s), or use a vector of labels (one for each ## group); See the Multiple groups section in the man page of model.syntax. summary(fit_multi, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-19 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## Number of equality constraints 3 ## ## Number of observations per group: ## A 100 ## B 100 ## ## Model Test User Model: ## ## Test statistic 6.839 ## Degrees of freedom 3 ## P-value (Chi-square) 0.077 ## Test statistic for each group: ## A 3.618 ## B 3.220 ## ## Model Test Baseline Model: ## ## Test statistic 131.688 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.969 ## Tucker-Lewis Index (TLI) 0.939 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -556.173 ## Loglikelihood unrestricted model (H1) -552.754 ## ## Akaike (AIC) 1134.347 ## Bayesian (BIC) 1170.628 ## Sample-size adjusted Bayesian (SABIC) 1135.779 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.113 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.228 ## P-value H_0: RMSEA &lt;= 0.050 0.139 ## P-value H_0: RMSEA &gt;= 0.080 0.755 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.083 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [A]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) 0.453 0.075 6.064 0.000 0.453 0.401 ## y ~ ## m (b) 0.542 0.068 7.947 0.000 0.542 0.460 ## x (c) 0.295 0.078 3.756 0.000 0.295 0.222 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.125 0.094 1.323 0.186 0.125 0.122 ## .y 0.116 0.098 1.178 0.239 0.116 0.096 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.885 0.125 7.071 0.000 0.885 0.840 ## .y 0.958 0.135 7.071 0.000 0.958 0.657 ## ## ## Group 2 [B]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) 0.453 0.075 6.064 0.000 0.453 0.387 ## y ~ ## m (b) 0.542 0.068 7.947 0.000 0.542 0.504 ## x (c) 0.295 0.078 3.756 0.000 0.295 0.235 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m -0.041 0.104 -0.397 0.691 -0.041 -0.037 ## .y -0.048 0.094 -0.513 0.608 -0.048 -0.040 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 1.074 0.152 7.071 0.000 1.074 0.850 ## .y 0.874 0.124 7.071 0.000 0.874 0.599 Test for Invariance To test for invariance, you can constrain parameters to be equal across groups. # Constrain &#39;a&#39; and &#39;b&#39; to be equal across groups model_constrained &lt;- &#39; m ~ c(a, a)*x y ~ c(b, b)*m + c*x &#39; fit_constrained &lt;- sem(model_constrained, data = data, group = &quot;group&quot;) ## Warning: lavaan-&gt;lavParTable(): ## using a single label per parameter in a multiple group setting implies imposing equality constraints across ## all the groups; If this is not intended, either remove the label(s), or use a vector of labels (one for each ## group); See the Multiple groups section in the man page of model.syntax. anova(fit_multi, fit_constrained) # Chi-square test for invariance ## Warning: lavaan-&gt;lavTestLRT(): ## some models have the same degrees of freedom ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## fit_multi 3 1134.3 1170.6 6.8386 ## fit_constrained 3 1134.3 1170.6 6.8386 0 0 0 Interpretation: • If the constrained model does not significantly worsen fit, the paths a and b are likely invariant. • If the fit gets significantly worse, the relationship differs between groups and should be modeled separately. Visualizing Standardized Results library(semPlot) semPaths(fit_multi, &quot;std&quot;, layout = &quot;tree&quot;, whatLabels = &quot;std&quot;, edge.label.cex = 1.2) Summary Multigroup SEM lets you: • Evaluate moderation by group. • Test for measurement invariance. • Gain deeper insight into context-dependent pathways. 2.9.3 Chapter 17: Yes! Based on the content in SEM9.2 – Mixed Models Part 2, here’s an R Markdown tutorial on using mixed models in piecewise SEM, covering: • Fixed vs. random effects • Adding group-level predictors • Understanding R² and model comparison • Dealing with hierarchical structure and sample size Introduction This tutorial introduces how to incorporate mixed effects into Structural Equation Modeling using the piecewiseSEM package. Mixed models are essential when your data are hierarchically structured (e.g., plots within sites, streams within watersheds). Step 1: Load Data and Create Variables # Log-transform predictors cardinale$logN &lt;- log10(cardinale$N + 1e-6) cardinale$logN2 &lt;- cardinale$logN^2 cardinale$logChl &lt;- log10(cardinale$Chl) # Centering predictors to reduce multicollinearity cardinale$logN.cen &lt;- scale(cardinale$logN, scale = FALSE) cardinale$logN2.cen &lt;- scale(cardinale$logN^2, scale = FALSE) Step 2: Fit Fixed Effects SEM cardinale.sem &lt;- psem( lm(SA ~ logN.cen + logN2.cen + SR, data = cardinale), lm(logChl ~ SA + logN.cen + logN2.cen, data = cardinale), logN.cen %~~% logN2.cen, data = cardinale ) summary(cardinale.sem) ## | | | 0% | |=========================================================================================================| 100% ## ## Structural Equation Model of cardinale.sem ## ## Call: ## SA ~ logN.cen + logN2.cen + SR ## logChl ~ SA + logN.cen + logN2.cen ## logN.cen ~~ logN2.cen ## ## AIC ## 1192.444 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## logChl ~ SR + ... coef 122 0.6639 0.508 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.458 with P-value = 0.499 and on 1 degrees of freedom ## Fisher&#39;s C = 1.355 with P-value = 0.508 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## SA logN.cen -2.9944 1.5375 123 -1.9476 0.0537 -0.5044 ## SA logN2.cen -0.4742 0.2424 123 -1.9568 0.0526 -0.5067 ## SA SR 0.3838 0.0359 123 10.6844 0.0000 0.6893 *** ## logChl SA 0.0201 0.004 123 5.0327 0.0000 0.3946 *** ## logChl logN.cen 0.1168 0.0953 123 1.2258 0.2226 0.3858 ## logChl logN2.cen 0.0032 0.015 123 0.2108 0.8334 0.0664 ## ~~logN.cen ~~logN2.cen -0.9685 - 125 -43.4652 0.0000 -0.9685 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## SA none 0.49 ## logChl none 0.25 Step 3: Add Random Effects with lme() #cardinale.mixed &lt;- psem( # lme(SA ~ logN.cen + logN2.cen + SR, random = ~1 | Stream, data = cardinale), # lme(logChl ~ SA + logN.cen + logN2.cen, random = ~1 | Stream, data = cardinale), # logN.cen %~~% logN2.cen, # data = cardinale #) #summary(cardinale.mixed) Step 4: Compare Models # Compare R-squared rsquared(cardinale.sem) ## Response family link method R.squared ## 1 SA gaussian identity none 0.4882395 ## 2 logChl gaussian identity none 0.2538638 #rsquared(cardinale.mixed) • Marginal R²: Variance explained by fixed effects. • Conditional R²: Variance explained by both fixed and random effects. Step 5: Optional — Add Group-Level Predictors To address possible Simpson’s paradox or correlated random effects: # Example if &quot;site_mean&quot; were available cardinale$stream_mean_logN &lt;- ave(cardinale$logN, cardinale$Stream) cardinale$deviation_logN &lt;- cardinale$logN - cardinale$stream_mean_logN This lets you include: • stream_mean_logN (group-level predictor) • deviation_logN (individual-level deviation) Step 6: Basis Set and Fisher’s C #fisherC(cardinale.mixed) Summary: • Mixed models let you account for non-independence among groups. • Use piecewiseSEM with lme() to include random effects. • Be cautious of group-level confounding — use centering and group-level predictors. • Use rsquared() and fisherC() for model comparison and goodness-of-fit. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
