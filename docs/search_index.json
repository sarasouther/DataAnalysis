[["index.html", "Statistics for Ecology Chapter 1 Introduction 1.1 Student Learning Objectives 1.2 Downloads 1.3 Description 1.4 Grading 1.5 Overview", " Statistics for Ecology Dr. Sara Souther 2025-06-10 Chapter 1 Introduction 1.1 Student Learning Objectives 1.2 Downloads 1.3 Description 1.4 Grading 1.5 Overview "],["intro-1.html", "Chapter 2 Intro", " Chapter 2 Intro Add an intro or just use the index file? "],["back-to-the-basics.html", "Chapter 3 Back to the basics 3.1 What can statistics tell us? 3.2 Sampling populations 3.3 Data types 3.4 Describing data", " Chapter 3 Back to the basics 3.1 What can statistics tell us? Welcome to our statistical exploration of the natural world! I want you to have an intuitive understanding of what we do when we conduct statistical analysis, understand how to select the appropriate statistical analysis and the assumptions of that analysis, and make a connection between running an analysis in a statistical software package and the statisticese of those unique individuals that we call statisticians. I think this will give you the confidence to tackle any analytical situation! First off, we are not statisticians - whew! We are ecologists and social scientists. This means that we do not need to understand theoretical mathematical frameworks. We need to APPLY statistics appropriately. There will be limits to our mathematical understanding of statistics, and this is OKAY! Think of all of the knowledge about the natural world and conducting field work that we possess that statisticians don’t! I’m happy to dive down math rabbit holes with you, BUT we don’t need to and CAN’T know everything. Almost all statistical analysis boils down to answering 1 of 2 questions: Do these groups differ? Is there a relationship between these variables? These seem like relatively simple questions to answer, perhaps just by looking at our data, so Why do we need statistics? The short answer is: error and sampling! Whenever we collect data, we introduce error; our instruments are imprecise and do not capture an exact measure of whatever you are measuring (e.g., height, weight), and humans make mistakes during measurement collection. Secondly, we are always measuring a sub-sample of the true population (true population meaning all representatives of whatever you are trying to measure; this can be grass, marbles, or the tibia of humans). Not only is in intractable in most cases to measure all individuals of whatever you are interested in, even when it is possible to attempt to measure all individuals (like in the case of rare plant work), statistics acknowledges that it is still unlikely that we are able to completely measure all individuals in your focal population, since individuals may be dormant or challenging to locate. If we could measure all individuals of our population of interest with perfect accurately, we could calculate population parameters, or quantities describing populations like averages and variation, rather than estimating these metrics, and in many cases just compare them. In this way, statistics is inherently practical, and asks, what can we say about whatever we are looking at, given our numerous flaws! 3.2 Sampling populations After a few classes, we will explore sampling methodology in greater depth in order to design appropriate experiments that test a statistical hypothesis. Let’s quickly talk about sampling now so that we have a shared understanding and vocabulary to build on - after all, statistics really centers around estimating characteristics of a true population from a sample. The really, truly amazing thing is that by properly applying statistics, we can learn practically anything about almost any population using samples! In statistics, a population refers to the all units of the thing that you are interested (i.e., all suriname frogs, all grains of sand, all aspen leaves from a genotype found in southern Arizona). Note: Population in statistics differs from the term population in population ecology, where a population refers to a group of individuals in a particular area that interbreed. A sample is a subset of the population that we measure to infer something about the population. Statistical analyses depend on a random sample or must account for non-randomness within the sample. Just imagine, for instance, that you were interested in whether coat color in cats differed between house cats and feral cats. To select the house cat sample, you randomly select house numbers, visit the house and record coat color, thus collecting a random sample. However, to survey feral cats, you go to several cat colonies at night and record the first cat that you see, which are always white or tan. The sample of feral cats introduces bias, and causes you to overestimate the number of light colored feral cats, and underestimate dark feral cats. We can conduct statistical analysis until the cats come home (ha!), but if your sample is biased, our results will always be meaningless. In the cat example, it was pretty obvious that the researcher was introducing bias, BUT it is REALLY easy to introduce bias in ecological and social research on accident! Imagine that you looking at fire effects on vegetative communities in the Sonoran. In high severity burn areas, there are thickets of cat’s claw (a pokey plant). Without proper field sampling protocols, it is very tempting to avoid establishing plots in the cat claw thickets, thus not capturing true differences in vegetation along burn severity gradients. We can go over field sampling methodology later, but let’s talk about several types of appropriate sampling strategies. The key is that we want our sample to be representative of the true population, so that estimates of values (i.e., means, variance) from the sample represent true population parameters. Simple Random Sampling is when every sample has an equal likelihood of being selected. We can quickly and easily generate such a sample in R, using the sample function. sample(1:100, 10, replace=FALSE) ## [1] 8 48 44 96 49 9 21 60 73 18 #1:10000 = numbers to chose among #number of random numbers you wish to generate #to replace or not (in other words do you wish for the same number to be selected multiple times) Random Cluster Sampling randomly select groups (aka clusters) within a population. This sampling design is used commonly in ecology, when we select random locations for plots, then measure all individuals within those plots. If for instance, we are interested in Ponderosa Pine growth rates on the Coconino National Forest, we would randomly assign points across Pondo habitat on the Coconino. At each point, we would set up a plot in which we measure Ponderosa Pines within an 11.71m radius plot. Why wouldn’t we just go out to a point and measure 1 tree to create a totally random sample? The plots are randomly assigned (yay!), but the trees within the plots are not independent. In other words, we might expect measures of trees within plot A to be more similar to each other than they are to trees within plot B, due to differences in microsite characteristics, genetic similarity among co-occurring trees, or site history (logging, fire). Luckily, we can account for this non-independence, as long as the plots are random! Stratified Sampling draws samples using proportionality based on homogeneous groupings known as strata. This type of sampling is frequently used in ecology to account for landscape differences in key factors. For instance, say you asked to classify vegetation types for a Nature preserve in southern Arizona. The reserve has a large riparian area (25% of the property) with completely different vegetation from the upland area (75%). Random sampling might, by chance, under or over represent one of these two areas. To create a stratified sampling design, you would ensure proportional representation of both areas by randomly placing 25% of the sample points within the riparian area, and 75% of the sample points within the upland area. Sample size More is better. However, practically we are often limited by time and money! Statistical analysis is only one part of presenting your research results. Generally, a results section in a manuscript includes: statistical results, data description (e.g., describing means, ranges, maxima, minima of groups of interest), and data visualization (i.e., creating beautiful figures). For each analysis that we cover, we will talk about how to present statistical results, describe data, and create appropriate supporting figures. 3.3 Data types Before we start learning to present research results (analysis, description, visualization), let’s talk about data! Data comes in several varieties, and the variety dictates which statistical analysis we choose! Categorical variables are non-numeric variables. Examples: Pet type (dog, cat, fish, bird), Size (small, medium, large), Car type (sedan, SUV), Present/Absent Numerical variables are variables that are numbers, and occur in two forms: *Discrete = Counts of things (no decimal points/fractions) Data are discrete when it does not make sense to have a partial number of the variable. For instance, if counting the number of insects in a pond, it does not make sense to count a half a species. Examples: Number of people in a building, number of trees in a plot, number of bugs in a pond *Continuous = Numerical data that can occur at any value. These are variables that can occur in any quantity. If you can have a fraction of this variable, it is continuous. Examples = Height, Weight, Length Ordinal variables (sometimes referred to as ranked) can be categorical or numerical, but the order matters. Examples = Grades (A, B, C, D, E), Likert scale variables (Strongly disagree, Agree, Strongly Agree), Class rank (1, 2, 3, 4, 5) 3.4 Describing data First, let’s take a spin with data description. We are starting here to introduce a few concepts that will be important to understand, as we launch into statistical analysis. We will start by describing continuous data. Let’s use a simplified version of a dataset that I’m working with right now to look at the performance of several species of pollinator-friendly native species in agricultural gardens. Eventually, we’d like to develop seed to provide to restorationists for restoration of arid and semiarid grasslands. To do this, we need to understand how reliable these species are at establishing, producing seed, and attracting pollinators. Initially, we are conducting experiments with multiple populations of each species to determine how consistently plants grow, reproduce, and perform. Here, We will take a look at the initial heights of 1 population of one species, Asclepias subverticulata. When doing an actual research write-up, I ask myself ‘What is the most important information for my audience to know about this dataset?’ to guide what descriptions of the data to include. Here, we are just going to play around with numbers and R code! #create vector of heights (cm) of one population of A. subverticulata sedonapopulation &lt;- c(3, 3, 3, 3, 7, 8, 9) #take the mean mean(sedonapopulation) ## [1] 5.142857 #calculate variance var(sedonapopulation) ## [1] 7.47619 #calculate standard deviation sd(sedonapopulation) ## [1] 2.734262 #calculate standard error #base r doesn&#39;t have this function #so we have to write our own std_error &lt;- function(x) sd(x)/sqrt(length(x)) std_error(sedonapopulation) ## [1] 1.033454 Most of the time when writing up results, you present a mean (sum of numbers divided by the number of observations), and an estimate of variation (a measure of how different the observations are). Here, we calculated three estimates variation, variance, standard deviation, and standard error. Since you will occasionally need to include equations in your write-ups, let’s get use to mathematical syntax, with these simple examples. The formula for the sample mean is: \\(\\mu = \\frac{\\Sigma x_i}{n}\\); where \\(\\mu\\) indicates the sample mean (sample = group of numbers we are looking at); \\(\\Sigma\\) means to add what ever follows; \\(x_{i}\\) is the value of one observation; (subscript i is often used to indicate that the action should be repeated for all values); \\(n\\) is the number of observations Why didn’t we just use \\(\\bar{x}\\) to indicate the mean? Because statisticians typically use \\(\\bar{x}\\) to indicate the true mean of the population, and \\(\\mu\\) to indicate the sample mean! Just to show you, what the mean() function is doing, let’s run: sum = 3+3+3+3+7+8+9 #add all the numbers in the sample n = length(sedonapopulation) #or you can just calculate the number of height measurements mean = sum/n; mean #divide sum by number ## [1] 5.142857 This formula is simple, but sometimes with more complex formulas, I will solve the equations by hand, to make sure that I understand what is happening! The formula for variance is: \\(S^{2} = \\frac{\\Sigma(x_i - \\mu)^{2}}{n - 1}\\) where \\(S^{2}\\) is the sample variance; \\(\\mu\\) is the sample mean (remember from above); \\(x_{i}\\) is the value of one observation; \\(n\\) is the number of observations In other words: #We determine how much each observation varies from the mean. diffobs1 = mean - 3 diffobs2 = mean - 3 diffobs3 = mean - 3 diffobs4 = mean - 3 diffobs5 = mean - 7 diffobs6 = mean - 8 diffobs7 = mean - 9 #Then we square each of these. diffobj1_sq = diffobs1^2 diffobj2_sq = diffobs2^2 diffobj3_sq = diffobs3^2 diffobj4_sq = diffobs4^2 diffobj5_sq = diffobs5^2 diffobj6_sq = diffobs6^2 diffobj7_sq = diffobs7^2 Why do we square the differences rather than just adding them up? Because differences will be positive and negative. If we added them without squaring, sample differences would negate each other. We want an estimate of the absolute differences of samples from the mean. #Then we add the differences up. sumofsquares = sum(diffobj1_sq, diffobj2_sq, diffobj3_sq, diffobj4_sq, diffobj5_sq, diffobj6_sq, diffobj7_sq) #Divide the sum of squares by n - 1. variance = sumofsquares/(n-1); variance ## [1] 7.47619 Why n - 1 instead of n? One reason is that, theoretically, because we are taking the mean of a sample, rather than all individuals, we underestimate the variance, so taking n-1 corrects that bias. Consider it a penalty for measuring a sample, not the entire population! Another practical reason is that dividing by n-1 makes the variance of a single sample undefined (unsolvable) rather than zero (solvable) For standard deviation, we just take the square root of the variance, to remove the effect of squaring the differences when calculating the variance, and thus contextualizing our estimate of variation with regard to the mean. For example, the variance for the Sedona population is 7.48, larger than the sample mean of 5.12; while the standard deviation is 2.73, indicating that you would expect most observations to be 5.12 +/- 2.73 (we’ll get to quantiles in a minute). The formula for standard deviation is: \\(\\sigma = \\sqrt\\frac{\\Sigma(x_i - \\mu)^{2}}{n - 1}\\) where \\(\\sigma\\) is the sample variance; \\(\\mu\\) is the sample mean; \\(x_{i}\\) is the value of one observation; \\(n\\) is the number of observations. Finally, standard error and confidence intervals (we’ll get to confidence intervals later) are the most common metrics of variance presented in journals. The formula for standard error is: \\(SE = \\frac{\\sigma}{\\sqrt n}\\) where \\(SE\\) is standard error of the sample; \\(\\sigma\\) is the standard deviation; and \\(n\\) is the number of samples. Why do we divide the standard deviation by the square root of the sample size to get standard error? While standard deviation measures the variation of the sample, standard error is meant to estimate the variation of the entire population of samples, if we could measure all individuals accurately. By dividing by the \\(\\sqrt n\\), the larger the sample size, the lower the error, because you have a more complete estimate of the true mean. In other words, standard deviation is just a measure of the variation of our sample, while standard error also incorporates information about our sampling process (how many individuals we have sampled). Want to delve deep into standard error and deviation (me neither - ha)?: Google central limit theorem + standard error / standard deviation. Means and variance measures are the most common way to describe quantitative data. However, several other metrics are useful for understanding the nature of your data and making decisions about analyses. A comprehensive understanding of your dataset includes describing these four features: Location (Mean, Median) Spread (Variability) Shape (Normal, skewed) Outliers We’ve talked about means. The median is just the central number in the dataset, and helps you identify skewness. #an example of an unskewed population sedona_unskewed &lt;- c(1, 2, 3, 4, 5, 6, 7) mean(sedona_unskewed) ## [1] 4 median(sedona_unskewed) ## [1] 4 #previous sedona population; skewed sedonapopulation &lt;- c(3, 3, 3, 3, 7, 8, 9) mean(sedonapopulation) ## [1] 5.142857 median(sedonapopulation) ## [1] 3 In an unskewed population, the mean will equal the median. Skew may not seem important, but it has statistical ramifications, AND it tells us something meaningful about the data. For instance, what if I said that mean price of a home in Flagstaff is 350K, but the median price of a home is 300K? We would know the that average house prices are driven up by a smaller number of expensive homes. We can quantify skew by comparing means and medians (mean &gt; median = right-skewed; median &gt; mean = left-skewed), but it is helpful to visualize the shape of data with a histogram. A histogram is a graph of the frequency of different measurements. Let’s add a few more observations to our Sedona populations (skewed and unskewed) and check out the look of the data! sedona_unskewed &lt;- c(7, 2, 2, 3, 3, 3, 3, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 0.5) mean(sedona_unskewed) ## [1] 3.975 median(sedona_unskewed) ## [1] 4 #I&#39;m renaming sedonapopulation, sedona_skewed for this example sedona_skewed &lt;- c(3, 3, 3, 3, 7, 3, 4, 5, 6, 3, 3, 3, 4, 4, 6, 7, 8, 9, 3, 4, 5, 2) mean(sedona_skewed) ## [1] 4.454545 median(sedona_skewed) ## [1] 4 In this relatively unskewed example, the tails are approximately even. This shape is also referred to as a normal or Gaussian distribution. Here, we superimposed the bellshaped Normal or Gaussian distribution. In this example of skewed data, the tail tapers to the right, indicated that the data is skewed to the right. In order to explain outliers, we need to look at quantiles! Quantiles are proportions of your data, in other words a way to break your data into chunks to understand spread. You can break your data into as many quantiles as you would like, but it is most common to break your data into 4 parts, also called quartiles. (If you break data into 5 parts, the components are called quintiles, 10 parts = deciles, 100 parts = percentiles). When you break data into quartiles, roughly 25 percent of the data occurs within each data chunk. The first chunk of the dataset contains 25% of the data (25th percentile; 25% of the data fall at or below this cut-off) is called the first quartile, the 50th percentile is called the sample median or the second quartile, the 75th percentile is called the third quartile. Box and whisker plots are commonly used to quickly examine quartiles. Let’s check out our plant height data again, using a box and whisker plot. In the plot shown here, the box encapsulates the Interquartile Range (IQR); the center of the data ranging from the 25th percentile to the 75th. The black line in the middle of the box is the median (also called the 50th percentile, because it bisects the dataset; half of the data occur above the median and half below). The lines emerging from the box (whiskers) indicate the extent of the first and third quartiles, and usually corresponding with the minimum and maximum values of the dataset, unless there are outliers. An outlier is a datapoint that occurs outside of the 1st or 3rd quantile. Let’s add one to our Sedona dataset, and see how it is represented on the box and whisker plot. #Let&#39;s add a plant height of 20. sedona_skewed &lt;- c(3, 3, 3, 3, 7, 3, 4, 5, 6, 3, 3, 3, 4, 4, 6, 7, 8, 9, 3, 4, 5, 2, 20) boxplot(sedona_skewed, main=&quot;Skewed&quot;, ylab=&quot;Plant height (cm)&quot;) The outlier appears as a dot on the box and whisker plot, and is the maximum value of the dataset. One other thing to note: Standard deviation also breaks data into meaningful segments, but is only used when data conform to a normal distribution; the mean +/- 1 SD accounts for 68% of the data, +/-2 SDs contains 95% of data, and +/- 3SD includes 99% of data. That said, I’ve never presented standard deviation in a manuscript; it is much more common to include standard error or confidence intervals (discussed later). We’ve played around a lot with data, but what do you actually need to take away from this? Data types (Categorical, Numerical discrete, Numerical continuous, Ordinal) Why? We will select analyses based on data type. The two basic questions that most statistical analyses answer. Why? This will help you define what statistics can and can’t do and bound our learning space! Ways to describe numerical continuous data (Location, Spread, Shape, Outliers). Why? You will describe your results using these concepts in write-up AND these concepts will be important for certain analyses. Know how to calculate mean, median, and standard error. Why? These are typical ways to describe data in results sections. Start to familiarize yourself with mathematical annotation. Why? You may need to include equations in your methods section. Start to familiarize yourself with R code. Why? Most researchers now use R to analyze, describe, and visualize their data. *Be able to interpret a histogram and box-whisker plot. Why? These are commonly used ways to visualize data. "],["randomness-and-probability.html", "Chapter 4 Randomness and probability 4.1 Prework 4.2 Randomness 4.3 The sample point method 4.4 Combinatorials 4.5 Union and intersection of events 4.6 Conditional probability 4.7 Multiplicative Law of Probability of Independent Events 4.8 Additive Law of Probability of Mutually Exclusive Events 4.9 General Additive Law of Probability 4.10 Key concepts 4.11 Practice problems", " Chapter 4 Randomness and probability 4.1 Prework Listen to the podcast on stochasticity Reads on randomness and determinism: https://towardsdatascience.com/when-science-and-philosophy-meet-randomness-determinism-and-chaos-abdb825c3114 *Load packages and datasets ## Rows: 8 Columns: 6 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (4): Toss1, Toss2, Toss3, Abbreviated ## dbl (2): Outcome, Probability ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 4.2 Randomness Randomness is statistically a simple concept with incredible repercussions for understanding natural systems, human decisions, and, well, life! One of the most interesting panels that I attended as a graduate student was a debate between scientific and religious philosophers over the meaning of randomness. There were a range of viewpoints; the theologian believed that randomness was where the divine could work; at the other extreme, the science philosopher believed that there is no such thing as random, only that we haven’t developed the capacity to measure what we perceive as random, and that everything we experience is a deterministic outcome of physics manifest in the world around us. As I write this lesson, I’m sitting on the couch with my younger child watching Jurassic Park. Ian Malcolm has just introduced himself as a ‘chaotician’ (he he). Chaos theory is one way to explain what appears to us as ‘randomness’, because small initial differences in conditions of complex systems leads to extremely different end states, even when the same deterministic rules apply to that system. The ‘butterfly effect’ (a butterfly flaps its wings in Asia causes a tornado in Texas) is often used to describe this core concept of chaos (the example originally created by Edward Norton Lorenz used a seagull instead of a butterfly). Chaos theory is incorporated into modeling simulations (particularly in population biology). In statistics, we have specific definition of randomness: The case when each item in a set has an equal probably of being selected. Statistics also acknowledges unexplained variation in nature, but doesn’t distinguish between true randomness or the inability to perfectly measure all variables explaining an observed phenomenon. Instead, all unpredictability is lumped together. The difference between the true value of an estimate (e.g., mean hieght of giraffes, slope describing the relationship between snail shell length and speed) and the observed estimate is called ‘error’. We use statistical models to parse ‘error’ (unexplained variation in the response variable) from treatment effects (variation in the response variable explained by the explanatory variables). [Add info on random effects here or introduce later?] Statistical error can arise for several reasons: random variation inherent to natural systems, mistakes during measurement or data collection, inaccuracy of measuring devices, omitted variables. As long as this error isn’t ‘systematic’, we accept that it is there, complete statistical analysis and move-on. However, systematic error, whenever you are directionally biasing your sample, or biasing your sampling in the same way, is something to be avoided when designing your experiment and collecting data. An example of a systematic measure would be if you are trying to estimate the mean weight of blue morpho butterflies in southern Brazil, and you never zero your balance. The true mean weight is lower than the mean weight that you calculate. Since randomness is inherent, we need to calculate the probability that we Our ability to say that an observed effect is statistically significant is based on probability: the proportion of times an event occurs in repeated trials or more generally , the quantitative measure of one’s belief in the occurrence of a future event. In other words, when conducting a statistical analysis, we need to be able to determine the likelihood that two or more groups differ by chance, or that an observed a relationship occurred by chance. Remember that probabilities vary between 0 and 1, with 0 indicating no chance of an event occurring and 1 indicating a 100% chance of an event occurring. Because statistics relies so heavily on probability, let’s review some basic concepts. Prepare yourself for lots of focus on ‘true coins’ (coins that have the equal likelihood have being heads or tails) and dice! Rolling dice and flipping coins are random events, and we will use them to look at predictability! [When to introduce Bayesian vs Frequentist concepts?] 4.3 The sample point method The sample point method is one method of determining the likelihood of an event, and entails the following steps: Define the sample space, S, by listing all possible outcomes of the ‘experiment’ or ‘trial’ you are conducting. Assign probabilities to all sample points, Pi, such that the probability of all events within the experiment tally to 1. *Sum all sample points that constitute the outcome you are interested in to find the probability of that outcome. Let’s practice this concept with an example. Let’s calculate the probability of getting two heads in three tosses of a fair coin. Table 4.1: Sample Space Outcome Toss1 Toss2 Toss3 Abbreviated Probability 1 Heads Heads Heads HHH 0.125 2 Heads Heads Tails HHT 0.125 3 Heads Tails Heads HTH 0.125 4 Tails Heads Heads THH 0.125 5 Tails Tails Heads TTH 0.125 6 Tails Heads Tails THT 0.125 7 Heads Tails Tails HTT 0.125 8 Tails Tails Tails TTT 0.125 The first step is to define the sample space for this experiment, by showing all possible outcomes of tossing a coin 3 times, as we have done in the table, called ‘Sample Space’. Since this is a fair or balanced coin, all of the outcomes are equally likely, so we assign them a probability of 1/8. Then, we can simply sum the runs that meet our criterion of 2 heads (HHT, HTH, THH): 1/8 + 1/8 + 1/8 = 0.375. The equation of an event occurring (A) is: \\(P(A) =\\frac{n_a}{N}\\) where P(A) is the probability of event A equals the number of points constituting event A (\\(n_a\\)) divided by the total number of sample points (N). Applying this equation to the above example, P(A) = \\(\\frac{3}{8} = 0.375\\) 4.4 Combinatorials Because defining the sample space can be cumbersome for larger sample spaces, we can use combinatorial math, specifically the mn rule, to calculate sample space! The mn rule simply states that if one group contains m elements and another group contains n elements, you can for m x n pairs containing an element from each group. For instance, if we were determining the sample space for outcomes of rolling 2, 6-sided dice, we could multiply 6 x 6 (or \\(6^2\\)). Using the equation for calculating the probability of an event, what is the probability of rolling double 6s? answer = 1/36; answer ## [1] 0.02777778 Let’s try another example with even larger sample space: Calculate the probability of each person in a class of 14 students has a different birthday. In this case, a sample point consists of 14 dates, since there are 14 different students. Assuming that each student has the same probability of being born on any one of 365 days (over course in the real world the likelihood of being born on particular dates differs). The total number of sample points is \\(N = 365^{14}\\) (i.e., there are 365 possible birthdays for Student 1, 365 possible birthdays for student 2, yielding 716835279219613000000000000000000000 possible combinations of b-days). To calculate the probability that each student has a different birthday, you would apply the same equation as above, but calculate the number of points to satisfy our event, keeping in mind that there are 365 birthdays possible for Student 1, 264 possible birthdays for Student 2, 263 possible birthdays for Student 3, and so on, as: P(A) = \\(\\frac{n_a}{N} = \\frac{365 \\times 364 \\times 363 \\dots \\times 352}{365^{14}}=0.7769\\) Sample points can be represented as sequences of numbers or symbols. An ordered arrangement of of distinct objects is called a permutation. We can calculate the sample space as total number of distinct ways of arranging these symbols or numbers in a sequence, using the following equation: \\(P_r^n = n(n-1)(n-2)\\dots(n-r+1) = \\frac{n!}{(n-r)!}\\) where \\(n! = n \\times (n-1) \\times (n-2) \\times\\dots\\times2\\times1\\), where the total number of ordering \\(n\\) objects taken \\(r\\) at a time. Quick factorial review and calculating in r #factorial of 5 or 5! factorial &lt;- 5*4*3*2*1; factorial ## [1] 120 factorialR &lt;- factorial(5); factorialR ## [1] 120 #remember that factorial(0) ## [1] 1 Let’s apply this equation: How many trinucleotide sequences can be formed without repeating a nucleotide? To put this another way, we ar interested in the number of ways of ordering \\(n = 4\\) elements (A, T, C, and G) taken 3 at a time (trinucleotide = 3; \\(r = 3\\)). permutations = (factorial(4))/factorial(4-3); permutations ## [1] 24 When the sequence is not important, we use a different formula. For unordered sets of r elements chosen without replacement from n available elements are called combinations, with total number of combinations calculated as: \\(C^n_r = \\frac{n!}{r!(n-r)!}\\) What are the number of combinations of 2 colors of m&amp;ms that we can select out of the 5 total colors? combinations = (factorial(5))/(factorial(2)*factorial(5-2)); combinations ## [1] 10 If it’s helpful, you can test the answer by hand, using these colors: tan, brown, orange, red, and green. Again, the basic concepts of the sample-point method, are to define sample space, and either assign probabilities to all sample points, then sum the probabilities OR calculate \\(P(A) =\\frac{n_a}{N}\\) where P(A) is the probability of event A equals the number of points constituting event A (\\(n_a\\)) divided by the total number of sample points (N). Note: these are the same equation, they only differ in terms of when you calculate the probability (of the points individually, or at the end after you define the sample space / and event number). When sample space is large, it is easier to use combinatorial math to calculate the points and the sample space. Let’s apply this understanding of probability to calculate the likelihood of two events occurring. 4.5 Union and intersection of events You might be gleaning that defining the appropriate sample space is critically important to correctly calculate probabilities of particular events occurring. Also important is the understanding relationship between events of interest for which we calculate probabilities. These concepts are also important for understanding Boolean operators, computer coding concepts, and modeling. Note: Boolean operators form the basis of mathematical sets and database logic. They connect your search words together to either narrow or broaden your set of results. The three basic boolean operators are: AND, OR, and NOT. (I copied and pasted this from the internet, should rewrite a bit - this is just taking a while) Union of events Union of events: What is the likelihood of both events A and B (this can be written as \\(A \\cup B\\))? contains all sampling points for A or B. Example calculating probability of union event: What is the likelihood of rolling an odd number (Event A) on a 6-side fair die OR that is less than 4 (Event B)? Event A = 1, 3, 5 Event B = 1, 2, 3 #The sample space is all possible rolls samplespace &lt;- c(1, 2, 3, 4, 5, 6); samplespace ## [1] 1 2 3 4 5 6 #We will use Boolean operators in R. They will return TRUE / FALSE statements. #Below we tell R to look for odd numbers OR (indicated by line) #numbers less than 4. unionevent &lt;- (samplespace%%2==1) | (samplespace &lt; 4); unionevent ## [1] TRUE TRUE TRUE FALSE TRUE FALSE #If the conditions are satisfied, TRUE will be returned. #If conditions are not met, FALSE will be returned. #Now, let&#39;s calculate the probability using techniques that you #have already seen above. probunion &lt;- 4/6; probunion ## [1] 0.6666667 Intersection of events Intersection of events: What is the likelihood of both events A and B (this can be written as \\(A \\cap B\\))? contains all sampling points for A and B. Example calculating probability of an intersection event: What is the likelihood of rolling an odd number (Event A) on a 6-side fair die AND that is less than 4 (Event B)? Event A = 1, 3, 5 Event B = 1, 2, 3 #The sample space is all possible rolls samplespace &lt;- c(1, 2, 3, 4, 5, 6); samplespace ## [1] 1 2 3 4 5 6 #We will use Boolean operators in R. They will return TRUE / FALSE statements. #Below we tell R to look for odd numbers OR (indicated by line) #numbers less than 4. intersectionevent &lt;- (samplespace%%2==1) &amp; (samplespace &lt; 4); intersectionevent ## [1] TRUE FALSE TRUE FALSE FALSE FALSE #If the conditions are satisfied, TRUE will be returned. #If conditions are not met, FALSE will be returned. #Now, let&#39;s calculate the probability using techniques that you #have already seen above. probintersection &lt;- 2/6; probintersection ## [1] 0.3333333 #Just for fun #The other common Boolean operator used in computer code is &#39;not&#39; #We want to roll any number except 2 nottwo &lt;- samplespace != 2; nottwo ## [1] TRUE FALSE TRUE TRUE TRUE TRUE probNOTtwo &lt;- 5/6; probNOTtwo ## [1] 0.8333333 4.6 Conditional probability Conditional probability indicates a situation when the probability of one event depends on the occurrence of another event. Conditional probability (written as P(A|B), and read as the probability of event A given B) is calculated with the following equation: \\(P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\) where \\(A \\cap B\\) indicates the probability of both event A and event B occurring. Example: I roll a die and ask you to guess the number. I want to increase your odds of guessing the correct number, so I tell you that the number is odd. You are going to guess the number 3 - What are your odd of guessing the correct number (odds that the answer is 3; event A) given that the result is an odd number (the answer is an odd number; event B)? To break this down, the probability of both A and B being true (\\(A \\cap B\\)) is 1/6. The likelihood of the roll being a 3 is the limiting factor in essence, so an ‘and’ probability is constrained by the least likely event. Then, we divide 1/6 by the probability of the roll being odd, which is 1/2 since there are 3 odd and 3 even numbers. conditional &lt;- (1/6)/(1/2); conditional ## [1] 0.3333333 #the likelihood of guessing the correct number, if the number is odd is 1/3. 4.7 Multiplicative Law of Probability of Independent Events For independent events, in other words, two events in which the occurrence of one event doesn’t depend on the occurrence of the other event, the probability of BOTH events occurring is the found by multiplying the probability of each event A and B happening: \\(P(A \\cap B)= P(A) \\times P(B)\\) where \\(A \\cap B\\) indicates the probability of both event A and event B occurring. Example: What is the probability of getting heads in two consecutive coin tosses? heads &lt;- (1/2)*(1/2); heads ## [1] 0.25 4.8 Additive Law of Probability of Mutually Exclusive Events For two events that are mutually exclusive (if event A occurs, then event B cannot occur), the probability of one of two mutually exclusive events happening is found by adding their individuals probabilities. \\(P(A \\cup B)= P(A) + P(B)\\) where \\(A \\cap B\\) indicates the probability of both event A and event B occurring. Example: What is the probability of rolling a 5 and a 6 on a fair die? That’s right! It’s zero, because the definition of mutual exclusivity indicates that if one event occurs the other can’t. Real example: What is the probability of rolling a 5 or a 6 on a fair die? fivesix &lt;- (1/6)+(1/6); fivesix ## [1] 0.3333333 4.9 General Additive Law of Probability When events are not mutually exclusive, we apply the general additive law of probability, which is written as: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) where \\(P(A \\cap B)\\) is the intersection of event A and event B. Why do we remove the intersection event? We essentially don’t want to double count the intersecting area of Event A and Event B. You’ve already completed one of these problems, we just hadn’t gone over intersection events. Let’s apply the general additive law of probability to the previous problem: What is the likelihood of rolling an odd number (Event A) on a 6-side fair die OR that is less than 4 (Event B)? Event A = 1, 3, 5 Event B = 1, 2, 3 Original calculation: #The sample space is all possible rolls samplespace &lt;- c(1, 2, 3, 4, 5, 6); samplespace ## [1] 1 2 3 4 5 6 #We will use Boolean operators in R. They will return TRUE / FALSE statements. #Below we tell R to look for odd numbers OR (indicated by line) #numbers less than 4. unionevent &lt;- (samplespace%%2==1) | (samplespace &lt; 4); unionevent ## [1] TRUE TRUE TRUE FALSE TRUE FALSE #If the conditions are satisfied, TRUE will be returned. #If conditions are not met, FALSE will be returned. #Now, let&#39;s calculate the probability using techniques that you #have already seen above. probunion &lt;- 4/6; probunion ## [1] 0.6666667 Calculation using the equation: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) probA &lt;- 3/6 probB &lt;- 3/6 intersection &lt;- 1/3 finalprob &lt;- probA + probB - intersection; finalprob ## [1] 0.6666667 4.10 Key concepts Randomness is inherent in natural systems. Whether this randomness is truly random or simply appears random due to the complexity of natural systems, we treat it the same in statistics. We want to avoid any form of systematic bias in experimental design. Randomness and other factors are lumped together in what we call statistical error; the difference between a true estimate of a parameter and what we estimate in our sample. Probability is the proportion of times an event occurs in repeated trials, and varies between 0 and 1. We practiced a bunch of different ways of calculating probability, for the long-term, what you really need to know is that to calculate probabilities, you need to understand your sample space and the likelihood and relationship of events of interest. Two great rules of probability are: Additive Law of Probability &amp; the Multiplicative Law of Probability *Union events and Intersection events 4.11 Practice problems "],["a-simple-statistical-test.html", "Chapter 5 A Simple Statistical Test 5.1 Simplicity at its finest 5.2 Basic hypothesis testing review 5.3 Tailed tests 5.4 Drawing conclusions from statistics 5.5 Reporting results 5.6 Review problems", " Chapter 5 A Simple Statistical Test 5.1 Simplicity at its finest We will highlight the basic components of a statistical test with a very simple statistical ‘tailed’ test. For this example, we will: State a hypothesis Calculate a test statistic Determine the p-value Interpret results 5.2 Basic hypothesis testing review We are all familiar with the basic scientific process: A researcher makes observations about the natural world, then generates a hypothesis to test a particular explanation of an observed phenomenon within the system, rejects or fails to reject the hypothesis, and reports these findings, growing the understanding of the system. In most day-to-day research, researchers perform the scientific process intuitively, rather than explicitly. For instance, I have never included hypotheses in a manuscript. This isn’t to say you can’t include hypotheses, just that most journals don’t require writing your methods this way, and often space constraints and readability considerations lead authors to avoid including specific hypotheses in write-ups. Frequently, I will include hypotheses in a research proposal to help illustrate key aspects of the system and demonstrate that the statistical tests are testable, but not always! In other words, hypothesis testing is the foundation of scientific inquiry, but easily overlooked, since it often isn’t necessary to explicitly state hypotheses to create scientific products. Statistical hypothesis testing answers very simple questions, while scientists work in very complex knowledge environments. For beginning researchers, it is important to understand what statistics can tell us, and how this builds information to address amazing and very interesting research questions. Hypotheses allow us to articulate exactly what we are testing in statistics and to organize thoughts and analyses. In Week 1, we discussed that most statistical tests are designed to answer one of two questions: 1) Are there differences between these groups? and 2) are there relationships between these variables? Let’s connect these concepts directly to hypothesis testing. The null hypothesis (\\(H_0\\)) is a statement about a population parameter that would be interesting to reject. The null hypothesis typically asserts that there is no effect or relationship or that results will not deviate from established knowledge. For instance: The mean height of giraffes in captivity and in the wild do NOT differ. The incidence of toenail fungus is the SAME in the control group and the group given anti-fungal medicine. There is NO relationship between sea grass height and the number of sea snails. The mean human body temperature is 98.6 degrees Fahrenheit. Null hypotheses are paired with alternative hypotheses (\\(H_A\\)) that represent ALL other possibilities other than that stated in the null hypothesis. For instance: The mean height of giraffes in captivity and in the wild differs. The incidence of toenail fungus is different in the control group and the group given anti-fungal medicine. There is a relationship between sea grass height and the number of sea snails. The mean human body temperature is not 98.6 degrees Fahrenheit. Note that the null hypothesis is very specific, while the alternative hypothesis is general. Statistical tests are designed to either reject or fail to reject the null hypothesis. 5.3 Tailed tests Tailed tests are very simple tests for comparing means or proportions, and are great for illustrating the basic components of a frequentist statistical analysis. Let’s walk through an example! Handedness is common in humans. Around 90% of humans preferentially use their right hand. You’ve been watching your cat, Geraldo, play with his toy mouse, and you notice that he preferentially uses his right paw to bat the mouse around. You start to wonder if cats display handedness like humans! You run around visiting cats and observing their paw usage and determine that 14 cats of the 18 you observe appear to be right-pawed, while 4 preferentially use their left paw. Is this enough evidence to suggest that cats display ‘handedness’ or did this pattern just emerge by chance? Generate hypotheses What is the null hypothesis in this case? Remember it must be specific so that we can either reject or fail to reject the null! \\(H_0\\): Left and right-pawed cats are equally frequent in the population (i.e., Cats are not right or left-pawed). Note that this null hypothesis is very specific. If we would describe this mathematically, we would say that we expect half the cats (9 / 18) to use their right paws and half to use their left paws. If we express this as a proportion, (\\(p\\)), we are testing whether \\(p\\) = 0.5. Very specific. What is the alternative hypothesis? \\(H_A\\): Left and right-pawed cats are not equally frequent in the population. Note that the alternative hypothesis is very broad and encompasses all other possibilities. Because, in theory, we could observe proportions below 0.5 (1/18 cats are right-handed) or above 0.5 (16/19 cats are right-handed), we refer to this as two-tailed. In a two-tailed test, the alternative hypothesis includes parameters on both sides of the value specified by the null hypothesis. Before we move on, can you think of an example of a 1-tailed test? Examples: Did you score better than the class average? Is the time to ARD slower when you avoid driving through campus? Note that we are only interested in values in one direction. In the first example, we are interested in whether your score is &gt;80% (class average). In the second, we are interested in whether your drive time is &lt;10 minutes (time to ARD driving through campus from your house). How could you phrase the first example to be a 2-sided test? Answer: Was my score different than the class average? In this case, your score could be higher or lower. Calculate a test statistic We generated our hypotheses. Let’s calculate a test statistic. What is a test statistic? A test statistic is a quantity calculated from that data used in statistical analysis to evaluate the null hypothesis. For this simple test, our test statistic will be 14, since this is the number of right-handed cats we observed. We want to ask whether observing 14 out of 18 cats using their right paw is truly different from the null (9 out of 18 cats using their right paw), or did this pattern occur by chance in our sample? Determine the p-value Frequentists statistics is based on the concept of statistical distributions. If we run many trials, we can determine the likelihood of certain events occurring. We refer to the patterns of occurrence of trials as frequency distributions. Let’s illustrate using the data above. A cat can either be right-handed or left-handed (in this case there are no ambidextrous cats). To determine the likelihood that our pattern arose by chance, we conduct numerous trials like a coin toss. We would randomly flip a coin 18 times and record the outcome of each trial; heads being right-pawed, and tails being left-pawed. Let’s do that now: Each student take a coin. Flip the coin 18 times and record number of heads. *Divide by total number of trials (in this case students) to derive relative frequency for each event. We probably generated something that looks like the figure below. This is referred to a probability distribution and expresses the relative frequency of particular events occurring. Frequentist statistics derives its name from this probability distribution. paws &lt;- 0:18 plot(paws, dbinom(paws, 18, 0.5), type=&#39;h&#39;, ylab=&quot;Probability&quot;, xlab=&quot;Number Right-pawed Cats&quot;) Here is a table of those probabilities: probability &lt;- dbinom(paws, 18, 0.5) N &lt;- 0:18 pawtable &lt;- cbind(N, probability) pawtableF &lt;- as.data.frame(pawtable); pawtableF ## N probability ## 1 0 3.814697e-06 ## 2 1 6.866455e-05 ## 3 2 5.836487e-04 ## 4 3 3.112793e-03 ## 5 4 1.167297e-02 ## 6 5 3.268433e-02 ## 7 6 7.081604e-02 ## 8 7 1.213989e-01 ## 9 8 1.669235e-01 ## 10 9 1.854706e-01 ## 11 10 1.669235e-01 ## 12 11 1.213989e-01 ## 13 12 7.081604e-02 ## 14 13 3.268433e-02 ## 15 14 1.167297e-02 ## 16 15 3.112793e-03 ## 17 16 5.836487e-04 ## 18 17 6.866455e-05 ## 19 18 3.814697e-06 Take a look at the chart. What is the probability of observing right pawedness in 14 out of 18 cats? Is this difference from the null (9 cats are right-handed, no better than random) big enough to reject the null? To determine this we calculate a p-value. A p-value is the probability of obtaining the data that we observe if the null hypothesis were true. In this case, we will generate a p-value for a two-tailed test. To do this, we will add the probabilities of observing 14 right pawed cats or more by chance AND for the possibility of observing 4 or fewer right paws, which would indicate left pawedness. P-value = Pr[14] + Pr[15] + Pr[16] + Pr[17] + Pr[18] + Pr[4] + Pr[3] + Pr[2] + Pr[1] + Pr[0] Recall that we can add these probabilities up, since, in our example, we say that being right or left pawed are mutually exclusive events. pvalue &lt;- (0.0117 + 0.0031 + 0.0006 + 0.00007 + 0.000004)*2; pvalue ## [1] 0.030948 We generate a P-value of 0.031. In most sciences, we have agreed on a threshold of 0.05 for establishing statistical significance. If p-values are less than or equal to 0.05, we reject the null hypothesis. If larger, we fail to reject. The significance level, \\(\\alpha\\), is a probability used as a criterion for rejecting the null hypothesis. This significance level is important. In the sciences, we would rather err on the side of not identifying a pattern, rather than saying there is a pattern, when there it doesn’t actually exist. This concept is included in the in the discussion of *statistical errors**. A Type I error is when you reject a true null hypothesis. You are saying there is a difference, when actually, if could perfectly measure your focal population, there is no difference. By establishing a significance level (\\(\\alpha\\)) of 0.05, we are saying that we are willing to accept that 5% of the time, we will say there is an effect when there isn’t one. A Type II error is failing to find a pattern or a difference when there actually is one. If you reduce your \\(\\alpha\\) to reduce your likelihood of making a Type I, you increase the likelihood of committing a Type II error. The probability of committing a Type II error is more challenging to quantify and is related to the concept of statistical power. A study with high power has a low likelihood of committing a Type II error. Statistical power depends on several things, including sample size, the magnitude of the effect of the treatment, and variation within the sample. A study with a LARGE sample size, a BIG treatment effect, and SMALL variation within samples will have high statistical power. We will talk about calculating statistical power later. 5.4 Drawing conclusions from statistics In this case, we reject the null hypothesis, and state that our results support the alternative hypothesis that there is handedness in cats. Note that we ‘support’ the alternative hypothesis, rather than saying that there is pawedness in cats or accepting the alternative hypothesis. Statements about statistics are phrased to reflect that we are dealing in probabilities, and there is always a chance that our findings are incorrect. Additionally, statistical tests specifically test the null hypothesis, not the alternative (for which there are often many possibilities). What would we say if we didn’t reject the null? We would state that we failed to reject the null hypothesis. Failing to reject the null indicates that our sample did not provide sufficient evidence to conclude that the effect exists, but lack of evidence doesn’t prove that the effect does not exist. For this reason, we never accept the null. 5.5 Reporting results When we report findings, we will provide: 1) A statement of findings 2) The test statistic 3) The P-value 4) A description of differences, if differences exist 5) A visualization of differences, if differences exist Here, let’s focus on reporting information 1 - 3. We might say something like: Cats displayed higher levels of handedness than expected by chance (t = 14, p = 0.031). 5.6 Review problems "],["selecting-statistical-tests.html", "Chapter 6 Selecting Statistical Tests 6.1 A foray into statistics 6.2 Selecting statistical analyses", " Chapter 6 Selecting Statistical Tests 6.1 A foray into statistics Last week we talked about data types and describing data. This week we are applying concepts that we learned last week to select (and run) a statistical analysis. When conducting an analysis, a dataset will contain one or more response variables (aka y-variables, dependent variables) and one or more explanatory variables (aka x-variables, independent variables). The data types of the explanatory and response variables determine which statistical test we use. Brief review: Explanatory variables are used to explain variation in response variables. For instance, imagine that we want to identify areas that support high numbers of plants from the genus Mitella. We hypothesize that Mitella occurrence is positively related to water supply (i.e., the number of Mitella plants goes up as water availability increases). In this case, my response variable would be number of plants in genus Mitella, and my explanatory variable would be some measure of water supply. 6.2 Selecting statistical analyses Below, you will see a very simple decision matrix for selecting statistical analyses. We will build on this basic matrix and move into alternative ways of conducting the analyses indicated within the matrix throughout the semester. However, this basic structure gives us a great launch point for structuring the next discussions. Also, the statistical analyses presented below are super common in social and ecological sciences! When both the response and explanatory variables are categorical, we use what is called a G-test! "],["experimental-design.html", "Chapter 7 Experimental design", " Chapter 7 Experimental design This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: 6.3 Orthogonality When designing an experiment, I want to make sure than none of my covariates are confounded with each other and I’d also like for them to not be correlated. Consider the following three experimental designs, where the number in each bin is the number of subjects of that type. I am interested in testing 2 different drugs and studying its effect on heart disease within the gender groups. Design 1 Males Females Design 2 Males Females Treatment A 0 10 Treatment A 1 9 Treatment B 6 0 Treatment B 5 1 Design 3 Males Females Design 4 Males Females Treatment A 3 5 Treatment A 4 4 Treatment B 3 5 Treatment B 4 4 This design is very bad. Because we have no males taking drug 1, and no females taking drug 2, we can’t say if any observed differences are due to the effect of drug 1 versus 2, or gender. When this situation happens, we say that the gender effect is confounded with the drug effect. This design is not much better. Because we only have one observation in the Male-Drug 1 group, any inference we make about the effect of drug 1 on males is based on one observation. In general that is a bad idea. Design 3 is better than the previous 2 because it evenly distributes the males and females among the two drug categories. However, it seems wasteful to have more females than males because estimating average of the male groups, I only have 6 observations while I have 10 females. This is the ideal design, with equal numbers of observations in each gender-drug group. Designs 3 and 4 are good because the correlation among my predictors is 0. In design 1, the drug covariate is perfectly correlated to the gender covariate. The correlation is less in design 2, but is zero in designs 3 and 4.We could show this by calculating the design matrix for each design and calculating the correlation coefficients between each of pairs of columns. Having an orthogonal design with equal numbers of observations in each group has many nice ramifications. Most importantly, with an orthogonal design, the interpretation of parameter is not dependent on what other factors are in the model. Balanced designs are also usually optimal in the sense that the variances of ^ β are as small as possible given the number of observations we have (barring any other a priori information). "],["power-analysis.html", "Chapter 8 Power analysis 8.1 Prework 8.2 Statistical power", " Chapter 8 Power analysis 8.1 Prework Install packages ‘pwr’, ‘faraway’, ‘simr’, ‘simglm’, and ‘Superpower’. 8.2 Statistical power As we learned in lesson 3, Basic Statistical Test, statistical power is a measure of making a Type II error - saying that there is no treatment effect when, in fact, there is one. A power analysis is a way of estimating statistical power to either speak to the ability of your experiment to detect treatment effects or estimate sample size needed to answer the question that you are interested in with your experimental design. library(pwr) library(tidyverse) library(simr) library(simglm) library(Superpower) #load data data(&#39;oatvar&#39;, package=&#39;faraway&#39;) #Using power analysis to estimate needed sample size. To conduct a power analysis, you will need to know: The number of groups in your study Significance level. It is standard to use a significance level of 0.05. The power required for your experiment, which is typically set at 0.8. Effect size, which can be calculated from data from a pilot study or estimated. Effect size is a relativized estimate of difference between groups being compared. Generally, effect size is calculated by taking the difference between the two groups (e.g., the mean of treatment group minus the mean of the control group) and dividing it by the standard deviation of one of the groups. Because effect size is converted to standard deviations units, it tells you how many standard deviations lie between the two means, and can be compared across datasets regardless of the original units of the study (which is why effect sizes are calculated for meta-analyses). A classic effect size calculation is Cohen’s D calculated as \\(\\frac{\\overline{x}_1 - \\overline{x}_2}{s_{pooled}}\\), where \\({\\overline{x}_1}\\) is the mean of one group, \\({\\overline{x}_2}\\) is the mean of the second group, and \\(s_{pooled}\\) is standard deviation (typically pooled; sometimes of the control or pretest data) calculated as \\(s_{pooled} = \\sqrt{\\frac{sd_{a}^{2}+ sd_{b}^{2}}{2}}\\). Note that Cohen’s D is one of many ways of calculating effect size. Two other metrics are used by the ‘pwr package’: \\(f\\), calculated as expected standard deviation of the group means divided by the pooled within-group standard deviation, is used in our example below. Another option is eta-squared (η2). The eta-squared is the proportion of the total variance explained by the means variance. It is harder to detect a smaller effect of the treatment, and easier to detect larger effects. Cohen in his 1988 book (citation below) classified effect sizes into 3 general categories for Cohen’s D (CONFIRM): a small effect is typically set at 0.1 - 0.3, a medium effect at 0.3 - 0.5, and a large effect at &gt; 0.5. Note that the range for small, medium, and large effects differ by effect size calculation Using these classifications, you can estimate the sample size for a proposed study even if you have no data. Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale,NJ: Lawrence Erlbaum. Below, we will run a power analysis with actual data, but let’s start by running a power analysis to guide experimental planning for a study when no opportunity to conduct a pilot. Imagine you are planning an experiment in which you are trialing the effect of a new fertilizer on oat growth. You plan to include three levels, a control (no fertilizer added), a moderate fertilization treatment, and a high fertilization treatment and measure height as a dependent variable. In the code below, k indicates the number of groups (here: control, moderate, high), f indicates an effect size (we selected a medium effect size), sig.level indicates the significance level (0.05), and power (standard to use 0.8). To calculate the number of groups, if you have two factors, simply multiply the number of groups in each factor to get the value k. For instance, say you are looking at the effect of 2 levels of pesticide and 3 temperature levels on bee longevity, you would multiply 2 * 3, yielding 6 groups. Why do we use 0.8? It is by convention, much like we set a significance level (\\(\\alpha\\)) of 0.05. At some point scientists agreed that a power level of 0.8, which means that the probability of rejecting a false null hypothesis is 0.8 (or 80%), is an acceptable risk of committing a Type II error. pwr.anova.test(k=3, f=0.3, sig.level=0.05, power=0.8) ## ## Balanced one-way analysis of variance power calculation ## ## k = 3 ## n = 36.70126 ## f = 0.3 ## sig.level = 0.05 ## power = 0.8 ## ## NOTE: n is number in each group In this case, we’d should include at least 37 individuals (plus a few extra) to accommodate loss of individuals during the experiment. When possible, it is preferable to run a pilot experiment in order to improve experimental design. The ‘pwr’ package in R allows you to run a power analysis on various forms of data. Let’s run through several quick analyses using the ‘pwr’ package. The effect size calculations will depend on your statistical test (for a full description of tests, go to the package). Since Cohen’s description of effect sizes as small, medium and large depend on each statistical test, there is an easy way to generate this information in the ‘pwr’ dataset. For example, if you are interested in determining the value associated with a small effect size for a regression, you would run the following code: cohen.ES(test = &#39;r&#39;, size = &#39;small&#39;) ## ## Conventional effect size from Cohen (1982) ## ## test = r ## size = small ## effect.size = 0.1 Note that you have several test options here: r = regression r; alternative = ‘two.sided’ = correlation (can specify direction ‘greater’) - not 100 percent sure I’m right here… p = t = for t-test (type = ‘paired’; you can select between two population to a repeated measure test) anov = anova chisq = chi sq tests *f2 = glms The ‘pwr’ package is nice for quick looks at power for simple analyses with no previous data. Note: that you can calculate effect sizes from data by hand and plug them into ‘pwr’ to generate sample sizes or power assessments. Let’s check this out with the ‘oatvar dataset’. ggplot(oatvar, aes(y=yield, x=block, color=variety)) + geom_point() + geom_line(aes(x=as.integer(block))) The cbpp is a dataset on contagious bovine pleuropneumonia. Data description is here: https://rdrr.io/cran/lme4/man/cbpp.html library(simr) head(cbpp) ## herd incidence size period obs ## 1 1 2 14 1 1 ## 2 1 3 12 2 2 ## 3 1 4 9 3 3 ## 4 1 0 5 4 4 ## 5 2 3 22 1 5 ## 6 2 1 18 2 6 cbpp$obs &lt;- 1:nrow(cbpp) "],["generalized-linear-models-glms.html", "Chapter 9 Generalized Linear Models (GLMs) 9.1 Linear models", " Chapter 9 Generalized Linear Models (GLMs) 9.1 Linear models What is a linear model, you say? A linear model is a model where the data and parameters of interest interact only via addition and multiplication. Most commonly, the term ‘linear model’ refers to statistical models that involve linear regression. Clearly, linear models include linear regression in all its beautiful forms, like ANCOVA, and logistic regression. However, ANOVA is actually considered a linear model as well, since it meets our basic definition. Though it may not seem so from the outset, ANOVA and regression, statistically are related. Here’s an example: Let’s pretend we are looking at SLA (Specific Leaf Area) measurements of 3 different populations of plants, one population from Flagstaff, one from Sedona, and one from Camp Verde. library(tidyverse) data &lt;- tribble(~Population, ~SLA, &quot;Flagstaff&quot;, 2, &quot;Flagstaff&quot;, 1, &quot;Flagstaff&quot;, 2, &quot;Sedona&quot;, 5, &quot;Sedona&quot;, 6, &quot;Sedona&quot;, 7, &quot;CampV&quot;, 10, &quot;CampV&quot;, 11, &quot;CampV&quot;, 10) pop &lt;- group_by(data, Population) SLAtable &lt;- summarise(pop, meanSLA = mean(SLA)); SLAtable ## # A tibble: 3 × 2 ## Population meanSLA ## &lt;chr&gt; &lt;dbl&gt; ## 1 CampV 10.3 ## 2 Flagstaff 1.67 ## 3 Sedona 6 #ANOVA model1 &lt;- aov(SLA ~ Population, data=data) summary(model1) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 2 112.67 56.33 101.4 2.37e-05 *** ## Residuals 6 3.33 0.56 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #ANOVA run as a linear model model2 &lt;- lm(SLA ~ Population, data=data) summary(model2) ## ## Call: ## lm(formula = SLA ~ Population, data = data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.0000 -0.3333 0.0000 0.3333 1.0000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.3333 0.4303 24.01 3.43e-07 *** ## PopulationFlagstaff -8.6667 0.6086 -14.24 7.50e-06 *** ## PopulationSedona -4.3333 0.6086 -7.12 0.000386 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7454 on 6 degrees of freedom ## Multiple R-squared: 0.9713, Adjusted R-squared: 0.9617 ## F-statistic: 101.4 on 2 and 6 DF, p-value: 2.373e-05 #in this simple model, #notice that the Intercept is the mean SLA of the #reference group #then population Flagstaff Coefficient #is 10.333 - 8.667 = 1.67 (mean of the Flagstaff pop) #Sedona population = 10.3333 - 4.3333 #6 = mean SLA of the Sedona population In other words, ANOVA compares means and provides a p-value that tells us that at least two groups are different, while the linear model form reports 1 mean for the reference group (intercept), and p-values indicate whether each group is different from the reference group. Well, hopefully that was an interesting aside and served to connect a variety of different statistical techniques. Now, let’s have a b***ing session about old-school linear models, like ANOVA. They have SO many assumptions that need to be met - in particular, linear models assume a normal distribution of residuals. Then, if that assumption isn’t met, you have to transform your dependent variable, and back transform your error bars, and on and on until you achieve normality, which - let’s be honest - may never happen. If only there was an easy way to deal with the assumption of normality of residuals! Enter a new(ish) generation of models - Generalized Linear Models (GLMS)! "],["generalized-linear-models.html", "Chapter 10 Generalized Linear Models 10.1 Link functions 10.2 GLMs in R", " Chapter 10 Generalized Linear Models A Generalized Linear Model (GLM) is an flexible statistical framework that allows us to model relationships between variables when the assumptions of ordinary linear regression are not met. Generalized Linear Models makes linear regression generalizable by using a link function to relate the linear model (i.e., relationship between x and y; systematic component) to the error (random component), allowing the magnitude of the variance for each measurement to be a function of the mean value and allowing us to specify different types of error distributions. Practically, GLMs allow you to analyze a variety of regression models, including linear regression, ANOVA, models for examining count data, models for predicting likelihood of events, under one statistical umbrella. Model assumptions What are the assumptions of GLMs? 1. The data are independent. No problem - this is always important! 2. The residuals do NOT need to be normally distributed, but you do need to specify a distribution from an exponential family that best fits your data (i.e., normal/Gaussian, binomial, Poisson, etc.) 3. The homogeneity of variance does NOT need to be satisfied. Wow! This is great - let’s go over the components of GLMs - like what is this “link” function that everyone is talking about? A little aside Don’t confused Generalized linear models with General linear models. The term “general” linear model (GLM) usually refers to a linear regression models that assumes a Gaussian (normal) distribution, while generalized linear models allow you to specify other distribution from the exponential family (a set of distributions which include normal, Poisson, gamma and other commonly used distributions) for residuals. Parameter estimation uses maximum likelihood estimation (MLE) rather than ordinary least squares (OLS). Remember from our previous discussion of linear regression, that we determine the line of best fit for our data points by using ordinary least squares. Maximum likelihood estimation is Components of the GLM The linear predictor: This is the function that describes the relationship between the independent variable/s and the dependent variable. This essentially has the same structure as a linear model: \\(\\eta_{i}\\) = \\(\\beta_{0}\\) + \\(\\beta_{1}X_{i1}\\) + … \\(\\beta_{p}X_{ip}\\) Where \\(\\eta_{i}\\) is the equivalent of y; the linear predictor - the predicted point on the y-axis according to the coefficients of predictors in combination with . \\(\\beta\\) values indicate coefficients of the predictors and x values indicate values of the predictors. The error or random component. The link function which brings together the linear predictor and the error distribution. How do link functions work? Let’s start with the formula for a line. \\(y =\\beta_{0} + \\beta_{1}*x\\) where y is the y-value (point along the y-axis); x is the value along the x-axis; \\(\\beta_{1}\\) (aka m) is the slope, or how much the line rises for 1 unit increase in x; \\(\\beta_{0}\\) is the intercept, or the value of y, when x = 0. In GLMs, the y term is \\(\\eta_{i}\\), so: \\(\\eta_{i}\\) = \\(\\beta_{0}\\) + \\(\\beta_{1}X_{i1}\\) + … \\(\\beta_{p}X_{ip}\\) You can keep adding predictor variables (i.e., explanatory variables, independent variables) to the model as \\(\\beta_{2}\\) and so on! You can also specify non-linear relationships, like polynomial relationships, by including the second order term \\(\\beta_{2}X_{i2}^2\\) like so: \\(\\eta_{i}\\) = \\(\\beta_{0}\\) + \\(\\beta_{1}X_{i1}\\) + \\(\\beta_{2}X_{i2}^2\\) For our classic regression models, you obtain an equation for the line of best fit presented in the syntax above. These classic linear regression models make several important assumptions: Additive relationships: Model variables have an additive relationship with each other, rather than multiplicative. Homoskedastic data: Constant variance, in order words, variance does not increase or decrease as a function of the mean. Normally distributed errors (residuals): Residuals are normally distributed, with mean 0 Non-correlated variables: Variables are independent. In standard linear model analysis, we use transformations to meet these assumptions, but GLMs are really cool, because we can avoid using transformation, by applying the link function. Through the beauty of link functions and conditional probabilities, we do not need to worry about homoskedasticity, normal distribution of residuals, or additive relations. 10.1 Link functions Let’s talk link functions, as this is what modifies our regression models, depending on the error distribution of our response variable. You specify the link function based on error distribution of your response variables, let’s go over the most common functions. Keep in mind that statisticians have developed many, many distributions! Some interesting ones include Pareto distribution, often applied to model Gross Domestic Product, and the von Mises distribution, for circular data, applied to days of the year! We will go over the most common link functions and their formulas used in the ecological sciences. Error distribution = Gaussian or Normal Link name = Identity This is the generalized linear model corresponding to a Gaussian distribution, in other words, regression/ANOVA. We do not need to transform the data in anyway, so the link is called identity. Link function = Revisiting the standard equation for a line, \\(y =\\beta_{0} + \\beta_{1}*x\\), we are saying that the mean y value will be what is calculated by the predicted model; we are not transforming anything. This is sometimes indicated with the following equation: \\(g(\\pi)=\\pi\\) where g indicates the link function, and \\(\\pi\\) indicates the mean y. Error distribution = Binomial Link name = Logit Link function = \\(g(\\pi_i)=ln(\\frac{\\pi_i}{1-\\pi_i})\\) This function can be rewritten to solve for y, as: \\(\\pi_i=ln(\\frac{e^{\\pi_i\\beta}}{1+e^{\\pi_i\\beta}})\\) Why would you want to do this? Back in the day, I used this function to create the line of best fit for logistic regressions! See excel example from my very first publication ever! Error distribution = Poisson Count data cannot be less than 0, so we use the Poisson distribution, so we transform the y variable by taking the natural log. Link name = Log Link function = \\(g(\\pi_i)=ln(\\pi_i)\\) This function can be rewritten to solve for y, as: \\(\\pi_i=e^{\\pi_i\\beta}\\) Error distribution = Gamma Used for data that are continuous and positive but may have skewed distributions. Often uses for lengths, durations or amounts (i.e., time to death). Link name = Reciprocal Link function = \\(g(\\pi_i)=1/pi_i)\\) This function can be rewritten to solve for y, as: 10.2 GLMs in R Let’s generate some code for statistical analysis, and create some figures! First, we will start by examining data with a Gaussian/normal distribution. data(iris) #Is petal width related to petal length? #model &lt;- glm(y-value ~ x-values, family = specifylinkingfunctions, data = dataset) iris_model &lt;- glm(Petal.Length ~ Petal.Width, family = gaussian(), data = iris) #y data is continuous, use gaussian summary(iris_model) ## ## Call: ## glm(formula = Petal.Length ~ Petal.Width, family = gaussian(), ## data = iris) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08356 0.07297 14.85 &lt;2e-16 *** ## Petal.Width 2.22994 0.05140 43.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.2286808) ## ## Null deviance: 464.325 on 149 degrees of freedom ## Residual deviance: 33.845 on 148 degrees of freedom ## AIC: 208.35 ## ## Number of Fisher Scoring iterations: 2 #for comparison run a linear regression for comparison iris_modelR &lt;- lm(Petal.Length ~ Petal.Width, data = iris) #y data is continuous, use gaussian summary(iris_modelR) ## ## Call: ## lm(formula = Petal.Length ~ Petal.Width, data = iris) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.33542 -0.30347 -0.02955 0.25776 1.39453 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08356 0.07297 14.85 &lt;2e-16 *** ## Petal.Width 2.22994 0.05140 43.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4782 on 148 degrees of freedom ## Multiple R-squared: 0.9271, Adjusted R-squared: 0.9266 ## F-statistic: 1882 on 1 and 148 DF, p-value: &lt; 2.2e-16 When we call the summary, you can see several interesting things. First, the first species alphabetically is represented by the intercept - essentially each species is compared against this first species. Then, you see the dispersion parameter (smaller the better), and AIC for model comparison (smaller the better). The dispersion parameter is a measure of variance - how the actual data points scatter around a mean, similar to the sum of squares in a linear regression. The AIC or the Akaike Information Criterion is usually calculated for model comparison. Here, it is presented as a general assessment of goodness of fit. AIC = 2K – 2ln(L) where: K: The number of model parameters. ln(L): The log-likelihood of the model. This tells us how likely the model is, given the data. Let’s take a direct look at the results of a statistical test using the Anova function in the ‘car’ package. library(car) #glm #iris_model &lt;- glm(Petal.Length ~ Petal.Width, family = gaussian(), data = iris) #y data is continuous, use gaussian Anova(iris_model) ## Analysis of Deviance Table (Type II tests) ## ## Response: Petal.Length ## LR Chisq Df Pr(&gt;Chisq) ## Petal.Width 1882.5 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #for comparison run a linear regression for comparison #iris_modelR &lt;- lm(Petal.Length ~ Petal.Width, data = iris) #y data is continuous, use gaussian Anova(iris_modelR) ## Anova Table (Type II tests) ## ## Response: Petal.Length ## Sum Sq Df F value Pr(&gt;F) ## Petal.Width 430.48 1 1882.5 &lt; 2.2e-16 *** ## Residuals 33.84 148 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #Look at residuals res_irismodel &lt;- rstandard(iris_model) plot(iris_model$fitted.values, res_irismodel, pch=20, ylab = &quot;Standarized residuals&quot;, xlab = &quot;fitted values&quot;) The residuals should have a random pattern, yet here we see a distinctive shape in which they increase at mid-size values and decrease, particularly at the largest values. We could try a different error distribution. iris_model &lt;- glm(Petal.Length ~ Petal.Width, family = Gamma (), data = iris) #y data is continuous, use gaussian summary(iris_model) ## ## Call: ## glm(formula = Petal.Length ~ Petal.Width, family = Gamma(), data = iris) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.532340 0.017386 30.62 &lt;2e-16 *** ## Petal.Width -0.172682 0.008915 -19.37 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Gamma family taken to be 0.08000084) ## ## Null deviance: 44.655 on 149 degrees of freedom ## Residual deviance: 12.444 on 148 degrees of freedom ## AIC: 412.86 ## ## Number of Fisher Scoring iterations: 5 res_irismodel &lt;- rstandard(iris_model) plot(iris_model$fitted.values, res_irismodel, pch=20, ylab = &quot;Standarized residuals&quot;, xlab = &quot;fitted values&quot;) iris_model &lt;- glm(Petal.Length ~ Petal.Width, family = gaussian (link=&quot;log&quot;), data = iris) #y data is continuous, use gaussian summary(iris_model) ## ## Call: ## glm(formula = Petal.Length ~ Petal.Width, family = gaussian(link = &quot;log&quot;), ## data = iris) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.59485 0.04195 14.18 &lt;2e-16 *** ## Petal.Width 0.54740 0.02283 23.98 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.4808902) ## ## Null deviance: 464.325 on 149 degrees of freedom ## Residual deviance: 71.172 on 148 degrees of freedom ## AIC: 319.85 ## ## Number of Fisher Scoring iterations: 7 res_irismodel &lt;- rstandard(iris_model) plot(iris_model$fitted.values, res_irismodel, pch=20, ylab = &quot;Standarized residuals&quot;, xlab = &quot;fitted values&quot;) iris_model &lt;- glm(Petal.Length ~ Petal.Width, family = gaussian (link=power(0.25)), data = iris) #y data is continuous, use gaussian summary(iris_model) ## ## Call: ## glm(formula = Petal.Length ~ Petal.Width, family = gaussian(link = power(0.25)), ## data = iris) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.133377 0.012699 89.25 &lt;2e-16 *** ## Petal.Width 0.198171 0.007213 27.47 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 0.4035011) ## ## Null deviance: 464.325 on 149 degrees of freedom ## Residual deviance: 59.718 on 148 degrees of freedom ## AIC: 293.53 ## ## Number of Fisher Scoring iterations: 6 res_irismodel &lt;- rstandard(iris_model) plot(iris_model$fitted.values, res_irismodel, pch=20, ylab = &quot;Standarized residuals&quot;, xlab = &quot;fitted values&quot;) All of this to point out that you can in essence perform transformations within the link and you can use the AIC values to compare different models and select the best one for your data (lowest AIC value). In the end, our first model was the best fit to our data, though it wasn’t perfect. Over Dispersion is measured by comparing variance of the response variable to the variance of the linear predictors, when the variance of the response variable is larger than that of the linear predictor, we have overdispersion, meaning those data are more weidley spread than the explanatory variables, and indicates that additional covariates need to be identified. At this time, let’s assume that we don’t have additional variables to add to our GLM model. Let’s run this and generate a nice plot! iris_model &lt;- glm(Petal.Length ~ Petal.Width, family = gaussian(), data = iris) If you want to make predictions: add this info if you want to helpful summary page 507. "],["mixed-effects-models.html", "Chapter 11 Mixed Effects Models 11.1 Examples in R", " Chapter 11 Mixed Effects Models Observations of data aren’t always independent, violating an assumption of our GLMs. Never fear, there are statistical models that deal with that; namely, our aforementioned mixed effects models. Let’s first cite some examples of non-independent observations: Time-series data, in which we are resampling the same points multiple times. Blocked data. For example, plants within a plot. We might expect individuals within 1 plot to respond similarly given higher likelihood of relatedness or greater similarity in microsite conditions, relative to plants in other plots. *Nested data. For example, multiple blood samples taken from an individual. We’d want to indicate that these data are non-independent and likely to be more similar to each other than to samples taken from another individual. Why call these models ‘mixed effects’? Because these models contain both fixed and random effects. Fixed effects = Variables of interest. We are interested in how these factors affect our response variable. Factors and levels of factors have been specifically chosen, and are replicable in future experiments. Random effects = Variables that may explain variation, but that we are not interested in and that we are not interested in replicating in future experiments. For instance, we don’t care if we have the same exact block or we sample blood from the same exact person or we collect data in the same year. 11.1 Examples in R Let’s take a look at running mixed effects models in R. Since agricultural researchers are the royals of blocking, we will take a look at the oat dataset. Be sure to load the lmerTest package, since this package will actually generate p-values. Want to read some snarky statistics back and forth? Check out commentary on developers of the lmer package on why they don’t include p-values… sigh… #load libraries and datasets suppressWarnings(library(&#39;faraway&#39;)) suppressWarnings(library(&#39;lmerTest&#39;)) suppressWarnings(library(&#39;lme4&#39;)) suppressWarnings(library(&#39;tidyverse&#39;)) suppressWarnings(library(&#39;emmeans&#39;)) suppressWarnings(library(&#39;multcomp&#39;)) data(&#39;oatvar&#39;, package=&#39;faraway&#39;) First, let’s talk about oats. The oats dataset ‘Data from an experiment to compare 8 varieties of oats. The growing area was heterogeneous and so was grouped into 5 blocks. Each variety was sown once within each block and the yield in grams per 16ft row was recorded.’ - Official description see ?faraway::oats When we are blocking, I find it really helpful to draw out the statistical design (Draw out what is happening). For now, let’s just take a look at the data. ggplot(oatvar, aes(y=yield, x=block, color=variety)) + geom_point() + geom_line(aes(x=as.integer(block))) Let’s just ask how does variety affect yield. The oat-folks have planted the varieties in 5 replicate blocks. This is a smart thing to do, since variation in microsite conditions will cause differences in yield, and they want their yield estimates to reflect this. While microsite effects are interesting, keep in mind, this is not the variable of interest and they would not try to precisely place the same plots. In other words, the blocks are a random variable. We would expect individuals within blocks to respond more similarly to each other than to those plants outside and individuals block, so let’s account for that variation! The syntax is simple. model &lt;- lmer(responsevariable ~ predictorvariable + (1|block), data=dataset). oatmodel.1 &lt;- lmer( yield ~ variety + (1|block), data=oatvar) anova(oatmodel.1) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## variety 77524 11075 7 28 8.2839 1.804e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This equation basically says let yield means vary by block. There is a statistically significant effect of variety. So let’s do a post-hoc test to see which varieties are different from the other. oatmodel.1 &lt;- lmerTest::lmer( yield ~ variety + (1|block), data=oatvar) dispairwise1 &lt;- emmeans::emmeans(oatmodel.1, ~variety, type = &quot;response&quot;); dispairwise1 ## variety emmean SE df lower.CL upper.CL ## 1 334 21 15.2 290 379 ## 2 377 21 15.2 332 421 ## 3 363 21 15.2 318 407 ## 4 287 21 15.2 242 332 ## 5 439 21 15.2 395 484 ## 6 331 21 15.2 286 375 ## 7 318 21 15.2 274 363 ## 8 384 21 15.2 339 429 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 dis_glht &lt;- glht(oatmodel.1, mcp(variety = &quot;Tukey&quot;)) #need to fix the below line #LetterResults &lt;- multcomp::cld(dis_glht, alpha=0.05, Letters = LETTERS) Add content on nested designs - need to find a decent dataset #why can&#39;t i use the oat dataset from MASS? #model &lt;- lmer(yield ~ variety + Fertilizer + (1|plot/subplot), data=oats) Add content crossed design Add content random slope and intercept "],["describing-ecological-patterns.html", "Chapter 12 Describing ecological patterns", " Chapter 12 Describing ecological patterns When working at landscape-scales, we often need to employ model selection techniques to identify important predictor variables or co-variates that explain the patterns in response variables of interest. There are several discrete steps and important considerations when completing such analyses. "],["step-1-identifying-predictor-variables-or-co-variates-of-importance.html", "Chapter 13 Step 1: Identifying predictor variables or co-variates of importance", " Chapter 13 Step 1: Identifying predictor variables or co-variates of importance "],["step-2-removing-colinear-variables.html", "Chapter 14 Step 2: Removing colinear variables", " Chapter 14 Step 2: Removing colinear variables Explain why this is important! Calculate the correlation matrix to identify highly correlated variables and use Variance Inflation Factor (VIF) to detect multicollinearity. USDM package provides a stepwise removal process, again removing terms with high VIF. library(car) # Calculate correlation matrix #cor_matrix &lt;- cor(data) # View the correlation matrix #print(cor_matrix) # Calculate VIF #library(car) #vif_model &lt;- lm(response ~ ., data = data) #vif_values &lt;- vif(vif_model) # View VIF values #print(vif_values) # Remove variables with high VIF (e.g., VIF &gt; 5) #data_reduced &lt;- data[, !(names(data) %in% names(vif_values[vif_values &gt; 5]))] library(usdm) #vif_step &lt;- vifstep(data, th = 10) # Set threshold (e.g., 10) #data_reduced &lt;- exclude(data, vif_step) "],["step-3-select-the-best-model.html", "Chapter 15 Step 3: Select the best model", " Chapter 15 Step 3: Select the best model Perform stepwise regression based on AIC (Akaike Information Criterion). library(MASS) #full_model &lt;- glm(response ~ ., data = data_reduced, family = binomial) #stepwise_model &lt;- stepAIC(full_model, direction = &quot;both&quot;) #summary(stepwise_model) LASSO (Least Absolute Shrinkage and Selection Operator) Regularization technique that performs variable selection and regularization simultaneously. Benefits: Automatic Variable Selection: LASSO can automatically shrink some coefficients to zero, effectively performing variable selection and reducing model complexity. Handles High Dimensionality: Particularly useful when the number of predictors is much larger than the number of observations. Regularization: Helps prevent overfitting by adding a penalty to the magnitude of coefficients. Interpretability: The resulting model is often simpler and easier to interpret, as it includes only a subset of the predictors. Computational Efficiency: Computationally efficient and can be solved using convex optimization methods. Drawbacks: Bias: The shrinkage can introduce bias into the estimates of the coefficients. Model Assumptions: Assumes linear relationships between predictors and the response variable. Choice of Penalty Parameter: The performance depends on the choice of the penalty parameter (lambda), which requires cross-validation to tune. Cannot Handle Multicollinearity Well: While LASSO can select variables, it might not perform well in the presence of high multicollinearity compared to other techniques like ridge regression. library(glmnet) #x &lt;- model.matrix(response ~ ., data_reduced)[, -1] #y &lt;- data_reduced$response #lasso_model &lt;- cv.glmnet(x, y, alpha = 1, family = &quot;binomial&quot;) #best_lambda &lt;- lasso_model$lambda.min #best_model &lt;- glmnet(x, y, alpha = 1, family = &quot;binomial&quot;, lambda = best_lambda) #coef(best_model) Best Subset Selection; Evaluate all possible subsets of predictors and select the best model based on a criterion like AIC, BIC, or adjusted R-squared. Benefits: Exhaustive Search: Considers all possible combinations of predictors, ensuring the best possible model according to a specified criterion (e.g., AIC, BIC). Model Performance: Often provides the best fit in terms of the criterion used for selection. Flexibility: Can be used with any type of regression model and can include interactions and polynomial terms. Drawbacks: Computationally Intensive: Becomes impractical with a large number of predictors due to the exponential increase in the number of models to be evaluated. Overfitting: Risk of overfitting, especially when the number of predictors is large relative to the number of observations. Multicollinearity: Does not address multicollinearity among predictors, which can lead to unstable coefficient estimates. library(leaps) #best_subset &lt;- regsubsets(response ~ ., data = data_reduced, nbest = 1, really.big = TRUE) #summary(best_subset) Model Averaging; Perform model averaging to account for model uncertainty. Benefits: Accounts for Model Uncertainty: By averaging over multiple models, it incorporates model uncertainty into the final predictions. Improved Predictive Performance: Often leads to better predictive performance by averaging out the errors of individual models. Flexibility: Can be applied to various types of models and selection criteria. Stability: Provides more stable estimates by considering multiple models rather than relying on a single best model. Drawbacks: Complexity: Results in a more complex model that can be harder to interpret compared to selecting a single best model. Computationally Intensive: Requires fitting and averaging a large number of models, which can be computationally demanding. Weight Selection: The choice of weights for averaging (e.g., based on AIC, BIC, or cross-validation errors) can influence the final model and may not always be straightforward. Potential for Overfitting: If not carefully managed, averaging over too many models can still lead to overfitting, particularly if the individual models are not regularized. library(MuMIn) options(na.action = &quot;na.fail&quot;) #global_model &lt;- glm(response ~ ., data = data_reduced, family = binomial) #dredged_models &lt;- dredge(global_model) #averaged_model &lt;- model.avg(dredged_models, subset = delta &lt; 2) #summary(averaged_model) "],["plot-establishment.html", "Chapter 16 Plot establishment", " Chapter 16 Plot establishment One other thing to consider, is the placement of plots across the landscape. Though this is step one of the process, since many folks will have previously established plots, we will discuss here! Spatially balanced random sampling offers several benefits, particularly in ecological and environmental studies. Here are some of the key advantages: Improved Representativeness Coverage: Ensures that the sample points are spread out evenly across the study area, which helps in capturing the spatial heterogeneity of the environment. Reduction of Bias: Minimizes the chances of over-sampling or under-sampling specific areas, leading to more accurate and generalizable results. Enhanced Statistical Efficiency Reduced Variance: By evenly distributing sample points, spatially balanced sampling often reduces the variance of the estimates compared to simple random sampling. Better Inference: Provides better estimates of population parameters and improves the precision of spatially explicit models. Flexibility and Adaptability Multiple Scales: Can be applied at various spatial scales, making it suitable for different types of studies ranging from local to regional levels. Integration with GIS: Easily integrated with Geographic Information Systems (GIS) to facilitate sample design and data collection. Cost-Effectiveness Efficient Use of Resources: Reduces travel time and costs associated with fieldwork by ensuring that sample locations are optimally distributed. Focused Sampling Effort: Enables targeted sampling in areas of interest while still maintaining a representative coverage. Robustness to Spatial Autocorrelation Handling Spatial Dependencies: Helps in accounting for spatial autocorrelation by ensuring that samples are not clustered, which can lead to more reliable statistical analyses. Examples of Applications Ecological Monitoring: Used to monitor biodiversity, habitat quality, and species distributions across large landscapes. Environmental Assessment: Applied in studies assessing soil contamination, water quality, and air pollution to ensure comprehensive spatial coverage. Resource Management: Useful in forestry, agriculture, and fisheries to assess the distribution and abundance of resources. Key Methods and Tools Spatially Balanced Sampling Algorithms: Such as the Generalized Random-Tessellation Stratified (GRTS) design. R Packages: spsurvey package in R provides tools for implementing spatially balanced sampling designs. library(spsurvey) # Define the study area and number of sample points #study_area &lt;- as.spatialPolygons(my_shapefile) #n_samples &lt;- 100 # Generate spatially balanced sample points #sample_points &lt;- grts(study_area, n = n_samples) # Plot the sample points #plot(study_area) #points(sample_points, col = &quot;red&quot;) "],["before-after-control-impact.html", "Chapter 17 Before-After-Control-Impact 17.1 Which design 17.2 BACI Design 2", " Chapter 17 Before-After-Control-Impact 17.1 Which design Four standard BACI DesignsFour standard BACI designs: 1. Single impact site; single control site; one year before; oneyear after 2. Single/multiple impact site; multiple control sites; one yearbefore; one year after 3. Single impact site; single control site; multiple yearsbefore; multiple years after. 4. Single impact site; multiple control sites; multiple yearsbefore; multiple years after See this tutorial 17.2 BACI Design 2 #test1 &lt;- lmer(responsevariable ~ GrazingTreatment+Time+GrazingTreatment:Time, random~1|Plot/Time, data=yourdata) "],["time-is-a-flat-circle.html", "Chapter 18 Time is a flat circle", " Chapter 18 Time is a flat circle "],["spatial-data.html", "Chapter 19 Spatial data", " Chapter 19 Spatial data "],["species-distribution-modeling.html", "Chapter 20 Species Distribution Modeling 20.1 Introduction 20.2 A brief review of niche theory 20.3 Downloads for this lab 20.4 Objectives 20.5 Methods", " Chapter 20 Species Distribution Modeling 20.1 Introduction Species distribution models (a.k.a.; ecological niche models, habitat models) relate environmental predictors like climate, elevation, or soil characteristics to species presence or abundance. These relationships are used to project likelihood of occurrence across space, by calculating the likelihood of occurrence across the study area using values associated with raster maps of the environmental variables in the model (Fig. 1). Most SDMs are correlative models that mathematically describe observed patterns of occurrence, and that do not incorporate underlying mechanisms in model projections. Understanding the limitations of correlative models (discussed below) is important for deciding when to use these models and interpreting your results. Figure 1. Raster layers are stacked to predict likely habitat. 20.2 A brief review of niche theory In spatial and population ecology, we define a species’ niche as all the conditions under which populations of a species maintain growth rates that are at or exceed replacement rates. The niche is often defined in n dimensional space, since so many factors contribute to the performance of a species (Fig. 2). In ecology, there tends to be a lot of confusion about spatial niche concepts, since ecology students often first learn about species niches from an evolutionary standpoint. In evolutionary ecology, a species’ niche is defined by a suite of traits possessed by an organism related to how this species ‘makes its living’ (attains food, nutrients, water). However, niche concepts and definitions are inherently related, and broadly describe the role of species within ecosystems. Figure 2. Species niches are complex. In theory, we could describe the niche of a species by adding N number of axes and create a cloud representing all the habitats where a species could persist. Ecologists break a species niche into two components: the fundamental niche and the realized niche. The fundamental niche is similar to the Grinnellian niche concept (Introduced by Joseph Grinnell in is 1917 paper, The niche relationships of the California Thrasher) niche concept. The fundamental niche of the species describes all the abiotic conditions that a species can physiologically tolerate and maintain population growth at or above replacement rates (we don’t count areas where species persist, but are not maintaining themselves; these area are known as demographic sinks). The realized niche refers to all of the areas that we actually observe a species on the landscape. The concept emerged out of the work of Elton, who highlighted the importance of species interactions in defining species distributions. The realized niche, therefore, reflects the combined effects of the abiotic and biotic environment on persistence across the landscape. Typically, the fundamental niche is larger than the realized niche. In other words, a species can be found in a lot of places, but processes such as competition for resources or predation, reduce the total area occupied by a species. In some cases, the realized niche can be larger than the fundamental niche. This occurs when positive species interactions, like mutualisms or facilitation, allow a species to overcome some sort of environmental resistance and occupy sites that would be inhospitable for the species without ‘help from a friend’. Finally, stochastic events, like disturbances, or landscape features, such as barriers to dispersal, can influence where as a species is found on the landscape. The distinction between fundamental and realized niches is important in order to understand the limitation of correlative SDMs, since these models cannot distinguish between the fundamental and realized niche of a species. Recall that species distribution models relate environmental factors to current occupation of a species. In most cases, habitat suitability is predicted using abiotic factors, including climate, soils, topographic information; all of which essentially describe the fundamental niche of a species (i.e., physiological tolerance to abiotic characteristics). However, the data used to build these models quantifies the current occupation of a species on the landscape, or the realized niche. This presents several issues: While we can learn generally about the abiotic factors that affect a species range, it is not a perfect picture of these tolerances, since distribution is affected by a variety of factors not included in the model. When a species fundamental niche and realized niche are really different due to factors not included in the model, current habitat projects can be inaccurate. This is a common problem when modeling habitat suitability for wild-harvested species, since they are underrepresented in suitable habitat, because those habitats are targeted for harvest. Using these models to predict future suitability should be interpreted skeptically. Future habitat suitability predictions are strong working hypotheses for ecological investigations or management actions. We don’t actually expect many species to track their bioclimatic niches, since many of the factors that influence a species range aren’t directly measured when building SDMs. SDMs are particularly unreliable when factors that shape a species niche change as a function of climate change. For instance, species interactions shape species distributions, and since species respond idiosyncratically to climate change, represent a major source of uncertainty in model predictions. These caveats and limitations, particularly related to your data, should be included in the discussion of your results. All of that said, SDMs can provide us with a lot of information with relatively little effort, and are often the best hypothesis on which to base decisions. 20.3 Downloads for this lab Create a file on your desktop called ‘speciesdistributionmodels’ and place the following files within it: R script for creating the SDM Occurrence data Worksheet to turn-in 20.4 Objectives Project the effects of climate change on Pinus ponderosa in Arizona. 20.5 Methods 20.5.1 Let’s get modeling Now, let’s walk through an example to discuss the various considerations and options for creating SDMs. For this exercise, we will use bioclimatic variables as environmental predictors. Bioclimatic variables are derived from downscaled climate models and created to be more more ecologically relevant compared to simple temperature and precipitation means. Bioclimatic variables are a great first step in model building, but depending on your study species and area, you may need to download finer scale layers or include other factors, like soil data. Resolution Raster layers are spatially mapped grids comprised of hundreds, thousands, or millions of cells (aka pixels) with values related to a variable assigned to each pixel. The smaller the pixel, the higher the resolution, but this greatly affects processing speed and may exceed computer storage. Map projections Different methods are used to project the 3D earth into a 2D map. We have to specify the projection of the layers used in our models or our data layers will not align properly. In the example below, we will specify a coordinate reference system (CRS), which defines, with the help of coordinates, how the projected map relates to locations on the earth. A CRS contains the following information: Coordinate system: The X, Y grid that defines where a point is located in space. Horizontal and vertical units: The units used to define the grid along the x, y (and z) axis. Datum: A modeled version of the shape of the Earth which defines the origin used to place the coordinate system in space. You will learn this further below. Projection Information: The mathematical equation used to flatten objects that are on a round surface (e.g. the Earth) so you can view them on a flat surface (e.g. your computer screens or a paper map). Luckily, we can pull all of this information from a spatial object, use the CRS function and reproject our data so that we are working with all data using the same CRS. Let’s start by installing and loading the libraries that we will need for our analysis, and by importing both current climate and future climate projections. For this exercise, we will download bioclimatic variables to characterize current climate. Bioclimatic variables are variables derived from mean, maximum and minimum temperature and precipitation data summarized from weather station data from across the globe, then interpolated based on various landscape features, most importantly elevation, in order to assign climatic values to locations with no climate stations. These bioclimatic variables have been created in order to represent climate data in a way that is biologically-relevant. Specifically, these bioclimatic include: Annual Mean Temperature (bio1) Mean Diurnal Range (Mean of monthly (max temp - min temp); bio2) Isothermality (bio3), Temperature Seasonality (standard deviation ×100; bio4) Max Temperature of Warmest Month (bio5) Min Temperature of Coldest Month (bio6) Temperature Annual Range (bio7) Mean Temperature of Wettest Quarter (bio8) Mean Temperature of Driest Quarter (bio9) Mean Temperature of Warmest Quarter (bio10) Mean Temperature of Coldest Quarter (bio11) Annual Precipitation (bio12) Precipitation of Wettest Month (bio13) Precipitation of Driest Month (bio14) Precipitation Seasonality (Coefficient of Variation; bio13) Precipitation of Wettest Quarter (bio16) Precipitation of Driest Quarter (bio17) Precipitation of Warmest Quarter (bio18) Precipitation of Coldest Quarter (bio19). Additionally, we will download climate projections. Climate projections are generated by Global Climate Models, which predict future climatic conditions based on complex algorithms describing the atmosphere. In order to project future species distributions, we need to select a particular climate model and a time period for which we are making predictions. Here, we are projecting suitable habitat for the time period 2061-2080. Since several facilities equipped with climate models generate climatic projections, we also have selected the CNRM-CM6-1 modeling group. Finally, we select the ‘socio-economic pathway’ utilized by our climate model. The degree of warming that occurs depends primarily on decisions that humans make around fossil fuel use and other climate mitigation strategies. The Intergovermental Panel on Climate Change (IPCC) works with social scientists, legislators and other to generate possible carbon use futures, which are then used to generate climate predictions. Here, we will use the Shared Socio-economic Pathway (SSP) ‘585’. Take a minute to search SSP 585. Record the answers to these questions on your worksheet: Provide a description of SSP 585. How do countries respond to climate change in this scenario? What climate-related technologies are assumed to be used in this future? Is a low, middle-of-the-road, or high degree of warming predicted in this scenario? If you were to repeat this exercise, which SSP would you choose and why? 20.5.2 Run the R code Load your R file run and walk through the exercise. Now that we’ve loaded our current and future climate models, let’s input our occurrence data. For our data on species occurrence, we will use data from the Global Biodiversity Information Facility (GBIF), a repository for species observations and locations derived from multiple sources, including citizen science, herbarium and museum collections. The GBIF data are biased by observer behavior, since many observations are derived from citizen science projects. Humans tend to collect data from easily accessed areas around roads, popular hiking trails or congregating areas. One way to reduce bias in presence only data is to use spatial thinning to reduce weighting observations from heavily trafficked areas more than observations made in other areas. There is no standard thinning distance, but it is typical to require a minimum of 5 km between observations. If you are working at smaller or larger scales, you could reduce or increase this distance! Also, if you are using presence and absence data or have employed an unbiased sampling strategy to collect data, you can skip this next step. While there are various quality control measures used by GBIF to ensure high data quality, we will run a few other data cleaning codes to remove anomalous points. This step is not necessary if using nonGBIF data! However, when producing your own spatial datasets, some form of data quality control is necessary. For this exercise, we will investigate whether models predict that Ponderosa pine forests that surround Flagstaff are predicted to persist as climate changes. Let’s download occurrence data for Ponderosa pine (Pinus ponderosa). Great! Now we have data! Let’s build an SDM. Our first decision point on model construction is based on the response variable. In this case, we have presence-only data; in other words, no one went out to the field to confirm locations where a species is absent across the landscape. Since location information often suffers from absence of absence data (ha), researchers have found statistical workarounds, which produce amazingly similar results to models fitted with presence or absence data. The method most commonly used to model habitat suitability with presence-only data involves generated numerous background points across your study area to compare with areas where your species is present. Note that your focal species could occur at any one of these background points, but that doesn’t matter. Essentially your model is characterizing habitat available to the species and the habitat of known occurrence to identify the environmental factors that best distinguish occupied habitat. Let’s breakdown the major model types used for SDMs: Profile techniques Profile techniques are simple algorithms that use environmental distance to known sites of occurrence to ‘profile’ habitat characteristics. These techniques are rarely used any more, so I won’t discuss further! Profile techniques include: Mahalanobis distance Ecological niche factor analysis (ENFA) Isodar analysis Bioclim Regression-based approaches You are familiar with regression based approaches from other statistical analyses! All regression approaches build upon standard regression models (Fig. 3), but differ in subtle ways to address common challenges to data modeling, like issues of nonnormality or heterogeneity of variance. Figure 3. Regression basics. Term y is related to term x. If the slope of the line differs significantly from 0, then there is a relationship between the variables. Error terms are derived from the residuals of the model; how much each individual point deviates from the line of best fit. The best fit is determined by repeatedly mapping lines across the data to identify that which most reduces error. Regression-based techniques include: Generalized Linear Modeling (GLM) (parametric) Flexible Discriminant Analysis (FDA) (parametric) Multivariate Adaptive Regression Splines (MARS) (nonparametric) Generalized Additive Modeling (GAM) (nonparametric) Generalized Linear Models are a flexible form of regression models. GLMs are ‘generalized’ by using a link function to relate the linear model to the response variable (which can be binomial, continuous, count data or other) and by relativizing the variance of each model term to its predicted value. Generalized Additive Models incorporate ‘smoothing functions’ to allow nonparametric estimates to be generated using a Bayesian approach. Multivariate Adaptive Regression Splines automatically models data nonlinearities and interactions between variables. Flexible Discriminant Analysis uses optimal scoring to transform the response variable so that the data are in a better form for linear separation. Machine learning approaches: Machine learning techniques use training data to ‘learn’ about the dataset in order to make predictions. Machine learning approaches include: Random Forest (RF) Boosted Regression Trees (BRT) Maximum Entropy (MaxEnt) There are other machine learning techniques, like Artificial Neural Networks (ANN), but the list above is most commonly used for distribution modeling! Random forest and boosted regression trees are similar, in that they create different ‘trees’ by iteratively bifurcating the dataset using predictor factors and identifying the tree that best predicts species occurrence. Figure 4. An illustration of random forest tree construction. MaxEnt models are a little different. According to the principle of maximum entropy, high entropy is when the probability distribution best represents the current state of knowledge about a system, in the context of precisely stated prior data. These models evaluate the set of all trial probability distributions that would encode the prior data and select the distribution with maximal information entropy. 20.5.2.1 Model selection The world of species distribution modeling is a contentious one! Many leaders in the field have their own ‘pet’ models that invariably they helped to develop software or methodology for! The general consensus is that each of the different modeling techniques has various strengths and weaknesses, and they should be combined into ensemble models for habitat predictions. However, other approaches exist. One line of thinking in distribution modeling is to use solely GLMs, spending great care to identify critical predictor variables in a way that is tied to current ecological understanding and that reduces nonlinearities among these variables. By taking these steps, models are created, which in theory, should provide better inferential power for both current and future habitats. For presence only data, maximum entropy models are generally considered an excellent model choice. In my experience, there is no perfect model, rather model accuracy varies from species to species. For this reason, I typically build ensemble models to integrate the strengths of different model types. 20.5.2.2 Build an SDM Deal with environmental predictor colinearity In general, it is recommended to avoid having correlated features (variables that have different numbers, but are following the same pattern) in your dataset. Indeed, a group of highly correlated features will not bring additional information to our analyses, but will increase the complexity of the algorithm, thus increasing the risk of errors. Including highly correlated variables in models also, in essence, weights the correlated variables more than independent variables, again leading to less accurate model outputs. In other words, we need to remove highly correlated variables. We will do this by generating Variable Inflation Factor (VIF) values, a measure of collinearity, for all predictor variables. Then, we will remove one of the two correlated variables. Build the dataframe for the SDM Building the SDM, requires two additional steps. In the first, we assemble the final dataset to be used in the model. Using the sdmData function, we indicate the following: The column that contains presence data. The environmental predictors. Absence data or how to create background data. Specify model evaluation parameters When we build the final model using the SDM function, we specify replication. Replication is the method used to partition the dataset into training and test data. Ideally, we would have collected completely independent training and test datasets; however, I’ve never actually seen this done, except for researchers who are investigating SDM methods. Ninety nine point nine percent of the time, datasets are split into test and training datasets. As the names imply, training data are used to build the model, and then test data are used to measure how good our predictions are by quantifying how often our model correctly predicts presence or absence. Splitting or partitioning data into test and training datasets is often conducted several times, since outcomes may depend on the test or training data used to build and evaluate models. There are several methods to create training and test datasets. The three available in the package that we will use are subsampling (sub), crossvalidation (cv), bootstrapping (boot). For sub and boot, you must indicate what proportion of test and training data. A 30% test data, 70% training data split is common (test.percent=30). Finally, you will also the models how many times to repeat evaluations using the n equals code. This can eat up a lot of memory, so I typically use an n of 5. Choosing the evaluation model Crossvalidation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods, multiple rounds of crossvalidation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance. Crossvalidation does not rely on random sampling, but rather splits the dataset into k unique subsets. This is the preferred method for spatial model evaluation and estimating generalization capability. Note that you have to select the number of ‘folds’ or data partitions, which is typically set at 5. Bootstrapping iteratively creates separate datasets from randomly sampling with replacement. Bootstrapping it is not as strong as crossvalidation when it is used for model validation, since it contains repeated elements in every subset. Bootstrapping is typically repeated 30 times in SDM model evaluation! Subsampling randomly splits the dataset into training and test datasets, but doesn’t maintain the independence of the datasets. In other words, due to random sampling, you might wind up with similar training and test datasets in each trial. For this reason, the more structured crossvalidation method is typically preferred. Build the SDM Once the data is appropriately compiled, we use the sdm function to build the actual model. Within this function, we specify: The column that contains presence or presence or absence The dataframe that we are using (d1) The types of models that we are using Replication type (cv), number of folds (5), how many times to repeat partitioning (1) THIS STEP WILL TAKE SOME TIME - JUST LET THE PROGRAM RUN! The model object (m1) tells you several things. First, it gives a brief summary of the model you ran. You can double check this to be sure that the model did what you told it to do. Here, everything seems fine: We ran a model for one species, we used two modeling methods, glm and maxent, we used cross_validation with 5 partitions. The model runs were successful (100% each). Finally, we are provided with 4 measures of model performance: AUC, COR, TSS, and Deviance. What types of models are we using to predict habitat suitability for Ponderosa pines (i.e., machine learning, regression, profile techniques)? 20.5.2.3 Model evaluation explained AUC stands for Area Under the Curve. AUC refers to a ROC plot, which plots sensitivity over 1 minus specificity. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. AUC is desirable for SDM model evaluation for two main reasons: AUC is scale invariant. AUC is classification threshold invariant. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen. This curve plots two parameters: True Positive Rate (Sensitivity): the proportion of presences correctly predicted as presence, False Positive Rate (1 minus Specificity): The specificity denotes the proportion of absences that are correctly predicted as absence, so the false positive rate indicates how many times the model predicted an occurrence when there was none. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0. As a rule, an AUC &gt; 0.75 indicates a high performing model. SDMs predict a probability of occurrence across the landscape. Different thresholds can be used to create a cutoff to predict presences or absences. For instance, we could say that if there is a 90% chance or more that a pixel is suitable habitat, then we consider those areas as occupied. We want to identify a cutoff that maximizes true presences, while minimizes false positives (i.e., areas that you incorrectly say contain a population, but don’t). You can see that as you decrease the cutoff, from say 90% to 70%, then your likelihood of correctly predicting presences goes up, BUT so does your likelihood of false positives. So, let’s check out the ROC plot below. Looking at AUC, a common form of model performance assessment, which is the best performing model? Generally, there is strong agreement between the test and the training data. As you increase the cutoff threshold, the likelihood that you correctly assign presence goes up, but so does the false positive rate (Fig. 5). Note that on the far right hand side of each ROC plot, if the cutoff is high enough, you will have a 100% true positive rate, and a 100% false positive rate (the cutoff is so low, that all habitats are predicted to support the focal species). Alternatively, with a low enough cutoff (left hand side of the ROC plot), you won’t have any positives or any false positives! This AUC cutoff will be important for building ensemble models; explained below! Figure 5 This figure (lifted from an ARCGIS website) shows the relationship between omission rates and ROC plots. So if we want an Omission Rate that is slightly less than 10%, we can use 0.29 as the Cutoff instead, and in this case, we will pick up 20.48% background points as potential presence locations, which is also a good rate. Like AUC, True Skill Statistic (TSS) values are calculated across the range of possible thresholds for classifying model scores.The TSS similarly incorporates sensitivity and specificity comparing models against random, yielding values that range from negative 1 to positive 1, where positive 1 indicates perfect prediction and greater than or equal to 0 indicates a model that performs no better than random. TSS is typically considered a better indicator of model performance for presence only models. A TSS of 0.5 or higher indicates high model performance. Pearson correlation (COR) between the predicted likelihood of presence and the presence or absence testing data. Deviance Lastly, if a model is interpreted as estimating species’ probability of presence, rather than just giving an index of habitat suitability, then the model predictions can be evaluated using deviance, defined as 2 times the log probability of the test data. 20.5.2.4 Ensemble model assembly Finally, we will merge models into an ensemble model. You may want to exclude models that didn’t have high predictive performance. We will give higher weights to the models with higher accuracy, in this cause using the TSS score. 20.5.2.5 Investigating model components We can run code to look at the model components that best predict presence. According this figure, which climatic variable best predicts habitat suitability for Ponderosa pine? 20.5.2.6 Convert to presence or absense predictions We use the test statistics to identify a threshold that maximized true positives and reduced false negatives. In order to do this, we will create a new raster and populate it with predictions of presence, using that threshold. 20.5.2.7 Plotting and predictions Let’s take a quick look at the predictions we have created for the current time period. To predict response of your focal species to future climate, just plug the novel climate conditions into the model! Let’s run this model. Now, let’s plot this prediction against our original! First, we’ll take a zoomed out look, then we will focus into our region! Let’s convert to predicted occurrence and plot! According to these maps, how will the amount of suitable habitat for Pinus ponderosa in Flagstaff change if climate change continues along the SSP 585 projection? How certain are you of these projections? Why might these models NOT be accurate? What type of vegetation do you think might be more common around Flagstaff as climate changes? Submit your answers to the questions presented throughout this tutorial and the figures that you generated (Occurrence of Pinus ponderosa currently and in the future) to your TA. "],["bootstrapping-and-resampling.html", "Chapter 21 Bootstrapping and resampling", " Chapter 21 Bootstrapping and resampling "],["population-sensation.html", "Chapter 22 Population sensation 22.1 Population characteristics 22.2 Planning a census 22.3 Life cycle diagrams 22.4 Population demographic models 22.5 Integral projection models 22.6 An example: Population ecology of an endangered plant 22.7 Putting it all together 22.8 Generating confidence intervals 22.9 Different life histories", " Chapter 22 Population sensation Populations contain multitudes - numerous interacting individuals of the same species with slightly different genetic backgrounds, sharing genes, colonizing new areas, responding to environmental stimuli. Populations are hot-pots of evolution and essential for understanding species viability, especially when species are rare. Population-level studies have contributed fundamental ecological knowledge in evolution, range dynamics, and species conservation. Now that we are all jazzed about populations, let’s learn about how to describe them and their dynamics quantitative! May your knowledge acquisition be exponential! 22.1 Population characteristics In population ecology, we commonly describe populations in terms of size and structure. Size is simply the number of individuals in a population and structure refers to the proportion of individuals across age or size classes. For rare species or species with low population sizes, population size can be directly estimated through a census. Just as it seems, a census is when each individual is counted. We often collect information pertinent to understanding demographic vital rates of individuals that are censused - just like we do in the U.S. census. For plants, rather than collect information on religious background or age as we would for populations of humans, we typically record the size of the plant and other salient characteristics derived from an understanding of the species’ breeding system, dispersal mechanism, life history, and ecology. When species are abundant, we use other methods to estimate population size, such as density (i.e., how many individuals occur within a particular area) or sub-sampling (i.e., collecting census data within a plot). If a species if cryptic, meaning hard to observe (as is the case for most animals), we use mark-recapture or occupancy modeling to estimate population size and structure. 22.2 Planning a census Censuses are the ideal method for collecting demographic data, since it allows in-depth and comprehensive examination of the population (and it’s dynamics!). Let’s discuss some considerations for establishing a census protocol. When conducting a census, you want to plan on capturing critical life events and ensure that you are able to capture those life events through time (for multiple years). Let’s look at an example and establish a plan to census individuals within a population of this hypothetical species. The figure above shows a typical phenological pattern for an understory plant species in the eastern deciduous forest. These plants are dormant throughout the winter when temperatures are cold enough to freeze plant tissue. All plants, adults and new seedlings, emerge in the spring, and grow to their full size for the summer over a few week period in early spring. Then, they begin flowering and seed development in late summer, then disperse seeds into the fall. The first census indicated in this figure takes place after emergence in the spring once plants have reached their final size for the growing season. The census is planned to occur as early as possible post-growth in order to assess recruitment (new seedlings that enter the population), since seedlings tend to start to die-off through time. Seeds of this species require an 18-month stratification period prior to germination. In order to census across all members of the population, including seeds, we need to include the second census to quantify the number of seeds present at census 1, since counting seeds post-dispersal is almost impossible. Note that we could capture individuals across all important classes, adult plants, seedlings and seeds, if we conducted a single census at time point 2. Why not conduct a single census? In this case, the researchers were interested in causes of mortality of plants, particularly new germinants, across the growing season. By conducting the census in the spring, they were able to identify all plants that emerged that year and note if those plants were lost during the growing season and in many cases ascribe a reason for the mortality of the plant. 22.3 Life cycle diagrams In the above example, the census was timed to measure the performance and fate of important classes within the population. A useful tool for developing census protocols and demographic models is called a life cycle diagram. A life cycle diagram indicates important life stages and possible transitions among those stages. Continuing with our example above, we develop the following life history diagram. Here, you will note several important demographic characteristics of this species. First, on the right hand side of the diagram, you can see that the researchers have classified plants into several groups: new seedlings, &gt;1 year old seedlings, juveniles, small adults and large adults. These categories were selected because individuals within the categories share similar rates of reproduction and survival (vital rates). On the left hand side of the diagram, you will see that this species forms a seedbank and that seeds within this seed back persist around 45 months. 22.4 Population demographic models 22.5 Integral projection models Instead of binning individuals into stages, Integral Projection Models (IPMs) are built upon linear models that relate a state variable (usually a metric of size in plants) to important vital rates. 22.6 An example: Population ecology of an endangered plant Pectis imberbis is an endangered plant species endemic to southern Arizona. In 2019, several researchers from NAU traveled to the largest population of P. imberbis in the state located on the Coronado National Memorial. They tagged, mapped and measured a sub-sample of this population in order to identify an appropriate state variable for P. imberbis. Using model selection tools (i.e., Aikaike Information Criterion (AIC)), the size metric that best explained variation in growth, survival and reproduction was selected. For P. imberbis, researchers explored a variety of state variables, including number of leaves, length of the longest leaf, stem number, height, and composite variables based on this metrics (like leaf number*number of leaves to estimate total plant foliage). In this case, height was the best predictor of performance and will serve as the state variable for demographic analyses. First, let’s load a few important packages and set our working directory. #Note that you have to go to the CRAN website to install IPMpack for the first time: #install.packages(&quot;IPMpack&quot;, repos = &quot;http://R-Forge.R-project.org&quot;, type = &quot;source&quot;) #devtools::install_github(&quot;CRAN/IPMpack&quot;) #Load packages #library(IPMpack) library(fields) library(car) library(tidyverse) library(grid) library(gridExtra) library(ggthemes) #remember in your own work that is it really convenient to set up a working directory! Here is an example: #setwd(&quot;/Users/sks379/Desktop/pectisanalyses/FinalPectisAnalyses&quot;) #laptop Once a state variable was identified, the team went back the next several years and collected demographic data on all plants. Let’s build our model for the first pair of transition years. #read in data (it needs to be imported as a dataframe names pectisdata) #pectisdata &lt;- read.csv(&quot;/Users/sks379/Desktop/pectisanalyses/FinalPectisAnalyses/FinalTransitionMatrices/exampleIPMendangeredplant.csv&quot;, header=TRUE) url &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=10vtDeB4yXtic7lLa8oTpCDQft10vtYDM&quot; pectisdata &lt;- read.csv(url) #exclude poor quality data #pectisipm &lt;- dplyr::filter(pectisdata, Exclude == &quot;0&quot;) #prepare the dataset pectis &lt;- as.data.frame(pectisdata) pectis$size &lt;- as.numeric(as.character(pectis$size)) ## Warning: NAs introduced by coercion pectis$sizeNext &lt;- as.numeric(as.character(pectis$sizeNext)) head(pectis) ## id size surv sizeNext fec1 fec2 Site DeerBrowse2020 BrowseLevel2020 Cover2020 ProtectedFRBrowse2020 ## 1 1 69.3 0 NA 1 154.5 CNM N &lt;NA&gt; L N ## 2 4 57.7 1 58.4 1 30.9 CNM Y L L N ## 3 6 68.5 1 48.3 1 72.1 CNM N &lt;NA&gt; BG N ## 4 7 101.6 1 57.4 1 257.5 CNM Y L L N ## 5 8 57.9 1 68.0 1 370.8 CNM Y L BG N ## 6 10 84.7 1 18.5 1 1339.0 CNM N &lt;NA&gt; BG N ## Notes2020 Notes2021 ## 1 ## 2 ## 3 INSECT DAMAGE ## 4 ## 5 ## 6 Notice the structure of the dataset! This dataframe tracks the fate of all individuals in the population from time 1 to time 2. The ‘id’ column corresponds with the tag number for each plant. The ‘size’ column contains sizes (in this case heights) of individuals in time 1, while ‘sizeNext’ indicates the size at time 2. In the ‘surv’ column, the fate of the individual at time 2 is indicated by either a 0 (died) or 1 (survived). For individuals that died, we place an ‘NA’ in the sizeNext column, since that individual didn’t exist that year! Reproduction is always the most complex part of demographic modeling for plant species. Often, a reproductive cycle includes cryptic stages (seeds!) that are hard to track, may include vegetative and sexual reproductive components, and may involve several processes (like flower production and seed production) that may be important to examine to understand why population growth rates vary! Pectis imberbis forms a seed bank with seeds remaining viable for at least 3 years (this was determined through a seed cage experiment). We do not include seed bank dynamics in this model, since seed dynamics are difficult to track across multiple sites and years, and manipulating seeds seems to affect their fate (germination, viability). For this reason, we have built the models such that seeds produced in year 1 has the potential to germinate in year 2 or perish. Luckily, ignoring the seed bank for species with high rates of seed production has negligible affects on population growth, and this pattern (higher rates of viability in the year following seed production) reflects the biology of this species. The ‘IPM package’ recognizes seedlings as seedlings based on the data structure. A new seedling won’t have a size, survival or reproductive values in year 1 (NAs are added to this column), but will have a measurement in the ‘sizeNext’ column. We’ve broken reproduction into two components, fec1 and fec2. If an individual reproduced, we mark a ‘1’ in fec1, if the individual didn’t reproduce, then we add a ‘0’ to the fec1 column. For those individuals that did reproduce, the fec2 column contains the number of seeds produced. Again notice that if an individual didn’t reproduce, we place an ‘NA’ in the fec2 column. Finally, we’ve documented other observations for each individual that we think might explain population growth rates, like whether a plant has been browsed by deer or is in competition with other vegetation. IPMs are only as good as the linear models that comprise them! As a critical first step in analyses, we have to take a look at the relationship between our state variable and growth, reproduction and survival. We want to be sure to address any issues with normality and unequal variance. First, examine variance; if variation depends on the state variable, include this in the model (we’ll do this in a minute). It is fairly common in plants to see, for instance, that growth rates are more variable in larger individuals. #evaluating data for IPM construction growth &lt;- lm(sizeNext ~ size, na.action = na.omit, data=pectis) resid &lt;- residuals(growth) shapiro.test(resid) ## ## Shapiro-Wilk normality test ## ## data: resid ## W = 0.29287, p-value &lt; 2.2e-16 plot(fitted(growth), residuals(growth)) abline(lm(residuals(growth)~fitted(growth))) This figure is showing a scatter plot of residuals on the y axis and fitted values (estimated responses) on the x axis. We use these scatterplots to detect non-linearity, unequal error variances, and outliers. The residuals should “bounce randomly” around the 0 line, indicating that the assumption that the relationship is linear is reasonable. You also show see that the residuals roughly form a “horizontal band” around the 0 line, indicating that the variances of the error terms are equal. Finally, we looking for a dataset in which no one residual “stands out” from the basic random pattern of residuals. Let’s look at these residuals another way and use label the outliers so that we can remove them if necessary! id.n = length(pectis$id) qqPlot(growth, distribution = &quot;norm&quot;, id.method=&quot;y&quot;, id.cex = 0.6, id.n=id.n, id.col = &quot;blue&quot;, id.location = &quot;ab&quot;) ## [1] 551 628 A quantile-quantile plot, often abbreviated as Q-Q plot, is a graphical tool used to assess whether a dataset follows a particular theoretical distribution, such as the normal distribution. It compares the quantiles of the observed data against the quantiles of the expected theoretical distribution. Here’s how to interpret a Q-Q plot: The x-axis of the Q-Q plot represents the theoretical quantiles from a specified distribution (e.g., the normal distribution). The y-axis represents the quantiles of the observed data. If the points on the Q-Q plot fall approximately along a straight line, it suggests that the data follows the theoretical distribution. Deviations from the straight line indicate departures from the assumed distribution. If points are above the line, it suggests that the observed values are higher than expected for that quantile. If points are below the line, it suggests that the observed values are lower than expected for that quantile. The ends of the Q-Q plot are often of particular interest. Deviations in the tails can indicate differences in tail behavior. Note to self: (In this example, there is no pattern to the residuals, so we don’t need to include size based variance models, but in final tutorial, maybe add a picture of size based variation (can grab pic from mee312146)). Uh oh. Our slope is wonky and there are those darn outliers. From a brief once-over, the outliers maybe driving the slope issue that we are observing. It is good practice to try to determine whether it is legitimate to remove outliers from any statistical analysis. In demographic studies, there are so many plants and many reasonable possibilities for why we may observe a strange transition from one year to the next. For example: 1) In the first year, someone measured the plant, but it had already been browsed by a deer and so was extra short, and then appears to grow like wild in the following year 2) Someone took data on the wrong space on the datasheet 3) Someone couldn’t find a mama plant, who actually died, so accidentally measured a baby that had germinated in a nearby spot Given this, unlike with standard statistical analyses, we tend to be less stringent about when we remove outliers. Let’s take a look at those residuals. row_index &lt;- 161 value &lt;- pectis[row_index,] print(value) ## id size surv sizeNext fec1 fec2 Site DeerBrowse2020 BrowseLevel2020 Cover2020 ProtectedFRBrowse2020 Notes2020 ## 161 191 71 1 47.3 1 154.5 CNM N &lt;NA&gt; PG N ## Notes2021 ## 161 #or you can generate the id and use that to subset (just looking at the id number from above) target_id &lt;- 68 row_data_index &lt;- pectis[pectis$ID == target_id, ] print(row_data_index) ## [1] id size surv sizeNext fec1 ## [6] fec2 Site DeerBrowse2020 BrowseLevel2020 Cover2020 ## [11] ProtectedFRBrowse2020 Notes2020 Notes2021 ## &lt;0 rows&gt; (or 0-length row.names) #check out the other outlier row_index &lt;- 84 value &lt;- pectis[row_index,] print(value) #note id number is 627 ## id size surv sizeNext fec1 fec2 Site DeerBrowse2020 BrowseLevel2020 Cover2020 ProtectedFRBrowse2020 Notes2020 ## 84 106 30.2 1 38.7 1 30.9 CNM Y M PG Y ## Notes2021 ## 84 Unsurprisingly, there is no ready reason why individuals 161 and 84 would be acting strangely BUT they do behave in weird ways!!! (Did someone miss a decimal point?) AND I want to favor the the numerous individuals that conform to the model. Let’s remove those one or more outliers! I take a gentle hand with this removal of outliers and will start with just removing 161. pectis &lt;- pectis[-c(161),] growth &lt;- lm(sizeNext ~ size, na.action = na.omit, data=pectis) par(mfrow=c(1,2),mar=c(4,4,2,1)) id.n = length(pectis$id) plot(sizeNext ~ size, xlab=&quot;Height (cm) year 1&quot;, ylab=&quot;Height (cm) year 2&quot;, data=pectis) qqPlot(growth, distribution = &quot;norm&quot;, id.method=&quot;y&quot;, id.cex = 0.6, id.n=id.n, id.col = &quot;blue&quot;, id.location = &quot;ab&quot;) ## 551 628 ## 550 627 abline(lm(residuals(growth)~fitted(growth))) Uh oh. Still not normal. Let’s remove additional outliers. pectis &lt;- pectis[-c(82, 84),] growth &lt;- lm(sizeNext ~ size, na.action = na.omit, data=pectis) resid &lt;- residuals(growth) shapiro.test(resid) ## ## Shapiro-Wilk normality test ## ## data: resid ## W = 0.29196, p-value &lt; 2.2e-16 par(mfrow=c(1,2),mar=c(4,4,2,1)) id.n = length(pectis$id) plot(sizeNext ~ size, xlab=&quot;Height (cm) year 1&quot;, ylab=&quot;Height (cm) year 2&quot;, data=pectis) qqPlot(growth, distribution = &quot;norm&quot;, id.method=&quot;y&quot;, id.cex = 0.6, id.n=id.n, id.col = &quot;blue&quot;, id.location = &quot;ab&quot;) ## 551 628 ## 548 625 abline(lm(residuals(growth)~fitted(growth))) Yay! This is looking better now. While we are looking at size data, let’s take a look set a couple of parameters that we will need to build an IPM. Set bounds for IPM============================ To integrate the kernel, we will need: 1) boundary points (the edges of the cells defining the matrix) 2) mesh points (the centers of the cells defining the matrix and the points at which the matrix is evaluated for the midpoint rule of numerical integration) 3) step size (the widths of the cells) Boundary points==== Because size based predictions are generated from regressions, you can sometimes get projected sizes that are biologically dubious. For this reason, we create bounds on how big or small plants can become. To do this, I use the size based data from the population, and create a max and min limits that coorespond to the min/max +/- 1 standard deviation minimumsize1 &lt;- min(pectis$size, na.rm=T); minimumsize1 ## [1] 23 minimumsize2 &lt;- min(pectis$sizeNext, na.rm=T); minimumsize2 ## [1] 8.7 maximumsize1 &lt;- max(pectis$size, na.rm=T); maximumsize1 ## [1] 116.4 maximumsize2 &lt;- max(pectis$sizeNext, na.rm=T); maximumsize2 ## [1] 631 standarddev &lt;- sd(pectis$size, na.rm=T); standarddev ## [1] 15.1035 minSize &lt;- minimumsize2; minSize #true minimum ## [1] 8.7 maxSize &lt;- maximumsize2 + standarddev; maxSize #1 standard deviation all adult plants year 1 ## [1] 646.1035 nBigMatrix = 100 #dictates bin size x&lt;-seq(from=0,to=135,length=1001) #Seems fairly standard to create a matrix with these dimensions x0&lt;-data.frame(size=x,size2=x*x) Alrighty, now let’s explore relationships that will form the rest of the model! Let’s start with survival. We will use the DHARMa package to test for deviation from a binomial distribution and identify outliers. #create a glm to predict survival survival &lt;- glm(surv ~ size, na.action = na.omit, family = &quot;binomial&quot;, data=pectis) # Install and load the DHARMa package #install.packages(&quot;DHARMa&quot;, dependencies = TRUE) library(DHARMa) residuals &lt;- residuals(survival, type = &quot;response&quot;) test &lt;- testResiduals(survival) #if you need to identify outliers object1 &lt;- simulateResiduals(fittedModel = survival) outliers(object1, lowerQuantile = 0, upperQuantile = 1, return = c(&quot;index&quot;, &quot;logical&quot;)) ## integer(0) #you can do this for flowering, but this year everything flowered so you will get an error flowering &lt;- glm(fec1 ~ size, na.action = na.omit, family = &quot;binomial&quot;, data = pectis) ## Warning: glm.fit: algorithm did not converge residuals &lt;- residuals(flowering, type = &quot;response&quot;) #test &lt;- testResiduals(flowering) Everything looks good with the survival (deviation and outlier tests are non-significant (n.s.))! Now, let’s look at seed production, which is count-based and conforms to a poisson distribution. The package below isn’t working - don’t worry about it at this time - it is unusual to do this anyway! reproduction &lt;- filter(pectis, fec1 == &quot;1&quot;) seeds &lt;- glm(fec2 ~ size, na.action = na.omit, family = &quot;poisson&quot;, data = reproduction) ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1009.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 741.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 772.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 988.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1266.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 473.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 473.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 545.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1215.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 339.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 566.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 762.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 566.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 329.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2317.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2657.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 700.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 360.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 319.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 319.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 659.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 916.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 350.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 597.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 865.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 679.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 679.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 896.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 648.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1678.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 607.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 453.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 556.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 360.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1524.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1586.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 957.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 659.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 896.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1277.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 360.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 339.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 885.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 350.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 556.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2544.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 700.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1575.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3543.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 947.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1884.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 298.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3162.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1112.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 865.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 937.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 710.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1503.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1915.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 319.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 607.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 4418.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2173.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 751.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 638.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1843.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 463.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1091.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2111.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2090.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 453.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 504.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 957.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 669.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 494.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 494.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 854.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1318.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2873.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 329.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 422.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 339.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 988.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 772.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 607.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 339.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1524.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 648.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 813.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 988.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1143.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3522.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3419.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1153.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 329.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 6942.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1318.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2482.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1297.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 556.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 473.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1060.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2101.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 329.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1884.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 360.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 566.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 329.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 772.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1174.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 288.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 298.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 885.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1812.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 350.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1060.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 937.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 638.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 473.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 535.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 350.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 329.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 453.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 422.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3738.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 957.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 339.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1884.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1050.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 484.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1617.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 587.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 741.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 576.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2080.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 669.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 896.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1205.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 463.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1112.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2348.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3883.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1472.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 710.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1163.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2204.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1658.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1369.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 350.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1977.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 319.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 854.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 566.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 597.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 453.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 669.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1400.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 957.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1143.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1102.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 339.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1575.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2173.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 844.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 772.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2070.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1019.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 638.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 690.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 937.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 566.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2925.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1452.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 782.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2049.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3687.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 597.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 587.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 82.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 648.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2183.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 875.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 896.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 422.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 854.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1287.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1091.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1380.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1483.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1709.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1256.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 834.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2544.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 865.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 4233.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1421.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 370.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 535.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 504.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 731.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 834.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 679.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 401.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 267.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1586.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1823.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 854.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 576.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1060.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2008.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1194.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 422.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3347.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 638.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1019.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 92.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 751.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 257.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 463.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 494.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 432.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 607.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 41.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 710.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1575.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 803.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 133.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 51.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 247.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 381.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 175.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 504.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 164.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 442.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 154.500000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 690.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 525.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 236.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 906.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 123.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 638.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 278.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 988.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1359.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 30.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1266.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 947.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 391.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 947.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1246.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 5088.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 854.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2801.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1050.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 2039.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1359.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 597.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 350.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 319.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 195.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 422.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 1163.900000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 298.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 3594.700000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 144.200000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 20.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 10.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 216.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 185.400000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 72.100000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 113.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 226.600000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 319.300000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 61.800000 ## Warning in dpois(y, mu, log = TRUE): non-integer x = 628.300000 residuals &lt;- residuals(seeds, type = &quot;response&quot;) qqnorm(residuals) qqline(residuals, add=TRUE) ## Warning in int_abline(a = a, b = b, h = h, v = v, untf = untf, ...): &quot;add&quot; is not a graphical parameter test_seeds &lt;- testResiduals(seeds) ## DHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = &#39;bootstrap&#39;. See ?testOutliers for details ## DHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = &#39;bootstrap&#39;. See ?testOutliers for details object2 &lt;- simulateResiduals(fittedModel = seeds) outliers(object2, lowerQuantile = 0, upperQuantile = 1, return = c(&quot;index&quot;, &quot;logical&quot;)) ## [1] 1 2 3 4 6 7 8 9 10 11 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 ## [29] 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 ## [57] 59 60 61 62 63 64 65 68 69 70 71 72 73 74 75 76 77 78 79 80 81 83 84 85 86 88 89 90 ## [85] 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 113 114 115 116 117 118 119 ## [113] 120 121 122 123 124 125 126 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 ## [141] 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 172 173 174 175 176 177 ## [169] 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 197 198 199 200 202 203 204 205 206 207 ## [197] 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 226 227 228 229 230 231 232 233 234 235 236 ## [225] 237 239 240 241 242 243 244 245 246 247 249 250 251 252 253 254 255 256 257 258 259 260 262 263 264 265 266 267 ## [253] 268 269 270 271 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 292 293 294 295 296 297 ## [281] 298 299 300 301 302 303 304 305 306 307 309 310 311 312 313 314 315 316 317 318 319 322 323 324 325 326 327 328 ## [309] 331 332 333 335 336 337 338 339 340 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 361 ## [337] 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 381 382 383 384 385 386 387 388 389 390 ## [365] 391 392 393 394 395 397 398 399 400 402 403 404 405 406 408 409 410 411 413 414 415 417 418 419 420 421 422 423 ## [393] 425 426 428 429 430 432 433 434 436 437 438 439 440 441 442 443 444 445 447 448 449 450 451 452 453 455 456 457 ## [421] 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 480 481 482 483 484 485 486 ## [449] 487 488 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 ## [477] 516 517 518 519 520 521 522 523 524 525 526 527 528 529 531 532 534 535 536 537 538 539 540 541 542 543 545 546 ## [505] 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 567 568 569 570 572 573 574 575 576 ## [533] 577 578 579 580 581 582 583 584 585 586 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 606 ## [561] 607 608 609 610 611 612 614 615 616 617 618 619 621 622 623 624 625 626 627 629 630 632 633 634 635 637 638 639 ## [589] 640 641 642 643 644 646 647 648 649 650 651 652 653 655 656 658 659 660 661 662 664 665 666 667 669 670 671 672 ## [617] 673 674 675 676 677 678 679 680 682 684 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 Let’s take a comprehensive look at our different regressions that will be used to build the IPM. #explore different regression models for relating growth, survival and reproduction to state variable #============================================= #explore vital rate data par(mfrow=c(3,2),mar=c(4,4,2,1)) plot(pectis$size,jitter(pectis$surv),xlab=&quot;Size (t)&quot;, ylab=&quot;Survival to t+1&quot;) plot(pectis$size,pectis$sizeNext,xlab=&quot;Size (t)&quot;,ylab=&quot;Size (t+1)&quot;) plot(pectis$size,jitter(pectis$fec1),xlab=&quot;Size (t)&quot;, ylab=&quot;Flowering probability&quot;) plot(pectis$size,pectis$fec2,xlab=&quot;Size (t)&quot;,ylab=&quot;Flower Head Number&quot;) hist(pectis$sizeNext[is.na(pectis$size)], xlab=&quot;Recruit Size&quot;, main=&quot;&quot;) All of these relationships look good. While we will retain the same state variable across all years, the nature of the relationship may change annually. For this reason, each year that we build a model, we will need to run model comparisons and select the most appropriate model. Let’s start with growth! #install.packages(&quot;IPMpack&quot;, repos = &quot;http://R-Forge.R-project.org&quot;, type = &quot;source&quot;) #devtools::install_github(&quot;CRAN/IPMpack&quot;) #see https://levisc8.github.io/ipmr/articles/ipmr-introduction.html #Load packages library(IPMpack) #growth model constuction growthModelComp(dataf = pectis, makePlot = TRUE, legendPos = &quot;bottomright&quot;, mainTitle = &quot;Growth&quot;) ## $summaryTable ## Exp. Vars Reg. Type AIC ## 1 sizeNext ~ 1 constantVar 5927.06461395333 ## 2 sizeNext ~ size constantVar 5920.14756906622 ## 3 sizeNext ~ size + size2 constantVar 5921.98357878777 ## ## $growthObjects ## $growthObjects[[1]] ## An object of class &quot;growthObj&quot; ## Slot &quot;fit&quot;: ## ## Call: ## lm(formula = Formula, data = dataf) ## ## Coefficients: ## (Intercept) ## 60.6 ## ## ## Slot &quot;sd&quot;: ## [1] 37.60413 ## ## ## $growthObjects[[2]] ## An object of class &quot;growthObj&quot; ## Slot &quot;fit&quot;: ## ## Call: ## lm(formula = Formula, data = dataf) ## ## Coefficients: ## (Intercept) size ## 39.8024 0.3132 ## ## ## Slot &quot;sd&quot;: ## [1] 37.35147 ## ## ## $growthObjects[[3]] ## An object of class &quot;growthObj&quot; ## Slot &quot;fit&quot;: ## ## Call: ## lm(formula = Formula, data = dataf) ## ## Coefficients: ## (Intercept) size size2 ## 32.075502 0.556869 -0.001828 ## ## ## Slot &quot;sd&quot;: ## [1] 37.37822 This handy piece of code quickly compares different model fits, the slope, a linear fit, and a polynomial fit. Since the lowest AIC value indicates the model-of-best-fit, then we will choose the 2nd order polynomial model. Now, let’s build our growth model! We also know from our previous analyses that the variation was equal, so we select ‘constantVar’. Finally size is continuous, so we select ‘gaussian’. #select best model - model with lowest values go &lt;-makeGrowthObj(dataf = pectis, Formula = sizeNext~size + size2, regType = &quot;constantVar&quot;, Family=&quot;gaussian&quot;) Let’s compare survival models now! pectis_clean &lt;- na.omit(pectis_surv) survModelComp(dataf = pectis_clean, makePlot = TRUE, legendPos = &quot;bottomright&quot;, mainTitle = &quot;Survival&quot;) ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: algorithm did not converge ## Warning: glm.fit: algorithm did not converge ## $summaryTable ## Exp. Vars AIC ## 1 surv ~ 1 2.00000000179842 ## 2 surv ~ size 4.00000000179842 ## 3 surv ~ size + size2 6.00000000179842 ## ## $survObjects ## $survObjects[[1]] ## An object of class &quot;survObj&quot; ## Slot &quot;fit&quot;: ## ## Call: glm(formula = Formula, family = binomial, data = dataf) ## ## Coefficients: ## (Intercept) ## 26.57 ## ## Degrees of Freedom: 309 Total (i.e. Null); 309 Residual ## Null Deviance: 0 ## Residual Deviance: 1.798e-09 AIC: 2 ## ## ## $survObjects[[2]] ## An object of class &quot;survObj&quot; ## Slot &quot;fit&quot;: ## ## Call: glm(formula = Formula, family = binomial, data = dataf) ## ## Coefficients: ## (Intercept) size ## 2.657e+01 4.349e-09 ## ## Degrees of Freedom: 309 Total (i.e. Null); 308 Residual ## Null Deviance: 0 ## Residual Deviance: 1.798e-09 AIC: 4 ## ## ## $survObjects[[3]] ## An object of class &quot;survObj&quot; ## Slot &quot;fit&quot;: ## ## Call: glm(formula = Formula, family = binomial, data = dataf) ## ## Coefficients: ## (Intercept) size size2 ## 2.657e+01 -7.019e-09 4.933e-11 ## ## Degrees of Freedom: 309 Total (i.e. Null); 307 Residual ## Null Deviance: 0 ## Residual Deviance: 1.798e-09 AIC: 6 #survModelComp(dataf = pectis, makePlot = TRUE, legendPos = &quot;bottomright&quot;, mainTitle = &quot;Survival&quot;) Here, the model of best fit is the first order model. Let’s build the survival object. #select best model - model with lowest values so &lt;- makeSurvObj(dataf = pectis_clean, Formula = surv~size+size2) ## Warning: glm.fit: algorithm did not converge The process for creating fecundity objects is not quite as streamlined, so let’s develop our own code to compare fecundity models. For P. imberbis, we break reproduction into two components: 1) whether a plant reproduces, and 2) if a plant reproduces, how many seeds are produced as a function of size. We will select the linear model. Now let’s look at seed production. #Test various fertility models to select best model #Seed production of reproductive plants Now let’s build the fecundity kernel. I will explain all of this later - all you need to do is change the formula structure following the ‘fo &lt;-’ code to reflect the models of best fit. 22.7 Putting it all together 22.8 Generating confidence intervals We will rerun it all over again - remember to update your models! #create dataframe to store results 22.9 Different life histories Let’s develop an IPM that includes both vegetative and sexual reproduction. We will use the endangered species, Stenogyne angustifolia, a species of mint endemic to Hawai’i, as an example. "],["permuting-your-brains-out.html", "Chapter 23 Permuting your brains out", " Chapter 23 Permuting your brains out "],["ordination-station.html", "Chapter 24 Ordination station", " Chapter 24 Ordination station "],["diversity-statistics.html", "Chapter 25 Diversity statistics 25.1 iNext 25.2 Coverage‐based R/E sampling curves 25.3 Data manipulation", " Chapter 25 Diversity statistics How do we describe the variety of species, traits or phylogenies in a particular area? With diversity statistics! Simple species counts, or species richness, is a time-old method for describing diversity. Since the olden days of creating ‘species lists’ for every habitat under the sun, new methods have emerged to describe the variety of life within ecological communities. These statistics attempt to deal with a problem inherent to diversity assessments: It is really hard to find all the species within an ecosystem and the more you look, the more species you find! There are a variety of different diversity statistics, which vary in terms of how much they weight relative abundances of species. These metrics are: Species richness: Simple counts of species in an area. Common species and rare species are given the same weight, since each species no matter how abundant it is, increases species richness by 1 unit! Includes no measure of evenness. Evenness is a measure of relative abundance of species at a site. The more equal species are in proportion to each other the greater the evenness of the site. The Shannon diversity index is a diversity index based on both richness (species counts) and evenness (relative abundance). The Simpson diversity index, like the Shannon diversity index, combines richness and evenness, but more strongly weights evenness relative to richness. These three metrics, richness, Shannon diversity and Simpson diversity index can be expressed as Hill numbers, or the effective number of species. The advantage of using Hill numbers, rather than generating diversity metrics using other indices, is that they are more easily interpreted, since they follow the doubling rule, and more easily compared to better understand diversity in a system. Hill numbers are parameterized by a diversity order q, which determines the measures’ sensitivity to species relative abundances. Hill numbers include the three most widely used species diversity measures as special cases: species richness (q = 0), Shannon diversity (q = 1) and Simpson diversity (q = 2). Specifically: \\[^qD = \\left(\\sum_{i=1}^{S} p_i^q\\right)^{1/(1-q)}\\] When q = 0, \\(^0D\\) is simply species richness, which counts species equally without regard to their relative abundances. For q = 1, the equation above is undefined, since 1/0, but the limit as q tends to 1 is the exponential of the familiar Shannon index, referred to as Shannon diversity. The measure for q = 1 counts individuals equally and thus counts species in proportion to their abundances; the measure \\(^1D\\) can be interpreted as the effective number of common species in the assemblage. The measure for q = 2 (\\(^2D\\)), referred to as Simpson diversity, discounts all but the dominant species and can be interpreted as the effective number of dominant species in the assemblage. The doubling rule refers to the fact that as a species assemblage goes from 8 species to 16 the calculated Hill numbers double. This seems like, of course, that would be the case, BUT diversity indices like the Shannon entropy (“Shannon-Wiener index”) and the Gini-Simpson index do not follow the doubling rule! By calling all of these indices “diversities” and treating them as if they were interchangeable in formulas or analyses requiring diversities, we will often generate misleading results.So, Hill numbers are true diversities (also referred to as the effective number of species), rather than just indices of diversity, unanchored to the actual diversity at a site. In other words, a community with eight equally-common species has a diversity of eight species, or a community with S equally-common species has a diversity of S species. This definition behaves as we expect of a diversity; the diversity of a community of sixteen equally-common species is double that of a community with eight equally-common species. One reason to calculate the effective number of species rather than diversity indices is that it allows straightforward and intuitive comparisons among communities. The goal in many diversity analyses is to make fair comparison of diversities across multiple assemblages that may vary in the size of their species pools or in the way in which they are sampled. Diversity metrics are strongly affected by both search time and search area. Even when researchers standardize search effort, both in terms of time spent surveying a plot and by plot area, diversity metrics are extremely sensitive to the diversity of the community itself. A more diverse community with high unevenness (i.e., lots of rare species) would require more search effort to estimate true diversity relative to a low diversity community with high evenness. For that reason, even when we’ve established appropriate sampling stategies, we have to standardize communities in order to compare them. For this reason, community ecologists use statistical techniques like extrapolation and rarefaction to estimate the ‘true’ number of species within an area based on your sample. Rarefaction deals with unequal sampling by down-sampling the larger samples until they contain the same number of observed individuals or sampling units as the smallest sample. In the past, rarefaction was the principal means to compare assemblages. When using rarefaction, you lose data, since you cull your other assemblages to be equivalently comparable with your “worst” sample. For this reason, many community ecologists now choose to generate asymptotic estimates of diversity, which involves combining rarefaction with extrapolation. To calculate asymptotic diversity estimates, you first generate a rarefaction/extrapolation sampling curve (R/E curve) that estimates diversity over a series of sampling units. Diversity estimates below your sample’s observed diversity estimate is derived from rarefaction (referred to as the sample-size-based approach to standardize data based on sample effort), and estimates above the observed diversity are derived from extrapolation. There are several ways of extrapolating diversity data beyond your actual sample. Parametric methods extend the species accumulation curve by fitting parametric functions. Most commonly, however, a non-parametric method that uses the frequency of rare species in a sample is used for extrapolating samples (a method developed by Alan Turing when designing the enigma machine). This approach standardizes data based on sample completeness, an estimated assemblage characteristic. Sampling completeness is the ratio between the detected and estimated diversity and indicates the proportion of detected species. The comparison between detected and estimated diversity is made by calculating and comparing sample coverage (thus these methods for extrapolating data are referred to as coverage-based approaches), or the total relative abundances of the observed species. The sample completeness curve provides a bridge between the size- and coverage-based R/E sampling curves. In other words, since sample completeness can be calculated for rarefied samples (i.e., use your actual data to compare detected to estimated diversity), sample completeness can be used to anchor and unite the rarefaction and extrapolation curves. By combining R/E curves to estimate of diversity and expressing diversity estimates as Hill numbers, iNEXT is a great all around package to compare diversity among assemblages of ecological data. This framework allows ecologists to compare of the species diversity of different assemblages in time or space, with reliable statistical inferences about these comparisons. 25.1 iNext What type of data do you have? The first decision to make is to determine what type of data you will analyze. For calculating diversity statistics, there are two principle forms of diversity data: Abundance data are data that include a measure of abundance - This could be data with counts of species or that includes cover data. When R/E curves are generated, the sampling unit is an individual. For example, to generate rarefaction curves, rarefied assemblages are created by randomly removing individuals within an assemblage. Note that many ecological studies collect abundance data within a plot or quadrat. In these cases, species are not independently sampled, and many ecologist recommend converting these datasets to incidence data (discussed below) to calculate diversity estimates, while others simply treat it as abundance data. Currently, statisticians are deriving methods to deal with sample-based abundance data (see Chui 2023) again using Turing’s methods of estimate completeness - hopefully, these new equations are integrated into the iNEXT package soon! Sample-based incidence data are data that note whether a give species is present or absent, and does not include information on how abundant that species is within an assemblage. In these cases the sampling unit is a trap, net, quadrat, plot, or timed survey. For sample-based incidence data, the sampling units (i.e., plots, quadrats, etc.), rather than individuals, are removed to generate rarefied diversity estimates. Note that for both abundance data and incidence data, the rarefaction process is repeated numerous times, allowing us to calculate confidence intervals (described in more detail below). There are two kinds of incidence input data for iNEXT: (1) incidence-raw data: for each assemblage, input data consist of a species-by-sampling-unit matrix; when there are N assemblages, input data consist of N matrices via a list object, with each matrix being a species-by-sampling-unit matrix. In iNEXT, this type of data is specified by datatype=“incidence_raw”. (2) Incidence-frequency data: input data for each assemblage consist of the number of sampling units (T) followed by the observed incidence frequencies (Y1, Y2, …, YS). Let’s check out a few of these examples. First, we will begin with the abundance dataset, spider. The spider dataset was collected at Harvard Experimental Forest. Researchers killed hemlock trees using two different methods (girdling or logging), in order to observe the effect of hemlock loss on the remnant community. Here, Sackett et al. (2011) provides abundance data for spiders collected on trees killed by girdling the trunk or by logging. library(iNEXT) library(tidyverse) #or #library(devtools) #install_github(&#39;AnneChao/iNEXT&#39;) #First let&#39;s check out two abundance datasets #Individual‐based abundance data (datatype=&quot;abundance&quot;): Input data for each assemblage/site include species abundances in an empirical sample of n individuals (“reference sample”). When there are N assemblages, input data consist of an S by N abundance matrix, or N lists of species abundances. #Check out spider data(&quot;spider&quot;); spider ## $Girdled ## [1] 46 22 17 15 15 9 8 6 6 4 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 ## ## $Logged ## [1] 88 22 16 15 13 10 8 8 7 7 7 5 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 #The spider dataset consists of abundance data from two canopy manipulation treatments (“Girdled” and “Logged”) of hemlock. Counts of individuals for species are provided for girdled and logged trees. #Go ahead and estimate diversity out &lt;- iNEXT(spider, q=c(0, 1, 2), datatype=&quot;abundance&quot;, endpoint=500) ggiNEXT(out, type=1, facet.var=&quot;Assemblage&quot;) The iNEXT functions calculates our R/E curve. There are several components of this function (taken from the package information): 1. The first term in the iNEXT function specifies the dataset. 2. Argument q allows you to select the type of diversity estimate that you are interested in. Here, we have chosen to calculate Hill numbers q = 0 (richness), 1 (Shannons), and 2 (Simpsons). 3. The argument datatype allows you to specify which type of data you are inputting (“abundance”, “incidence_raw” or “incidence_freq”). 4. The argument size to specify sample sizes for which diversity estimates are computed. If NULL, then diversity estimates will be calculated for those sample sizes determined by the specified/default endpoint and knots. 5. Argument endpoint is an integer specifying the sample size that is the endpoint for R/E calculation; If NULL, then endpoint=double the sample size of your assemblage (total sample size for abundance data; total sampling units for incidence data). As a general rule of thumb, for species richness, the size in the R/E curve can be extrapolated to at most double or triple the minimum observed sample size, guided by an estimated asymptote. For Shannon diversity and Simpson diversity, if the data are not too sparse, the extrapolation can be reliably extended to infinity to attain the estimated asymptote. 6. Knots an integer specifying the number of equally‐spaced knots between size 1 and the endpoint; default is 40. se a logical variable to calculate the bootstrap standard error and conf confidence interval. nboot an integer specifying the number of bootstrap replications; default is 50. Let’s take a look at another abundance dataset and use this next analysis to discuss the components of the output from the iNEXT function. The bird dataset consists of abundance tallies for birds at two sites: North site and South site. #Check out the other abundance dataset, called &#39;bird&#39; data(&quot;bird&quot;); bird ## North.site South.site ## Acanthiza_lineata 0 3 ## Acanthiza_nana 0 18 ## Acanthiza_pusilla 41 31 ## Acanthorhynchus_tenuirostris 0 2 ## Alisterus_scapularis 3 1 ## Cacatua_galerita 1 2 ## Cacomantis_flabelliformis 5 5 ## Calyptorhynchus_funereus 4 1 ## Colluricincla_harmonica 4 6 ## Cormobates_leucophaea 11 32 ## Corvus_coronoides 1 0 ## Dacelo_novaeguineae 2 0 ## Eopsaltria_australis 5 5 ## Gerygone_mouki 12 10 ## Leucosarcia_melanoleuca 1 1 ## Lichenostomus_chrysops 0 4 ## Malurus_cyaneus 0 6 ## Malurus_lamberti 0 6 ## Manorina_melanophrys 0 9 ## Meliphaga_lewinii 11 18 ## Menura_novaehollandiae 9 5 ## Monarcha_melanopsis 1 10 ## Neochmia_temporalis 0 9 ## Oriolus_sagittatus 1 0 ## Pachycephala_olivacea 0 2 ## Pachycephala_pectoralis 16 15 ## Pachycephala_rufiventris 0 3 ## Pardalotus_punctatus 15 17 ## Petroica_rosea 1 1 ## Phylidonyris_niger 0 2 ## Platycercus_elegans 2 7 ## Psophodes_olivaceus 7 7 ## Ptilonorhynchus_violaceus 2 2 ## Ptiloris_paradiseus 0 3 ## Rhipidura_albicollis 18 20 ## Rhipidura_rufifrons 8 14 ## Sericornis_citreogularis 0 2 ## Sericornis_frontalis 2 6 ## Strepera_graculina 3 4 ## Zoothera_lunulata 0 1 ## Zosterops_lateralis 16 17 #calculate diversity metrics for the bird dataset birdtest &lt;- iNEXT(bird, q=c(0,1,2), datatype=&quot;abundance&quot;, size=NULL, endpoint=NULL, knots=40, se=TRUE, conf=0.95, nboot=50) birdtest$AsyEst ## Assemblage Diversity Observed Estimator s.e. LCL UCL ## 1 North.site Species richness 27.00000 31.47772 6.929001 27.000000 45.05832 ## 2 North.site Shannon diversity 16.53585 17.96568 1.309010 15.400065 20.53129 ## 3 North.site Simpson diversity 11.84785 12.52375 1.312643 9.951019 15.09648 ## 4 South.site Species richness 38.00000 40.07655 6.626617 38.000000 53.06448 ## 5 South.site Shannon diversity 25.44707 27.25065 1.216105 24.867124 29.63417 ## 6 South.site Simpson diversity 19.63930 20.91318 1.288039 18.388668 23.43769 #plot your bird data ggiNEXT(birdtest, type=1, se=TRUE, facet.var=&quot;None&quot;, color.var=&quot;Assemblage&quot;, grey=FALSE) The iNEXT() function returns the “iNEXT” object, which includes three output lists (taken from the iNEXT package documentation): 1. $DataInfo for summarizing data information; 2. $iNextEst for showing size- and coverage-based diversity estimates along with related statistics for a series of rarefied and extrapolated samples; 3. $AsyEst for showing asymptotic diversity estimates along with related statistics. 4. $DataInfo, as shown below, returns basic data information including the reference sample size (n), observed species richness (S.obs), sample coverage estimate for the reference sample (SC), and the first ten frequency counts (f1‐f10). This part of output can also be computed by the function DataInfo() You can take a look at the entire output by calling ‘birdtest’ or you can grab the data that you will use in your analyses (like we did above), by calling ‘birdtest$AsyEst’. For each estimate, 95% confidence intervals are calculated by bootstrapping samples. In the size-based standardization (i.e., rarefaction), the sample size is fixed in each regenerated bootstrap sample. In the coverage-based standardization (i.e., extrapolated), for a given standardized coverage value, bootstrapping is again used, but using random draws based on simulated data. The result is that the sampling uncertainty is greater in the coverage-based standardization and the resulting confidence interval is wider than that in the corresponding size-based standardization. The bootstrapping default is 50, and since it is a random process expect CI to differ each time you calculate them. Now let’s look at incidence data. First let’s look at the tropical ant data (in the dataset ant included in the package) at five elevations (50m, 500m, 1070m, 1500m, and 2000m) collected by Longino &amp; Colwell (2011) from Costa Rica. The 5 lists of incidence frequencies are shown below. The first entry of each list must be the total number of sampling units, followed by the species incidence frequencies. #incidence dataset data(ant) str(ant) ## List of 5 ## $ h50m : num [1:228] 599 330 263 236 222 195 186 183 182 129 ... ## $ h500m : num [1:242] 230 133 131 123 78 73 65 60 60 56 ... ## $ h1070m: num [1:123] 150 99 96 80 74 68 60 54 46 45 ... ## $ h1500m: num [1:57] 200 144 113 79 76 74 73 53 50 43 ... ## $ h2000m: num [1:15] 200 80 59 34 23 19 15 13 8 8 ... Look at the second incidence dataset. The ciliates data were collected from three coastal dune habitats to demonstrate the use of the input datatype=“incidence_raw”. The data set (ciliates) included in the package is a list of three species-by-plot matrices. data(ciliates) str(ciliates) ## List of 3 ## $ EtoshaPan : int [1:365, 1:19] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:365] &quot;Acaryophrya.collaris&quot; &quot;Actinobolina.multinucleata.n..sp.&quot; &quot;Afroamphisiella.multinucleata.n..sp.&quot; &quot;Afrothrix.multinucleata.n..sp.&quot; ... ## .. ..$ : chr [1:19] &quot;x53&quot; &quot;x54&quot; &quot;x55&quot; &quot;x56&quot; ... ## $ CentralNamibDesert : int [1:365, 1:17] 0 0 0 0 0 1 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:365] &quot;Acaryophrya.collaris&quot; &quot;Actinobolina.multinucleata.n..sp.&quot; &quot;Afroamphisiella.multinucleata.n..sp.&quot; &quot;Afrothrix.multinucleata.n..sp.&quot; ... ## .. ..$ : chr [1:17] &quot;x31&quot; &quot;x32&quot; &quot;x34&quot; &quot;x35&quot; ... ## $ SouthernNamibDesert: int [1:365, 1:15] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:365] &quot;Acaryophrya.collaris&quot; &quot;Actinobolina.multinucleata.n..sp.&quot; &quot;Afroamphisiella.multinucleata.n..sp.&quot; &quot;Afrothrix.multinucleata.n..sp.&quot; ... ## .. ..$ : chr [1:15] &quot;x9&quot; &quot;x17&quot; &quot;x19&quot; &quot;x20&quot; ... #Run the following commands to get the output as shown below. out.raw &lt;- iNEXT(ciliates, q = 0, datatype=&quot;incidence_raw&quot;, endpoint=150) 25.2 Coverage‐based R/E sampling curves You can derive a single measure of diversity for the same coverage/sample to compare among communities using estimateD. #The following command returns the species diversity with a specified level of sample coverage of 98.5% for the ant data. For some assemblages, this coverage value corresponds to rarefaction (i.e., less than the coverage of the reference sample), while for the others it corresponds to extrapolation (i.e., greater than the coverage of the reference sample), as indicated under the method column of the output. estimateD(ant, datatype=&quot;incidence_freq&quot;, base=&quot;coverage&quot;, level=0.985, conf=0.95) ## Assemblage t Method Order.q SC qD qD.LCL qD.UCL ## 1 h50m 327.1646 Rarefaction 0 0.985 197.487977 186.351661 208.624293 ## 2 h50m 327.1646 Rarefaction 1 0.985 78.052670 75.603394 80.501945 ## 3 h50m 327.1646 Rarefaction 2 0.985 50.461029 48.897486 52.024572 ## 4 h500m 342.8592 Extrapolation 0 0.985 268.725933 245.159581 292.292284 ## 5 h500m 342.8592 Extrapolation 1 0.985 103.847150 99.916734 107.777566 ## 6 h500m 342.8592 Extrapolation 2 0.985 64.758264 61.807251 67.709277 ## 7 h1070m 158.9508 Extrapolation 0 0.985 123.608792 108.600074 138.617509 ## 8 h1070m 158.9508 Extrapolation 1 0.985 59.591818 56.905446 62.278191 ## 9 h1070m 158.9508 Extrapolation 2 0.985 41.775173 39.620709 43.929638 ## 10 h1500m 125.9590 Rarefaction 0 0.985 50.478877 41.924784 59.032970 ## 11 h1500m 125.9590 Rarefaction 1 0.985 26.248998 24.682079 27.815917 ## 12 h1500m 125.9590 Rarefaction 2 0.985 18.648902 17.514626 19.783178 ## 13 h2000m 104.6306 Rarefaction 0 0.985 12.909623 10.970702 14.848544 ## 14 h2000m 104.6306 Rarefaction 1 0.985 7.710717 7.018771 8.402664 ## 15 h2000m 104.6306 Rarefaction 2 0.985 5.794580 5.127607 6.461552 25.3 Data manipulation Scripts from the iNEXT package are fairly straightforward and easy to run. The challenge with these datasets is wrangling the data into the correct format for analysis. Since diversity statistics are often paired with NMDS data, let’s start with an example in which we wrangle data formatted for the Vegan package. library(readr) # Download and read the first dataset url1 &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1c1guQTQwGNfA5Kx-xNPKRDsSJrBR-HxW&quot; env_matrix &lt;- read_csv(url1) ## New names: ## Rows: 44 Columns: 5 ## ── Column specification ## ───────────────────────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (2): Treatment, Treatment_status dbl (3): ...1, Year, PlotN ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set ## `show_col_types = FALSE` to quiet this message. ## • `` -&gt; `...1` # Download and read the second dataset url2 &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1jCqfJPEuEWo4uNZfxW43VkSvEDDtayF5&quot; species_matrix &lt;- read_csv(url2) ## New names: ## Rows: 44 Columns: 136 ## ── Column specification ## ───────────────────────────────────────────────────────────────────────────────────────────── Delimiter: &quot;,&quot; dbl ## (136): ...1, Acalypha.neomexicana, Allium.sp, Ambrosia.psilostachya, Arabis.perennans, Arctostaphylos.pringle... ## ℹ Use `spec()` to retrieve the full column specification for this data. ℹ Specify the column types or set ## `show_col_types = FALSE` to quiet this message. ## • `` -&gt; `...1` This data is incidence data collected at 3 replicate control plots and 6 treatment plots before and after disturbance. # Load necessary libraries library(tidyverse) print(colnames(species_matrix)) ## [1] &quot;...1&quot; &quot;Acalypha.neomexicana&quot; ## [3] &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; ## [5] &quot;Arabis.perennans&quot; &quot;Arctostaphylos.pringlei&quot; ## [7] &quot;Arctostaphylos.pungens&quot; &quot;Arenaria.lanuginosa.var.saxosa&quot; ## [9] &quot;Aristida.purpurea&quot; &quot;Aristida.schiedeana.var.orcuttiana&quot; ## [11] &quot;Aristida.sp&quot; &quot;Aristida.ternipes&quot; ## [13] &quot;Artemisia.ludoviciana&quot; &quot;Artemisia.sp&quot; ## [15] &quot;Asclepias.nyctaginifolia&quot; &quot;Astragalus.sp&quot; ## [17] &quot;Baccharis.pteronioides&quot; &quot;Bahia.biternata&quot; ## [19] &quot;Bidens.sp&quot; &quot;Boechera.perennans&quot; ## [21] &quot;Bothriochloa.ischaemum&quot; &quot;Bouteloua.curtipendula&quot; ## [23] &quot;Bouteloua.eriopoda&quot; &quot;Bouteloua.gracilis&quot; ## [25] &quot;Bouteloua.hirsuta&quot; &quot;Bouteloua.sp&quot; ## [27] &quot;Brickellia.betonicifolia&quot; &quot;Brickellia.californica&quot; ## [29] &quot;Brickellia.sp&quot; &quot;Bromus.ciliatus&quot; ## [31] &quot;Bromus.tectorum&quot; &quot;Bryophyta.sp&quot; ## [33] &quot;Calliandra.humilis&quot; &quot;Calliandra.humilis.var.reticulata&quot; ## [35] &quot;Calochortus.sp&quot; &quot;Carex.sp&quot; ## [37] &quot;Ceanothus.fendleri&quot; &quot;Chenopodium.fremontii&quot; ## [39] &quot;Coleogyne.ramosissima&quot; &quot;Collinsia.parviflora&quot; ## [41] &quot;Comandra.umbellata&quot; &quot;Conyza.canadensis&quot; ## [43] &quot;Cylindropuntia.sp&quot; &quot;Cynoglossum.officinale&quot; ## [45] &quot;Cyperus.fendlerianus&quot; &quot;Cyperus.sp&quot; ## [47] &quot;Dalea.albiflora&quot; &quot;Datura.sp&quot; ## [49] &quot;Descurainia.obtusa&quot; &quot;Descurainia.sophia&quot; ## [51] &quot;Desmanthus.cooleyi&quot; &quot;Desmodium.batocaulon&quot; ## [53] &quot;Dysphania.graveolens&quot; &quot;Dysphania.pumilio&quot; ## [55] &quot;Echinocereus.sp&quot; &quot;Elymus.elymoides&quot; ## [57] &quot;Eragrostis.curvula&quot; &quot;Eragrostis.intermedia&quot; ## [59] &quot;Erigeron.divergens&quot; &quot;Erigeron.flagellaris&quot; ## [61] &quot;Erigeron.oreophilus&quot; &quot;Eriogonum.jamesii&quot; ## [63] &quot;Eriogonum.wrightii&quot; &quot;Escobaria.sp&quot; ## [65] &quot;Escobaria.vivipara&quot; &quot;Euphorbia.albomarginata&quot; ## [67] &quot;Euphorbia.revoluta&quot; &quot;Euphorbia.schizoloba&quot; ## [69] &quot;Euphorbia.serpyllifolia&quot; &quot;Evolvulus.sericeus&quot; ## [71] &quot;Festuca.arizonica&quot; &quot;Galactia.wrightii&quot; ## [73] &quot;Galium.microphyllum&quot; &quot;Garrya.wrightii&quot; ## [75] &quot;Glandularia.bipinnatifida&quot; &quot;Glandularia.gooddingii&quot; ## [77] &quot;Gutierrezia.sarothrae&quot; &quot;Heliomeris.longifolia.var.annua&quot; ## [79] &quot;Heliomeris.multiflora&quot; &quot;Hesperidanthus.linearifolius&quot; ## [81] &quot;Heterosperma.pinnatum&quot; &quot;Houstonia.wrightii&quot; ## [83] &quot;Hybanthus.verticillatus&quot; &quot;Hymenopappus.filifolius&quot; ## [85] &quot;Juncus.saximontanus&quot; &quot;Koeleria.macrantha&quot; ## [87] &quot;Lepidium.lasiocarpum&quot; &quot;Lonicera.arizonica&quot; ## [89] &quot;Lotus.wrightii&quot; &quot;Lycurus.setosus&quot; ## [91] &quot;Machaeranthera.gracilis&quot; &quot;Mammillaria.sp&quot; ## [93] &quot;Mimosa.aculeaticarpa&quot; &quot;Mimosa.biuncifera&quot; ## [95] &quot;Mirabilis.sp&quot; &quot;Mollugo.verticillata&quot; ## [97] &quot;Monarda.sp&quot; &quot;Muhlenbergia.emersleyi&quot; ## [99] &quot;Muhlenbergia.longiligula&quot; &quot;Muhlenbergia.sp&quot; ## [101] &quot;Nolina.microcarpa&quot; &quot;Ophioglossum.engelmannii&quot; ## [103] &quot;Opuntia.chlorotica&quot; &quot;Opuntia.sp&quot; ## [105] &quot;Packera.neomexicana&quot; &quot;Pediomelum.tenuiflorum&quot; ## [107] &quot;Penstemon.barbatus&quot; &quot;Penstemon.eatonii&quot; ## [109] &quot;Penstemon.linarioides&quot; &quot;Penstemon.sp&quot; ## [111] &quot;Phemeranthus.parviflorus&quot; &quot;Phoradendron.leucarpum.ssp.tomentosum&quot; ## [113] &quot;Physalis.hederifolia&quot; &quot;Poa.fendleriana&quot; ## [115] &quot;Polygala.alba&quot; &quot;Polygala.obscura&quot; ## [117] &quot;Polygonum.douglasii&quot; &quot;Portulaca.umbraticola&quot; ## [119] &quot;Pseudognaphalium.canescens&quot; &quot;Psoralidium.tenuiflorum&quot; ## [121] &quot;Quercus.turbinella&quot; &quot;Rhamnus.ilicifolia&quot; ## [123] &quot;Rhus.trilobata&quot; &quot;richness_aggregated&quot; ## [125] &quot;Solanum.elaeagnifolium&quot; &quot;Sporobolus.contractus&quot; ## [127] &quot;Sporobolus.cryptandrus&quot; &quot;Sporobolus.interruptus&quot; ## [129] &quot;Symphyotrichum.falcatum&quot; &quot;Tradescantia.sp&quot; ## [131] &quot;Verbascum.densiflorum&quot; &quot;Verbascum.thapsus&quot; ## [133] &quot;Verbena.bracteata&quot; &quot;Vulpia.microstachys&quot; ## [135] &quot;Vulpia.octoflora&quot; &quot;Xanthisma.gracile&quot; print(colnames(env_matrix)) ## [1] &quot;...1&quot; &quot;Year&quot; &quot;PlotN&quot; &quot;Treatment&quot; &quot;Treatment_status&quot; # combine the datasets, by hand or with code! combined_data &lt;- inner_join(species_matrix, env_matrix, by = c(&quot;...1&quot; = &quot;...1&quot;)) # Split the data into different groups control_pre &lt;- combined_data %&gt;% filter(Treatment == &quot;Control&quot; &amp; Treatment_status == &quot;PreTreatment&quot;) #remove unnecessary columns colnames(control_pre) ## [1] &quot;...1&quot; &quot;Acalypha.neomexicana&quot; ## [3] &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; ## [5] &quot;Arabis.perennans&quot; &quot;Arctostaphylos.pringlei&quot; ## [7] &quot;Arctostaphylos.pungens&quot; &quot;Arenaria.lanuginosa.var.saxosa&quot; ## [9] &quot;Aristida.purpurea&quot; &quot;Aristida.schiedeana.var.orcuttiana&quot; ## [11] &quot;Aristida.sp&quot; &quot;Aristida.ternipes&quot; ## [13] &quot;Artemisia.ludoviciana&quot; &quot;Artemisia.sp&quot; ## [15] &quot;Asclepias.nyctaginifolia&quot; &quot;Astragalus.sp&quot; ## [17] &quot;Baccharis.pteronioides&quot; &quot;Bahia.biternata&quot; ## [19] &quot;Bidens.sp&quot; &quot;Boechera.perennans&quot; ## [21] &quot;Bothriochloa.ischaemum&quot; &quot;Bouteloua.curtipendula&quot; ## [23] &quot;Bouteloua.eriopoda&quot; &quot;Bouteloua.gracilis&quot; ## [25] &quot;Bouteloua.hirsuta&quot; &quot;Bouteloua.sp&quot; ## [27] &quot;Brickellia.betonicifolia&quot; &quot;Brickellia.californica&quot; ## [29] &quot;Brickellia.sp&quot; &quot;Bromus.ciliatus&quot; ## [31] &quot;Bromus.tectorum&quot; &quot;Bryophyta.sp&quot; ## [33] &quot;Calliandra.humilis&quot; &quot;Calliandra.humilis.var.reticulata&quot; ## [35] &quot;Calochortus.sp&quot; &quot;Carex.sp&quot; ## [37] &quot;Ceanothus.fendleri&quot; &quot;Chenopodium.fremontii&quot; ## [39] &quot;Coleogyne.ramosissima&quot; &quot;Collinsia.parviflora&quot; ## [41] &quot;Comandra.umbellata&quot; &quot;Conyza.canadensis&quot; ## [43] &quot;Cylindropuntia.sp&quot; &quot;Cynoglossum.officinale&quot; ## [45] &quot;Cyperus.fendlerianus&quot; &quot;Cyperus.sp&quot; ## [47] &quot;Dalea.albiflora&quot; &quot;Datura.sp&quot; ## [49] &quot;Descurainia.obtusa&quot; &quot;Descurainia.sophia&quot; ## [51] &quot;Desmanthus.cooleyi&quot; &quot;Desmodium.batocaulon&quot; ## [53] &quot;Dysphania.graveolens&quot; &quot;Dysphania.pumilio&quot; ## [55] &quot;Echinocereus.sp&quot; &quot;Elymus.elymoides&quot; ## [57] &quot;Eragrostis.curvula&quot; &quot;Eragrostis.intermedia&quot; ## [59] &quot;Erigeron.divergens&quot; &quot;Erigeron.flagellaris&quot; ## [61] &quot;Erigeron.oreophilus&quot; &quot;Eriogonum.jamesii&quot; ## [63] &quot;Eriogonum.wrightii&quot; &quot;Escobaria.sp&quot; ## [65] &quot;Escobaria.vivipara&quot; &quot;Euphorbia.albomarginata&quot; ## [67] &quot;Euphorbia.revoluta&quot; &quot;Euphorbia.schizoloba&quot; ## [69] &quot;Euphorbia.serpyllifolia&quot; &quot;Evolvulus.sericeus&quot; ## [71] &quot;Festuca.arizonica&quot; &quot;Galactia.wrightii&quot; ## [73] &quot;Galium.microphyllum&quot; &quot;Garrya.wrightii&quot; ## [75] &quot;Glandularia.bipinnatifida&quot; &quot;Glandularia.gooddingii&quot; ## [77] &quot;Gutierrezia.sarothrae&quot; &quot;Heliomeris.longifolia.var.annua&quot; ## [79] &quot;Heliomeris.multiflora&quot; &quot;Hesperidanthus.linearifolius&quot; ## [81] &quot;Heterosperma.pinnatum&quot; &quot;Houstonia.wrightii&quot; ## [83] &quot;Hybanthus.verticillatus&quot; &quot;Hymenopappus.filifolius&quot; ## [85] &quot;Juncus.saximontanus&quot; &quot;Koeleria.macrantha&quot; ## [87] &quot;Lepidium.lasiocarpum&quot; &quot;Lonicera.arizonica&quot; ## [89] &quot;Lotus.wrightii&quot; &quot;Lycurus.setosus&quot; ## [91] &quot;Machaeranthera.gracilis&quot; &quot;Mammillaria.sp&quot; ## [93] &quot;Mimosa.aculeaticarpa&quot; &quot;Mimosa.biuncifera&quot; ## [95] &quot;Mirabilis.sp&quot; &quot;Mollugo.verticillata&quot; ## [97] &quot;Monarda.sp&quot; &quot;Muhlenbergia.emersleyi&quot; ## [99] &quot;Muhlenbergia.longiligula&quot; &quot;Muhlenbergia.sp&quot; ## [101] &quot;Nolina.microcarpa&quot; &quot;Ophioglossum.engelmannii&quot; ## [103] &quot;Opuntia.chlorotica&quot; &quot;Opuntia.sp&quot; ## [105] &quot;Packera.neomexicana&quot; &quot;Pediomelum.tenuiflorum&quot; ## [107] &quot;Penstemon.barbatus&quot; &quot;Penstemon.eatonii&quot; ## [109] &quot;Penstemon.linarioides&quot; &quot;Penstemon.sp&quot; ## [111] &quot;Phemeranthus.parviflorus&quot; &quot;Phoradendron.leucarpum.ssp.tomentosum&quot; ## [113] &quot;Physalis.hederifolia&quot; &quot;Poa.fendleriana&quot; ## [115] &quot;Polygala.alba&quot; &quot;Polygala.obscura&quot; ## [117] &quot;Polygonum.douglasii&quot; &quot;Portulaca.umbraticola&quot; ## [119] &quot;Pseudognaphalium.canescens&quot; &quot;Psoralidium.tenuiflorum&quot; ## [121] &quot;Quercus.turbinella&quot; &quot;Rhamnus.ilicifolia&quot; ## [123] &quot;Rhus.trilobata&quot; &quot;richness_aggregated&quot; ## [125] &quot;Solanum.elaeagnifolium&quot; &quot;Sporobolus.contractus&quot; ## [127] &quot;Sporobolus.cryptandrus&quot; &quot;Sporobolus.interruptus&quot; ## [129] &quot;Symphyotrichum.falcatum&quot; &quot;Tradescantia.sp&quot; ## [131] &quot;Verbascum.densiflorum&quot; &quot;Verbascum.thapsus&quot; ## [133] &quot;Verbena.bracteata&quot; &quot;Vulpia.microstachys&quot; ## [135] &quot;Vulpia.octoflora&quot; &quot;Xanthisma.gracile&quot; ## [137] &quot;Year&quot; &quot;PlotN&quot; ## [139] &quot;Treatment&quot; &quot;Treatment_status&quot; control_pre &lt;- dplyr::select(control_pre, -c(Year, PlotN, Treatment, Treatment_status, ...1)) control_pret &lt;- t(control_pre) control_post &lt;- combined_data %&gt;% filter(Treatment == &quot;Control&quot; &amp; Treatment_status == &quot;PostTreatment&quot;) colnames(control_post) ## [1] &quot;...1&quot; &quot;Acalypha.neomexicana&quot; ## [3] &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; ## [5] &quot;Arabis.perennans&quot; &quot;Arctostaphylos.pringlei&quot; ## [7] &quot;Arctostaphylos.pungens&quot; &quot;Arenaria.lanuginosa.var.saxosa&quot; ## [9] &quot;Aristida.purpurea&quot; &quot;Aristida.schiedeana.var.orcuttiana&quot; ## [11] &quot;Aristida.sp&quot; &quot;Aristida.ternipes&quot; ## [13] &quot;Artemisia.ludoviciana&quot; &quot;Artemisia.sp&quot; ## [15] &quot;Asclepias.nyctaginifolia&quot; &quot;Astragalus.sp&quot; ## [17] &quot;Baccharis.pteronioides&quot; &quot;Bahia.biternata&quot; ## [19] &quot;Bidens.sp&quot; &quot;Boechera.perennans&quot; ## [21] &quot;Bothriochloa.ischaemum&quot; &quot;Bouteloua.curtipendula&quot; ## [23] &quot;Bouteloua.eriopoda&quot; &quot;Bouteloua.gracilis&quot; ## [25] &quot;Bouteloua.hirsuta&quot; &quot;Bouteloua.sp&quot; ## [27] &quot;Brickellia.betonicifolia&quot; &quot;Brickellia.californica&quot; ## [29] &quot;Brickellia.sp&quot; &quot;Bromus.ciliatus&quot; ## [31] &quot;Bromus.tectorum&quot; &quot;Bryophyta.sp&quot; ## [33] &quot;Calliandra.humilis&quot; &quot;Calliandra.humilis.var.reticulata&quot; ## [35] &quot;Calochortus.sp&quot; &quot;Carex.sp&quot; ## [37] &quot;Ceanothus.fendleri&quot; &quot;Chenopodium.fremontii&quot; ## [39] &quot;Coleogyne.ramosissima&quot; &quot;Collinsia.parviflora&quot; ## [41] &quot;Comandra.umbellata&quot; &quot;Conyza.canadensis&quot; ## [43] &quot;Cylindropuntia.sp&quot; &quot;Cynoglossum.officinale&quot; ## [45] &quot;Cyperus.fendlerianus&quot; &quot;Cyperus.sp&quot; ## [47] &quot;Dalea.albiflora&quot; &quot;Datura.sp&quot; ## [49] &quot;Descurainia.obtusa&quot; &quot;Descurainia.sophia&quot; ## [51] &quot;Desmanthus.cooleyi&quot; &quot;Desmodium.batocaulon&quot; ## [53] &quot;Dysphania.graveolens&quot; &quot;Dysphania.pumilio&quot; ## [55] &quot;Echinocereus.sp&quot; &quot;Elymus.elymoides&quot; ## [57] &quot;Eragrostis.curvula&quot; &quot;Eragrostis.intermedia&quot; ## [59] &quot;Erigeron.divergens&quot; &quot;Erigeron.flagellaris&quot; ## [61] &quot;Erigeron.oreophilus&quot; &quot;Eriogonum.jamesii&quot; ## [63] &quot;Eriogonum.wrightii&quot; &quot;Escobaria.sp&quot; ## [65] &quot;Escobaria.vivipara&quot; &quot;Euphorbia.albomarginata&quot; ## [67] &quot;Euphorbia.revoluta&quot; &quot;Euphorbia.schizoloba&quot; ## [69] &quot;Euphorbia.serpyllifolia&quot; &quot;Evolvulus.sericeus&quot; ## [71] &quot;Festuca.arizonica&quot; &quot;Galactia.wrightii&quot; ## [73] &quot;Galium.microphyllum&quot; &quot;Garrya.wrightii&quot; ## [75] &quot;Glandularia.bipinnatifida&quot; &quot;Glandularia.gooddingii&quot; ## [77] &quot;Gutierrezia.sarothrae&quot; &quot;Heliomeris.longifolia.var.annua&quot; ## [79] &quot;Heliomeris.multiflora&quot; &quot;Hesperidanthus.linearifolius&quot; ## [81] &quot;Heterosperma.pinnatum&quot; &quot;Houstonia.wrightii&quot; ## [83] &quot;Hybanthus.verticillatus&quot; &quot;Hymenopappus.filifolius&quot; ## [85] &quot;Juncus.saximontanus&quot; &quot;Koeleria.macrantha&quot; ## [87] &quot;Lepidium.lasiocarpum&quot; &quot;Lonicera.arizonica&quot; ## [89] &quot;Lotus.wrightii&quot; &quot;Lycurus.setosus&quot; ## [91] &quot;Machaeranthera.gracilis&quot; &quot;Mammillaria.sp&quot; ## [93] &quot;Mimosa.aculeaticarpa&quot; &quot;Mimosa.biuncifera&quot; ## [95] &quot;Mirabilis.sp&quot; &quot;Mollugo.verticillata&quot; ## [97] &quot;Monarda.sp&quot; &quot;Muhlenbergia.emersleyi&quot; ## [99] &quot;Muhlenbergia.longiligula&quot; &quot;Muhlenbergia.sp&quot; ## [101] &quot;Nolina.microcarpa&quot; &quot;Ophioglossum.engelmannii&quot; ## [103] &quot;Opuntia.chlorotica&quot; &quot;Opuntia.sp&quot; ## [105] &quot;Packera.neomexicana&quot; &quot;Pediomelum.tenuiflorum&quot; ## [107] &quot;Penstemon.barbatus&quot; &quot;Penstemon.eatonii&quot; ## [109] &quot;Penstemon.linarioides&quot; &quot;Penstemon.sp&quot; ## [111] &quot;Phemeranthus.parviflorus&quot; &quot;Phoradendron.leucarpum.ssp.tomentosum&quot; ## [113] &quot;Physalis.hederifolia&quot; &quot;Poa.fendleriana&quot; ## [115] &quot;Polygala.alba&quot; &quot;Polygala.obscura&quot; ## [117] &quot;Polygonum.douglasii&quot; &quot;Portulaca.umbraticola&quot; ## [119] &quot;Pseudognaphalium.canescens&quot; &quot;Psoralidium.tenuiflorum&quot; ## [121] &quot;Quercus.turbinella&quot; &quot;Rhamnus.ilicifolia&quot; ## [123] &quot;Rhus.trilobata&quot; &quot;richness_aggregated&quot; ## [125] &quot;Solanum.elaeagnifolium&quot; &quot;Sporobolus.contractus&quot; ## [127] &quot;Sporobolus.cryptandrus&quot; &quot;Sporobolus.interruptus&quot; ## [129] &quot;Symphyotrichum.falcatum&quot; &quot;Tradescantia.sp&quot; ## [131] &quot;Verbascum.densiflorum&quot; &quot;Verbascum.thapsus&quot; ## [133] &quot;Verbena.bracteata&quot; &quot;Vulpia.microstachys&quot; ## [135] &quot;Vulpia.octoflora&quot; &quot;Xanthisma.gracile&quot; ## [137] &quot;Year&quot; &quot;PlotN&quot; ## [139] &quot;Treatment&quot; &quot;Treatment_status&quot; control_post &lt;- dplyr::select(control_post, -c(Year, PlotN, Treatment, Treatment_status, ...1)) control_postt &lt;- t(control_post) treatment_pre &lt;- combined_data %&gt;% filter(Treatment == &quot;Treatment&quot; &amp; Treatment_status == &quot;PreTreatment&quot;) colnames(treatment_pre) ## [1] &quot;...1&quot; &quot;Acalypha.neomexicana&quot; ## [3] &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; ## [5] &quot;Arabis.perennans&quot; &quot;Arctostaphylos.pringlei&quot; ## [7] &quot;Arctostaphylos.pungens&quot; &quot;Arenaria.lanuginosa.var.saxosa&quot; ## [9] &quot;Aristida.purpurea&quot; &quot;Aristida.schiedeana.var.orcuttiana&quot; ## [11] &quot;Aristida.sp&quot; &quot;Aristida.ternipes&quot; ## [13] &quot;Artemisia.ludoviciana&quot; &quot;Artemisia.sp&quot; ## [15] &quot;Asclepias.nyctaginifolia&quot; &quot;Astragalus.sp&quot; ## [17] &quot;Baccharis.pteronioides&quot; &quot;Bahia.biternata&quot; ## [19] &quot;Bidens.sp&quot; &quot;Boechera.perennans&quot; ## [21] &quot;Bothriochloa.ischaemum&quot; &quot;Bouteloua.curtipendula&quot; ## [23] &quot;Bouteloua.eriopoda&quot; &quot;Bouteloua.gracilis&quot; ## [25] &quot;Bouteloua.hirsuta&quot; &quot;Bouteloua.sp&quot; ## [27] &quot;Brickellia.betonicifolia&quot; &quot;Brickellia.californica&quot; ## [29] &quot;Brickellia.sp&quot; &quot;Bromus.ciliatus&quot; ## [31] &quot;Bromus.tectorum&quot; &quot;Bryophyta.sp&quot; ## [33] &quot;Calliandra.humilis&quot; &quot;Calliandra.humilis.var.reticulata&quot; ## [35] &quot;Calochortus.sp&quot; &quot;Carex.sp&quot; ## [37] &quot;Ceanothus.fendleri&quot; &quot;Chenopodium.fremontii&quot; ## [39] &quot;Coleogyne.ramosissima&quot; &quot;Collinsia.parviflora&quot; ## [41] &quot;Comandra.umbellata&quot; &quot;Conyza.canadensis&quot; ## [43] &quot;Cylindropuntia.sp&quot; &quot;Cynoglossum.officinale&quot; ## [45] &quot;Cyperus.fendlerianus&quot; &quot;Cyperus.sp&quot; ## [47] &quot;Dalea.albiflora&quot; &quot;Datura.sp&quot; ## [49] &quot;Descurainia.obtusa&quot; &quot;Descurainia.sophia&quot; ## [51] &quot;Desmanthus.cooleyi&quot; &quot;Desmodium.batocaulon&quot; ## [53] &quot;Dysphania.graveolens&quot; &quot;Dysphania.pumilio&quot; ## [55] &quot;Echinocereus.sp&quot; &quot;Elymus.elymoides&quot; ## [57] &quot;Eragrostis.curvula&quot; &quot;Eragrostis.intermedia&quot; ## [59] &quot;Erigeron.divergens&quot; &quot;Erigeron.flagellaris&quot; ## [61] &quot;Erigeron.oreophilus&quot; &quot;Eriogonum.jamesii&quot; ## [63] &quot;Eriogonum.wrightii&quot; &quot;Escobaria.sp&quot; ## [65] &quot;Escobaria.vivipara&quot; &quot;Euphorbia.albomarginata&quot; ## [67] &quot;Euphorbia.revoluta&quot; &quot;Euphorbia.schizoloba&quot; ## [69] &quot;Euphorbia.serpyllifolia&quot; &quot;Evolvulus.sericeus&quot; ## [71] &quot;Festuca.arizonica&quot; &quot;Galactia.wrightii&quot; ## [73] &quot;Galium.microphyllum&quot; &quot;Garrya.wrightii&quot; ## [75] &quot;Glandularia.bipinnatifida&quot; &quot;Glandularia.gooddingii&quot; ## [77] &quot;Gutierrezia.sarothrae&quot; &quot;Heliomeris.longifolia.var.annua&quot; ## [79] &quot;Heliomeris.multiflora&quot; &quot;Hesperidanthus.linearifolius&quot; ## [81] &quot;Heterosperma.pinnatum&quot; &quot;Houstonia.wrightii&quot; ## [83] &quot;Hybanthus.verticillatus&quot; &quot;Hymenopappus.filifolius&quot; ## [85] &quot;Juncus.saximontanus&quot; &quot;Koeleria.macrantha&quot; ## [87] &quot;Lepidium.lasiocarpum&quot; &quot;Lonicera.arizonica&quot; ## [89] &quot;Lotus.wrightii&quot; &quot;Lycurus.setosus&quot; ## [91] &quot;Machaeranthera.gracilis&quot; &quot;Mammillaria.sp&quot; ## [93] &quot;Mimosa.aculeaticarpa&quot; &quot;Mimosa.biuncifera&quot; ## [95] &quot;Mirabilis.sp&quot; &quot;Mollugo.verticillata&quot; ## [97] &quot;Monarda.sp&quot; &quot;Muhlenbergia.emersleyi&quot; ## [99] &quot;Muhlenbergia.longiligula&quot; &quot;Muhlenbergia.sp&quot; ## [101] &quot;Nolina.microcarpa&quot; &quot;Ophioglossum.engelmannii&quot; ## [103] &quot;Opuntia.chlorotica&quot; &quot;Opuntia.sp&quot; ## [105] &quot;Packera.neomexicana&quot; &quot;Pediomelum.tenuiflorum&quot; ## [107] &quot;Penstemon.barbatus&quot; &quot;Penstemon.eatonii&quot; ## [109] &quot;Penstemon.linarioides&quot; &quot;Penstemon.sp&quot; ## [111] &quot;Phemeranthus.parviflorus&quot; &quot;Phoradendron.leucarpum.ssp.tomentosum&quot; ## [113] &quot;Physalis.hederifolia&quot; &quot;Poa.fendleriana&quot; ## [115] &quot;Polygala.alba&quot; &quot;Polygala.obscura&quot; ## [117] &quot;Polygonum.douglasii&quot; &quot;Portulaca.umbraticola&quot; ## [119] &quot;Pseudognaphalium.canescens&quot; &quot;Psoralidium.tenuiflorum&quot; ## [121] &quot;Quercus.turbinella&quot; &quot;Rhamnus.ilicifolia&quot; ## [123] &quot;Rhus.trilobata&quot; &quot;richness_aggregated&quot; ## [125] &quot;Solanum.elaeagnifolium&quot; &quot;Sporobolus.contractus&quot; ## [127] &quot;Sporobolus.cryptandrus&quot; &quot;Sporobolus.interruptus&quot; ## [129] &quot;Symphyotrichum.falcatum&quot; &quot;Tradescantia.sp&quot; ## [131] &quot;Verbascum.densiflorum&quot; &quot;Verbascum.thapsus&quot; ## [133] &quot;Verbena.bracteata&quot; &quot;Vulpia.microstachys&quot; ## [135] &quot;Vulpia.octoflora&quot; &quot;Xanthisma.gracile&quot; ## [137] &quot;Year&quot; &quot;PlotN&quot; ## [139] &quot;Treatment&quot; &quot;Treatment_status&quot; treatment_pre &lt;- dplyr::select(treatment_pre, -c(Year, PlotN, Treatment, Treatment_status, ...1)) treatment_pret &lt;- t(treatment_pre) treatment_post &lt;- combined_data %&gt;% filter(Treatment == &quot;Treatment&quot; &amp; Treatment_status == &quot;PostTreatment&quot;) colnames(treatment_post) ## [1] &quot;...1&quot; &quot;Acalypha.neomexicana&quot; ## [3] &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; ## [5] &quot;Arabis.perennans&quot; &quot;Arctostaphylos.pringlei&quot; ## [7] &quot;Arctostaphylos.pungens&quot; &quot;Arenaria.lanuginosa.var.saxosa&quot; ## [9] &quot;Aristida.purpurea&quot; &quot;Aristida.schiedeana.var.orcuttiana&quot; ## [11] &quot;Aristida.sp&quot; &quot;Aristida.ternipes&quot; ## [13] &quot;Artemisia.ludoviciana&quot; &quot;Artemisia.sp&quot; ## [15] &quot;Asclepias.nyctaginifolia&quot; &quot;Astragalus.sp&quot; ## [17] &quot;Baccharis.pteronioides&quot; &quot;Bahia.biternata&quot; ## [19] &quot;Bidens.sp&quot; &quot;Boechera.perennans&quot; ## [21] &quot;Bothriochloa.ischaemum&quot; &quot;Bouteloua.curtipendula&quot; ## [23] &quot;Bouteloua.eriopoda&quot; &quot;Bouteloua.gracilis&quot; ## [25] &quot;Bouteloua.hirsuta&quot; &quot;Bouteloua.sp&quot; ## [27] &quot;Brickellia.betonicifolia&quot; &quot;Brickellia.californica&quot; ## [29] &quot;Brickellia.sp&quot; &quot;Bromus.ciliatus&quot; ## [31] &quot;Bromus.tectorum&quot; &quot;Bryophyta.sp&quot; ## [33] &quot;Calliandra.humilis&quot; &quot;Calliandra.humilis.var.reticulata&quot; ## [35] &quot;Calochortus.sp&quot; &quot;Carex.sp&quot; ## [37] &quot;Ceanothus.fendleri&quot; &quot;Chenopodium.fremontii&quot; ## [39] &quot;Coleogyne.ramosissima&quot; &quot;Collinsia.parviflora&quot; ## [41] &quot;Comandra.umbellata&quot; &quot;Conyza.canadensis&quot; ## [43] &quot;Cylindropuntia.sp&quot; &quot;Cynoglossum.officinale&quot; ## [45] &quot;Cyperus.fendlerianus&quot; &quot;Cyperus.sp&quot; ## [47] &quot;Dalea.albiflora&quot; &quot;Datura.sp&quot; ## [49] &quot;Descurainia.obtusa&quot; &quot;Descurainia.sophia&quot; ## [51] &quot;Desmanthus.cooleyi&quot; &quot;Desmodium.batocaulon&quot; ## [53] &quot;Dysphania.graveolens&quot; &quot;Dysphania.pumilio&quot; ## [55] &quot;Echinocereus.sp&quot; &quot;Elymus.elymoides&quot; ## [57] &quot;Eragrostis.curvula&quot; &quot;Eragrostis.intermedia&quot; ## [59] &quot;Erigeron.divergens&quot; &quot;Erigeron.flagellaris&quot; ## [61] &quot;Erigeron.oreophilus&quot; &quot;Eriogonum.jamesii&quot; ## [63] &quot;Eriogonum.wrightii&quot; &quot;Escobaria.sp&quot; ## [65] &quot;Escobaria.vivipara&quot; &quot;Euphorbia.albomarginata&quot; ## [67] &quot;Euphorbia.revoluta&quot; &quot;Euphorbia.schizoloba&quot; ## [69] &quot;Euphorbia.serpyllifolia&quot; &quot;Evolvulus.sericeus&quot; ## [71] &quot;Festuca.arizonica&quot; &quot;Galactia.wrightii&quot; ## [73] &quot;Galium.microphyllum&quot; &quot;Garrya.wrightii&quot; ## [75] &quot;Glandularia.bipinnatifida&quot; &quot;Glandularia.gooddingii&quot; ## [77] &quot;Gutierrezia.sarothrae&quot; &quot;Heliomeris.longifolia.var.annua&quot; ## [79] &quot;Heliomeris.multiflora&quot; &quot;Hesperidanthus.linearifolius&quot; ## [81] &quot;Heterosperma.pinnatum&quot; &quot;Houstonia.wrightii&quot; ## [83] &quot;Hybanthus.verticillatus&quot; &quot;Hymenopappus.filifolius&quot; ## [85] &quot;Juncus.saximontanus&quot; &quot;Koeleria.macrantha&quot; ## [87] &quot;Lepidium.lasiocarpum&quot; &quot;Lonicera.arizonica&quot; ## [89] &quot;Lotus.wrightii&quot; &quot;Lycurus.setosus&quot; ## [91] &quot;Machaeranthera.gracilis&quot; &quot;Mammillaria.sp&quot; ## [93] &quot;Mimosa.aculeaticarpa&quot; &quot;Mimosa.biuncifera&quot; ## [95] &quot;Mirabilis.sp&quot; &quot;Mollugo.verticillata&quot; ## [97] &quot;Monarda.sp&quot; &quot;Muhlenbergia.emersleyi&quot; ## [99] &quot;Muhlenbergia.longiligula&quot; &quot;Muhlenbergia.sp&quot; ## [101] &quot;Nolina.microcarpa&quot; &quot;Ophioglossum.engelmannii&quot; ## [103] &quot;Opuntia.chlorotica&quot; &quot;Opuntia.sp&quot; ## [105] &quot;Packera.neomexicana&quot; &quot;Pediomelum.tenuiflorum&quot; ## [107] &quot;Penstemon.barbatus&quot; &quot;Penstemon.eatonii&quot; ## [109] &quot;Penstemon.linarioides&quot; &quot;Penstemon.sp&quot; ## [111] &quot;Phemeranthus.parviflorus&quot; &quot;Phoradendron.leucarpum.ssp.tomentosum&quot; ## [113] &quot;Physalis.hederifolia&quot; &quot;Poa.fendleriana&quot; ## [115] &quot;Polygala.alba&quot; &quot;Polygala.obscura&quot; ## [117] &quot;Polygonum.douglasii&quot; &quot;Portulaca.umbraticola&quot; ## [119] &quot;Pseudognaphalium.canescens&quot; &quot;Psoralidium.tenuiflorum&quot; ## [121] &quot;Quercus.turbinella&quot; &quot;Rhamnus.ilicifolia&quot; ## [123] &quot;Rhus.trilobata&quot; &quot;richness_aggregated&quot; ## [125] &quot;Solanum.elaeagnifolium&quot; &quot;Sporobolus.contractus&quot; ## [127] &quot;Sporobolus.cryptandrus&quot; &quot;Sporobolus.interruptus&quot; ## [129] &quot;Symphyotrichum.falcatum&quot; &quot;Tradescantia.sp&quot; ## [131] &quot;Verbascum.densiflorum&quot; &quot;Verbascum.thapsus&quot; ## [133] &quot;Verbena.bracteata&quot; &quot;Vulpia.microstachys&quot; ## [135] &quot;Vulpia.octoflora&quot; &quot;Xanthisma.gracile&quot; ## [137] &quot;Year&quot; &quot;PlotN&quot; ## [139] &quot;Treatment&quot; &quot;Treatment_status&quot; treatment_post &lt;- dplyr::select(treatment_post, -c(Year, PlotN, Treatment, Treatment_status, ...1)) treatment_postt &lt;- t(treatment_post) #combine matrices, control_pre, control_post, treatment_pre, treatment_post, in a matrix of N lists combined_list &lt;- list(control_pre = control_pre, control_post = control_post, treatment_pre = treatment_pre, treatment_post = treatment_post) lapply(combined_list, head) ## $control_pre ## # A tibble: 6 × 135 ## Acalypha.neomexicana Allium.sp Ambrosia.psilostachya Arabis.perennans Arctostaphylos.pring…¹ Arctostaphylos.pungens ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 1 ## 2 0 0 0 0 0 1 ## 3 0 0 0 0 0 1 ## 4 0 1 0 0 0 1 ## 5 0 0 0 0 0 1 ## 6 0 0 0 1 0 1 ## # ℹ abbreviated name: ¹​Arctostaphylos.pringlei ## # ℹ 129 more variables: Arenaria.lanuginosa.var.saxosa &lt;dbl&gt;, Aristida.purpurea &lt;dbl&gt;, ## # Aristida.schiedeana.var.orcuttiana &lt;dbl&gt;, Aristida.sp &lt;dbl&gt;, Aristida.ternipes &lt;dbl&gt;, ## # Artemisia.ludoviciana &lt;dbl&gt;, Artemisia.sp &lt;dbl&gt;, Asclepias.nyctaginifolia &lt;dbl&gt;, Astragalus.sp &lt;dbl&gt;, ## # Baccharis.pteronioides &lt;dbl&gt;, Bahia.biternata &lt;dbl&gt;, Bidens.sp &lt;dbl&gt;, Boechera.perennans &lt;dbl&gt;, ## # Bothriochloa.ischaemum &lt;dbl&gt;, Bouteloua.curtipendula &lt;dbl&gt;, Bouteloua.eriopoda &lt;dbl&gt;, Bouteloua.gracilis &lt;dbl&gt;, ## # Bouteloua.hirsuta &lt;dbl&gt;, Bouteloua.sp &lt;dbl&gt;, Brickellia.betonicifolia &lt;dbl&gt;, Brickellia.californica &lt;dbl&gt;, … ## ## $control_post ## # A tibble: 6 × 135 ## Acalypha.neomexicana Allium.sp Ambrosia.psilostachya Arabis.perennans Arctostaphylos.pring…¹ Arctostaphylos.pungens ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 0 ## 2 0 0 0 0 0 1 ## 3 0 0 0 0 0 1 ## 4 0 0 0 0 0 1 ## 5 0 0 0 0 0 1 ## 6 0 0 0 0 0 1 ## # ℹ abbreviated name: ¹​Arctostaphylos.pringlei ## # ℹ 129 more variables: Arenaria.lanuginosa.var.saxosa &lt;dbl&gt;, Aristida.purpurea &lt;dbl&gt;, ## # Aristida.schiedeana.var.orcuttiana &lt;dbl&gt;, Aristida.sp &lt;dbl&gt;, Aristida.ternipes &lt;dbl&gt;, ## # Artemisia.ludoviciana &lt;dbl&gt;, Artemisia.sp &lt;dbl&gt;, Asclepias.nyctaginifolia &lt;dbl&gt;, Astragalus.sp &lt;dbl&gt;, ## # Baccharis.pteronioides &lt;dbl&gt;, Bahia.biternata &lt;dbl&gt;, Bidens.sp &lt;dbl&gt;, Boechera.perennans &lt;dbl&gt;, ## # Bothriochloa.ischaemum &lt;dbl&gt;, Bouteloua.curtipendula &lt;dbl&gt;, Bouteloua.eriopoda &lt;dbl&gt;, Bouteloua.gracilis &lt;dbl&gt;, ## # Bouteloua.hirsuta &lt;dbl&gt;, Bouteloua.sp &lt;dbl&gt;, Brickellia.betonicifolia &lt;dbl&gt;, Brickellia.californica &lt;dbl&gt;, … ## ## $treatment_pre ## # A tibble: 6 × 135 ## Acalypha.neomexicana Allium.sp Ambrosia.psilostachya Arabis.perennans Arctostaphylos.pring…¹ Arctostaphylos.pungens ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 1 ## 2 0 0 0 0 0 1 ## 3 0 0 0 0 0 1 ## 4 0 0 0 0 0 1 ## 5 0 0 0 0 0 1 ## 6 0 0 0 0 0 1 ## # ℹ abbreviated name: ¹​Arctostaphylos.pringlei ## # ℹ 129 more variables: Arenaria.lanuginosa.var.saxosa &lt;dbl&gt;, Aristida.purpurea &lt;dbl&gt;, ## # Aristida.schiedeana.var.orcuttiana &lt;dbl&gt;, Aristida.sp &lt;dbl&gt;, Aristida.ternipes &lt;dbl&gt;, ## # Artemisia.ludoviciana &lt;dbl&gt;, Artemisia.sp &lt;dbl&gt;, Asclepias.nyctaginifolia &lt;dbl&gt;, Astragalus.sp &lt;dbl&gt;, ## # Baccharis.pteronioides &lt;dbl&gt;, Bahia.biternata &lt;dbl&gt;, Bidens.sp &lt;dbl&gt;, Boechera.perennans &lt;dbl&gt;, ## # Bothriochloa.ischaemum &lt;dbl&gt;, Bouteloua.curtipendula &lt;dbl&gt;, Bouteloua.eriopoda &lt;dbl&gt;, Bouteloua.gracilis &lt;dbl&gt;, ## # Bouteloua.hirsuta &lt;dbl&gt;, Bouteloua.sp &lt;dbl&gt;, Brickellia.betonicifolia &lt;dbl&gt;, Brickellia.californica &lt;dbl&gt;, … ## ## $treatment_post ## # A tibble: 6 × 135 ## Acalypha.neomexicana Allium.sp Ambrosia.psilostachya Arabis.perennans Arctostaphylos.pring…¹ Arctostaphylos.pungens ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 0 0 1 ## 2 0 0 0 0 0 1 ## 3 0 0 0 0 0 1 ## 4 0 0 0 0 0 1 ## 5 0 0 0 0 0 1 ## 6 0 0 0 0 0 1 ## # ℹ abbreviated name: ¹​Arctostaphylos.pringlei ## # ℹ 129 more variables: Arenaria.lanuginosa.var.saxosa &lt;dbl&gt;, Aristida.purpurea &lt;dbl&gt;, ## # Aristida.schiedeana.var.orcuttiana &lt;dbl&gt;, Aristida.sp &lt;dbl&gt;, Aristida.ternipes &lt;dbl&gt;, ## # Artemisia.ludoviciana &lt;dbl&gt;, Artemisia.sp &lt;dbl&gt;, Asclepias.nyctaginifolia &lt;dbl&gt;, Astragalus.sp &lt;dbl&gt;, ## # Baccharis.pteronioides &lt;dbl&gt;, Bahia.biternata &lt;dbl&gt;, Bidens.sp &lt;dbl&gt;, Boechera.perennans &lt;dbl&gt;, ## # Bothriochloa.ischaemum &lt;dbl&gt;, Bouteloua.curtipendula &lt;dbl&gt;, Bouteloua.eriopoda &lt;dbl&gt;, Bouteloua.gracilis &lt;dbl&gt;, ## # Bouteloua.hirsuta &lt;dbl&gt;, Bouteloua.sp &lt;dbl&gt;, Brickellia.betonicifolia &lt;dbl&gt;, Brickellia.californica &lt;dbl&gt;, … combined_list &lt;- lapply(combined_list, function(x) { if(is.data.frame(x)) as.matrix(x) else x }) str(combined_list) ## List of 4 ## $ control_pre : num [1:6, 1:135] 0 0 0 0 0 0 0 0 0 1 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:135] &quot;Acalypha.neomexicana&quot; &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; &quot;Arabis.perennans&quot; ... ## $ control_post : num [1:6, 1:135] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:135] &quot;Acalypha.neomexicana&quot; &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; &quot;Arabis.perennans&quot; ... ## $ treatment_pre : num [1:16, 1:135] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:135] &quot;Acalypha.neomexicana&quot; &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; &quot;Arabis.perennans&quot; ... ## $ treatment_post: num [1:16, 1:135] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : NULL ## .. ..$ : chr [1:135] &quot;Acalypha.neomexicana&quot; &quot;Allium.sp&quot; &quot;Ambrosia.psilostachya&quot; &quot;Arabis.perennans&quot; ... Now that we have the data in the correct format, let’s analyze! library(iNEXT) # Example of running iNEXT on incidence_raw data results &lt;- suppressWarnings({iNEXT(combined_list, q=0, datatype=&quot;incidence_raw&quot;)}) # Plot the results plot(results) Let’s transform an abundance dataset! This dataset consists of observations of pollinators within an ecoregion for two time periods. Far more observations were collected after the development of cell phones and app-based species identification software like iNaturalist. In order to compare the two assemblages, you must calculate asymptotic diversity estimates using R/E curves. The dataset has been cleaned and spatially-thinned, so that we assume that each observation indicates a distinct individual and thus we have an estimate of abundance within this ecoregion. First, check out the dataset. Here, each row corresponds to an observation. In order to run analyses on this data, we must transform the data into either an S by N abundance matrix (bird data), or N lists of species abundances (spider data). Let’s transform our data into the list structure. #you have to adjust the code to port in this datasets! library(readr) url3 &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1bpLmtDDEPAdSaTPFPNg33hyj6y0viqxk&quot; arcticlocs &lt;- read_csv(url3) ## Rows: 388 Columns: 24 ## ── Column specification ───────────────────────────────────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (12): NA_L3CODE, NA_L3NAME, NA_L2NAME, NA_L1NAME, NA_L3KEY, NA_L2KEY, NA_L1KEY, genus, countryCode, family, t... ## dbl (11): id.y, NA_L2CODE, NA_L1CODE, Shape_Leng, Shape_Area, LON, LAT, gbifID, year, coords.x1, coords.x2 ## lgl (1): optional ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. #time periods arcticlocsTP1 &lt;- filter(arcticlocs, year &gt;= 1939 &amp; year &lt; 1979) arcticlocsTP2 &lt;- filter(arcticlocs, year &gt;= 1980 &amp; year &lt; 2021) obsTP1 &lt;- length(arcticlocsTP1$genus) obsTP2 &lt;- length(arcticlocsTP2$genus) lengthsofobs &lt;- c(obsTP1, obsTP2) #Time period 1 arcticlocsTP1sum &lt;- plyr::count(arcticlocsTP1, &#39;genus&#39;) arcticlocsTP1unitnum &lt;- length(arcticlocsTP1$genus) arcticlocsTP1V &lt;- c(arcticlocsTP1unitnum, arcticlocsTP1sum$freq) arcticlocsTP1Vn &lt;- as.numeric(arcticlocsTP1V) #Time period 2 arcticlocsTP2sum &lt;- plyr::count(arcticlocsTP2, &#39;genus&#39;) arcticlocsTP2unitnum &lt;- length(arcticlocsTP2$genus) arcticlocsTP2V &lt;- c(arcticlocsTP2unitnum, arcticlocsTP2sum$freq) arcticlocsTP2Vn &lt;- as.numeric(arcticlocsTP2V) arctic &lt;- list(&quot;TimePeriod1&quot; = arcticlocsTP1Vn, &quot;TimePeriod2&quot; = arcticlocsTP2Vn) str(arctic) ## List of 2 ## $ TimePeriod1: num [1:11] 13 1 2 1 1 1 3 1 1 1 ... ## $ TimePeriod2: num [1:163] 365 3 4 1 1 1 2 2 3 3 ... Now, our data consists of two lists - one for time period one and one describing time period two. Let’s run our analyses. Note that ggiNEXT is a wrapper for the standard ggplot function, and can be manipulated just like ggplot. #establish the max extrapolation number (two times the largest sample) and the knots manually maxrun &lt;- (max(obsTP1, obsTP2)*2) t &lt;- seq(1, maxrun, by=1) out.inc &lt;- iNEXT(arctic, q=c(0,1,2), datatype=&quot;incidence_freq&quot;, size=t, nboot=200) arcticfigure &lt;- ggiNEXT(out.inc, facet.var=&quot;Order.q&quot;, se =TRUE, grey = TRUE) + ylab(&quot;Generic diversity&quot;) + ggtitle(&quot;Arctic cordillera&quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)); arcticfigure ## Scale for fill is already present. ## Adding another scale for fill, which will replace the existing scale. ## Scale for colour is already present. ## Adding another scale for colour, which will replace the existing scale. Wonderful! You’ve transformed your data into the iNEXT format according to your data type. Now, work through an example with your own data! Put your data into the correct format based on your data, and analyze away. "],["what-are-structural-equation-models.html", "Chapter 26 What are structural equation models? 26.1 Anatomy of a Structural Equation Model 26.2 Building Multivariate Models 26.3 From Concept to Model 26.4 Common Structures in Causal Diagrams 26.5 Chapter 5: Covariance-Based Estimation in SEM 26.6 Getting Started with lavaan – Model Specification, Estimation, and Interpretation 26.7 Part 1: What is Piecewise SEM? 26.8 Overview", " Chapter 26 What are structural equation models? Structural Equation Modeling (SEM) is a statistical technique that enables the modeling of complex causal relationships among variables. It extends regression by allowing: Simultaneous estimation of multiple equations Inclusion of both observed and latent (unmeasured) variables Specification of indirect effects and feedback loops Assessment of model fit against observed data SEM is often visualized as a path diagram, where arrows represent hypothesized relationships between variables. 26.0.1 When Should You Use SEM? Use SEM when: You have multiple dependent variables that may influence one another You want to estimate direct and indirect effects between variables You need to account for measurement error in survey or ecological constructs You are testing a theory or conceptual model with multiple pathways Your system includes latent constructs like “habitat quality” or “disturbance pressure” inferred from several indicators SEM is ideal when a single regression model is too simplistic to capture the interdependent relationships in your system. 26.0.2 How is SEM Different from Multiple Regression? Feature Multiple Regression SEM Number of equations One Many simultaneously Latent variables ❌ Not allowed ✅ Allowed Indirect effects ❌ Manual calculation ✅ Modeled explicitly Measurement error (in predictors) ❌ Ignored ✅ Can be modeled Model fit assessment R², AIC, etc. Chi-square test, RMSEA, CFI, TLI, etc. In multiple regression, you can only estimate one equation at a time, with no ability to model feedback loops or measurement error. In contrast, SEM treats your whole system as a network, testing how well your full model fits the data. 26.0.3 Summary SEM is a flexible and powerful framework for: Exploring and testing theoretical models Modeling complex causal chains Incorporating unobservable constructs It provides more rigorous insights into systems where variables are interdependent, influenced by hidden factors, and subject to error. 26.1 Anatomy of a Structural Equation Model Structural Equation Modeling (SEM) provides a flexible framework for representing complex causal relationships between variables. In this chapter, we break down the structure of an SEM into its key components. 26.1.1 Types of Variables SEM uses two main types of variables: Observed variables: Directly measured values (e.g., soil nitrogen, species richness). Latent variables: Not directly observed, but inferred from multiple observed indicators (e.g., a latent “Disturbance Pressure” factor inferred from road density, fire frequency, and grazing). Each variable also plays one of two roles in the model: Variable Role Description Exogenous Predictor variables not explained by any other variables in the model. Endogenous Response variables influenced by one or more other variables in the model. Note: Endogenous variables can act as both outcomes and predictors of other endogenous variables. 26.1.2 Path Diagrams Path diagrams are a central feature of SEM. They visually represent the hypothesized relationships between variables using arrows: Single-headed arrows (→) represent causal/predictive paths Double-headed arrows (↔︎) represent covariances (associations not assumed to be causal) 26.2 Building Multivariate Models 26.2.1 What is Causality? Causality refers to understanding the directional effect one variable has on another. In SEM, causal relationships are explicitly modeled, unlike in multiple regression where they may be implied but not specified. Judea Pearl’s Ladder of Causation is a framework for understanding levels of causal reasoning: 1. Observation — associational reasoning (“what is”) 2. Intervention — action-based reasoning (“what if I do”) 3. Counterfactuals — imagining alternate realities (“what if I had done”) We ascend the ladder by incorporating more assumptions and a model of the system. 26.2.2 Why move beyond Multiple Regression? Multiple regression models: • Estimate associations while “controlling for” other variables • Assume independence among predictors • Do not explicitly encode causal structure Causal models (via SEM): • Represent direction and strength of causal pathways • Account for mediators, confounders, and latent variables • Enable testing of hypotheses about mechanisms 26.2.3 Meta-Modeling Your System Before diving into data, build a conceptual model: 1. Define your research Purpose: • Discovery: Are you exploring patterns or relationships for the first time? • Hypothesis Testing: Are you evaluating a specific theoretical prediction? • Prediction: Are you aiming to forecast outcomes under new conditions? 🔍 Tip: Discovery-based models might be more flexible or exploratory, while hypothesis testing demands tighter causal logic and pre-specified relationships. 2. Identify the Focus: What role do your variables play in the system?: • Drivers: Variables that initiate causal change (e.g., treatments, environmental context). • Responses: Outcome variables (e.g., pollinator abundance). • Mediators: Variables that transmit effects from causes to outcomes (e.g., plant cover). • Theory Testing: Are you evaluating a known causal pathway or testing a novel ecological hypothesis? 🧠 Make sure the pieces of your model are causal! Avoid throwing in all your variables just because you measured them — each should have a theorized role. 3. Determine Span of Inference: Are your findings meant to be context-specific or to inform broader generalizations?: • Local/System-Specific: Targeted insights for a particular site or management action. • Generalizable Process: Testing broad ecological principles or multi-site patterns. 📏 This choice influences how you frame your model structure and what covariates (like “site”) you might treat as fixed or random. 26.3 From Concept to Model Once your meta-model is constructed: • Reify with data availability in mind • Ensure your DAG closes appropriate backdoors • Align structure with your research purpose “Make sure the pieces of your model are causal!” 26.4 Common Structures in Causal Diagrams In structural equation modeling (SEM) and causal inference, understanding the basic building blocks of causal diagrams is crucial. These structures form the foundation for determining what to control for and what to avoid controlling for. Below are the most common structures you’ll encounter in Directed Acyclic Graphs (DAGs), with brief explanations and examples. 26.4.1 Chains (Mediation Paths) Structure: Cause → Mediator → Effect Interpretation: • The mediator is an intermediate variable through which the cause affects the effect. • If you control for the mediator, you block the indirect path, potentially underestimating the total effect of the cause. Example: Fire frequency → Canopy cover → Soil moisture • If you control for canopy cover, you might miss the full effect that fire frequency has on soil moisture via changes in canopy cover. Visual: library(dagitty) library(ggdag) dag &lt;- dagify( SoilMoisture ~ CanopyCover, CanopyCover ~ Fire, labels = c(Fire = &quot;Fire Frequency&quot;, CanopyCover = &quot;Canopy Cover&quot;, SoilMoisture = &quot;Soil Moisture&quot;), exposure = &quot;Fire&quot;, outcome = &quot;SoilMoisture&quot; ) ggdag(dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() 26.4.2 Colliders Structure: Cause1 → Collider ← Cause2 Interpretation: • A collider is a variable that is influenced by two (or more) variables. • Controlling for the collider opens a path between the two causes, creating spurious associations where none may exist. Example: Soil nitrogen → Plant growth ← Pathogen load • Both soil nitrogen and pathogen load affect plant growth. If you control for plant growth (the collider), it might look like soil nitrogen and pathogens are correlated—even if they’re not. Visual: dag &lt;- dagify( PlantGrowth ~ SoilNitrogen + PathogenLoad, labels = c(SoilNitrogen = &quot;Soil N&quot;, PathogenLoad = &quot;Pathogens&quot;, PlantGrowth = &quot;Plant Growth&quot;), exposure = &quot;SoilNitrogen&quot;, outcome = &quot;PathogenLoad&quot; ) ggdag(dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() 26.4.3 Forks (Confounding Paths) Structure: CommonCause → Cause, CommonCause → Effect Interpretation: • This is a confounding structure, where the common cause explains the observed correlation between two variables. • Controlling for the common cause blocks the backdoor path, helping isolate the causal effect. Example: Soil type → Invasive species cover Soil type → Native species richness • If you don’t control for soil type, you may falsely conclude that invasive cover affects native richness, when both are simply driven by the soil. Visual: dag &lt;- dagify( InvasiveCover ~ SoilType, NativeRichness ~ SoilType, labels = c(SoilType = &quot;Soil Type&quot;, InvasiveCover = &quot;Invasive Cover&quot;, NativeRichness = &quot;Native Richness&quot;), exposure = &quot;InvasiveCover&quot;, outcome = &quot;NativeRichness&quot; ) ggdag(dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() 26.4.4 Descendants of Colliders Key Point: • Controlling for descendants of colliders can also open spurious paths. Example: Imagine you control for a variable like Plant biomass, which is a descendant of a collider (e.g., Herbivory ← Plant defense → Biomass). This can reintroduce bias just like controlling for the collider itself. 26.4.5 Summary of strcutures Structure Do You Control For It? Why? Chain (mediator) ❌ (if estimating total effect) Controls block indirect paths Collider ❌ Conditioning opens spurious paths Fork (common cause) ✅ Controls block confounding Descendant of Collider ❌ Opens collider paths indirectly Understanding these structures helps you build better SEMs, choose correct adjustment sets, and avoid introducing bias into your models. 26.4.6 Identifying Causality You don’t need to know all mechanisms to make causal claims. But you do need to: • Map the system (e.g., using DAGs) • Identify and control for confounders • Avoid controlling for mediators 26.4.7 Backdoor Criterion To estimate a causal effect of X → Y, block all backdoor paths (those that flow into X) by conditioning on appropriate variables not affected by X. 26.4.8 Frontdoor Criterion Useful when you can’t block all backdoor paths. Identify a mediator that: • Is affected by X • Affects Y • Is not affected by any confounders of X and Y 26.4.9 Build your own DAG: SEM Case Study on Pollinator Impacts of Vegetation Management This research project explores how different Integrated Vegetation Management (IVM) treatments conducted by Arizona Public Service (APS) on powerline rights-of-way (ROWs) affect pollinator communities, using Structural Equation Modeling (SEM) to untangle direct and indirect effects. 26.4.9.1 Study Context APS manages vegetation beneath powerlines for safety and access, using a mix of: • Mechanical removal • Herbicide application • Combined Mechanical + Herbicide • Untreated (Control) plots Understanding how these treatments influence floral resources is key to predicting changes in pollinator abundance and diversity. 26.4.9.2 Data Collection At 3 sites, treatments were applied in a randomized block design across plots categorized as: • Control • Herbicide • Mechanical • Mechanical + Herbicide Researchers want to test whether management effects on pollinators are mediated through floral resources, or whether there are residual (direct) effects of treatment type beyond vegetation structure. 26.4.9.3 Key Variables and Their Roles Exogenous (Independent) Variables: • Treatment (main predictor—e.g., herbicide, mowing) • Soil substrate (moderator/stratifier of treatment effects) • Cattle presence (a confounder—not caused by treatment) Plant Community Mediators: • Plant richness • Plant cover • Plant height • Ceanothus presence/abundance • Woody debris Pollinator Response Variables: • Pollinator abundance • Pollinator richness library(dagitty) library(ggdag) library(tidyverse) # Define the DAG ivm_dag &lt;- dagitty(&quot; dag { Treatment Soil_Substrate Cattle Plant_Richness Plant_Cover Plant_Height Ceanothus Woody_Debris Pollinator_Richness Pollinator_Abundance Treatment -&gt; Plant_Richness Treatment -&gt; Plant_Cover Treatment -&gt; Plant_Height Treatment -&gt; Ceanothus Treatment -&gt; Woody_Debris Soil_Substrate -&gt; Treatment Soil_Substrate -&gt; Plant_Richness Soil_Substrate -&gt; Plant_Cover Soil_Substrate -&gt; Woody_Debris Cattle -&gt; Plant_Richness Cattle -&gt; Plant_Cover Cattle -&gt; Pollinator_Abundance Cattle -&gt; Pollinator_Richness Plant_Richness -&gt; Pollinator_Richness Plant_Richness -&gt; Pollinator_Abundance Plant_Cover -&gt; Pollinator_Richness Plant_Cover -&gt; Pollinator_Abundance Plant_Height -&gt; Pollinator_Richness Plant_Height -&gt; Pollinator_Abundance Ceanothus -&gt; Pollinator_Richness Ceanothus -&gt; Pollinator_Abundance Woody_Debris -&gt; Pollinator_Richness Woody_Debris -&gt; Pollinator_Abundance } &quot;) node_roles &lt;- tibble( name = c( &quot;Treatment&quot;, &quot;Soil_Substrate&quot;, &quot;Cattle&quot;, &quot;Plant_Richness&quot;, &quot;Plant_Cover&quot;, &quot;Plant_Height&quot;, &quot;Ceanothus&quot;, &quot;Woody_Debris&quot;, &quot;Pollinator_Richness&quot;, &quot;Pollinator_Abundance&quot; ), role = c( &quot;Treatment&quot;, &quot;Context&quot;, &quot;Context&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Pollinator&quot;, &quot;Pollinator&quot; ) ) # Get layout and merge roles dag_df &lt;- tidy_dagitty(ivm_dag, layout = &quot;nicely&quot;) %&gt;% left_join(node_roles, by = &quot;name&quot;) # Plot using ggplot2 with corrected edge structure ggplot() + geom_segment( data = dag_df %&gt;% filter(!is.na(xend)), aes(x = x, y = y, xend = xend, yend = yend), arrow = arrow(length = unit(0.02, &quot;npc&quot;)), color = &quot;grey40&quot; ) + geom_point( data = dag_df %&gt;% filter(!is.na(x)), aes(x = x, y = y, color = role), size = 8, alpha = 0.85 ) + geom_text( data = dag_df %&gt;% filter(!is.na(x)), aes(x = x, y = y, label = name), color = &quot;black&quot;, size = 4 ) + scale_color_manual(values = c( &quot;Treatment&quot; = &quot;#FB7E21FF&quot;, &quot;Context&quot; = &quot;#A91601FF&quot;, &quot;Plant&quot; = &quot;#18DDC2FF&quot;, &quot;Pollinator&quot; = &quot;#00468BFF&quot; )) + labs( title = &quot;Hypothesized DAG for Pollinator Response to IVM Treatment&quot;, color = &quot;Variable Type&quot; ) + theme_void() 26.4.10 Adjustment Sets An adjustment set is a group of variables you control for (include as covariates) in order to estimate the causal effect of one variable (say, Treatment) on another (say, Pollinator_Richness) without bias. In short, it blocks backdoor paths — those sneaky alternative routes through which spurious associations can travel. To find variables to condition on: adjustmentSets(ivm_dag, exposure = &quot;Treatment&quot;, outcome = &quot;Pollinator_Richness&quot;) ## { Soil_Substrate } adjustmentSets(ivm_dag, exposure = &quot;Treatment&quot;, outcome = &quot;Pollinator_Abundance&quot;) ## { Soil_Substrate } Interpretation: To estimate the total causal effect of Treatment on Pollinator_Richness, you only need to adjust for Soil_Substrate. This means: • Soil_Substrate is a backdoor variable — it opens a non-causal path because it influences both Treatment and plant variables that, in turn, influence pollinators. • Controlling for Soil_Substrate blocks that spurious path and gives you an unbiased estimate of the effect of Treatment. Do not control for: • Mediators, like Plant_Richness, Plant_Cover, or Woody_Debris, if you want the total effect of treatment. • Colliders, such as anything caused by both Treatment and another variable (you don’t have a clear collider in this DAG, but good to keep in mind). Example in a model: • model &lt;- lm(Pollinator_Richness ~ Treatment + Soil_Substrate, data = your_data) GREG HERE YOU NEED TO CREATE YOUR FINAL DATASET BY SUMMARIZING THE VEG DATA AND ADDING YOUR POLLINATOR DATA PER PLOT. THEN WE CAN ADJUST THE CODE BELOW TO TEST WHETHER YOU REPLICATION / DATA ARE OK AND THEN MOVE ONTO NEXT STEPS 26.5 Chapter 5: Covariance-Based Estimation in SEM library(lavaan) library(mvtnorm) library(mvnormtest) library(psych) ## ## Attaching package: &#39;psych&#39; ## The following object is masked from &#39;package:lavaan&#39;: ## ## cor2cov ## The following objects are masked from &#39;package:terra&#39;: ## ## describe, distance, rescale ## The following object is masked from &#39;package:faraway&#39;: ## ## logit ## The following objects are masked from &#39;package:ggplot2&#39;: ## ## %+%, alpha ## The following object is masked from &#39;package:car&#39;: ## ## logit ## The following object is masked from &#39;package:fields&#39;: ## ## describe library(Matrix) 26.5.1 What is Covariance-Based SEM? Structural Equation Modeling (SEM) using covariance-based maximum likelihood estimation fits parameters so that the model-implied covariance matrix matches the observed covariance matrix as closely as possible. Unlike individual regression models: SEM accounts for how estimation of one parameter affects others. SEM allows for modeling feedbacks and latent variables. SEM is based on maximum likelihood estimation (MLE). 26.5.2 Maximum Likelihood Estimation In SEM, MLE identifies parameters that maximize the likelihood of observing the data, given the model. This involves: Exploring the parameter space iteratively. Estimating model fit using a fitting function like ML. Computationally intensive with more parameters. 26.5.3 Assumptions Behind ML Estimation Multivariate normality (use mvnormtest::mshapiro.test()) # Load necessary package if (!requireNamespace(&quot;mvnormtest&quot;, quietly = TRUE)) { install.packages(&quot;mvnormtest&quot;) } library(mvnormtest) # Simulate multivariate normal data set.seed(123) data &lt;- data.frame( x1 = rnorm(100), x2 = rnorm(100), x3 = rnorm(100) ) # Apply Mardia-Shapiro-Wilk test (requires transpose) mshapiro.test(t(data)) ## ## Shapiro-Wilk normality test ## ## data: Z ## W = 0.97635, p-value = 0.0689 No severe skew or missing data No redundant variables (covariance matrix must be positive definite) # Check skew and kurtosis psych::describe(data) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## x1 1 100 0.09 0.91 0.06 0.08 0.89 -2.31 2.19 4.50 0.06 -0.22 0.09 ## x2 2 100 -0.11 0.97 -0.23 -0.17 0.97 -2.05 3.24 5.29 0.63 0.58 0.10 ## x3 3 100 0.12 0.95 0.04 0.09 0.91 -1.76 2.29 4.05 0.32 -0.59 0.09 # Check for missing data summary(is.na(data)) # Should be all FALSE ## x1 x2 x3 ## Mode :logical Mode :logical Mode :logical ## FALSE:100 FALSE:100 FALSE:100 Sufficient sample size relative to parameters # Load the lavaan package if (!requireNamespace(&quot;lavaan&quot;, quietly = TRUE)) { install.packages(&quot;lavaan&quot;) } library(lavaan) # Define a simple SEM model model &lt;- &#39; y1 ~ x1 + x2 y2 ~ y1 &#39; # Simulate data set.seed(123) data &lt;- data.frame( x1 = rnorm(100), x2 = rnorm(100), y1 = rnorm(100), y2 = rnorm(100) ) # Fit the model fit &lt;- sem(model, data = data) # Print summary summary(fit) ## lavaan 0.6-19 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 100 ## ## Model Test User Model: ## ## Test statistic 0.439 ## Degrees of freedom 2 ## P-value (Chi-square) 0.803 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y1 ~ ## x1 -0.133 0.103 -1.289 0.197 ## x2 0.024 0.097 0.244 0.807 ## y2 ~ ## y1 -0.049 0.109 -0.449 0.653 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y1 0.878 0.124 7.071 0.000 ## .y2 1.066 0.151 7.071 0.000 # Count parameters n_params &lt;- lavInspect(fit, &quot;npar&quot;) # Number of free parameters n_obs &lt;- nrow(data) # Portnoy&#39;s rule portnoy_value &lt;- n_params^(3/2) / n_obs portnoy_value ## [1] 0.1118034 26.5.4 Identifiability Before fitting a SEM, you must ensure it is identified — meaning you have enough unique information to estimate all model parameters. 26.5.4.1 Key Rules T-rule: Number of parameters ≤ unique entries in covariance matrix. # Number of unique observed covariances p &lt;- ncol(data) n_cov &lt;- p * (p + 1) / 2 # Compare to number of free parameters n_cov ## [1] 10 lavInspect(fit, &quot;npar&quot;) # Should be ≤ n_cov ## [1] 5 Order condition: For each endogenous variable, incoming paths ≤ connected variables. There’s no direct test — but: • Draw your DAG. • Count how many arrows go into each endogenous variable. • Make sure that number ≤ number of variables related to it. Rank condition: Variables in feedback loops must be influenced by different causes. • For feedback loops: ensure that each variable has at least one unique exogenous predictor. • Use DAG tools (ggdag or dagitty) to visualize and confirm. If these rules are violated, the model is underidentified and cannot be estimated. 26.5.5 Degrees of Freedom DF = number of observed variances/covariances – number of estimated parameters Just-identified models (DF = 0): Can’t test fit Overidentified models (DF &gt; 0): Preferred — allows model fit evaluation library(lavaan) # define model as a lavaan-style string model &lt;- &#39; cover ~ age + elev firesev ~ age + cover &#39; fit &lt;- sem(model, data = keeley) fitMeasures(fit, c(&quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;cfi&quot;, &quot;rmsea&quot;)) ## chisq df pvalue cfi rmsea ## 0.900 1.000 0.343 1.000 0.000 26.5.6 Sample Size Considerations A model’s complexity is limited by your sample size: Rule of thumb: ≥ 5 observations per estimated parameter Preferably: ≥ 20 observations per parameter Use Portnoy’s rule: ρ3/2 / n → 0 Account for: Exogenous variable variances/covariances (often directly estimated from data) Endogenous variable error variances (often derived, not estimated directly) 26.5.7 Summary Covariance-based SEM is powerful but demands careful model design: Check identifiability before estimation Be aware of sample size constraints Use model diagnostics to evaluate fit after estimation 26.5.8 Chapter 6: Structural Equation Modeling in R with lavaan 26.6 Getting Started with lavaan – Model Specification, Estimation, and Interpretation Setting Up Before you begin, make sure the lavaan and lavaanPlot packages are installed: What is lavaan? • lavaan stands for Latent Variable Analysis. • Developed by Yves Rosseel (2010). • Syntax is similar to regression in R using formulas. • It supports latent and observed variables, covariance-based SEM, mediation, path analysis, and more. Example: Post-Fire Plant Recovery We’ll analyze a dataset from Keeley et al. (2006) studying how stand age, fire severity, and other factors affect plant cover. Step 1: Start Simple – A Regression as SEM # Load dataset # Example: keeley &lt;- read.csv(&quot;path/to/your/keeley_data.csv&quot;) # Fit SEM #model1 &lt;- &#39;cover ~ age&#39; #fit1 &lt;- sem(model1, data = keeley) #summary(fit1, standardized = TRUE, rsquare = TRUE) Intercepts and Mean Structures To explicitly estimate intercepts: #fit1_mean &lt;- sem(model1, data = keeley, meanstructure = TRUE) #summary(fit1_mean) Viewing the Model #lavaanPlot(model = fit1, coefs = TRUE, stand = TRUE) Standardized Estimates standardizedSolution(fit1) This gives standardized coefficients and helps compare the relative strength of predictors. Mediation Example: Indirect Effects #model2 &lt;- &#39; # firesev ~ age # cover ~ firesev + age #&#39; #fit2 &lt;- sem(model2, data = keeley) #summary(fit2, standardized = TRUE, rsquare = TRUE) Direct, Indirect, and Total Effects #model3 &lt;- &#39; # firesev ~ af*age # cover ~ fc*firesev + ac*age # Derived # indirect := af * fc # total := ac + (af * fc) #&#39; #fit3 &lt;- sem(model3, data = keeley) #standardizedSolution(fit3) Warnings: Variance Scaling #varTable(fit3) If you get a warning about variances differing by orders of magnitude, consider rescaling your variables or using standardized solutions. Visualizing Complex Models #lavaanPlot( # model = fit3, # coefs = TRUE, # stand = TRUE, # sig = 0.05, # graph_options = list(layout = &quot;circo&quot;) #) Final Exercise Prompt Try fitting the following model: #model_final &lt;- &#39; # rich ~ distance + abiotic + hetero # hetero ~ distance # abiotic ~ distance # abiotic ~~ hetero #&#39; #fit_final &lt;- sem(model_final, data = keeley) #summary(fit_final, standardized = TRUE) 26.6.1 Chapter 7:Assessing Fit and Normality library(lavaan) library(mvnormtest) library(MVN) ## ## Attaching package: &#39;MVN&#39; ## The following object is masked from &#39;package:psych&#39;: ## ## mardia ## The following object is masked from &#39;package:survival&#39;: ## ## royston Overview In this tutorial, we learn how to evaluate model fit and test for normality, key assumptions in covariance-based SEM. We’ll cover: • Standard fit indices (e.g., RMSEA, CFI) • Residuals and modification indices • Normality diagnostics • Remedies for assumption violations Example: Fully Mediated SEM #model_full &lt;- &#39; # firesev ~ age # cover ~ firesev #&#39; #fit_full &lt;- sem(model_full, data = keeley, meanstructure = TRUE) #summary(fit_full, fit.measures = TRUE) Interpretation: • Chi-square (p &gt; 0.05) → Model is not significantly different from the observed data. • Check additional fit indices: RMSEA, CFI, SRMR, AIC, BIC. Fit Indices (Kline 2023 Recommendations) Fit Measure Interpretation Chi-square test Prefer p &gt; 0.05 RMSEA 90% CI lower bound &lt; 0.05 CFI &gt; 0.90 SRMR &lt; 0.10 Diagnosing Misfit with Residuals #residuals(fit_full, type = &quot;cor&quot;) # residual correlations #modificationIndices(fit_full, standardized = FALSE, sort. = TRUE) • Large residuals or modification indices &gt; 3.84 suggest misfit. • Inspect residual correlation between rich and distance. Testing Normality of Residuals library(MVN) # Step 1: Get residuals from lavaan model #resids &lt;- lavPredict(fit_full, type = &quot;ov&quot;) # residuals for observed variables # Optional: check univariate normality visually #apply(resids[, 1:2], 2, function(x) { # qqnorm(x); qqline(x) #}) # Step 2: Multivariate Shapiro-Wilk (for n ≤ 50) #mshapiro.test(t(resids[, 1:2])) # Step 3: Mardia&#39;s test for multivariate normality #MVN::mvn(data = resids[, 1:2], mvn_test = &quot;mardia&quot;) # Step 1: Get residuals from lavaan model #tryCatch({ # resids &lt;- lavPredict(fit_full, type = &quot;ov&quot;) # Make sure residuals are numeric and have no missing values # if (!is.null(resids) &amp;&amp; is.matrix(resids) &amp;&amp; all(is.finite(resids[, 1:2]))) { # Step 2: Multivariate Shapiro-Wilk (for n ≤ 50) # print(mshapiro.test(t(resids[, 1:2]))) # Step 3: Mardia&#39;s test # print(MVN::mvn(data = resids[, 1:2], mvn_test = &quot;mardia&quot;)) # } else { # message(&quot;Residuals are missing, not numeric, or contain NA/Inf values.&quot;) # } #}, error = function(e) { # message(&quot;MVN test failed: &quot;, conditionMessage(e)) #}) ⸻ If Assumptions Are Violated… Option 1: Satorra-Bentler Correction #fit_sb &lt;- sem(model_full, data = keeley, test = &quot;Satorra.Bentler&quot;) #summary(fit_sb) Option 2: Bollen-Stine Bootstrap #fit_bs &lt;- sem(model_full, data = keeley, test = &quot;bollen.stine&quot;, se = &quot;boot&quot;, bootstrap = 1000) #summary(fit_bs) Summary: • Use fit indices and residuals to assess model performance. • Check assumptions of normality; consider corrections if violated. • Explore modification indices to identify potential improvements. 26.6.2 Chapter 8: Comparing Models and Testing Mediation This section introduces two major approaches for comparing SEM models: • Likelihood Ratio Tests (LRTs) for nested models. • Information Criteria (e.g., AIC, AICc) for both nested and non-nested models. We also explore mediation, which refers to how a relationship between two variables is explained by one or more intervening variables. # Load required libraries library(lavaan) library(AICcmodavg) ## ## Attaching package: &#39;AICcmodavg&#39; ## The following objects are masked from &#39;package:MuMIn&#39;: ## ## AICc, DIC, importance ## The following object is masked from &#39;package:lme4&#39;: ## ## checkConv ## The following object is masked from &#39;package:fields&#39;: ## ## predictSE # Fully Mediated Model fullMedModel &lt;- &#39; firesev ~ age cover ~ firesev &#39; fullMedSEM &lt;- sem(fullMedModel, data = keeley) # Partially Mediated Model partialMedModel &lt;- &#39; firesev ~ age cover ~ firesev + age &#39; partialMedSEM &lt;- sem(partialMedModel, data = keeley) # Likelihood Ratio Test (nested models) anova(partialMedSEM, fullMedSEM) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## partialMedSEM 0 359.4 371.9 0.0000 ## fullMedSEM 1 360.7 370.7 3.2974 3.2974 0.15977 1 0.06939 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Interpretation: A non-significant LRT suggests that the simpler (fully mediated) model fits the data about as well as the more complex model. AIC-Based Model Comparison # AICc model comparison aictab( cand.set = list(fullMedSEM, partialMedSEM), modnames = c(&quot;Full&quot;, &quot;Partial&quot;) ) ## ## Model selection based on AICc: ## ## K AICc Delta_AICc AICcWt Cum.Wt LL ## Partial 5 360.11 0.00 0.63 0.63 -174.70 ## Full 4 361.17 1.05 0.37 1.00 -176.35 Interpretation: Models within 2 ΔAICc units are considered roughly equivalent. Higher AIC weight (AICcWt) indicates stronger support for that model. Additional Example: Distance and Species Richness Key Takeaways: • Use LRT for nested models; lower chi-square and higher p-value = simpler model may suffice. • Use AICc for broader comparisons; lower AICc and higher weight = better. • Mediation is central to SEM and can be tested with both approaches. • Fully vs. Partially Mediated models differ by whether direct paths bypass mediators. 26.6.3 Chapter 8: Latent Variables as Drivers What is a Latent Variable? Latent variables are unobserved constructs that we infer from multiple observed indicators. They represent abstract concepts like intelligence, disturbance, or biodiversity. • In SEM, latent variables are drawn as circles. • Observed indicators are squares or rectangles. • Latent variables are typically estimated through Confirmatory Factor Analysis (CFA). Confirmatory Factor Analysis (CFA) CFA allows you to test whether certain observed variables co-vary in ways consistent with an underlying theoretical construct. Example: Aposematism in Poison Frogs # Sample covariance matrix from Santos &amp; Cannatella (2011) # santosCov &lt;- read.table(&quot;https://raw.githubusercontent.com/username/santosCov.txt&quot;, na.strings = #&quot;.&quot;) #santosCov &lt;- as.matrix(santosCov) # Create covariance matrix manually (example values) santosCov &lt;- matrix(c( 1.00, 0.45, 0.38, 0.45, 1.00, 0.50, 0.38, 0.50, 1.00 ), nrow = 3, byrow = TRUE) # Add row and column names (must match your CFA model exactly) colnames(santosCov) &lt;- rownames(santosCov) &lt;- c(&quot;Alkaloid.quantity&quot;, &quot;Alkaloid.diversity&quot;, &quot;Conspicuous.coloration&quot;) # Specify CFA model santosCFA1 &lt;- &#39; Aposematism =~ Alkaloid.quantity + Alkaloid.diversity + Conspicuous.coloration &#39; # Fit the model santosFit1 &lt;- sem(santosCFA1, sample.cov = santosCov, sample.nobs = 21) summary(santosFit1, standardized = TRUE) ## lavaan 0.6-19 ended normally after 23 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 21 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Aposematism =~ ## Alkaloid.qntty 1.000 0.571 0.585 ## Alkalod.dvrsty 1.316 0.714 1.842 0.065 0.751 0.769 ## Conspics.clrtn 1.111 0.572 1.943 0.052 0.634 0.650 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .Alkaloid.qntty 0.627 0.249 2.514 0.012 0.627 0.658 ## .Alkalod.dvrsty 0.388 0.298 1.305 0.192 0.388 0.408 ## .Conspics.clrtn 0.550 0.258 2.133 0.033 0.550 0.578 ## Aposematism 0.326 0.272 1.199 0.230 1.000 1.000 Why Use Latent Variables? • Increase accuracy by pooling information across multiple imperfect indicators. • Reduce measurement error. • Enable modeling of unobservable constructs. Identification Rules (How to Know Your Model Can Be Estimated): 1. T-Rule: Number of estimated parameters ≤ number of unique elements in the covariance matrix. 2. Three-indicator rule: Each latent variable has ≥ 3 uncorrelated indicators → SUFFICIENT. 3. Two-indicator rule: Works for multiple latent variables if indicators don’t share variance → SUFFICIENT. 4. Fixing scale: • Set variance of latent = 1.0, or • Set one loading to 1.0 to put latent on that indicator’s scale. Models with 2 indicators and shared error may be underidentified. Fit a Second Latent Variable: Body Size #santosSize &lt;- &#39; # Size =~ Log.Mass + Log.RMR + Log.Scope #&#39; #santosSizeFit &lt;- sem(santosSize, sample.cov = santosCov, sample.nobs = 21) #summary(santosSizeFit, standardized = TRUE) Combine Latent Variables You can model multiple latent variables and test how they relate to each other. #santosCFA2 &lt;- &#39; # Aposematism =~ Alkaloid.quantity + Alkaloid.diversity + Conspicuous.coloration + #Ant.Mite.Specialization + log.Prey # Scale =~ Log.Mass + Log.RMR + Log.Scope + Conspicuous.coloration #&#39; #santosFit2 &lt;- sem(santosCFA2, sample.cov = santosCov, sample.nobs = 21) #summary(santosFit2, standardized = TRUE) Summary: • Latent variables allow you to estimate unobservable concepts. • CFA is the method used to define latent variables in SEM. • Identification is critical—use the rules to check if your model can be estimated. • Measurement error is reduced by leveraging multiple indicators. 26.6.4 Chapter 10: Latent Responses and Measurement Error library(lavaan) library(semPlot) Overview In this section, we explore how latent variables can be modeled as responses and how to account for measurement error in observed indicators. Latent variables are constructs that cannot be measured directly (e.g., biodiversity, ecosystem health, intelligence), but are inferred from multiple observed indicators. Key Concepts Latent Variables as Responses Latent variables can be endogenous (influenced by other variables in the model). For instance: • A latent construct such as “Habitat Quality” could be influenced by soil moisture, disturbance, and vegetation cover. • Each latent construct is defined by observed indicators, which are imperfect and contain measurement error. Measurement Error SEM is powerful because it separates true score variance from error variance. Each observed variable has two components: • The true score linked to the latent construct. • The error term (random noise or instrument error). Accounting for measurement error prevents biased parameter estimates and inflated correlations. Example: Latent Response Model with Measurement Error We model a latent response Performance, influenced by an observed predictor Treatment, and measured via three indicators: perf1, perf2, perf3. # Define the SEM model &lt;- &#39; # Measurement model Performance =~ perf1 + perf2 + perf3 # Structural model Performance ~ Treatment &#39; # Simulate data set.seed(123) n &lt;- 200 Treatment &lt;- rnorm(n) perf1 &lt;- 0.6*Treatment + rnorm(n, sd = 1) perf2 &lt;- 0.6*Treatment + rnorm(n, sd = 1) perf3 &lt;- 0.6*Treatment + rnorm(n, sd = 1) data &lt;- data.frame(Treatment, perf1, perf2, perf3) # Fit the SEM fit &lt;- sem(model, data = data) summary(fit, standardized = TRUE) ## lavaan 0.6-19 ended normally after 22 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 7 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 1.871 ## Degrees of freedom 2 ## P-value (Chi-square) 0.392 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Performance =~ ## perf1 1.000 0.552 0.489 ## perf2 0.995 0.180 5.522 0.000 0.549 0.499 ## perf3 0.968 0.183 5.288 0.000 0.535 0.467 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Performance ~ ## Treatment 0.570 0.075 7.642 0.000 1.032 0.971 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .perf1 0.969 0.106 9.135 0.000 0.969 0.761 ## .perf2 0.910 0.100 9.058 0.000 0.910 0.751 ## .perf3 1.024 0.110 9.289 0.000 1.024 0.782 ## .Performance 0.017 0.042 0.410 0.682 0.056 0.056 Visualizing the SEM semPaths(fit, &quot;std&quot;, layout = &quot;tree&quot;, whatLabels = &quot;std&quot;) Why Model Latent Responses? • More reliable constructs by combining multiple indicators. • Reduces noise from any single observed variable. • Better reflects theoretical constructs (e.g., stress, biodiversity). Key Assumptions • Indicators are unidimensional (reflect one latent factor). • Measurement errors are uncorrelated. • Sufficient variation and correlation among indicators. Takeaway Modeling latent responses allows you to capture complex, unobserved constructs while accounting for error in measurements. SEM enables estimation of both the relationships among constructs and their measurement structure. 26.6.5 Chapter 11: Local Estimation and D-Separation Overview This chapter introduces local (piecewise) SEM — an approach that decomposes a full SEM into a series of local linear models. It contrasts with global covariance-based SEM in both assumptions and evaluation. Why Use Piecewise SEM? Covariance SEM Piecewise SEM Requires multivariate normality Robust to non-normal data Models all paths simultaneously Fits multiple linear models separately Fit assessed globally (χ², RMSEA) Fit assessed via D-separation tests Difficult with small N More flexible with limited sample size Example: Breaking SEM Into Local Regressions library(piecewiseSEM) # Now check if the functions are available ls(&quot;package:piecewiseSEM&quot;) ## [1] &quot;%~~%&quot; &quot;AIC_psem&quot; &quot;as.psem&quot; &quot;basisSet&quot; &quot;cerror&quot; &quot;coefs&quot; ## [7] &quot;dSep&quot; &quot;evaluateClasses&quot; &quot;fisherC&quot; &quot;getCoefficients&quot; &quot;getDAG&quot; &quot;getSortedPsem&quot; ## [13] &quot;keeley&quot; &quot;LLchisq&quot; &quot;meadows&quot; &quot;multigroup&quot; &quot;partialCorr&quot; &quot;partialResid&quot; ## [19] &quot;psem&quot; &quot;rsquared&quot; &quot;shipley&quot; &quot;stdCoefs&quot; &quot;unstdCoefs&quot; # Simulate data set.seed(123) n &lt;- 100 x &lt;- rnorm(n) y2 &lt;- 0.5 * x + rnorm(n) y1 &lt;- 0.6 * x + 0.4 * y2 + rnorm(n) example_data &lt;- data.frame(x, y1, y2) # Fit piecewise SEM mod_list &lt;- psem( lm(y2 ~ x, data = example_data), lm(y1 ~ x + y2, data = example_data) ) # Test model fit using Fisher&#39;s C fisherC(mod_list) ## Fisher.C df P.Value ## 1 NA 0 NA # Extract standardized coefficients stdCoefs(mod_list) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate sig ## 1 y2 x 0.4475284 0.10687862 98 4.187258 6.172903e-05 0.3895619 *** ## 2 y1 x 0.4549228 0.11372501 97 4.000200 1.236720e-04 0.3509054 *** ## 3 y1 y2 0.4238113 0.09899469 97 4.281152 4.371472e-05 0.3755510 *** dag &lt;- getDAG(mod_list) plot(dag) Directed Separation (D-sep) D-sep tests whether missing paths in your model are truly unnecessary. It evaluates conditional independence assumptions implied by the model. Understanding D-separation • Two variables are D-separated if they are conditionally independent, given a set of other variables. • D-sep claims are testable and form the basis set of the model. # Define DAG g &lt;- dagitty(&quot;dag { x -&gt; y2 -&gt; y1 x -&gt; y1 }&quot;) # View conditional independencies implied by the DAG impliedConditionalIndependencies(g) Model Fit via Fisher’s C library(piecewiseSEM) # Simulate example data set.seed(123) n &lt;- 100 x &lt;- rnorm(n) y2 &lt;- 0.5 * x + rnorm(n) y1 &lt;- 0.6 * x + 0.4 * y2 + rnorm(n) example_data &lt;- data.frame(x, y1, y2) # Fit piecewise SEM mod_list &lt;- psem( lm(y2 ~ x, data = example_data), lm(y1 ~ x + y2, data = example_data) ) # ✅ Get model fit statistics (replaces sem.fit) fisherC(mod_list) ## Fisher.C df P.Value ## 1 NA 0 NA # ✅ Get standardized path coefficients (replaces sem.coefs) stdCoefs(mod_list) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate sig ## 1 y2 x 0.4475284 0.10687862 98 4.187258 6.172903e-05 0.3895619 *** ## 2 y1 x 0.4549228 0.11372501 97 4.000200 1.236720e-04 0.3509054 *** ## 3 y1 y2 0.4238113 0.09899469 97 4.281152 4.371472e-05 0.3755510 *** # ✅ Optional: View DAG plot(getDAG(mod_list)) Interpretation: • p &gt; 0.05 → model is not rejected, D-sep claims hold. • p &lt; 0.05 → model is rejected, missing paths may be important. Summary: • Piecewise SEM is ideal when: • Data violate global SEM assumptions. • Sample size is too small for global SEM. • You want to understand model fit using D-sep logic. • Use dagitty to derive implied independencies. • Use sem.fit() and sem.coefs() for piecewise evaluation. 26.6.6 Chapter 12: Introduction to Piecewise SEM in R Learning Objectives By the end of this tutorial, you will be able to: • Understand how piecewise SEM differs from traditional covariance-based SEM. • Fit a multi-equation SEM using the psem() function. • Evaluate model fit using d-separation tests and Fisher’s C statistic. • Extract path coefficients and R² values. • Visualize model structure and relationships using visreg, DiagrammeR, or plot(). 26.7 Part 1: What is Piecewise SEM? Piecewise SEM uses a local estimation approach: each equation is estimated separately using standard regression (e.g., lm, lmer). This provides greater flexibility, such as: • Including non-normal or mixed-effect models. • Dealing with small sample sizes or nested structures. • Assessing model fit through d-sep tests (Shipley’s test of directed separation). Limitations: • Can’t handle latent variables. • Not suitable for cyclical feedback loops. • Difficult to interpret in overidentified models with correlated errors. Part 2: Load Packages and Data # Load libraries library(piecewiseSEM) library(visreg) library(DiagrammeR) # Load sample data (or use your own) data(keeley) # from piecewiseSEM Part 3: Specify and Fit Your Model # Fit individual models mod1 &lt;- lm(abiotic ~ distance, data = keeley) mod2 &lt;- lm(hetero ~ distance, data = keeley) mod3 &lt;- lm(rich ~ abiotic + hetero, data = keeley) # Combine into a psem object keeley_sem &lt;- psem(mod1, mod2, mod3) Evaluate Model Fit # Directed separation test #dSep(keeley_sem) # Fisher&#39;s C statistic #fisherC(keeley_sem) Interpret: • A non-significant p-value (&gt; 0.05) = model fits. • Significant missing paths → consider adding them and reassessing fit. Part 5: Summarize Coefficients and R² # Coefficients coefs(keeley_sem) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 abiotic distance 0.3998 0.0823 88 4.8562 0e+00 0.4597 *** ## 2 hetero distance 0.0045 0.0013 88 3.4593 8e-04 0.3460 *** ## 3 rich abiotic 0.8136 0.1746 87 4.6586 0e+00 0.4136 *** ## 4 rich hetero 45.0702 11.6797 87 3.8589 2e-04 0.3426 *** # R-squared values rsquared(keeley_sem) ## Response family link method R.squared ## 1 abiotic gaussian identity none 0.2113455 ## 2 hetero gaussian identity none 0.1197074 ## 3 rich gaussian identity none 0.3667967 Part 6: Plot Your Model Option A: Quick Base R Plot keeley_sem &lt;- psem( lm(firesev ~ age + cover, data = keeley), lm(cover ~ age + elev + firesev, data = keeley), data = keeley ) plot(keeley_sem) #Option B: Refined Graph with DiagrammeR # Optional customization plot(keeley_sem, node_attrs = list( x = c(2.5, 2.5, 4, 1), y = c(3, 1, 2, 2), shape = &quot;rectangle&quot;, fillcolor = &quot;white&quot; )) Part 7: Mediation Example mod_firesev &lt;- lm(firesev ~ age, data = keeley) mod_cover &lt;- lm(cover ~ firesev, data = keeley) firesev_model &lt;- psem(mod_firesev, mod_cover) summary(firesev_model) ## | | | 0% | |===========================================================================================================| 100% ## ## Structural Equation Model of firesev_model ## ## Call: ## firesev ~ age ## cover ~ firesev ## ## AIC ## 364.696 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## cover ~ age + ... coef 87 -1.8018 0.075 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 3.297 with P-value = 0.069 and on 1 degrees of freedom ## Fisher&#39;s C = 5.18 with P-value = 0.075 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## firesev age 0.0597 0.0125 88 4.7781 0 0.4539 *** ## cover firesev -0.0839 0.0184 88 -4.5594 0 -0.4371 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## firesev none 0.21 ## cover none 0.19 dSep(firesev_model) ## | | | 0% | |===========================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 cover ~ age + ... coef 87 -1.80184 0.07503437 rsquared(firesev_model) ## Response family link method R.squared ## 1 firesev gaussian identity none 0.2059938 ## 2 cover gaussian identity none 0.1910867 Bonus: Visualize with Covariates Held Constant # Visualize fire severity&#39;s effect on cover visreg(firesev_model[[2]], xvar = &quot;firesev&quot;) Wrap-Up Piecewise SEM offers flexibility and clarity for causal inference in ecology. By evaluating each path independently while testing the full model’s coherence through d-sep and Fisher’s C, you gain both transparency and statistical rigor. 26.7.1 Chapter 13: Nonlinearity and Interaction in SEM Overview In this tutorial, we’ll cover: • Nonlinearities (e.g., polynomial terms) • Centering variables to reduce multicollinearity • Interaction terms • Implementing these in SEM frameworks Example 1: Nonlinear Effects (Cardinale et al. 2009) Load and Prepare Data # Download data url &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1oHBul4_JcqlPFZgYsH3WOIZJRQRw1O4F&quot; cardinale &lt;- read.csv(url) # Check it loaded head(cardinale) ## Stream Well N Chl SR SA ## 1 Adobe Creek 1 0e+00 0.0094 105 26 ## 2 Adobe Creek 5 1e-06 0.0100 105 24 ## 3 Adobe Creek 3 1e-04 0.0613 105 20 ## 4 Adobe Creek 4 1e-02 0.1003 105 23 ## 5 Adobe Creek 2 1e+00 0.2000 105 20 ## 6 Adobe Creek 1 0e+00 0.0377 105 21 # Log-transform variables cardinale$logN &lt;- log10(cardinale$N + 1e-6) cardinale$logN2 &lt;- cardinale$logN^2 cardinale$logChl &lt;- log10(cardinale$Chl) #Fit SEM with piecewiseSEM model1 &lt;- psem( lm(SA ~ logN + logN2 + SR, data = cardinale), lm(logChl ~ SA + logN + logN2, data = cardinale), logN %~~% logN2, data = cardinale ) summary(model1) ## | | | 0% | |===========================================================================================================| 100% ## ## Structural Equation Model of model1 ## ## Call: ## SA ~ logN + logN2 + SR ## logChl ~ SA + logN + logN2 ## logN ~~ logN2 ## ## AIC ## 1192.444 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## logChl ~ SR + ... coef 122 0.6639 0.508 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.458 with P-value = 0.499 and on 1 degrees of freedom ## Fisher&#39;s C = 1.355 with P-value = 0.508 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## SA logN -2.9944 1.5375 123 -1.9476 0.0537 -0.5044 ## SA logN2 -0.4742 0.2424 123 -1.9568 0.0526 -0.5067 ## SA SR 0.3838 0.0359 123 10.6844 0.0000 0.6893 *** ## logChl SA 0.0201 0.004 123 5.0327 0.0000 0.3946 *** ## logChl logN 0.1168 0.0953 123 1.2258 0.2226 0.3858 ## logChl logN2 0.0032 0.015 123 0.2108 0.8334 0.0664 ## ~~logN ~~logN2 -0.9685 - 125 -43.4652 0.0000 -0.9685 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## SA none 0.49 ## logChl none 0.25 Reducing Collinearity via Centering # Center predictors cardinale$logN.cen &lt;- scale(cardinale$logN, scale = FALSE) cardinale$logN2.cen &lt;- cardinale$logN.cen^2 # Check correlation cor(cardinale$logN.cen, cardinale$logN2.cen) ## [,1] ## [1,] 0.5126311 Refit Model with Centered Predictors model2 &lt;- psem( lm(SA ~ logN.cen + logN2.cen + SR, data = cardinale), lm(logChl ~ SA + logN.cen + logN2.cen, data = cardinale), logN.cen %~~% logN2.cen, data = cardinale ) summary(model2) ## | | | 0% | |===========================================================================================================| 100% ## ## Structural Equation Model of model2 ## ## Call: ## SA ~ logN.cen + logN2.cen + SR ## logChl ~ SA + logN.cen + logN2.cen ## logN.cen ~~ logN2.cen ## ## AIC ## 1192.444 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## logChl ~ SR + ... coef 122 0.6639 0.508 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.458 with P-value = 0.499 and on 1 degrees of freedom ## Fisher&#39;s C = 1.355 with P-value = 0.508 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## SA logN.cen 0.3668 0.446 123 0.8223 0.4125 0.0618 ## SA logN2.cen -0.4742 0.2424 123 -1.9568 0.0526 -0.1470 ## SA SR 0.3838 0.0359 123 10.6844 0.0000 0.6893 *** ## logChl SA 0.0201 0.004 123 5.0327 0.0000 0.3946 *** ## logChl logN.cen 0.0944 0.0275 123 3.4320 0.0008 0.3116 *** ## logChl logN2.cen 0.0032 0.015 123 0.2108 0.8334 0.0193 ## ~~logN.cen ~~logN2.cen 0.5126 - 125 6.6752 0.0000 0.5126 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## SA none 0.49 ## logChl none 0.25 Try lavaan for the Same Model Example 2: Interaction Effects (Keeley et al.) Center and Create Interaction url2 &lt;- &quot;https://drive.google.com/uc?export=download&amp;id=1YTsFP1T__Hn13hTvj9TVOK-wbGDxLd01&quot; # Try to read the CSV directly keeley &lt;- read.csv(url2) keeley$age_cent &lt;- scale(keeley$age, scale = FALSE) keeley$fire_cent &lt;- scale(keeley$firesev, scale = FALSE) keeley$int_term &lt;- keeley$age_cent * keeley$fire_cent Fit SEM with Interaction in piecewiseSEM keeley_int &lt;- psem( lm(cover ~ age_cent * fire_cent, data = keeley), lm(fire_cent ~ age_cent, data = keeley), data = keeley ) summary(keeley_int) ## ## Structural Equation Model of keeley_int ## ## Call: ## cover ~ age_cent * fire_cent ## fire_cent ~ age_cent ## ## AIC ## 362.993 ## ## --- ## Tests of directed separation: ## ## No independence claims present. Tests of directed separation not possible. ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0 with P-value = 1 and on 0 degrees of freedom ## Fisher&#39;s C = NA with P-value = NA and on 0 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## cover age_cent -0.0050 0.0027 86 -1.8810 0.0634 -0.1985 ## cover fire_cent -0.0684 0.0203 86 -3.3752 0.0011 -0.3561 ** ## cover age_cent:fire_cent -0.0021 0.0014 86 -1.5263 0.1306 -0.1438 ## fire_cent age_cent 0.0597 0.0125 88 4.7781 0.0000 0.4539 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## cover none 0.24 ## fire_cent none 0.21 Optional: Fit Interaction SEM with lavaan Final Notes: • Polynomial terms allow us to model curvature. • Centering reduces collinearity and changes interpretation. • Interaction terms help model conditional effects. • Both lavaan and piecewiseSEM support these techniques. 26.7.2 Chapter 14: GLMs with SEM using PiecewiseSEM This section integrates Generalized Linear Models (GLMs) into Structural Equation Modeling, especially using piecewiseSEM. It also introduces important adjustments for working with non-normal data, and compares latent theoretic (LT) and observed error (OE) approaches for standardizing coefficients. 26.8 Overview In this tutorial, we’ll: • Learn how to incorporate GLMs into SEM using piecewiseSEM • Understand how to deal with non-normality and directed separation warnings • Compute standardized coefficients using both Latent Theoretic (LT) and Observed Error (OE) approaches Data from Anderson et al. 2010 Example # Simulated structure to mimic Anderson et al. set.seed(42) anderson &lt;- data.frame( biomass.kg = rnorm(100, mean = 10, sd = 2), leafN = rnorm(100, mean = 3, sd = 0.5), landscape = sample(0:1, 100, replace = TRUE), hotspotYN = rbinom(100, 1, 0.4) ) Model: Using GLM in psem library(piecewiseSEM) anderson.sem &lt;- psem( lm(leafN ~ biomass.kg, data = anderson), glm(hotspotYN ~ leafN + biomass.kg + landscape, family = &quot;binomial&quot;, data = anderson) ) summary(anderson.sem) ## | | | 0% | |===========================================================================================================| 100% ## ## Structural Equation Model of anderson.sem ## ## Call: ## leafN ~ biomass.kg ## hotspotYN ~ leafN + biomass.kg + landscape ## ## AIC ## 270.381 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## leafN ~ landscape + ... coef 97 1.1151 0.2676 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 1.274 with P-value = 0.259 and on 1 degrees of freedom ## Fisher&#39;s C = 2.637 with P-value = 0.268 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## leafN biomass.kg 0.0068 0.0219 98 0.3098 0.7574 0.0313 ## hotspotYN leafN -0.1212 0.4652 96 -0.2604 0.7945 -0.0301 ## hotspotYN biomass.kg 0.0374 0.1005 96 0.3720 0.7099 0.0428 ## hotspotYN landscape -0.1553 0.4181 96 -0.3715 0.7103 -0.0427 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## leafN none 0 ## hotspotYN nagelkerke 0 Directed Separation &amp; Non-Normality # Add the &#39;conserve = TRUE&#39; argument to be conservative in tests summary(anderson.sem, conserve = TRUE) ## | | | 0% | |===========================================================================================================| 100% ## ## Structural Equation Model of anderson.sem ## ## Call: ## leafN ~ biomass.kg ## hotspotYN ~ leafN + biomass.kg + landscape ## ## AIC ## 270.381 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## leafN ~ landscape + ... coef 97 1.1151 0.2676 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 1.274 with P-value = 0.259 and on 1 degrees of freedom ## Fisher&#39;s C = 2.637 with P-value = 0.268 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## leafN biomass.kg 0.0068 0.0219 98 0.3098 0.7574 0.0313 ## hotspotYN leafN -0.1212 0.4652 96 -0.2604 0.7945 -0.0301 ## hotspotYN biomass.kg 0.0374 0.1005 96 0.3720 0.7099 0.0428 ## hotspotYN landscape -0.1553 0.4181 96 -0.3715 0.7103 -0.0427 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## leafN none 0 ## hotspotYN nagelkerke 0 If the model includes non-normal endogenous variables (e.g., binary hotspotYN), the direction of independence tests matters. Use: dSep(anderson.sem, direction = c(&quot;hotspotYN &lt;- leafN&quot;)) ## | | | 0% | |===========================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 leafN ~ landscape + ... coef 97 1.115091 0.2675663 Or specify a correlated error structure: anderson.sem2 &lt;- update(anderson.sem, hotspotYN %~~% leafN) dSep(anderson.sem2) ## | | | 0% | |===========================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 leafN ~ landscape + ... coef 97 1.115091 0.2675663 Standardizing Coefficients Latent Theoretic (LT) Approach anderson.glm &lt;- anderson.sem[[2]] Betas &lt;- coefs(anderson.sem)[2:4, 3] # GLM coefficients preds &lt;- predict(anderson.glm, type = &quot;link&quot;) sd.y.LT &lt;- sqrt(var(preds) + pi^2/3) sd.x &lt;- sapply(anderson[, c(&quot;leafN&quot;, &quot;biomass.kg&quot;, &quot;landscape&quot;)], sd) Betas.LT &lt;- Betas * sd.x / sd.y.LT Betas.LT ## leafN biomass.kg landscape ## -0.03014129 0.04284880 -0.04271486 Observed Error (OE) Approach preds_response &lt;- predict(anderson.glm, type = &quot;response&quot;) R &lt;- cor(anderson$hotspotYN, preds_response) sd.y.OE &lt;- sqrt(var(preds_response)) / R Betas.OE &lt;- Betas * sd.x / sd.y.OE Betas.OE ## leafN biomass.kg landscape ## -0.1089265 0.1548496 -0.1543656 Compare LT vs OE Approaches # Indirect effect: leafN → hotspotYN (through biomass.kg) Beta.leafN &lt;- coefs(anderson.sem)$Std.Estimate[1] indirect_LT &lt;- Beta.leafN * Betas.LT[1] indirect_OE &lt;- Beta.leafN * Betas.OE[1] c(LT = indirect_LT, OE = indirect_OE) ## LT.leafN OE.leafN ## -0.0009434225 -0.0034093982 Summary: • Use glm() in psem() for binary or count outcomes. • Add conserve = TRUE for directed separation when variables are non-normal. • Use both LT and OE standardization to interpret effect sizes. 26.8.1 Chapter 15: Categorical Predictors &amp; Multigroup SEM library(lme4) library(piecewiseSEM) library(emmeans) library(lavaan) Overview In this tutorial, we explore: • Using categorical predictors in SEM. • Accounting for random effects (e.g., genotype). • Conducting multigroup SEM across different contexts or study sites. Categorical Predictors and Random Effects Example: Bowen et al. (2017) tested whether Phragmites genotype affects soil microbes and productivity. library(multcompView) # Simulated structure: Genotype nested within Phragmites status (e.g., native, invasive) set.seed(1) n &lt;- 90 bowen &lt;- data.frame( status = factor(rep(c(&quot;native&quot;, &quot;invasive&quot;, &quot;introduced&quot;), each = 30)), Genotype = rep(paste0(&quot;G&quot;, 1:9), each = 10), observed_otus = rnorm(n, mean = 2500, sd = 100), RNA.DNA = rnorm(n, 0.7, 0.05), below.C = rnorm(n, 43, 1), abovebiomass_g = rnorm(n, 2, 0.5) ) # Mixed models for each component div_mod &lt;- lmer(observed_otus ~ status + (1 | Genotype), data = bowen) ## boundary (singular) fit: see help(&#39;isSingular&#39;) activity_mod &lt;- lmer(RNA.DNA ~ status + observed_otus + (1 | Genotype), data = bowen) carbon_mod &lt;- lmer(below.C ~ observed_otus + status + (1 | Genotype), data = bowen) ## boundary (singular) fit: see help(&#39;isSingular&#39;) biomass_mod &lt;- lmer(abovebiomass_g ~ RNA.DNA + observed_otus + below.C + status + (1 | Genotype), data = bowen) ## Warning: Some predictor variables are on very different scales: consider rescaling ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning: Some predictor variables are on very different scales: consider rescaling # Build piecewise SEM bowen_mod &lt;- psem(div_mod, activity_mod, carbon_mod, biomass_mod, data = bowen) summary(bowen_mod) ## | | | 0% ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## | |===========================================================================================================| 100% ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning: Categorical or non-linear variables detected. Please refer to documentation for interpretation of ## Estimates! ## ## Structural Equation Model of bowen_mod ## ## Call: ## observed_otus ~ status ## RNA.DNA ~ status + observed_otus ## below.C ~ observed_otus + status ## abovebiomass_g ~ RNA.DNA + observed_otus + below.C + status ## ## AIC ## 1251.338 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## below.C ~ RNA.DNA + ... coef 85 -0.1384 0.8902 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 3.513 with P-value = 0.061 and on 1 degrees of freedom ## Fisher&#39;s C = 0.233 with P-value = 0.89 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## observed_otus status - - 2.0000 0.0237 0.9766 - ## observed_otus status = native 2508.2458 16.3598 6.0000 153.3172 0.0000 - *** ## observed_otus status = introduced 2511.0278 16.3598 6.0000 153.4872 0.0000 - *** ## observed_otus status = invasive 2513.2775 16.3598 6.0000 153.6247 0.0000 - *** ## RNA.DNA observed_otus 0 1e-04 84.2102 -0.3354 0.7381 - ## RNA.DNA status - - 2.0000 2.0441 0.2111 - ## RNA.DNA status = invasive 0.6835 0.0104 5.9386 65.7032 0.0000 - *** ## RNA.DNA status = native 0.7056 0.0104 5.9389 67.8238 0.0000 - *** ## RNA.DNA status = introduced 0.7119 0.0104 5.9365 68.4304 0.0000 - *** ## below.C observed_otus -4e-04 0.0012 86.0000 -0.3219 0.7483 - ## below.C status - - 2.0000 0.7820 0.4997 - ## below.C status = invasive 42.763 0.1857 5.9128 230.2879 0.0000 - *** ## below.C status = introduced 43.0245 0.1857 5.9100 231.7263 0.0000 - *** ## below.C status = native 43.0658 0.1857 5.9132 231.9142 0.0000 - *** ## abovebiomass_g RNA.DNA 0.1948 1.1068 84.0000 0.1760 0.8607 - ## abovebiomass_g observed_otus 2e-04 6e-04 84.0000 0.4172 0.6776 - ## abovebiomass_g below.C -0.1292 0.0523 84.0000 -2.4692 0.0156 - * ## abovebiomass_g status - - 2.0000 0.2393 0.7943 - ## abovebiomass_g status = introduced 2.0074 0.0911 5.8667 22.0333 0.0000 - *** ## abovebiomass_g status = native 2.0829 0.0905 5.7339 23.0193 0.0000 - *** ## abovebiomass_g status = invasive 2.0871 0.0927 6.2237 22.5236 0.0000 - *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method Marginal Conditional ## observed_otus none 0.00 0.00 ## RNA.DNA none 0.06 0.10 ## below.C none 0.02 0.02 ## abovebiomass_g none 0.07 0.07 Visualizing Categorical Effects Estimated Marginal Means by status: lapply(bowen_mod[-length(bowen_mod)], emmeans, specs = ~status) ## [[1]] ## status emmean SE df lower.CL upper.CL ## introduced 2511 16.4 6 2471 2551 ## invasive 2513 16.4 6 2473 2553 ## native 2508 16.4 6 2468 2548 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## [[2]] ## status emmean SE df lower.CL upper.CL ## introduced 0.712 0.0104 5.94 0.686 0.737 ## invasive 0.684 0.0104 5.94 0.658 0.709 ## native 0.706 0.0104 5.94 0.680 0.731 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## [[3]] ## status emmean SE df lower.CL upper.CL ## introduced 43.0 0.186 5.91 42.6 43.5 ## invasive 42.8 0.186 5.91 42.3 43.2 ## native 43.1 0.186 5.91 42.6 43.5 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## [[4]] ## status emmean SE df lower.CL upper.CL ## introduced 2.01 0.0911 5.87 1.78 2.23 ## invasive 2.09 0.0927 6.22 1.86 2.31 ## native 2.08 0.0905 5.73 1.86 2.31 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 # Post-hoc Tests (Tukey) generic_tukey &lt;- function(x) emmeans(x, list(pairwise ~ status)) lapply(bowen_mod[-length(bowen_mod)], generic_tukey) ## [[1]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 2511 16.4 6 2471 2551 ## invasive 2513 16.4 6 2473 2553 ## native 2508 16.4 6 2468 2548 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive -2.25 23.1 6 -0.097 0.9948 ## introduced - native 2.78 23.1 6 0.120 0.9921 ## invasive - native 5.03 23.1 6 0.217 0.9744 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates ## ## ## [[2]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 0.712 0.0104 5.94 0.686 0.737 ## invasive 0.684 0.0104 5.94 0.658 0.709 ## native 0.706 0.0104 5.94 0.680 0.731 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive 0.02831 0.0147 5.94 1.924 0.2130 ## introduced - native 0.00624 0.0147 5.94 0.424 0.9072 ## invasive - native -0.02207 0.0147 5.94 -1.500 0.3560 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates ## ## ## [[3]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 43.0 0.186 5.91 42.6 43.5 ## invasive 42.8 0.186 5.91 42.3 43.2 ## native 43.1 0.186 5.91 42.6 43.5 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive 0.2615 0.263 5.91 0.996 0.6061 ## introduced - native -0.0413 0.263 5.91 -0.157 0.9865 ## invasive - native -0.3029 0.263 5.92 -1.153 0.5204 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates ## ## ## [[4]] ## $`emmeans of status` ## status emmean SE df lower.CL upper.CL ## introduced 2.01 0.0911 5.87 1.78 2.23 ## invasive 2.09 0.0927 6.22 1.86 2.31 ## native 2.08 0.0905 5.73 1.86 2.31 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## ## $`pairwise differences of status` ## 1 estimate SE df t.ratio p.value ## introduced - invasive -0.07972 0.132 6.41 -0.603 0.8234 ## introduced - native -0.07557 0.128 5.67 -0.592 0.8295 ## invasive - native 0.00415 0.131 6.21 0.032 0.9994 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 3 estimates Multigroup SEM in lavaan Fit the same SEM model to different groups and test whether path coefficients differ. # Create simulated dataset group_df &lt;- data.frame( site = rep(c(&quot;A&quot;, &quot;B&quot;), each = 50), x = rnorm(100), m = rnorm(100), y = rnorm(100) ) # Define a simple SEM model sem_model &lt;- &#39; m ~ a*x y ~ b*m + c*x &#39; # Fit multi-group SEM fit_multi &lt;- lavaan::sem(sem_model, data = group_df, group = &quot;site&quot;) ## Warning: lavaan-&gt;lavParTable(): ## using a single label per parameter in a multiple group setting implies imposing equality constraints across all ## the groups; If this is not intended, either remove the label(s), or use a vector of labels (one for each ## group); See the Multiple groups section in the man page of model.syntax. # View summary summary(fit_multi, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-19 ended normally after 12 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## Number of equality constraints 3 ## ## Number of observations per group: ## A 50 ## B 50 ## ## Model Test User Model: ## ## Test statistic 0.790 ## Degrees of freedom 3 ## P-value (Chi-square) 0.852 ## Test statistic for each group: ## A 0.498 ## B 0.292 ## ## Model Test Baseline Model: ## ## Test statistic 2.093 ## Degrees of freedom 6 ## P-value 0.911 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) -0.131 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -293.784 ## Loglikelihood unrestricted model (H1) -293.389 ## ## Akaike (AIC) 609.568 ## Bayesian (BIC) 638.225 ## Sample-size adjusted Bayesian (SABIC) 603.484 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.130 ## P-value H_0: RMSEA &lt;= 0.050 0.874 ## P-value H_0: RMSEA &gt;= 0.080 0.098 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.029 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [A]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) -0.011 0.099 -0.116 0.908 -0.011 -0.009 ## y ~ ## m (b) -0.105 0.095 -1.106 0.269 -0.105 -0.128 ## x (c) 0.031 0.101 0.310 0.757 0.031 0.031 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.144 0.170 0.844 0.398 0.144 0.119 ## .y 0.076 0.139 0.544 0.586 0.076 0.077 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 1.445 0.289 5.000 0.000 1.445 1.000 ## .y 0.960 0.192 5.000 0.000 0.960 0.983 ## ## ## Group 2 [B]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) -0.011 0.099 -0.116 0.908 -0.011 -0.014 ## y ~ ## m (b) -0.105 0.095 -1.106 0.269 -0.105 -0.088 ## x (c) 0.031 0.101 0.310 0.757 0.031 0.031 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m -0.116 0.134 -0.860 0.390 -0.116 -0.124 ## .y -0.240 0.160 -1.506 0.132 -0.240 -0.215 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.872 0.174 5.000 0.000 0.872 1.000 ## .y 1.234 0.247 5.000 0.000 1.234 0.991 Constraining Parameters Across Groups You can test whether coefficients differ across groups: A significant p-value means the unconstrained model fits better — i.e., group differences do matter. Summary: • Use lme4 and piecewiseSEM for models with random effects. • lavaan enables multigroup comparisons to test generality across systems. • Post-hoc tools like emmeans help interpret categorical predictors. 26.8.2 Chapter 16: Multigroup SEM in R library(lavaan) What is Multigroup SEM? Multigroup SEM allows you to: • Test whether path coefficients differ between groups. • Assess whether a model holds equally well across groups. • Investigate measurement invariance (i.e., whether constructs are perceived similarly). Example Model Setup We’ll build a simple mediation model where group is a binary factor (“A” vs “B”). # Simulate data set.seed(123) n &lt;- 100 group &lt;- rep(c(&quot;A&quot;, &quot;B&quot;), each = n) x &lt;- rnorm(2 * n) m &lt;- 0.5 * x + rnorm(2 * n) y &lt;- 0.6 * m + 0.3 * x + rnorm(2 * n) data &lt;- data.frame(group, x, m, y) Define the SEM model &lt;- &#39; m ~ a*x y ~ b*m + c*x &#39; Fit Multigroup SEM fit_multi &lt;- sem(model, data = data, group = &quot;group&quot;) ## Warning: lavaan-&gt;lavParTable(): ## using a single label per parameter in a multiple group setting implies imposing equality constraints across all ## the groups; If this is not intended, either remove the label(s), or use a vector of labels (one for each ## group); See the Multiple groups section in the man page of model.syntax. summary(fit_multi, fit.measures = TRUE, standardized = TRUE) ## lavaan 0.6-19 ended normally after 13 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 14 ## Number of equality constraints 3 ## ## Number of observations per group: ## A 100 ## B 100 ## ## Model Test User Model: ## ## Test statistic 6.839 ## Degrees of freedom 3 ## P-value (Chi-square) 0.077 ## Test statistic for each group: ## A 3.618 ## B 3.220 ## ## Model Test Baseline Model: ## ## Test statistic 131.688 ## Degrees of freedom 6 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 0.969 ## Tucker-Lewis Index (TLI) 0.939 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -556.173 ## Loglikelihood unrestricted model (H1) -552.754 ## ## Akaike (AIC) 1134.347 ## Bayesian (BIC) 1170.628 ## Sample-size adjusted Bayesian (SABIC) 1135.779 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.113 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.228 ## P-value H_0: RMSEA &lt;= 0.050 0.139 ## P-value H_0: RMSEA &gt;= 0.080 0.755 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.083 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## ## Group 1 [A]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) 0.453 0.075 6.064 0.000 0.453 0.401 ## y ~ ## m (b) 0.542 0.068 7.947 0.000 0.542 0.460 ## x (c) 0.295 0.078 3.756 0.000 0.295 0.222 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.125 0.094 1.323 0.186 0.125 0.122 ## .y 0.116 0.098 1.178 0.239 0.116 0.096 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 0.885 0.125 7.071 0.000 0.885 0.840 ## .y 0.958 0.135 7.071 0.000 0.958 0.657 ## ## ## Group 2 [B]: ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## m ~ ## x (a) 0.453 0.075 6.064 0.000 0.453 0.387 ## y ~ ## m (b) 0.542 0.068 7.947 0.000 0.542 0.504 ## x (c) 0.295 0.078 3.756 0.000 0.295 0.235 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m -0.041 0.104 -0.397 0.691 -0.041 -0.037 ## .y -0.048 0.094 -0.513 0.608 -0.048 -0.040 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .m 1.074 0.152 7.071 0.000 1.074 0.850 ## .y 0.874 0.124 7.071 0.000 0.874 0.599 Test for Invariance To test for invariance, you can constrain parameters to be equal across groups. # Constrain &#39;a&#39; and &#39;b&#39; to be equal across groups model_constrained &lt;- &#39; m ~ c(a, a)*x y ~ c(b, b)*m + c*x &#39; fit_constrained &lt;- sem(model_constrained, data = data, group = &quot;group&quot;) ## Warning: lavaan-&gt;lavParTable(): ## using a single label per parameter in a multiple group setting implies imposing equality constraints across all ## the groups; If this is not intended, either remove the label(s), or use a vector of labels (one for each ## group); See the Multiple groups section in the man page of model.syntax. anova(fit_multi, fit_constrained) # Chi-square test for invariance ## Warning: lavaan-&gt;lavTestLRT(): ## some models have the same degrees of freedom ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## fit_multi 3 1134.3 1170.6 6.8386 ## fit_constrained 3 1134.3 1170.6 6.8386 0 0 0 Interpretation: • If the constrained model does not significantly worsen fit, the paths a and b are likely invariant. • If the fit gets significantly worse, the relationship differs between groups and should be modeled separately. Visualizing Standardized Results library(semPlot) semPaths(fit_multi, &quot;std&quot;, layout = &quot;tree&quot;, whatLabels = &quot;std&quot;, edge.label.cex = 1.2) Summary Multigroup SEM lets you: • Evaluate moderation by group. • Test for measurement invariance. • Gain deeper insight into context-dependent pathways. 26.8.3 Chapter 17: Yes! Based on the content in SEM9.2 – Mixed Models Part 2, here’s an R Markdown tutorial on using mixed models in piecewise SEM, covering: • Fixed vs. random effects • Adding group-level predictors • Understanding R² and model comparison • Dealing with hierarchical structure and sample size Introduction This tutorial introduces how to incorporate mixed effects into Structural Equation Modeling using the piecewiseSEM package. Mixed models are essential when your data are hierarchically structured (e.g., plots within sites, streams within watersheds). Step 1: Load Data and Create Variables # Log-transform predictors cardinale$logN &lt;- log10(cardinale$N + 1e-6) cardinale$logN2 &lt;- cardinale$logN^2 cardinale$logChl &lt;- log10(cardinale$Chl) # Centering predictors to reduce multicollinearity cardinale$logN.cen &lt;- scale(cardinale$logN, scale = FALSE) cardinale$logN2.cen &lt;- scale(cardinale$logN^2, scale = FALSE) Step 2: Fit Fixed Effects SEM cardinale.sem &lt;- psem( lm(SA ~ logN.cen + logN2.cen + SR, data = cardinale), lm(logChl ~ SA + logN.cen + logN2.cen, data = cardinale), logN.cen %~~% logN2.cen, data = cardinale ) summary(cardinale.sem) ## | | | 0% | |===========================================================================================================| 100% ## ## Structural Equation Model of cardinale.sem ## ## Call: ## SA ~ logN.cen + logN2.cen + SR ## logChl ~ SA + logN.cen + logN2.cen ## logN.cen ~~ logN2.cen ## ## AIC ## 1192.444 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## logChl ~ SR + ... coef 122 0.6639 0.508 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.458 with P-value = 0.499 and on 1 degrees of freedom ## Fisher&#39;s C = 1.355 with P-value = 0.508 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## SA logN.cen -2.9944 1.5375 123 -1.9476 0.0537 -0.5044 ## SA logN2.cen -0.4742 0.2424 123 -1.9568 0.0526 -0.5067 ## SA SR 0.3838 0.0359 123 10.6844 0.0000 0.6893 *** ## logChl SA 0.0201 0.004 123 5.0327 0.0000 0.3946 *** ## logChl logN.cen 0.1168 0.0953 123 1.2258 0.2226 0.3858 ## logChl logN2.cen 0.0032 0.015 123 0.2108 0.8334 0.0664 ## ~~logN.cen ~~logN2.cen -0.9685 - 125 -43.4652 0.0000 -0.9685 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## SA none 0.49 ## logChl none 0.25 Step 3: Add Random Effects with lme() #cardinale.mixed &lt;- psem( # lme(SA ~ logN.cen + logN2.cen + SR, random = ~1 | Stream, data = cardinale), # lme(logChl ~ SA + logN.cen + logN2.cen, random = ~1 | Stream, data = cardinale), # logN.cen %~~% logN2.cen, # data = cardinale #) #summary(cardinale.mixed) Step 4: Compare Models # Compare R-squared #rsquared(cardinale.sem) #rsquared(cardinale.mixed) • Marginal R²: Variance explained by fixed effects. • Conditional R²: Variance explained by both fixed and random effects. Step 5: Optional — Add Group-Level Predictors To address possible Simpson’s paradox or correlated random effects: # Example if &quot;site_mean&quot; were available cardinale$stream_mean_logN &lt;- ave(cardinale$logN, cardinale$Stream) cardinale$deviation_logN &lt;- cardinale$logN - cardinale$stream_mean_logN This lets you include: • stream_mean_logN (group-level predictor) • deviation_logN (individual-level deviation) Step 6: Basis Set and Fisher’s C #fisherC(cardinale.mixed) Summary: • Mixed models let you account for non-independence among groups. • Use piecewiseSEM with lme() to include random effects. • Be cautious of group-level confounding — use centering and group-level predictors. • Use rsquared() and fisherC() for model comparison and goodness-of-fit. "],["introduction-to-bayesian-statistics.html", "Chapter 27 Introduction To Bayesian Statistics", " Chapter 27 Introduction To Bayesian Statistics "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
