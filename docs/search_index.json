[["index.html", "Working with Spatial Data in R Chapter 1 Introduction 1.1 Learning Objectives 1.2 Using R in This Course 1.3 How to use this online manual 1.4 Course structure", " Working with Spatial Data in R Sara Souther Chapter 1 Introduction Welcome to our statistical exploration of the natural world! My goal is for you to develop an intuitive understanding of statistical analysis—how to choose the right test, understand its assumptions, visualize and interpret results, and communicate findings clearly in a report. By the end of this course, I hope you’ll feel confident tackling any analytical situation you encounter. Let’s get one thing out of the way early: we are not statisticians—whew! We are ecologists and social scientists. Our goal is not to master the theoretical mathematics behind statistics, but rather to apply statistical tools appropriately and thoughtfully. For this reason, we won’t spend much time on the underlying equations or proofs, but instead focus on practical application, critical interpretation, and making our data tell a clear story. For this reason, I will not spend much space describing the mathematical understanding of statistics! 1.1 Learning Objectives By the end of this course, you will be able to: Select appropriate statistical tests based on the type of data (categorical, continuous, ordinal), the structure of your study design, and the research question being asked (e.g., comparing groups, testing relationships, evaluating change over time). Properly run statistical analyses using R, including data cleaning, assumption checking, and the implementation of common tests such as t-tests, ANOVA, chi-squared tests, correlation, linear regression, and basic non-parametric alternatives. Report and interpret statistical results in clear, publication-ready language. This includes summarizing findings with appropriate measures of central tendency and variation (mean, median, standard error), reporting test statistics and p-values, and interpreting both statistical and biological/social significance. Visualize data effectively using plots and figures that aid interpretation and communicate key findings to scientific and non-scientific audiences. Understand the assumptions behind common statistical methods and recognize when those assumptions are violated, along with strategies for addressing or adjusting for these issues. Develop confidence and critical thinking in applying statistical tools to real-world ecological and social science data, with a focus on transparency, reproducibility, and responsible data use. 1.2 Using R in This Course In this course, we use R as a practical tool for ecological and social science data analysis. The focus is not on mathematical theory, but on learning how to choose appropriate statistical tests, check assumptions, visualize data, and interpret results clearly and responsibly. By working with real datasets in R, you will build confidence in reproducible, transparent analysis and develop the skills needed to tackle the kinds of statistical questions you will encounter in research and professional practice. To get started, we will go over a few R-best practices for organizing and running code in our first class lecture! 1.2.1 What you need to use R You need two programs: R (does the calculations) RStudio (helps you write and organize code) Think of R as the engine, and RStudio as the dashboard. Install R: Go to: https://www.r-project.org Click Download R from CRAN Choose any mirror (it truly does not matter) Download the version for your operating system (Mac, Windows, or Linux) Install using the default options Install RStudio: Go to: https://posit.co/download/rstudio-desktop/ Download RStudio Desktop (Free) Install using the default options RStudio will automatically find R once both are installed When you open RStudio, you’ll see four panes: Console is where R runs code Source is where you write and save code Environment shows what data you have loaded Plots/Files/Help is where figures and help appear You can move the order in which these panes appear. 1.2.2 Create an R file What is an R file? An R file is a script or a plain text file that contains R code. R files allow you to write, save, and rerun analyses step by step, which is essential for reproducible science. Instead of typing commands one-by-one into the Console (where they disappear), an R file lets you: Keep a permanent record of your work Add comments explaining what you’re doing Edit and rerun code easily Share your analysis with others Think of an R file as your lab notebook, written in code. How to open (create) an R file in RStudio: Open RStudio Go to File → New File Choose R Script A new tab will open with a blank file. This is a standard R script, usually saved with the extension .R. Save it right away: File → Save As Give it a clear, descriptive name (e.g., data_exploration.R) Hint: Create a folder for this course (you could organize by subfolders for each chapter) and store all of your data and R files in this folder. 1.2.3 Using working directories and opening files RStudio allows you to open files and perform other actions using drop down menus. However, it is far more efficient to set working directories and import files using script within your R file. When you are asked to revisit work for a big project, using working directories allows you to jump right back into the project, because the pertinent data files are linked (this is super great when you have to do revisions, for instance). Datasets are created in Excel and saved as .csv files that are readable by R. 1.2.4 R Packages R comes with built-in functions, but most ecological work uses packages—collections of tools made by other scientists. Using libraries takes two steps. The first time you use a new package, you will install that package, and then load the package. You will need to load packages every time that you use R. 1.2.5 Other information about R code Anything after a # is ignored by R. It is good practice to use hashtags to add information to your code. Use comments to explain: what the code does why you wrote it anything future-you will forget You will encounter issues with your R code. When you do this, your best friends are AI language models, like ChatGPT and Claude. Explain the error, providing the error code, and your code, and the AI assistant will help you. I cannot overstate how jealous I am of all young scientists - debugging used to take hours as you combed over the R documentation looking for hints as to why your code is breaking. 1.2.6 Getting started with R Some of you may be R experts, but for those who are new or need a refresher, check out this R code and import the associated csv file. Formatted R script csv file to import If you need a refresher on working directories and libraries, follow this link 1.3 How to use this online manual Within this manual, we have created chapters for key topics in the application of statistics to the natural world. Chapters contain broad explanations, R code, ecological examples, and assignments, and they are designed to be worked through with RStudio open beside your browser. So within each chapter: Copy the code. Run it. Change the numbers. Break it. Fix it. The learning is in the doing, not the reading. 1.3.1 How this manual fits into the course This manual contains the core concepts, R code, ecological examples, and detailed explanations. It is the reference you return to throughout the semester and beyond. Every chapter builds on previous chapters, so if something in a later chapter feels unfamiliar, look back—the foundation is here. Recorded lectures provide the narrative arc and motivation for each chapter. They hit the key ideas, walk through the reasoning, and connect concepts to the bigger picture. Watch the lecture first, then work through the corresponding chapter at your own pace. Canvas is where you submit assignments, take quizzes, check grades, and find course announcements. Assignment prompts appear at the end of each chapter in this book, but you will submit your work through Canvas. GitHub hosts the source code for this entire book. Every .Rmd file that produces these chapters is visible in the repository. You are welcome to explore it, clone it, or use it as a reference for your own projects. If you spot a typo or error, you can even submit a pull request—that is how open-source resources improve over time. 1.3.2 Tips for getting the most out of this book Run every code chunk yourself. Do not just read the output printed in the book. The act of running code, seeing it work (or fail), and tweaking it is where the learning happens. Experiment freely. Change the sample size. Swap the probability. Add noise. See what breaks. You cannot damage anything by running code in your own R session, and every “mistake” teaches you something about how R and statistics work. Read with RStudio open. The book and RStudio should be side by side. If you are reading on a laptop, split the screen. If you have a second monitor, even better. Use the search function. This book is a website, and your browser’s search (Ctrl+F or Cmd+F) works on every page. If you need to find where we introduced a concept, search for it. Come back to earlier chapters. Statistical concepts build on each other. If something in Chapter 8 feels unclear, the answer is often in Chapter 3 or 4. The chapters are designed to be revisited, not just read once. Use the reference tables. Several chapters include reference tables (such as the data type → distribution → model table in Chapter 4). You do not need to memorize these. Bookmark them and return to them as needed. 1.3.3 A note on R code in this book All code in this book is written in R and is designed to run in a standard R or RStudio environment. We primarily use base R to keep dependencies minimal, with occasional use of common packages like ggplot2 or knitr when they add clarity. If a code chunk requires a package you have not installed, R will tell you. You can install any missing package with: install.packages(&quot;package_name&quot;) If you encounter an error you cannot resolve, copy the error message and search for it—chances are good that someone else has had the same problem and the answer is on Stack Overflow or the RStudio Community forums. Learning to troubleshoot R errors is itself a valuable skill. 1.4 Course structure This course has been designed to be ‘flipped’. Watch the lecture before you come to class. Then, treat class like a workshop to work through quizzes, assignments, and most importantly your work for your thesis. Whether you are developing the methods for your proposal or conducting your final analysis, this course is meant to support you as you actually perform statistics. "],["back-to-the-basics.html", "Chapter 2 Back to the basics 2.1 Watch the video 2.2 What can statistics tell us? 2.3 The Frequentist Framework 2.4 Bayesian statistics 2.5 Fundaments of frequentist statistics 2.6 What is a model? 2.7 Assignment", " Chapter 2 Back to the basics 2.1 Watch the video Watch the chapter video 2.2 What can statistics tell us? Almost all statistical analysis boils down to answering one of two questions: 1. Do these groups differ? 2. Is there a relationship between these variables? These seem like simple questions — in theory you might just “look at the data” to answer the questions — so why do we need statistics? The short answer is: sampling. We are always measuring a subset of the true population. Even when we try to measure every individual (e.g., rare plant censuses), some individuals may be dormant, hidden, or unreachable. If we were omniscient, we would simply compute the true population parameters (mean, variance, etc.), compare them directly, and skip statistics entirely. But in the real world, we estimate these values from samples and quantify the uncertainty around them. Statistics is practical: it asks, what can we say given imperfect measurements, “small” samples (relative to the true population), and noisy biological systems? 2.3 The Frequentist Framework The statistics we use most commonly in ecology — t-tests, ANOVA, linear models, GLMs, mixed models — are all built within the frequentist or parametric framework. Frequentist statistics assumes: There is a true but unknown parameter The population mean, variance, slope, etc. are fixed constants — but we don’t know their values. Your sample is one of many possible samples you could have taken If you collected data again under the same conditions, you’d get a different sample and different estimates. Variation in your estimates comes from sampling error The parameter does not vary — it varies because you sampled imperfectly. Uncertainty is therefore in the estimator, not the parameter. Probability statements describe long-run frequencies of repeated sampling In other words, frequentist statistics asks: “If we repeated this experiment many times, how often would our method work?” A 95% confidence interval means that 95% of intervals generated this way would contain the true parameter. Once an interval is calculated, the true value is either inside it or not—there is no probability attached to that specific interval. What does “parametric” mean? Parametric tests assume the data follow a particular probability distribution (e.g., normal, Poisson, binomial). These assumptions influence: which model we choose how we estimate uncertainty what kinds of inferences we can make 2.4 Bayesian statistics The main alternative statistical framework to frequentist or parametric statistics is Bayesian statistics. Bayesian statistics is based on Bayes’ Theorem, which provides a mathematical way to update our beliefs in light of new data. In this framework, we start with a prior distribution representing what we believe about a parameter before collecting data. We then combine this prior with the likelihood (the information contained in the data) to obtain a posterior distribution, which reflects our updated beliefs after seeing the evidence. This allows us to make intuitive probability statements about parameters—such as the probability that a parameter lies within a certain range—and provides a flexible foundation for modeling uncertainty, especially in hierarchical and ecological systems. We (and most other ecologists) will learn frequentist methods first, because: Most classical ecological statistics (t-tests, ANOVA, GLMs) use the frequentist framework. Frequentist thinking aligns with standard experimental design. Historically, Bayesian methods were computationally difficult. Most journals still expect p-values and confidence intervals. 2.4.1 When might you choose Bayesian statistics? Although we begin with frequentist methods for their simplicity and historical use in ecology, there are many situations where Bayesian approaches offer clear advantages. Ecologists often choose Bayesian statistics when: Data are sparse, noisy, or imbalanced. Bayesian priors help stabilize estimates when sample sizes are small or data contain many zeros (common in rare species monitoring or demographic studies). Hierarchical or multilevel structure is important. Many ecological datasets have natural grouping—plots within sites, individuals within populations, years within climate regimes. Bayesian methods handle hierarchical models gracefully and provide full uncertainty estimates for every level.You want to incorporate prior information. In wildlife ecology, population modeling, or long-term monitoring, previous studies often contain valuable information that can inform current estimates. Priors allow you to formally combine past research with new data. You care more about parameter uncertainty than about p-values. Bayesian posterior distributions provide intuitive quantities such as: “There is a 94% probability that survival increased after treatment.” This is often more interpretable than a traditional p-value. Models are too complex for frequentist methods.Nonlinear state–space models, integrated population models, hierarchical occupancy models, and many SDM frameworks are more stable and easier to fit using Bayesian tools like Stan, JAGS, or NIMBLE. You need robust propagation of uncertainty. Bayesian methods naturally propagate uncertainty across multiple model components, which is critical for forecasting, resilience modeling, and structured decision making. In short, Bayesian statistics shines when ecological questions involve complex models, low sample sizes, hierarchical structure, prior knowledge, or full uncertainty estimation. 2.5 Fundaments of frequentist statistics 2.5.1 Statistical error Statistical error is the discrepancy between what we observe and the true value we seek to estimate. In statistics, error does not necessarily mean a mistake - it refers to unavoidable variation introduced by sampling, measurement, and natural processes. Whenever we collect data, we introduce error, because: – our instruments have limits – humans make mistakes – environmental conditions vary – biological systems are inherently noisy Frequentist statistical approaches classify and handle error in different ways. 2.5.1.1 Sampling error Sampling error (variation in estimates) arises because different random samples from the same population produce different estimates, even when the underlying population does not change. Imagine: There is a true population mean (fixed but unknown). You take a random sample and calculate the sample mean. You repeat this process many times. What happens? Each sample produces a slightly different sample mean. That variability among sample means is sampling error. 2.5.1.2 Process error Process variability reflects true heterogeneity in the system itself: Individuals differ Environments differ Years differ This variability exists even if you observed the entire population. Key characteristics: Does not disappear with larger sample size Reflects ecological mechanisms Example: Some trees grow faster than others because of microsite conditions. We are sometimes able to reduce process error by accounting for and measuring variables that drive process error, but often we can’t, because the driver of this error is unknown, too complex, or difficult to measure. 2.5.1.3 Measurement error Measurement error arises anytime imperfect humans with imperfect instruments (the situation all the time) measure a sample and can include error introduced by: Imprecise instruments Observer differences 2.5.1.4 Model misspecification Model misspecification occurs when the statistical model does not correctly represent the true data-generating process, arising from: Missing predictors: A relevant variable that influences the response is omitted from the model. Ecological example: Modeling plant growth without including soil moisture. Wrong functional form (linear vs non-linear): The relationship between predictor and response is modeled incorrectly (e.g., linear when nonlinear). Ecological example: Assuming linear temperature effects when responses are unimodal. Ignored interactions: The effect of one predictor depends on another, but this dependency is not modeled. Ecological example: Fire effects differing by precipitation regime. In theory, these are avoidable forms of error, but, particularly in the case of missing predictors, it is often extremely challenging to collect all the relevant predictors in a complex system. When planning an experiment, however, it is really important to think about the underlying processes in your system in order to capture the most important predictors of the variable that you are interested in. 2.5.1.5 Systematic error Systematic error refers to consistent, directional error that causes estimates to be biased in the same way across observations or samples. Unlike random error, systematic error does not average out with increased sample size. Examples include: Biased sampling (e.g., sampling only accessible sites) Consistent measurement bias (e.g., miscalibrated instruments; measuring all plants in the ambient treatment with a device is calibrated such at it adds .1 cm to each measure) Observer bias Why it matters: Systematic error leads to biased estimates and misleading conclusions, even when statistical uncertainty appears small. Because systematic error introduces bias, it should be minimized through design (mostly through randomization and careful consideration of experimental design) rather than absorbed into model error. Sampling error, and all unavoidable types of error (process, measurement, model misspecification) are specified in the error component of a statistical model. 2.6 What is a model? A statistical model is a mathematical expression that describes how a response variable changes as a function of one or more predictors. Since we are taking a frequentist approach, models we will use in this course follow the same structure: Response = Deterministic component + Random variation The deterministic component represents the systematic pattern we want to explain. For example, if asking how plant height changes with grazing treatment, the deterministic component is the grazing treatment (and, of course, the response is plant height!). The random variation component represents the noise or uncertainty that remains after accounting for the predictors, and it follows a probability distribution (We will talk about this in the next chapter). Recall that the other name for frequentist statistics is parametric statistics, which refers to the use of probability distributions. In the next chapter, we will explore probability distributions — the mathematical tools that allow us to represent different types of error and uncertainty in ecological data. 2.7 Assignment Decide your general approach to your statistical analyses. Will frequentists statistics generally work? Are there analysis for which you would like to use Bayesian statistics? "],["probability-the-language-of-uncertainty.html", "Chapter 3 Probability: The Language of Uncertainty 3.1 Watch the video 3.2 Probability is everywhere, and we often get it wrong 3.3 From uncertainty to inference 3.4 Defining the sample space 3.5 Probability as long-run frequency 3.6 Conditional probability and dependence 3.7 From probability to statistical thinking 3.8 Where probability appears in ecology and statistics 3.9 Summary 3.10 Assignment: Probability thinking in your research system", " Chapter 3 Probability: The Language of Uncertainty 3.1 Watch the video Watch the chapter video Across philosophy, physics, ecology, and statistics, “randomness” does not mean the same thing. Sometimes it reflects true unpredictability, sometimes incomplete knowledge, and sometimes the limits of measurement. One of the most interesting panels that I attended as a graduate student was a debate between scientific and religious philosophers over the meaning of randomness. There were a range of viewpoints; the theologian believed that randomness was where the divine could work; at the other extreme, the science philosopher believed that there is no such thing as random, only that we haven’t developed the capacity to measure what we perceive as random, and that everything we experience is a deterministic outcome of physics manifest in the world around us. Statistics does not try to resolve why variation exists. In statistics, randomness has a specific and practical meaning: it refers to situations in which outcomes cannot be predicted with certainty, but where each possible outcome has a known probability of occurring. Importantly, statistics does not distinguish between “true” randomness and unpredictability arising from incomplete information or measurement limitations. Instead, all sources of uncertainty are treated the same way. But probability is not just an abstract statistical concept—it shapes how science communicates with the rest of the world, and misinterpreting it has real consequences. 3.2 Probability is everywhere, and we often get it wrong Consider these statements: “It is extremely likely (95–100% probability) that human activities caused more than half of the observed increase in global mean surface temperature from 1951 to 2010.” — IPCC Fifth Assessment Report “There is a 30% chance of rain tomorrow.” “The probability of local extinction of this population within 50 years is 0.40.” Each of these communicates uncertainty using probability, and each is routinely misunderstood. The IPCC statement does not mean that scientists are 95% sure climate change is happening—it means that in 95–100% of the analyses, human influence exceeded natural variability. A 30% chance of rain does not mean 30% of the area gets wet, or that it will rain for 30% of the day—it means that under similar atmospheric conditions, about 30% of days produce measurable rainfall. An extinction probability of 0.40 does not mean the population will go extinct or won’t—it means that if we ran the future 1,000 times under the same conditions, roughly 400 of those futures would result in extinction. Getting probability right matters. Policy decisions about endangered species, climate adaptation, and public health all depend on whether decision-makers correctly interpret probabilistic statements. And as scientists, producing and interpreting those statements correctly is a core part of our job. 3.3 From uncertainty to inference The difference between the true value of a quantity (such as the mean height of giraffes or the strength of a relationship between snail shell length and movement speed) and the value we observe from a sample is called error. Error is not a mistake; it is the inevitable result of sampling and measurement in complex, variable systems. Statistical models are designed to separate systematic patterns in the data (such as treatment effects or relationships between variables) from this background uncertainty. Because error is unavoidable, statistics replaces certainty with probability. When conducting a statistical analysis, we use probability to evaluate how likely it is that an observed pattern could arise by chance alone. This single idea—comparing what we observed to what chance alone would produce—is the engine that drives almost all of statistical inference. It is what connects this chapter to every analysis you will do in this course. To make that connection, we need to understand a few core probability concepts. We will develop them primarily through ecological examples, using coins and dice only briefly as familiar scaffolding when a new idea needs a simple illustration. 3.4 Defining the sample space The first step in any probability calculation is to define the sample space: the complete set of all possible outcomes of an experiment, observation, or process. If the sample space is defined incorrectly, any probability calculation that follows will also be incorrect. In simple cases, the sample space is obvious. Flip a fair coin: the sample space is {heads, tails}. Roll a die: the sample space is {1, 2, 3, 4, 5, 6}. In ecological applications, defining the sample space requires more thought—and the question you ask determines the answer. Ecological example: You survey a set of plots to determine whether a focal plant species is present. For each plot, the outcome is either present or absent. The sample space consists of two possible outcomes. Importantly, the sample space does not include abundance, biomass, or flowering status—unless those outcomes are explicitly part of your question. Now suppose instead you are counting individuals in each plot. The sample space changes: it becomes {0, 1, 2, 3, …}, in principle extending to infinity. This is a fundamentally different question, and it will lead to different probability calculations and different statistical models. Presence/absence data and count data look different, behave differently, and require different analytical tools—and that distinction starts here, with the sample space. Defining the sample space forces you to be precise about what you are studying. Asking “Is the species present?” is not the same as asking “How many individuals are there?”, which is not the same as asking “What proportion of plots are occupied?” Each question defines a different sample space, and each leads to a different statistical framework. 3.5 Probability as long-run frequency In statistics, probability is commonly interpreted as a long-run frequency: the proportion of times an event would occur if the same process were repeated many times under identical conditions. Suppose you estimate seed germination by planting 100 seeds from the same species under identical greenhouse conditions. Even though the conditions are controlled, not all seeds germinate. If 30 out of 100 seeds germinate, we estimate the probability of germination as 0.30. But what does this number mean? It means that if you repeated this experiment many, many times—planting 100 seeds each time under the same conditions—the proportion of seeds that germinate would converge toward 0.30. In any single trial, the result might be 25, or 33, or 28. But as the number of trials increases, the average settles down. This idea—that estimates stabilize with more data—is one of the most important intuitions in statistics. Let’s see it in action. 3.5.1 Simulation: watching probability stabilize To build intuition, let’s use a simple example first: flipping a fair coin. We know the theoretical probability of heads is 0.5. But in small samples, the observed proportion can look nothing like 0.5. set.seed(226) # Flip a coin many times and track the running proportion of heads n_flips &lt;- 2000 flips &lt;- sample(c(&quot;H&quot;, &quot;T&quot;), size = n_flips, replace = TRUE) running_proportion &lt;- cumsum(flips == &quot;H&quot;) / (1:n_flips) plot(1:n_flips, running_proportion, type = &quot;l&quot;, col = &quot;steelblue&quot;, lwd = 1.5, xlab = &quot;Number of flips&quot;, ylab = &quot;Proportion of heads&quot;, main = &quot;Probability stabilizes with more observations&quot;, ylim = c(0, 1)) abline(h = 0.5, lty = 2, col = &quot;firebrick&quot;) text(n_flips * 0.8, 0.54, &quot;True probability = 0.5&quot;, col = &quot;firebrick&quot;, cex = 0.9) Figure 3.1: As the number of coin flips increases, the observed proportion of heads converges toward the true probability of 0.5. With few flips, estimates are noisy; with many flips, they stabilize. Notice the pattern: early on, the observed proportion swings wildly. With only 10 flips, you might see 70% heads or 30% heads, and neither would be surprising. But as the number of flips grows, the line settles toward 0.5. This is the law of large numbers in action, and it is the reason that larger samples give more reliable estimates. This same logic applies directly to ecological sampling. If the true probability of seed germination is 0.30, then planting 10 seeds gives a noisy estimate (you might see 1 or 5 germinate), while planting 500 seeds gives a much more stable one. This is not just a theoretical nicety—it is the practical reason that sample size matters in every ecological study. 3.5.2 What long-run frequency means for your research In real ecological studies, we rarely get to repeat the exact same experiment thousands of times. But we still rely on long-run frequency reasoning. When we say “the survival probability of juveniles is 0.65,” we mean: if we could observe the survival process for this species many times under these conditions, about 65% of juveniles would survive. Our observed data are one realization of this process, and probability describes the behavior we expect across many such realizations. This interpretation connects directly to ecological rates like survival, recruitment, germination, and flowering probability—all of which are estimated from repeated observations rather than single outcomes. 3.6 Conditional probability and dependence Many ecological processes are not isolated events. Whether a plant flowers depends on whether it survived the winter. Whether you detect a species depends on whether it is actually present. Whether a seed germinates may depend on whether it experienced a fire. These are all cases where the probability of one event depends on another. 3.6.1 How knowing one thing changes what we expect Conditional probability describes situations in which the probability of one event depends on whether another event has occurred. It is written as \\(P(A \\mid B)\\), read as “the probability of A given B,” and is calculated as: \\[P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)}\\] where \\(P(A \\cap B)\\) is the probability that both events occur simultaneously. The formula looks abstract, so let’s work through an ecological example. Example: flowering probability in a perennial plant Consider a perennial plant population that you have monitored for two years. In year one, you tagged 200 individuals. By spring of year two, you recorded whether each plant (a) survived the winter and (b) flowered. Here are the results: # Simulated monitoring data set.seed(42) n_plants &lt;- 200 survived &lt;- sample(c(TRUE, FALSE), n_plants, replace = TRUE, prob = c(0.7, 0.3)) flowered &lt;- ifelse(survived, sample(c(TRUE, FALSE), sum(survived), replace = TRUE, prob = c(0.4, 0.6)), FALSE) plant_data &lt;- data.frame( survived = survived, flowered = flowered ) # Summarize the data cat(&quot;Total plants tagged:&quot;, n_plants, &quot;\\n&quot;) ## Total plants tagged: 200 cat(&quot;Survived winter:&quot;, sum(survived), &quot;\\n&quot;) ## Survived winter: 129 cat(&quot;Flowered:&quot;, sum(flowered), &quot;\\n&quot;) ## Flowered: 37 cat(&quot;Died and flowered:&quot;, sum(!survived &amp; flowered), &quot;(should be 0)\\n&quot;) ## Died and flowered: 0 (should be 0) Now let’s calculate two different probabilities: # P(flowered): probability of flowering across ALL tagged plants p_flower &lt;- mean(plant_data$flowered) cat(&quot;P(flowered) =&quot;, round(p_flower, 3), &quot;\\n&quot;) ## P(flowered) = 0.185 # P(flowered | survived): probability of flowering GIVEN survival p_flower_given_survived &lt;- mean(plant_data$flowered[plant_data$survived]) cat(&quot;P(flowered | survived) =&quot;, round(p_flower_given_survived, 3), &quot;\\n&quot;) ## P(flowered | survived) = 0.287 These are different numbers, and the difference matters. \\(P(\\text{flowered})\\) treats all 200 plants the same—including the ones that died, which had zero chance of flowering. \\(P(\\text{flowered} \\mid \\text{survived})\\) restricts attention to the plants that were actually “eligible” to flower. The conditional probability is higher because knowing that a plant survived changes the sample space we are considering. This distinction is not just mathematical bookkeeping. In population ecology, the probability of reproduction is almost always conditional on survival. If you estimate \\(P(\\text{flowered})\\) without accounting for survival, you will underestimate the reproductive capacity of surviving plants. Stage-structured population models, life tables, and matrix projection models all depend on conditional probabilities like these. 3.6.2 Independence: when knowing one thing changes nothing Two events are independent if the occurrence of one does not affect the probability of the other. Mathematically, events A and B are independent if: \\[P(A \\mid B) = P(A)\\] In other words, learning that B happened tells you nothing new about the probability of A. When independence is reasonable: The survival of two plants growing far apart in similar habitat may be approximately independent. Whether it rains in Tucson today is independent of whether a coin lands heads. When independence is unreasonable: The survival of the same plant across consecutive years is almost certainly not independent—a plant that survived year one is likely healthier, larger, or in a better microsite, all of which increase its probability of surviving year two. For independent events, the probability of both occurring is the product of their individual probabilities: \\[P(A \\cap B) = P(A) \\times P(B)\\] This is the multiplication rule for independent events. It only works when the events truly are independent. # Two independent events: flipping two separate coins p_heads_coin1 &lt;- 0.5 p_heads_coin2 &lt;- 0.5 p_both_heads &lt;- p_heads_coin1 * p_heads_coin2 cat(&quot;P(heads on both coins) =&quot;, p_both_heads, &quot;\\n&quot;) ## P(heads on both coins) = 0.25 # Two dependent events: survival across years p_survive_yr1 &lt;- 0.70 p_survive_yr2_given_yr1 &lt;- 0.85 # higher because survivors are healthier p_survive_both &lt;- p_survive_yr1 * p_survive_yr2_given_yr1 cat(&quot;P(survive both years) =&quot;, p_survive_both, &quot;\\n&quot;) ## P(survive both years) = 0.595 # Compare: if we INCORRECTLY assumed independence p_survive_yr2_marginal &lt;- 0.70 # ignoring dependence p_survive_both_wrong &lt;- p_survive_yr1 * p_survive_yr2_marginal cat(&quot;P(survive both years), assuming independence =&quot;, p_survive_both_wrong, &quot;\\n&quot;) ## P(survive both years), assuming independence = 0.49 cat(&quot;Error from assuming independence:&quot;, round(p_survive_both - p_survive_both_wrong, 3), &quot;\\n&quot;) ## Error from assuming independence: 0.105 Assuming independence when events are dependent leads to incorrect conclusions. In this example, ignoring the dependence between years underestimates two-year survival. This is why statistical analyses often account for repeated measures, spatial structure, or shared environments—these are all forms of dependence that violate the independence assumption. Why this matters for your statistics: Many statistical tests assume that observations are independent. Violating this assumption—for example, by treating repeated measurements on the same individual as independent data points—is one of the most common mistakes in ecological statistics. Recognizing when events are dependent is not just a probability concept; it is a practical skill that will affect every analysis you run. 3.7 From probability to statistical thinking So far, we have built a vocabulary: sample spaces, long-run frequency, conditional probability, independence. These concepts matter, but they serve a larger purpose: they are the building blocks of statistical inference. This section shows how they come together. The key question in most statistical analyses is deceptively simple: Could the pattern I observed have arisen by chance alone? Answering that question requires three things: (1) a model of what “chance alone” looks like, (2) a way to describe the range of outcomes that chance could produce, and (3) a way to measure how surprising our actual observation is. Probability provides all three. 3.7.1 Probability distributions: the full picture of uncertainty In the previous sections, we talked about the probability of single events: the probability of germination, the probability of survival. But in practice, we usually care about the distribution of outcomes—not just whether a single seed germinates, but how many out of 50 germinate. A probability distribution describes all possible outcomes and how likely each one is. It is a complete picture of uncertainty for a given process. Let’s build one from scratch. Suppose the probability of seed germination for a particular species is 0.30. You plant 50 seeds. How many do you expect to germinate? The intuitive answer is \\(50 \\times 0.30 = 15\\). But you won’t always get exactly 15. Sometimes you’ll get 12, sometimes 18, occasionally 8 or 22. The question is: what is the full range of plausible outcomes, and how likely is each one? We can answer this with simulation: set.seed(226) # Simulate planting 50 seeds, 10000 times n_seeds &lt;- 50 p_germ &lt;- 0.30 n_reps &lt;- 10000 germination_counts &lt;- rbinom(n_reps, size = n_seeds, prob = p_germ) hist(germination_counts, breaks = seq(-0.5, max(germination_counts) + 0.5, by = 1), col = &quot;lightgreen&quot;, border = &quot;darkgreen&quot;, xlab = &quot;Number of seeds germinated (out of 50)&quot;, ylab = &quot;Frequency&quot;, main = &quot;Distribution of germination outcomes\\n(p = 0.30, n = 50 seeds)&quot;) abline(v = n_seeds * p_germ, lty = 2, lwd = 2, col = &quot;firebrick&quot;) text(n_seeds * p_germ + 3, n_reps * 0.12, &quot;Expected: 15&quot;, col = &quot;firebrick&quot;, cex = 0.9) Figure 3.2: If the true germination probability is 0.30 and we plant 50 seeds, this is the distribution of outcomes we would expect across many repetitions. The dashed line marks the expected value of 15. This histogram is a binomial distribution: it describes the number of “successes” (germinations) in a fixed number of independent trials (seeds), each with the same probability of success. You just built it by simulation; R has a built-in function that does the same thing analytically: # Compare simulation to the theoretical binomial distribution theoretical_probs &lt;- dbinom(0:n_seeds, size = n_seeds, prob = p_germ) # Most likely outcomes likely_range &lt;- which(theoretical_probs &gt; 0.01) - 1 # subtract 1 because index starts at 1 cat(&quot;Outcomes with &gt;1% probability:&quot;, paste(range(likely_range), collapse = &quot; to &quot;), &quot;\\n&quot;) ## Outcomes with &gt;1% probability: 8 to 22 cat(&quot;Expected value:&quot;, n_seeds * p_germ, &quot;\\n&quot;) ## Expected value: 15 cat(&quot;Standard deviation:&quot;, round(sqrt(n_seeds * p_germ * (1 - p_germ)), 2), &quot;\\n&quot;) ## Standard deviation: 3.24 The binomial distribution is just one probability distribution. Different kinds of ecological data call for different distributions—counts of individuals in quadrats often follow a Poisson distribution, continuous measurements like body mass often follow a normal distribution. We will meet these in later chapters. For now, the important idea is that a probability distribution gives us a way to describe what “normal variation” looks like for a given process. And that is exactly what we need to build a null model. 3.7.2 Null models: what does chance alone look like? Here is the central move in statistical thinking: once you can describe what chance alone would produce, you can ask whether your data look like they came from that process or whether something more interesting is going on. A null model is a probability model that represents the hypothesis of “no effect” or “nothing interesting happening.” It defines what we would expect to see if, for example, a treatment had no impact, two groups were identical, or a pattern were generated by chance. Ecological example: You are testing whether a restoration treatment increases seed germination. You set up an experiment with two groups: Treatment: 50 seeds planted in restored soil Control: 50 seeds planted in unrestored soil You observe that 22 out of 50 seeds germinate in the treatment group and 13 out of 50 in the control group. The difference in germination rates is \\(22/50 - 13/50 = 0.44 - 0.26 = 0.18\\). That seems like a meaningful difference. But could it have happened by chance, even if the treatment did nothing? To answer this, we need a null model. The null hypothesis is: the treatment has no effect on germination. Under this hypothesis, the 35 seeds that germinated would have germinated regardless of which group they were in. The group labels are meaningless. This gives us a strategy: shuffle the labels and see what happens. 3.7.3 A permutation test: building a null distribution A permutation test (also called a randomization test) works by: Combining all observations into a single pool (ignoring group labels). Randomly reassigning observations to groups. Calculating the difference between groups for each random assignment. Repeating many times to build a null distribution—the distribution of differences we would see if group membership were random. # Our observed data treatment &lt;- c(rep(1, 22), rep(0, 28)) # 22 germinated out of 50 control &lt;- c(rep(1, 13), rep(0, 37)) # 13 germinated out of 50 # Combine into one dataset germinated &lt;- c(treatment, control) group &lt;- c(rep(&quot;treatment&quot;, 50), rep(&quot;control&quot;, 50)) # Observed difference in germination rates obs_diff &lt;- mean(germinated[group == &quot;treatment&quot;]) - mean(germinated[group == &quot;control&quot;]) cat(&quot;Observed difference in germination rate:&quot;, obs_diff, &quot;\\n&quot;) ## Observed difference in germination rate: 0.18 Now we build the null distribution by shuffling group labels: set.seed(226) n_perms &lt;- 10000 perm_diffs &lt;- numeric(n_perms) for (i in 1:n_perms) { shuffled_group &lt;- sample(group) # randomly reassign labels perm_diffs[i] &lt;- mean(germinated[shuffled_group == &quot;treatment&quot;]) - mean(germinated[shuffled_group == &quot;control&quot;]) } # Plot the null distribution hist(perm_diffs, breaks = 40, col = &quot;grey80&quot;, border = &quot;grey50&quot;, xlab = &quot;Difference in germination rate (treatment - control)&quot;, ylab = &quot;Frequency&quot;, main = &quot;Null distribution: what chance alone produces&quot;) abline(v = obs_diff, col = &quot;firebrick&quot;, lwd = 2, lty = 2) abline(v = -obs_diff, col = &quot;firebrick&quot;, lwd = 2, lty = 2) text(obs_diff + 0.02, n_perms * 0.06, &quot;Observed\\ndifference&quot;, col = &quot;firebrick&quot;, cex = 0.85, adj = 0) Figure 3.3: The null distribution shows the range of differences in germination rate we would expect if the treatment had no effect. The red dashed line marks our observed difference. Values as extreme or more extreme than our observation are rare under the null model, suggesting the treatment had a real effect. Most of the null distribution is clustered near zero—which makes sense, because if the treatment does nothing, the difference between groups should be small. Our observed difference of 0.18 sits out in the tail of the distribution. 3.7.4 The p-value: measuring surprise The p-value is simply the proportion of the null distribution that is as extreme as, or more extreme than, what we actually observed. It answers the question: if the treatment truly had no effect, how often would chance alone produce a difference this large? # Two-sided p-value: proportion of permutations with |difference| &gt;= |observed| p_value &lt;- mean(abs(perm_diffs) &gt;= abs(obs_diff)) cat(&quot;p-value:&quot;, p_value, &quot;\\n&quot;) ## p-value: 0.0361 A small p-value means our observation would be unusual if nothing were going on. That’s it. It does not prove the treatment works. It does not measure the size of the effect. It does not tell us whether the result is ecologically important. It quantifies surprise under a specific assumption—the assumption that the null model is true. Let’s put all three pieces together: summary_df &lt;- data.frame( Concept = c(&quot;Null model&quot;, &quot;Null distribution&quot;, &quot;p-value&quot;), Definition = c(&quot;A probability model representing &#39;no effect&#39;&quot;, &quot;The distribution of outcomes under the null model&quot;, &quot;The probability of observing a result as extreme as ours, if the null model were true&quot;), In_our_example = c(&quot;Treatment has no effect on germination&quot;, &quot;Histogram of differences from 10,000 random shuffles&quot;, paste0(p_value, &quot; of shuffled differences were as extreme as ours&quot;)) ) knitr::kable(summary_df, col.names = c(&quot;Concept&quot;, &quot;What it means&quot;, &quot;In our example&quot;), caption = &quot;The three components of a statistical test&quot;) Table 3.1: The three components of a statistical test Concept What it means In our example Null model A probability model representing ‘no effect’ Treatment has no effect on germination Null distribution The distribution of outcomes under the null model Histogram of differences from 10,000 random shuffles p-value The probability of observing a result as extreme as ours, if the null model were true 0.0361 of shuffled differences were as extreme as ours This logic—build a null model, generate a null distribution, ask where your data fall—is the foundation of hypothesis testing. Every statistical test you encounter in this course, from t-tests to regression to ANOVA, follows this same structure. The tests differ in the specific null model they use and how they calculate the null distribution, but the reasoning is always the same. 3.8 Where probability appears in ecology and statistics The concepts from this chapter are not an isolated topic. They are the foundation for virtually everything that follows in this course and in quantitative ecology more broadly. Here is a brief roadmap of where these ideas reappear: Hypothesis testing formalizes the null model logic we developed in the previous section. Every test you will learn—t-tests, ANOVA, chi-square tests, regression significance tests—asks the same question: how surprising is my result under a null model? The tests differ in what null model they assume and how they compute the null distribution, but the reasoning is identical. Regression and linear models estimate relationships between variables, and every coefficient in a regression model has a probability distribution. Testing whether a coefficient “differs from zero” is a direct application of null model thinking: we ask how likely the observed coefficient would be if the true relationship were zero. Population modeling and Population Viability Analysis (PVA) rely on probability at every level. Survival probabilities, reproduction probabilities, and transition probabilities between life stages are all estimated from data and used to project population dynamics into the future. These models are often stochastic, meaning they incorporate randomness to generate distributions of possible outcomes (like the extinction probability example from the beginning of this chapter). Stage-structured population models use conditional probabilities across time steps—these are Markov chains, where the probability of being in a particular state next year depends on your current state, exactly as flowering probability depended on survival in our earlier example. Bayesian inference takes the conditional probability formula from this chapter and flips it. Instead of asking \\(P(\\text{data} \\mid \\text{hypothesis})\\)—which is what a p-value gives us—Bayesian methods ask \\(P(\\text{hypothesis} \\mid \\text{data})\\). This “flip” is Bayes’ theorem, and it is nothing more than the conditional probability formula applied in a new direction. If you choose to use Bayesian methods later, the concepts from this chapter are exactly what you need. Occupancy models separate two processes that are often conflated: whether a species is truly present at a site, and whether you detected it during your survey. The probability of detection given presence, \\(P(\\text{detected} \\mid \\text{present})\\), is a conditional probability. Occupancy models use this to estimate the true occurrence rate from imperfect survey data. You do not need to understand all of these yet. The point is that probability is not a standalone topic—it is the language that connects everything we will do this semester. 3.9 Summary Randomness and variability are unavoidable features of natural systems. In statistics, we do not attempt to explain why variation exists; instead, we develop tools to reason about uncertainty in the presence of that variation. The difference between a true population value and what we observe in a sample is called error. Error is not a mistake—it is an inherent consequence of sampling complex systems. Probability provides the language for reasoning about error and uncertainty. Key concepts from this chapter: The sample space defines all possible outcomes. Being explicit about what outcomes are possible—and what question you are asking—is essential for translating ecological questions into statistical ones. Probability as long-run frequency means that a probability represents the proportion of times an event would occur across many repetitions. Larger samples give more stable estimates. Conditional probability formalizes how probabilities change when additional information is available. Many ecological processes are conditional: reproduction depends on survival, detection depends on presence, and future states depend on current ones. Independence means that knowing one event tells you nothing about another. Assuming independence when events are dependent—such as repeated measurements on the same individual—is a common source of error in statistical analyses. A probability distribution describes the full range of possible outcomes and their likelihoods. It is the basis for building null models. A null model represents the hypothesis that nothing interesting is happening. The null distribution shows the range of outcomes we would expect under that hypothesis. The p-value is the proportion of the null distribution as extreme as or more extreme than our observed result. It quantifies surprise under a specific assumption—not the probability that the hypothesis is true. These concepts form the foundation for hypothesis testing, regression models, population modeling, and Bayesian inference, all of which will be developed in later chapters. Key takeaway: Probability does not remove uncertainty from ecological science—it allows us to work with it transparently and rigorously. 3.10 Assignment: Probability thinking in your research system Goal: Apply core probability concepts from this chapter to your own research system, without requiring advanced mathematics. Instructions: Answer the following questions using your thesis system (or a proposed research project if you are early-stage). Short answers are sufficient (1–3 sentences per question unless otherwise noted). 3.10.1 Part 1: Defining the sample space Identify one ecological outcome you measure or plan to measure (e.g., survival, presence/absence, flowering, infection). Explicitly define the sample space for this outcome. What outcomes are possible? What outcomes are not included? 3.10.2 Part 2: Probability as long-run frequency If you repeated your study many times under the same conditions, what does the probability of your chosen outcome represent? What would increasing your sample size change about your probability estimate? 3.10.3 Part 3: Conditional probability Identify one outcome in your system that depends on another condition (e.g., reproduction depends on survival, detection depends on presence, germination depends on fire). Explain the difference between \\(P(A)\\) and \\(P(A \\mid B)\\) in the context of your research question (e.g., “P(flowering) vs. P(flowering | survived winter)”). 3.10.4 Part 4: Independence and dependence Identify two events in your system that are likely dependent. Briefly explain why assuming independence would be inappropriate in this case. What might go wrong statistically? 3.10.5 Part 5: Null model thinking Describe a null model for one comparison or question in your research. What would you expect to see if there were no treatment effect, no relationship, or no difference? If you simulated data under this null model many times, what would the distribution of outcomes look like? (Describe in words—you do not need to run a simulation.) 3.10.6 Part 6: Connecting to your analyses Name one statistical analysis you plan to use (or expect to use) in your research. Which probability concept from this chapter does it rely on most heavily? Examples to get you started: If you plan to use logistic regression, it models conditional probability. If you plan to use a t-test, it relies on null model logic. If you plan to build a population model, it uses conditional (transition) probabilities across time steps. "],["data-types-and-why-they-matter.html", "Chapter 4 Data Types and Why They Matter 4.1 Types of variables 4.2 From data to residuals 4.3 Normal vs. non-normal error 4.4 Linking data types to distributions and models 4.5 Common statistical distributions 4.6 Summary 4.7 Assignment", " Chapter 4 Data Types and Why They Matter In the previous chapter, we built a binomial distribution by simulating seed germination 10,000 times. That distribution described the range of outcomes we would expect for a binary process (germinate or not) with a known probability. But the binomial is just one distribution for one kind of data. If we had been counting the number of seedlings per plot instead of asking whether each seed germinated, we would need a different distribution. If we had been measuring seedling height, we would need yet another. This is the core idea of this chapter: the type of data you collect determines the probability distribution, which determines the statistical model you should use. \\[\\text{Data type} \\rightarrow \\text{Distribution} \\rightarrow \\text{Model choice}\\] This connection is not optional—it is the reason why different ecological questions require different statistical approaches. A t-test assumes one kind of error structure; a Poisson regression assumes another. Choosing the wrong model for your data type doesn’t just give you a less precise answer—it can give you the wrong answer entirely. Before we learn any specific test, we need to understand the raw material we are working with. 4.1 Types of variables Ecological data come in several forms, and recognizing which form your data take is the first step toward choosing an appropriate analysis. 4.1.1 Categorical variables Categorical variables classify observations into groups or categories. The values are labels, not numbers, and arithmetic operations on them are meaningless. Examples: habitat type (forest, grassland, wetland), species identity (ponderosa pine, Douglas fir, white fir), treatment group (control, fertilized, burned), presence or absence of a species. Categorical variables with only two levels—such as present/absent, alive/dead, or germinated/not—are called binary variables. These are among the most common response variables in ecology, and they connect directly to the binomial distribution you built in Chapter 3. 4.1.2 Numerical variables Numerical variables represent measured quantities and come in two forms: Discrete (counts): These are whole numbers representing how many of something you observed. Counts cannot be negative, and fractions don’t make sense. If you are counting the number of insects in a pitfall trap, the number of flowers on a plant, or the number of seedlings in a quadrat, you are working with discrete data. Counts often follow Poisson or negative binomial distributions. Continuous: These are measurements that can take any value within a range, including fractions and decimals. Height, biomass, temperature, growth rate, and leaf area are all continuous variables. Continuous data often follow a normal (Gaussian) distribution, though not always—biomass, for instance, is always positive and often right-skewed. 4.1.3 Ordinal variables Ordinal variables have a meaningful order, but the distance between categories is not necessarily equal. A Likert scale response of “strongly disagree, disagree, neutral, agree, strongly agree” is ordinal: the categories have a clear ranking, but the difference between “agree” and “strongly agree” is not necessarily the same as between “disagree” and “neutral.” Examples: damage severity ratings (none, light, moderate, severe), Braun-Blanquet cover classes, stream condition indices. Ordinal data require specialized methods (such as cumulative link models) because treating them as either purely categorical or purely numerical can lead to incorrect conclusions. 4.2 From data to residuals Once you know your data type, the next question is: how do we evaluate whether a statistical model is doing a good job? The answer involves residuals. 4.2.1 What is a residual? When we fit a statistical model, the model generates a predicted value for each observation. The residual is the difference between what we actually observed and what the model predicted: \\[\\text{Residual} = \\text{Observed value} - \\text{Predicted value}\\] Residuals represent the variation that the model does not explain. If a model captures the major patterns in the data, residuals should be small and centered around zero. Think of it like throwing darts at a bullseye. Each throw lands slightly off center, but the pattern of misses forms a cloud around the target. The center of the cloud estimates the true target, while the spread of the cloud represents error. A good model puts the bullseye in the right place; residuals describe the scatter around it. 4.2.2 Why the shape of residuals matters Residuals are not just leftovers—they carry critical information about whether a model is appropriate. Statistical inference relies on assumptions about how residuals behave. If those assumptions are violated, our estimates of uncertainty, confidence intervals, and p-values may be misleading. When we examine residuals, we ask: Are they centered around zero? Are positive and negative residuals roughly balanced? Does the variability of residuals stay consistent across the range of predicted values? Do they follow the distribution we assumed? Patterns in residuals—such as systematic curvature, increasing spread, or heavy tails—signal that something about the model is wrong. Maybe a predictor is missing, maybe the relationship is nonlinear, or maybe the assumed error distribution doesn’t match the data. Residual diagnostics are a key part of responsible statistical analysis, not an optional afterthought. We will practice these diagnostics extensively when we begin fitting models. 4.3 Normal vs. non-normal error Many statistical models assume that residuals follow a normal (Gaussian) distribution: most residuals are small, extreme deviations are rare, and the distribution is symmetric around zero. This assumption often works well for continuous traits like height, biomass, or growth rates, where many small, independent factors add together to produce the observed value. However, not all data behave this way. The sample space matters—and this is where Chapter 3’s lessons become directly relevant: Count data (0, 1, 2, 3, …) cannot be negative and are often right-skewed, especially when the average count is small. They typically follow Poisson or negative binomial distributions. Binary data (0 or 1) can only take two values. Normal error makes no sense here; we use the binomial distribution instead. Proportion data are bounded between 0 and 1. Normal distributions can predict values outside these bounds, which is meaningless. Using a model that assumes normal error for non-normal data can produce biased estimates and incorrect inferences. Recognizing your data type—and choosing a model with the appropriate error structure—is one of the most consequential decisions you will make in any analysis. 4.4 Linking data types to distributions and models The table below is a reference that connects data types to the distributions and models commonly used in ecology. You do not need to memorize this table. Bookmark it. We will return to it repeatedly throughout the course, and each time we encounter a new analysis, I will point you back to the relevant row. For now, the goal is to see the pattern: different data types lead to different distributions, which lead to different models. Every row in this table represents a deliberate choice that a researcher must make based on the structure of their data. Table 4.1: Reference: Data types, distributions, and models commonly used in ecology. Bookmark this table—we will return to it throughout the course. Data Type Distribution Link Ecological Example Typical Models Continuous Normal (Gaussian) Identity Do burned plots have taller seedlings? t-test, ANOVA, linear regression, LMM Continuous (positive, skewed) Gamma Log Does fertilizer increase biomass (always &gt; 0)? Gamma GLM, Gamma GLMM Continuous (bounded 0–1) Beta Logit / log–log What proportion of cover is bare soil? Beta regression, Beta GLMM Counts (integers ≥ 0) Poisson Log Does precipitation affect pitfall-trapped insects? Poisson GLM, Poisson GLMM Counts with overdispersion Negative Binomial Log Does nutrient addition affect flower counts (variance &gt; mean)? Negative Binomial GLM, NB GLMM Counts with many zeros Zero-inflated Poisson / NB Log Do invasive grasses produce many zero seedling counts? ZIP / ZINB, hurdle models Binary outcomes (0/1) Binomial (Bernoulli) Logit / probit Does shade increase seedling survival probability? Logistic regression, Binomial GLM/GLMM Proportions from counts Binomial Logit What proportion of seeds germinate per treatment? Binomial GLM, logistic regression Categorical (unordered) Multinomial Logit Do grazing treatments shift vegetation type frequencies? Multinomial logistic, Chi-square Ordinal (ordered categories) Ordinal logistic Logit / probit Does grazing intensity affect seedling vigor ranks? Proportional odds (CLM/CLMM) Time-to-event Exponential / Weibull Log How long do seedlings survive under drought? Survival analysis, Cox regression Nonlinear continuous Non-normal, unknown Identity or specialized Is the relationship between age and growth curved? GAM, GAMM 4.5 Common statistical distributions Now let’s look at some of the distributions from the table above. For each one, we will simulate data in R so you can see the shape of the distribution and build intuition for what each one represents. 4.5.1 The normal distribution The normal distribution is the classic bell curve. It arises from the Central Limit Theorem: when many small, independent factors add together—genetics, microclimate, measurement error, nutrient availability—their combined effect tends to be normally distributed. This is why so many continuous biological measurements are approximately normal. Ecological examples: seedling height, tree diameter (DBH), leaf nitrogen content, body mass, daily temperature. set.seed(1) heights &lt;- rnorm(1000, mean = 10, sd = 2) hist(heights, breaks = 20, col = &quot;lightblue&quot;, border = &quot;steelblue&quot;, main = &quot;Simulated Normal Distribution&quot;, xlab = &quot;Height (cm)&quot;, ylab = &quot;Frequency&quot;) abline(v = 10, lty = 2, lwd = 2, col = &quot;firebrick&quot;) Figure 4.1: The normal distribution is symmetric and bell-shaped. Most values cluster near the mean, with fewer observations in the tails. The normal distribution is defined by two parameters: the mean (center) and the standard deviation (spread). Many common statistical tests—t-tests, ANOVA, linear regression—assume that residuals follow this distribution. 4.5.2 The binomial distribution You already built this distribution in Chapter 3 when you simulated seed germination. The binomial distribution arises when there are two possible outcomes (success or failure), each trial has the same probability of success, and you repeat the trial a fixed number of times. Ecological examples: seed germination (germinated / not), survival (alive / dead), flowering (reproductive / not), species presence or absence at survey points. set.seed(1) germination &lt;- rbinom(1000, size = 10, prob = 0.6) hist(germination, breaks = seq(-0.5, 10.5, by = 1), col = &quot;lightgreen&quot;, border = &quot;darkgreen&quot;, main = &quot;Simulated Binomial Distribution&quot;, xlab = &quot;Number of seeds germinated (out of 10)&quot;, ylab = &quot;Frequency&quot;) abline(v = 10 * 0.6, lty = 2, lwd = 2, col = &quot;firebrick&quot;) Figure 4.2: The binomial distribution describes the number of successes in a fixed number of trials. Here, we simulate germination of 10 seeds with a 60% probability of success. The binomial distribution is defined by two parameters: the number of trials (\\(n\\)) and the probability of success (\\(p\\)). The expected value is \\(n \\times p\\). When you use logistic regression to model binary outcomes, you are estimating \\(p\\) as a function of predictor variables. 4.5.3 The Poisson distribution The Poisson distribution describes counts of events that happen independently, with a constant average rate, over a fixed period of time or area of space. It often appears in ecology because many biological events behave like random arrivals. Ecological examples: number of flowers per plant, number of birds detected per point count, number of insects in a pitfall trap, seedling recruitment counts per quadrat. set.seed(1) flower_counts &lt;- rpois(1000, lambda = 4) hist(flower_counts, breaks = seq(-0.5, max(flower_counts) + 0.5, by = 1), col = &quot;salmon&quot;, border = &quot;brown&quot;, main = &quot;Simulated Poisson Distribution&quot;, xlab = &quot;Number of flowers&quot;, ylab = &quot;Frequency&quot;) abline(v = 4, lty = 2, lwd = 2, col = &quot;firebrick&quot;) Figure 4.3: The Poisson distribution describes the number of events occurring in a fixed interval. It is right-skewed when the mean is small and becomes more symmetric as the mean increases. A key property of the Poisson distribution is that the mean equals the variance. When this assumption is violated—specifically, when the variance is much larger than the mean—the data are overdispersed, and the Poisson model is no longer appropriate. 4.5.4 The negative binomial distribution When count data are more variable than the Poisson allows, we say they are overdispersed. Overdispersion frequently arises in ecology because individuals vary in productivity, organisms are spatially clustered, or environmental conditions change across the study area. The negative binomial distribution adds an extra parameter that allows the variance to exceed the mean. Ecological examples: insect counts in patchy habitat, seed production with high individual variation, flower counts with many zeros and a few very high values. set.seed(1) nb_counts &lt;- rnbinom(1000, size = 2, mu = 4) hist(nb_counts, breaks = seq(-0.5, max(nb_counts) + 0.5, by = 1), col = &quot;tan&quot;, border = &quot;sienna&quot;, main = &quot;Simulated Negative Binomial Distribution&quot;, xlab = &quot;Count&quot;, ylab = &quot;Frequency&quot;) abline(v = 4, lty = 2, lwd = 2, col = &quot;firebrick&quot;) Figure 4.4: The negative binomial distribution accommodates overdispersion—more variability than the Poisson allows. Notice the longer right tail compared to the Poisson. Compare this histogram to the Poisson above: both have a mean of 4, but the negative binomial has a much longer right tail. If you fit a Poisson model to overdispersed data, your standard errors will be too small, your p-values too optimistic, and your confidence intervals too narrow. Recognizing overdispersion—and switching to a negative binomial—is a practical skill you will use often. 4.5.5 The uniform distribution The uniform distribution assigns equal probability to all values within a range. It is relatively rare as a model for ecological data, but it plays an important role behind the scenes. Uses in ecology and statistics: simulating randomness, generating null models (as we did in Chapter 3’s permutation test), creating starting conditions for stochastic population models, and serving as a “non-informative” prior in Bayesian analysis. set.seed(1) uniform_vals &lt;- runif(1000, min = 0, max = 1) hist(uniform_vals, breaks = 20, col = &quot;gray80&quot;, border = &quot;gray50&quot;, main = &quot;Simulated Uniform Distribution&quot;, xlab = &quot;Value&quot;, ylab = &quot;Frequency&quot;) Figure 4.5: The uniform distribution assigns equal probability to all values. It is useful for simulation and null models, but rarely describes ecological data directly. 4.6 Summary The type of data you collect determines everything that follows in a statistical analysis: Categorical, numerical (discrete and continuous), and ordinal variables each require different treatment. Recognizing what kind of data you have is the first step toward choosing an appropriate model. Residuals are the portion of variation that a model does not explain. Their shape tells you whether your model is appropriate. Residual diagnostics are an essential part of every analysis. Different data types produce different error structures. Continuous data often produce normally distributed residuals; counts produce Poisson or negative binomial error; binary data produce binomial error. Using the wrong error structure gives wrong answers. The reference table in this chapter maps data types to distributions and models. You do not need to memorize it now—we will refer to it throughout the course as we encounter new analyses. Everything in this course builds on the chain: data type \\(\\rightarrow\\) distribution \\(\\rightarrow\\) model choice. Understanding this chain is more important than memorizing any single test. 4.7 Assignment Make a list of the response variables you plan to measure for your research project. For each variable, identify: The data type (categorical, discrete count, continuous, ordinal, binary). The most appropriate error distribution (normal, binomial, Poisson, negative binomial, etc.). A brief explanation (1–2 sentences) of why you chose that distribution. What features of your data led you to this choice? Hint: Refer to the reference table in this chapter. If you are unsure about a variable, describe what the data look like (e.g., “always positive, right-skewed, many zeros”) and reason from there. "],["data-exploration-and-assumption-checking.html", "Chapter 5 Data Exploration and Assumption Checking 5.1 Why explore before you analyze? 5.2 The dataset: Forest seedling survival 5.3 Getting to know your data 5.4 Visualizing distributions 5.5 Key questions to ask during exploration 5.6 Assumptions of parametric tests 5.7 Checking assumptions after fitting a model 5.8 Summary: The EDA workflow 5.9 Key takeaways 5.10 Assignment", " Chapter 5 Data Exploration and Assumption Checking Every statistical model makes assumptions about your data. Before you run any analysis, you need to know what you’re working with—and afterward, you need to verify that your model’s assumptions are reasonable. This chapter covers both phases of exploratory data analysis (EDA): the detective work you do before fitting a model, and the diagnostic checks you do after. Think of it this way: pre-modeling EDA helps you choose the right tool for the job, while post-modeling diagnostics confirm that you chose wisely. 5.1 Why explore before you analyze? It’s tempting to jump straight into analysis. You have a hypothesis, you collected data, and you want to know if your treatment worked. But skipping exploration is a recipe for problems: Data entry errors can produce impossible values that completely distort your results Missing data patterns might not be random, which affects how you handle them Unexpected distributions might indicate you need a different type of model Outliers might represent measurement errors, data entry mistakes, or genuinely unusual biology The goal of exploratory analysis is simple: no surprises when you run your model. If your residual plots look terrible after fitting a regression, you want to have seen that coming—and ideally, you want to have addressed it beforehand. 5.2 The dataset: Forest seedling survival Throughout this chapter, we’ll use a simulated dataset that mirrors what you might collect in an ecological field study. Imagine you’ve established plots across a burned forest landscape to monitor seedling regeneration. At each plot, you recorded: height_cm: Seedling height in centimeters diameter_mm: Stem diameter in millimeters burn_severity: Categorical burn severity (low, moderate, high) elevation_m: Plot elevation in meters canopy_cover: Percent canopy cover (0-100) survival: Whether the seedling survived to the end of the growing season (0/1) seedling_count: Number of seedlings in the plot Let’s create this dataset so you can follow along: # Set seed for reproducibility set.seed(42) # Number of observations n &lt;- 150 # Create the dataset seedlings &lt;- data.frame( plot_id = 1:n, # Continuous predictors elevation_m = round(rnorm(n, mean = 2200, sd = 300), 0), canopy_cover = round(pmin(100, pmax(0, rnorm(n, mean = 45, sd = 20))), 1), # Categorical predictor burn_severity = sample(c(&quot;low&quot;, &quot;moderate&quot;, &quot;high&quot;), n, replace = TRUE, prob = c(0.3, 0.4, 0.3)) ) # Make burn_severity an ordered factor seedlings$burn_severity &lt;- factor(seedlings$burn_severity, levels = c(&quot;low&quot;, &quot;moderate&quot;, &quot;high&quot;)) # Generate height as a function of canopy cover and burn severity # (with some noise and a few problematic values for teaching purposes) seedlings$height_cm &lt;- round( 15 + 0.2 * seedlings$canopy_cover + ifelse(seedlings$burn_severity == &quot;high&quot;, -5, ifelse(seedlings$burn_severity == &quot;moderate&quot;, -2, 0)) + rnorm(n, 0, 4), 1 ) # Add some realistic messiness: # A few implausible negative heights (data entry errors) seedlings$height_cm[c(23, 87)] &lt;- c(-3.2, -1.5) # One extremely large value (possible legacy tree, or typo?) seedlings$height_cm[112] &lt;- 89.5 # Diameter correlates with height seedlings$diameter_mm &lt;- round( 2 + 0.3 * seedlings$height_cm + rnorm(n, 0, 1.5), 1 ) # Count data (number of seedlings per plot) - Poisson-ish with some overdispersion seedlings$seedling_count &lt;- rpois(n, lambda = 3 + 0.02 * seedlings$canopy_cover) # Add some zeros for realism seedlings$seedling_count[seedlings$burn_severity == &quot;high&quot;][1:5] &lt;- 0 # Survival as binary outcome influenced by height and burn severity survival_prob &lt;- plogis(-2 + 0.1 * seedlings$height_cm + ifelse(seedlings$burn_severity == &quot;high&quot;, -1, 0)) seedlings$survival &lt;- rbinom(n, 1, prob = survival_prob) # Add some missing values (realistic pattern: harder to measure in high-burn areas) seedlings$canopy_cover[seedlings$burn_severity == &quot;high&quot;][1:3] &lt;- NA seedlings$height_cm[c(45, 98)] &lt;- NA This dataset intentionally includes some messy features you’ll encounter in real fieldwork: a few impossible values, some missing data, potential outliers, and variables of different types. Let’s explore it. 5.3 Getting to know your data 5.3.1 First look: Structure and dimensions Before anything else, understand what you’re working with. How many observations? How many variables? What type is each variable? # How big is the dataset? dim(seedlings) ## [1] 150 8 The dim() function returns rows (observations) and columns (variables). Here we have 150 seedlings and 8 variables. # What types of data do we have? str(seedlings) ## &#39;data.frame&#39;: 150 obs. of 8 variables: ## $ plot_id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ elevation_m : num 2611 2031 2309 2390 2321 ... ## $ canopy_cover : num NA 14 68.3 39.5 NA NA 44.8 29 34.3 70.8 ... ## $ burn_severity : Factor w/ 3 levels &quot;low&quot;,&quot;moderate&quot;,..: 3 2 1 2 3 3 1 2 3 3 ... ## $ height_cm : num 16.1 14.7 24.2 21.4 24.3 23.7 19.7 20.7 22.4 23.4 ... ## $ diameter_mm : num 8.2 4.6 8.4 9 6.4 6.4 6.2 7.7 10.6 8.6 ... ## $ seedling_count: num 0 4 2 3 0 0 2 3 0 0 ... ## $ survival : int 0 1 1 0 0 1 1 1 0 0 ... What to look for in str() output: Numeric variables (num or int): Are these actually continuous measurements, or should some be factors? Factor variables: Are the levels correct? In the right order? Character variables: Should these be converted to factors? Unexpected types: Sometimes numbers get read as characters due to typos or special characters Notice that burn_severity is already a factor with ordered levels (low &lt; moderate &lt; high). If it had been read as a character, we’d need to convert it. # Look at the first few rows head(seedlings) ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm seedling_count survival ## 1 1 2611 NA high 16.1 8.2 0 0 ## 2 2 2031 14.0 moderate 14.7 4.6 4 1 ## 3 3 2309 68.3 low 24.2 8.4 2 1 ## 4 4 2390 39.5 moderate 21.4 9.0 3 0 ## 5 5 2321 NA high 24.3 6.4 0 0 ## 6 6 2168 NA high 23.7 6.4 0 1 # And the last few tail(seedlings) ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm seedling_count survival ## 145 145 2076 42.5 low 23.3 9.0 3 1 ## 146 146 2534 35.5 high 23.5 10.1 4 0 ## 147 147 2056 41.7 moderate 20.0 8.7 4 0 ## 148 148 2070 62.3 low 29.9 10.7 5 0 ## 149 149 2409 46.9 low 25.3 7.8 3 1 ## 150 150 1883 12.5 moderate 28.4 11.5 1 1 Checking head() and tail() can reveal problems at the beginning or end of your data file—common locations for header issues or incomplete records. 5.3.2 Summary statistics: What are the ranges? summary(seedlings) ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm ## Min. : 1.00 Min. :1302 Min. : 0.00 low :49 Min. :-3.20 Min. : 0.100 ## 1st Qu.: 38.25 1st Qu.:2016 1st Qu.:31.05 moderate:55 1st Qu.:16.80 1st Qu.: 6.625 ## Median : 75.50 Median :2188 Median :45.70 high :46 Median :21.40 Median : 8.350 ## Mean : 75.50 Mean :2191 Mean :45.00 Mean :21.75 Mean : 8.489 ## 3rd Qu.:112.75 3rd Qu.:2392 3rd Qu.:57.25 3rd Qu.:25.93 3rd Qu.:10.200 ## Max. :150.00 Max. :3011 Max. :94.20 Max. :89.50 Max. :27.700 ## NA&#39;s :3 NA&#39;s :2 ## seedling_count survival ## Min. :0.000 Min. :0.00 ## 1st Qu.:2.000 1st Qu.:0.00 ## Median :3.000 Median :0.00 ## Mean :3.513 Mean :0.48 ## 3rd Qu.:5.000 3rd Qu.:1.00 ## Max. :9.000 Max. :1.00 ## What to look for in summary() output: Minimum and maximum values: Are they plausible? A negative height or a canopy cover of 150% would indicate problems. Means vs. medians: Large differences suggest skewness. NA’s count: How much missing data do we have, and where? Let’s interpret what we see: height_cm: The minimum is -3.2, which is impossible. We have a data entry error to investigate. canopy_cover: 3 missing values (NA’s). Range is 0-94%, which seems reasonable. seedling_count: Ranges from 0 to 14, with a median of 4. That’s plausible for count data. 5.3.3 Checking for missing values Missing data requires careful handling. First, let’s see the overall pattern: # Count missing values per column colSums(is.na(seedlings)) ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm seedling_count ## 0 0 3 0 2 0 0 ## survival ## 0 We have 2 missing heights and 3 missing canopy cover values. But where are they missing? Missing data patterns can be informative—or problematic. # Which rows have missing data? seedlings[!complete.cases(seedlings), ] ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm seedling_count survival ## 1 1 2611 NA high 16.1 8.2 0 0 ## 5 5 2321 NA high 24.3 6.4 0 0 ## 6 6 2168 NA high 23.7 6.4 0 1 ## 45 45 1790 25.8 high NA 7.9 0 0 ## 98 98 1762 44.5 low NA 9.1 6 1 Looking at the output, we can see that some missing canopy cover values occur in high-burn plots. This might not be random—perhaps the high-burn areas were harder to survey, or equipment failed in those conditions. This kind of non-random missingness can bias your results if not handled carefully. For a more visual exploration of missing data patterns, the naniar package is excellent: # install.packages(&quot;naniar&quot;) # if needed library(naniar) # Visual summary of missingness vis_miss(seedlings) # Missing data by variable gg_miss_var(seedlings) 5.3.4 Identifying impossible values Sometimes summary() reveals obvious problems. Let’s dig into that negative height: # Find rows with impossible heights seedlings[seedlings$height_cm &lt; 0 &amp; !is.na(seedlings$height_cm), ] ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm seedling_count survival ## 23 23 2148 10.1 moderate -3.2 0.1 2 0 ## 87 87 2135 72.0 moderate -1.5 1.9 3 0 Plots 23 and 87 have negative heights. These are clearly data entry errors. Before changing anything, document what you found: # Document the problematic values problem_rows &lt;- which(seedlings$height_cm &lt; 0 &amp; !is.na(seedlings$height_cm)) print(paste(&quot;Impossible negative heights in rows:&quot;, paste(problem_rows, collapse = &quot;, &quot;))) ## [1] &quot;Impossible negative heights in rows: 23, 87&quot; What should you do? Options include: Check original datasheets: Maybe you can recover the correct value Set to NA: If the correct value is unknown Remove the observation: Only if it’s a small fraction and you document it Never silently delete or change values. Your code should document every decision. # Option: Set impossible values to NA (documenting the decision) # seedlings$height_cm[seedlings$height_cm &lt; 0] &lt;- NA 5.3.5 Quick reference: Common data problems Problem How to spot it What to do Impossible values Check min/max in summary() Verify against original data, set to NA or correct Typos in factor levels Look at levels() or unique() Correct spelling, combine levels Wrong data types Check str() output Convert with as.factor(), as.numeric(), etc. Duplicated rows Use duplicated() Investigate and remove if true duplicates Inconsistent missing codes Look for -999, “.”, “NA”, etc. Convert all to proper NA Let’s check for factor level problems: # Are there any unexpected levels? levels(seedlings$burn_severity) ## [1] &quot;low&quot; &quot;moderate&quot; &quot;high&quot; # Any typos if this were character data? unique(seedlings$burn_severity) ## [1] high moderate low ## Levels: low moderate high Good—our burn severity levels are clean. In real data, you might find entries like “Low”, “low”, “LOW”, and “lo” that all need to be standardized. 5.4 Visualizing distributions Now that we’ve inspected the data structure and fixed obvious errors, let’s visualize what we’re working with. Visualization is your most powerful tool for understanding data—patterns that are invisible in summary statistics often jump out in plots. 5.4.1 Continuous variables: Histograms Histograms show the distribution of a continuous variable. We’re looking for: Shape: Is it roughly symmetric, or skewed left/right? Modes: Is there one peak (unimodal) or multiple (bimodal, multimodal)? Spread: How variable is the data? Outliers: Are there isolated values far from the rest? hist(seedlings$height_cm, breaks = 20, col = &quot;lightblue&quot;, border = &quot;white&quot;, main = &quot;Distribution of Seedling Heights&quot;, xlab = &quot;Height (cm)&quot;, ylab = &quot;Frequency&quot;) Figure 5.1: Distribution of seedling heights. Note the potential outlier above 80 cm and the gap between it and the main distribution. Interpretation: The distribution is roughly unimodal and symmetric, centered around 20 cm. But notice that isolated bar way out to the right near 90 cm—that’s our potential outlier (plot 112). There’s a clear gap between this value and the rest of the distribution, which warrants investigation. Let’s also look at a variable we expect to be non-normal: hist(seedlings$seedling_count, breaks = seq(-0.5, max(seedlings$seedling_count) + 0.5, by = 1), col = &quot;darkseagreen&quot;, border = &quot;white&quot;, main = &quot;Distribution of Seedling Counts&quot;, xlab = &quot;Number of Seedlings per Plot&quot;, ylab = &quot;Frequency&quot;) Figure 5.2: Distribution of seedling counts per plot. Count data typically show right skew and are bounded at zero. Interpretation: This looks like classic count data—discrete values, bounded at zero, with a right skew. This is exactly what we’d expect for Poisson-distributed data, and it tells us that a linear model with normal errors would be inappropriate. We’d want to use a Poisson or negative binomial GLM instead. 5.4.2 Continuous variables: Density plots Density plots are smoothed versions of histograms. They’re useful for comparing distributions across groups: # Base R approach plot(density(seedlings$height_cm[seedlings$burn_severity == &quot;low&quot;], na.rm = TRUE), col = &quot;forestgreen&quot;, lwd = 2, main = &quot;Height Distribution by Burn Severity&quot;, xlab = &quot;Height (cm)&quot;, xlim = range(seedlings$height_cm, na.rm = TRUE), ylim = c(0, 0.12)) lines(density(seedlings$height_cm[seedlings$burn_severity == &quot;moderate&quot;], na.rm = TRUE), col = &quot;orange&quot;, lwd = 2) lines(density(seedlings$height_cm[seedlings$burn_severity == &quot;high&quot;], na.rm = TRUE), col = &quot;firebrick&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;Low&quot;, &quot;Moderate&quot;, &quot;High&quot;), col = c(&quot;forestgreen&quot;, &quot;orange&quot;, &quot;firebrick&quot;), lwd = 2, title = &quot;Burn Severity&quot;) Figure 5.3: Seedling height distributions differ by burn severity, with high-severity burns producing shorter seedlings on average. Interpretation: The three burn severity groups show similar spread (variance) but different centers—high-burn seedlings tend to be shorter. The similar shapes suggest that comparing groups with ANOVA or linear regression is reasonable, though we’ll verify this with residual diagnostics later. 5.4.3 Boxplots: Comparing groups and spotting outliers Boxplots are excellent for comparing distributions across groups and identifying outliers: plot_df &lt;- seedlings %&gt;% dplyr::filter(!is.na(height_cm)) boxplot(height_cm ~ burn_severity, data = plot_df, col = c(&quot;forestgreen&quot;, &quot;orange&quot;, &quot;firebrick&quot;), main = &quot;Seedling Height by Burn Severity&quot;, xlab = &quot;Burn Severity&quot;, ylab = &quot;Height (cm)&quot;) Figure 5.4: Boxplot of seedling height by burn severity. The extreme outlier appears in the moderate burn category. Reading a boxplot: Box: The middle 50% of the data (interquartile range, IQR) Bold line: The median Whiskers: Extend to the most extreme points within 1.5 × IQR of the box Points beyond whiskers: Potential outliers That point way above the moderate-burn box is our 89.5 cm seedling. It’s flagged as an outlier not because boxplots have opinions, but because it’s more than 1.5 × IQR above the third quartile. Whether it’s a real outlier (biologically unusual) or a problem outlier (data error) requires investigation. 5.4.4 QQ plots: Checking normality visually Quantile-quantile (QQ) plots compare your data’s distribution to a theoretical distribution—usually the normal distribution. If your data are normally distributed, points should fall along a straight diagonal line. qqnorm(seedlings$height_cm, main = &quot;QQ Plot: Seedling Height&quot;, pch = 16, col = &quot;steelblue&quot;) qqline(seedlings$height_cm, col = &quot;firebrick&quot;, lwd = 2) Figure 5.5: QQ plot for seedling height. Most points follow the line well, but the extreme high value deviates substantially. Reading a QQ plot: Pattern What it suggests Points on the line Data approximately normal S-shape (both tails curve same way) Heavy tails (more extreme values than expected) Points curve up on right Right skew Points curve down on left Left skew Single points far from line Outliers Interpretation: Most points follow the line reasonably well, suggesting the height data are approximately normal. But that one point in the upper right deviates dramatically—this is our 89.5 cm outlier again. The lower-left deviation might be our impossible negative values (if we haven’t removed them yet). 5.4.5 Scatterplots: Exploring relationships For understanding relationships between variables before modeling: plot(height_cm ~ canopy_cover, data = seedlings, pch = 16, col = adjustcolor(&quot;steelblue&quot;, alpha = 0.6), main = &quot;Height vs. Canopy Cover&quot;, xlab = &quot;Canopy Cover (%)&quot;, ylab = &quot;Height (cm)&quot;) # Add a simple trend line abline(lm(height_cm ~ canopy_cover, data = seedlings), col = &quot;firebrick&quot;, lwd = 2) Figure 5.6: Seedling height increases with canopy cover, as expected. The relationship appears roughly linear. Key questions when examining scatterplots: Is the relationship linear? Or does it curve? Is the spread constant? Or does it fan out (heteroscedasticity)? Are there outliers? Points far from the overall pattern? Are there clusters? Might indicate subgroups in your data Interpretation: There’s a positive relationship between canopy cover and seedling height—more shade means taller seedlings. The relationship looks reasonably linear. That high point near the top is our outlier again. The spread of points around the line looks fairly constant (good for linear regression assumptions). 5.4.6 Scatterplot matrices: The big picture When you have multiple variables, a scatterplot matrix lets you see all pairwise relationships at once: # Select numeric columns numeric_vars &lt;- seedlings[, c(&quot;height_cm&quot;, &quot;diameter_mm&quot;, &quot;elevation_m&quot;, &quot;canopy_cover&quot;, &quot;seedling_count&quot;)] pairs(numeric_vars, pch = 16, col = adjustcolor(&quot;steelblue&quot;, alpha = 0.5), main = &quot;Pairwise Relationships&quot;) Figure 5.7: Scatterplot matrix showing relationships among continuous variables. Look for linear relationships, consistent spread, and outliers. What to look for: Strong linear relationships: Might indicate collinearity if both are predictors Curved relationships: Might need transformation or polynomial terms Outliers: Same point appearing as an outlier in multiple panels Clusters: Might indicate groups you should model separately For a more informative version, the GGally package creates scatterplot matrices with correlations and distributions: # install.packages(&quot;GGally&quot;) # if needed library(GGally) ggpairs(numeric_vars, lower = list(continuous = wrap(&quot;points&quot;, alpha = 0.5)), diag = list(continuous = &quot;densityDiag&quot;), upper = list(continuous = wrap(&quot;cor&quot;, size = 4))) 5.5 Key questions to ask during exploration After this initial exploration, you should be able to answer: Do I have data quality issues? Impossible values, unexpected missingness patterns, wrong data types? What do my distributions look like? Symmetric or skewed? Bounded? Discrete or continuous? What relationships exist? Are they linear? Is variance constant across the range? Are there outliers? How many? Are they errors or real? What type of model is appropriate? Normal linear model? GLM? Something else? For our seedling data, initial exploration suggests: - We have some data entry errors to fix (negative heights) - One potential outlier to investigate (89.5 cm height) - Some non-random missing data in high-burn plots - Height is roughly normal; count data is not - Height relates linearly to canopy cover - Burn severity affects height (potential treatment effect) We’re now ready to fit a preliminary model and check its assumptions more formally. 5.6 Assumptions of parametric tests 5.6.1 Why assumptions matter Every statistical model makes assumptions about how your data behave. These aren’t arbitrary rules—they’re the mathematical conditions under which the model’s estimates and p-values are valid. When assumptions are violated: Point estimates may be biased: Your estimate of the mean or slope might be systematically wrong Standard errors may be wrong: Usually underestimated, making effects seem more “significant” than they are P-values may be misleading: You might reject the null hypothesis more (or less) often than you should Confidence intervals may not contain the true value at the stated rate: Your 95% CI might only capture the true value 80% of the time The good news: not all violations are equally serious, and many can be addressed. 5.6.2 The Big Four assumptions for linear models When fitting a linear model (regression, ANOVA, t-tests), we assume: 1. Normality of residuals This is the most commonly misunderstood assumption. We do not assume that your raw data are normally distributed. We assume that the residuals—the differences between observed values and predicted values—are normally distributed. Why does this matter? Because many types of raw data shouldn’t be normal. Heights might vary by treatment group, so the overall distribution is multimodal. Counts are bounded at zero. But once we account for the systematic effects (treatment, predictors), the leftover variation should scatter normally around zero. When it matters most: Small sample sizes. With large samples (n &gt; 30 per group, roughly), the Central Limit Theorem helps protect your p-values even with some non-normality. 2. Homoscedasticity (equal variance) The spread of residuals should be constant across all predicted values. In other words, the model should be equally good at predicting small values and large values. Common violations: - Fan shape: Variance increases with the mean (common with count data, positive-only data) - Trumpet shape: Similar to fan, often seen with growth data - Groups with very different spread: One treatment much more variable than another Why it matters: Unequal variance means some observations contribute more to parameter estimates than they should, and standard errors are wrong. 3. Independence Observations should not influence each other. This is often the most important assumption—and the hardest to fix when violated. Common violations in ecology: - Repeated measures: Same plant measured at multiple time points - Spatial clustering: Plots near each other are more similar - Temporal autocorrelation: Measurements close in time are correlated - Hierarchical structure: Seedlings within plots within sites Why it matters: Non-independence inflates your effective sample size. You might have 100 observations, but if they’re clustered in 10 plots, your effective sample size is closer to 10. P-values will be far too small. 4. Linearity The relationship between predictors and response should be linear (for linear models). If the true relationship is curved, a linear model will systematically over- and under-predict in different regions. How to detect: Look for patterns in residual plots—if residuals curve systematically, the relationship isn’t linear. 5.7 Checking assumptions after fitting a model Here’s the key insight: most assumption checks require fitting a model first. You can’t examine residuals until you have residuals, and you can’t have residuals without a fitted model. This is why EDA happens in two phases: 1. Before fitting: Explore raw data, identify problems, choose an appropriate model 2. After fitting: Examine residuals to verify assumptions were reasonable Let’s fit a simple model and walk through the diagnostic process. 5.7.1 Fitting a preliminary model We’ll model seedling height as a function of canopy cover and burn severity: # First, let&#39;s clean up those impossible values seedlings_clean &lt;- seedlings seedlings_clean$height_cm[seedlings_clean$height_cm &lt; 0] &lt;- NA # Fit a linear model model1 &lt;- lm(height_cm ~ canopy_cover + burn_severity, data = seedlings_clean) # Quick look at results summary(model1) ## ## Call: ## lm(formula = height_cm ~ canopy_cover + burn_severity, data = seedlings_clean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.248 -2.861 -0.550 2.149 60.100 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.39920 1.73483 8.876 3.10e-15 *** ## canopy_cover 0.21741 0.02925 7.432 9.91e-12 *** ## burn_severitymoderate -3.31512 1.33442 -2.484 0.0142 * ## burn_severityhigh -6.45353 1.39770 -4.617 8.76e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.585 on 139 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.3821, Adjusted R-squared: 0.3688 ## F-statistic: 28.65 on 3 and 139 DF, p-value: 1.738e-14 The coefficients suggest that canopy cover has a positive effect on height, and high burn severity reduces height. But before we interpret these results, we need to check whether the model’s assumptions are met. 5.7.2 Base R diagnostic plots The simplest approach is plot(model), which produces four diagnostic plots: par(mfrow = c(2, 2)) plot(model1) Figure 5.8: The four default diagnostic plots from plot(model). Each reveals different potential problems. par(mfrow = c(1, 1)) Let’s interpret each plot: Plot 1: Residuals vs. Fitted This plot checks both linearity and homoscedasticity. X-axis: Predicted (fitted) values from the model Y-axis: Residuals (observed - predicted) Red line: Smoothed average of residuals What you want to see: - Residuals scattered randomly around zero (the horizontal dashed line) - No systematic curve in the red line (would indicate non-linearity) - Constant spread across the range of fitted values (if spread changes, you have heteroscedasticity) What our plot shows: [Interpret based on actual output] Plot 2: QQ Plot of Residuals This checks normality of residuals. X-axis: Theoretical quantiles (where points would fall if perfectly normal) Y-axis: Actual quantiles of your residuals What you want to see: - Points falling along the diagonal line What departures mean: - Tails curving away: Heavy-tailed distribution - S-shape: Skewness - Individual points off the line in corners: Outliers Plot 3: Scale-Location This is another check for homoscedasticity, but plots √|standardized residuals| against fitted values. What you want to see: - Horizontal red line - Points spread evenly across the range What increasing spread means: Variance increases with the mean—common with positive-only data, counts, or when you need a log transformation. Plot 4: Residuals vs. Leverage This identifies influential observations—points that have a large effect on the model fit. Leverage: How unusual a point is in predictor space (extreme X values) Standardized residuals: How unusual a point is in response space (extreme Y given X) Cook’s distance (dashed contours): Combines both; measures overall influence What you want to see: - No points outside the Cook’s distance contours (dashed red lines) - No points in the upper or lower right corners What problems look like: - Point with high leverage AND high residual = very influential, investigate - Points labeled with row numbers are R’s way of saying “look at these” 5.7.3 Using the performance package The performance package provides a more visual and comprehensive diagnostic dashboard: # install.packages(&quot;performance&quot;) # if needed library(performance) # All-in-one visual check check_model(model1) This produces a multi-panel display checking: - Linearity (posterior predictive check) - Homoscedasticity - Influential observations - Collinearity (for multiple predictors) - Normality of residuals You can also run specific checks: # Test normality formally check_normality(model1) # Test homoscedasticity check_heteroscedasticity(model1) # Check for influential observations check_outliers(model1) 5.7.4 Interpreting diagnostic output: A worked example Let’s walk through interpreting our model’s diagnostics step by step: # Extract residuals and fitted values residuals_model &lt;- residuals(model1) fitted_model &lt;- fitted(model1) # Keep only finite pairs (prevents lowess() failure) ok &lt;- is.finite(fitted_model) &amp; is.finite(residuals_model) f &lt;- fitted_model[ok] r &lt;- residuals_model[ok] # 1. Residuals vs fitted plot(f, r, pch = 16, col = adjustcolor(&quot;steelblue&quot;, alpha = 0.6), xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, main = &quot;Residuals vs. Fitted Values&quot;) abline(h = 0, col = &quot;firebrick&quot;, lwd = 2, lty = 2) # Smooth trend line (only if there are enough points + unique x&#39;s) if(length(f) &gt; 5 &amp;&amp; length(unique(f)) &gt; 2) { lines(lowess(f, r), col = &quot;forestgreen&quot;, lwd = 2) } else { message(&quot;Skipping lowess(): not enough finite/unique fitted values.&quot;) } Questions to ask: 1. Does the green smooth line stay close to zero? (Yes = good, linearity satisfied) 2. Does the vertical spread of points stay constant left to right? (Yes = homoscedasticity satisfied) 3. Are there extreme points? (Check if they’re outliers we identified earlier) # 2. Check normality of residuals qqnorm(residuals_model, pch = 16, col = &quot;steelblue&quot;, main = &quot;QQ Plot of Residuals&quot;) qqline(residuals_model, col = &quot;firebrick&quot;, lwd = 2) # Formal test (but use with caution - very sensitive with large samples) shapiro.test(residuals_model) ## ## Shapiro-Wilk normality test ## ## data: residuals_model ## W = 0.62711, p-value &lt; 2.2e-16 About the Shapiro-Wilk test: A significant p-value suggests non-normality. But be careful: - With small samples, the test has low power and may miss real non-normality - With large samples, the test detects trivial departures that don’t affect inference - Visual assessment is usually more useful than the formal test # 3. Check for influential points # Cook&#39;s distance cooks_d &lt;- cooks.distance(model1) plot(cooks_d, type = &quot;h&quot;, main = &quot;Cook&#39;s Distance&quot;, ylab = &quot;Cook&#39;s Distance&quot;, xlab = &quot;Observation&quot;) abline(h = 4/nrow(seedlings_clean), col = &quot;firebrick&quot;, lty = 2) # Common threshold # Which points are influential? influential &lt;- which(cooks_d &gt; 4/nrow(seedlings_clean)) if(length(influential) &gt; 0) { print(paste(&quot;Potentially influential points:&quot;, paste(influential, collapse = &quot;, &quot;))) print(seedlings_clean[influential, ]) } ## [1] &quot;Potentially influential points: 62, 112, 150&quot; ## plot_id elevation_m canopy_cover burn_severity height_cm diameter_mm seedling_count survival ## 62 62 2256 85.7 low 23.9 10.7 4 0 ## 112 112 2232 64.4 low 89.5 27.7 5 1 ## 150 150 1883 12.5 moderate 28.4 11.5 1 1 Cook’s distance thresholds: - Traditional: Cook’s D &gt; 1 is concerning - More sensitive: Cook’s D &gt; 4/n flags more points for inspection - Best approach: Look at the plot and investigate any points that stand out 5.8 Summary: The EDA workflow Let’s consolidate what we’ve learned into a repeatable workflow: 5.8.1 Before fitting a model: Inspect structure: dim(), str(), head(), summary() Check for impossible values: Look at min/max, find and document problems Assess missing data: Count and look for patterns Visualize distributions: Histograms, density plots, boxplots Explore relationships: Scatterplots, correlation Decide on model type: Based on data types and distributions 5.8.2 After fitting a model: Plot diagnostics: plot(model) or check_model() Check normality: QQ plot of residuals Check homoscedasticity: Residuals vs. fitted plot Check linearity: Residuals vs. fitted plot Identify influential points: Cook’s distance Address problems: Transform, use different model, or document limitations This iterative process—explore, model, diagnose, adjust—is how real data analysis works. The goal isn’t perfection; it’s understanding your data well enough to draw defensible conclusions. 5.9 Key takeaways Always explore before modeling: Catch errors and understand your data structure Check residuals, not raw data: Assumptions apply to residuals, not observations Visualization is your most powerful tool: Plots reveal patterns that statistics miss Document your decisions: Every data cleaning choice should be reproducible No model is perfect: The goal is “good enough” for valid inference, not mathematical perfection When in doubt, try a different model: GLMs, transformations, and robust methods are all available 5.10 Assignment Conduct a complete EDA on your own project dataset: Data inspection (2-3 paragraphs) Report the dimensions and structure of your data Summarize any missing data and their patterns Document any impossible or suspicious values Distribution visualization (3-4 figures with captions) Create histograms or density plots for your main response variable(s) Create boxplots if you have categorical predictors Describe what you observe about shape, spread, and potential outliers Relationship exploration (2-3 figures with captions) Create scatterplots of your response vs. key predictors Describe the relationships you see: linear? curved? strong? weak? Preliminary model and diagnostics Fit an appropriate preliminary model Run diagnostic checks and include the plots Interpret each diagnostic plot in 1-2 sentences Summary and plan (1 paragraph) What issues did you identify? How do you plan to address them? What model do you think is most appropriate for your data? "],["describing-data.html", "Chapter 6 Describing Data 6.1 The dataset: Milkweed seedling heights 6.2 Location: Where is the center? 6.3 Spread: How variable are the observations? 6.4 Shape: Is the distribution symmetric? 6.5 Outliers: Unusual observations 6.6 Describing categorical data 6.7 Confidence intervals 6.8 Putting it all together: Complete description 6.9 Reporting in publications 6.10 Key takeaways 6.11 Looking ahead 6.12 Assignment", " Chapter 6 Describing Data Before running any statistical test, we need to describe what we’re working with. Descriptive statistics summarize the key features of a dataset—where the values cluster, how spread out they are, what shape the distribution takes, and whether any observations seem unusual. These summaries serve two purposes: they help us understand our data before analysis, and they communicate our findings to readers afterward. When writing up results, I ask myself: What is the most important information for my audience to know about this dataset? That question guides which descriptive statistics to include. But before we can make those decisions, we need to understand what each measure tells us. A comprehensive description of any dataset addresses four features: Location: Where is the center of the data? (mean, median) Spread: How variable are the observations? (variance, standard deviation, range, IQR) Shape: Is the distribution symmetric or skewed? (histograms, comparing mean to median) Outliers: Are there unusual observations? (boxplots, extreme values) We’ll work through each of these, building intuition for what the numbers mean and when to use which measure. 6.1 The dataset: Milkweed seedling heights Let’s use data from a project I’m working on right now. We’re studying the performance of several pollinator-friendly native species in agricultural gardens across Arizona. The goal is to develop seed sources for restoration of arid and semiarid grasslands. To do this, we need to understand how reliably these species establish, produce seed, and attract pollinators. We’re conducting experiments with multiple populations of each species to see how consistently plants grow and perform. Here, we’ll look at the initial heights (in cm) of one population of Asclepias subverticillata (horsetail milkweed) from near Sedona. # Heights (cm) of A. subverticillata seedlings from the Sedona population sedona &lt;- c(3, 3, 3, 3, 7, 8, 9) This is a small dataset, which makes it easy to see exactly what each calculation is doing. In practice, you’ll work with much larger datasets, but the principles are the same. 6.2 Location: Where is the center? 6.2.1 The mean The most common measure of central tendency is the mean—the sum of all values divided by the number of observations. In R: mean(sedona) ## [1] 5.142857 The mean height is about 5.1 cm. But what is R actually calculating? The formula for the sample mean is: \\[\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\\] where: \\(\\bar{x}\\) (“x-bar”) is the sample mean \\(\\sum\\) means “add up” \\(x_i\\) is the value of observation \\(i\\) \\(n\\) is the number of observations Let’s verify this by hand: # Add all the values total &lt;- 3 + 3 + 3 + 3 + 7 + 8 + 9 total ## [1] 36 # Count the observations n &lt;- length(sedona) n ## [1] 7 # Divide sample_mean &lt;- total / n sample_mean ## [1] 5.142857 This matches mean(sedona). Sometimes with more complex formulas, I’ll work through the calculation by hand to make sure I understand what’s happening. A note on notation: You’ll see different symbols used in different textbooks. The convention is: \\(\\bar{x}\\) = sample mean (what we calculate from our data) \\(\\mu\\) = population mean (the true mean of the entire population, which we usually don’t know) When we calculate mean(sedona), we’re estimating the population mean using our sample. The sample mean is our best guess at what the true population mean might be. 6.2.2 The median The median is the middle value when observations are arranged in order. Half the data fall above the median, and half below. median(sedona) ## [1] 3 For our Sedona seedlings, the median height is 3 cm—quite different from the mean of 5.1 cm! To find the median by hand: 1. Arrange values in order: 3, 3, 3, 3, 7, 8, 9 2. Find the middle value (for n=7, that’s the 4th value): 3 For an even number of observations, the median is the average of the two middle values. 6.2.3 Mean vs. median: Which should you report? The difference between mean and median tells us something important about our data. When the mean is much larger than the median (5.1 vs. 3), it suggests that a few high values are pulling the average up. This is called right skew or positive skew. Guidelines: Situation Report Why Symmetric distribution Mean Mean and median are similar; mean uses all data Skewed distribution Median More representative of typical values Comparing to other studies Whatever they used Enables comparison Ecological interest in extremes Both Full picture of distribution Here’s a real-world example: If I told you the mean home price in Flagstaff is $550,000, but the median is $425,000, what would you conclude? A few expensive homes are pulling the mean up. The median better represents what a typical home buyer would pay. Similarly, if we’re interested in “typical” seedling size, the median of 3 cm might be more informative than the mean of 5.1 cm. But if we want to estimate total biomass (where large individuals contribute disproportionately), the mean matters more. 6.2.4 The mode For categorical data, we often report the mode—the most frequently occurring value. For continuous data, the mode is less useful because each value might occur only once. # For our seedling data, we can find the mode manually: table(sedona) ## sedona ## 3 7 8 9 ## 4 1 1 1 The mode is 3 cm, which appears four times. When the mode, median, and mean all differ substantially (mode=3, median=3, mean=5.1), the distribution is definitely skewed. 6.3 Spread: How variable are the observations? Knowing the center isn’t enough. Two datasets can have the same mean but very different amounts of variation. Measures of spread tell us how much individual observations differ from one another. 6.3.1 Range The simplest measure of spread is the range—the difference between the largest and smallest values. # The range max(sedona) - min(sedona) ## [1] 6 # Or see both endpoints range(sedona) ## [1] 3 9 The range of our seedling heights is 6 cm (from 3 to 9 cm). The range is easy to understand but has a major limitation: it depends entirely on the two most extreme values. A single outlier can dramatically change the range, making it sensitive to data entry errors or unusual observations. 6.3.2 Variance Variance measures the average squared deviation from the mean. It captures how spread out the data are around their center. The formula for sample variance is: \\[s^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n - 1}\\] where: \\(s^2\\) is the sample variance \\(x_i\\) is each observation \\(\\bar{x}\\) is the sample mean \\(n\\) is the number of observations Let’s calculate this step by step: # Step 1: Calculate the mean xbar &lt;- mean(sedona) xbar ## [1] 5.142857 # Step 2: Calculate each deviation from the mean deviations &lt;- sedona - xbar deviations ## [1] -2.142857 -2.142857 -2.142857 -2.142857 1.857143 2.857143 3.857143 # Step 3: Square each deviation squared_devs &lt;- deviations^2 squared_devs ## [1] 4.591837 4.591837 4.591837 4.591837 3.448980 8.163265 14.877551 # Step 4: Sum the squared deviations sum_sq &lt;- sum(squared_devs) sum_sq ## [1] 44.85714 # Step 5: Divide by n-1 variance &lt;- sum_sq / (n - 1) variance ## [1] 7.47619 # Verify with R&#39;s function var(sedona) ## [1] 7.47619 Why do we square the deviations? If we just added up the deviations, the positives and negatives would cancel out (the sum would always be zero—try it!). Squaring makes all values positive, so we can see the total magnitude of variation. Why divide by n-1 instead of n? This is called Bessel’s correction. When we estimate variance from a sample, we tend to underestimate the true population variance. Dividing by n-1 instead of n corrects this bias. Think of it as a penalty for using a sample instead of measuring the entire population. Another way to think about it: we used up one “degree of freedom” when we estimated the mean. Only n-1 of our observations are truly free to vary; once you know the mean and n-1 values, the last value is determined. 6.3.3 Standard deviation The standard deviation is simply the square root of the variance: \\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n - 1}}\\] # Square root of variance sqrt(variance) ## [1] 2.734262 # Or use R&#39;s function sd(sedona) ## [1] 2.734262 Why take the square root? Variance is in squared units (cm²), which is hard to interpret. Standard deviation puts us back in the original units (cm), making it easier to relate to the data. For our seedlings, the standard deviation is about 2.7 cm. This means observations typically deviate from the mean by roughly 2.7 cm. The 68-95-99.7 rule: For normally distributed data, approximately: - 68% of observations fall within 1 SD of the mean - 95% fall within 2 SDs - 99.7% fall within 3 SDs So if seedling heights were normally distributed with mean 5.1 cm and SD 2.7 cm, we’d expect about 68% of seedlings to be between 2.4 and 7.8 cm tall. 6.3.4 Standard error The standard error (SE) is different from standard deviation—it measures uncertainty in the mean itself, not variation in individual observations. \\[SE = \\frac{s}{\\sqrt{n}}\\] # R doesn&#39;t have a built-in SE function, so we write our own std_error &lt;- function(x) sd(x) / sqrt(length(x)) std_error(sedona) ## [1] 1.033454 What’s the difference between SD and SE? Standard deviation describes how variable the data are. It’s a property of the sample. Standard error describes how precisely we’ve estimated the mean. It decreases with larger samples. If you measured 7 seedlings and got SD = 2.7 cm, measuring 700 seedlings would give you roughly the same SD (the population is still variable). But your SE would be much smaller (you’d be more confident in your estimate of the mean). When to report which: Situation Report Why Describing the sample SD Shows actual variation in your data Comparing means across groups SE or CI Shows precision of estimates Publication results SE or 95% CI Standard practice in ecology 6.3.5 Interquartile range (IQR) The interquartile range is the difference between the 75th percentile (Q3) and 25th percentile (Q1). It captures the middle 50% of the data. # Get quartiles quantile(sedona) ## 0% 25% 50% 75% 100% ## 3.0 3.0 3.0 7.5 9.0 # IQR specifically IQR(sedona) ## [1] 4.5 For our seedlings, Q1 = 3 and Q3 = 7.5, so the IQR is 4.5 cm. The middle half of seedlings range from 3 to 7.5 cm tall. The IQR is useful because it’s resistant to outliers—unlike range, which can change dramatically with a single extreme value. 6.3.6 Summary table: Measures of spread Measure Formula Units Sensitive to outliers? Best for Range max - min Original Very Quick overview Variance \\(\\frac{\\sum(x_i - \\bar{x})^2}{n-1}\\) Squared Yes Calculations, ANOVA SD \\(\\sqrt{\\text{variance}}\\) Original Yes Describing variation SE \\(\\frac{s}{\\sqrt{n}}\\) Original Yes Precision of mean IQR Q3 - Q1 Original No Skewed data, outliers present 6.4 Shape: Is the distribution symmetric? The shape of a distribution tells us whether values are evenly spread around the center or bunched up on one side. This matters for choosing appropriate statistical methods—many tests assume symmetric (approximately normal) distributions. 6.4.1 Comparing mean and median A quick way to assess skewness: Mean ≈ Median: Distribution is roughly symmetric Mean &gt; Median: Right-skewed (tail extends toward high values) Mean &lt; Median: Left-skewed (tail extends toward low values) # Our Sedona data mean(sedona) ## [1] 5.142857 median(sedona) ## [1] 3 # Mean (5.1) &gt; Median (3) suggests right skew 6.4.2 Visualizing shape with histograms A histogram shows the frequency of values in different ranges (bins). It’s our primary tool for visualizing distribution shape. # Let&#39;s expand our dataset to better show distributions set.seed(123) sedona_large &lt;- c(3, 3, 3, 3, 7, 8, 9, # original data round(rnorm(30, mean = 4, sd = 2), 1)) # add more realistic variation sedona_large &lt;- sedona_large[sedona_large &gt; 0] # heights must be positive hist(sedona_large, breaks = 10, col = &quot;lightblue&quot;, border = &quot;white&quot;, main = &quot;Distribution of Seedling Heights&quot;, xlab = &quot;Height (cm)&quot;, ylab = &quot;Number of seedlings&quot;) # Add vertical lines for mean and median abline(v = mean(sedona_large), col = &quot;firebrick&quot;, lwd = 2, lty = 1) abline(v = median(sedona_large), col = &quot;forestgreen&quot;, lwd = 2, lty = 2) legend(&quot;topright&quot;, legend = c(paste(&quot;Mean =&quot;, round(mean(sedona_large), 1)), paste(&quot;Median =&quot;, round(median(sedona_large), 1))), col = c(&quot;firebrick&quot;, &quot;forestgreen&quot;), lty = c(1, 2), lwd = 2) Figure 6.1: Histogram of seedling heights showing the frequency of observations in different height ranges. Reading a histogram: X-axis: The variable being measured (height) Y-axis: Count or frequency of observations in each bin Bars: Each bar represents a range of values; taller bars mean more observations What to look for: Modes: How many peaks? One (unimodal), two (bimodal), or more? Symmetry: Are the left and right sides roughly mirror images? Skewness: Does one tail extend further than the other? Gaps: Are there ranges with no observations? Outliers: Are there isolated bars far from the rest? 6.4.3 The normal (Gaussian) distribution Many statistical methods assume data follow a normal distribution—the familiar bell-shaped curve. Key features: Symmetric around the mean Mean = Median = Mode Most observations cluster near the center Tails extend equally in both directions Figure 6.2: A symmetric distribution with a normal curve overlay. In a perfect normal distribution, the mean and median are identical. 6.4.4 Skewed distributions Real ecological data are often skewed. Right-skewed (positive skew) data are especially common for measurements that can’t be negative: Body sizes Count data (number of offspring, flowers, seeds) Time to event (seed germination, time to flowering) Many physiological rates Figure 6.3: Comparison of symmetric vs. right-skewed distributions. Note how the mean is pulled toward the tail in the skewed distribution. In the right-skewed example, notice how the mean (red line) is pulled toward the tail, while the median (green dashed line) stays closer to where most values cluster. 6.5 Outliers: Unusual observations Outliers are observations that fall far from the rest of the data. They might represent: Data entry errors: A decimal point in the wrong place Measurement problems: Equipment malfunction, observer error Real biological variation: Genuinely unusual individuals Different populations: A sample that doesn’t belong with the others The key is to investigate outliers before deciding what to do with them. 6.5.1 Detecting outliers with boxplots Boxplots (box-and-whisker plots) display the five-number summary and flag potential outliers: # Add an outlier to our data sedona_with_outlier &lt;- c(sedona, 25) boxplot(sedona_with_outlier, main = &quot;Seedling Heights with Outlier&quot;, ylab = &quot;Height (cm)&quot;, col = &quot;lightblue&quot;) Figure 6.4: Anatomy of a boxplot showing the median, interquartile range (IQR), whiskers, and outliers. Reading a boxplot: Component What it shows Bold line in box Median (50th percentile) Box IQR: middle 50% of data (Q1 to Q3) Whiskers Extend to most extreme points within 1.5 × IQR Points beyond whiskers Potential outliers The 1.5 × IQR rule is a common convention for flagging outliers, but it’s not absolute. In small samples, even normal data may have points beyond the whiskers. 6.5.2 Boxplots for comparing groups Boxplots are especially useful for comparing distributions across groups: # Simulate data from three populations set.seed(42) flagstaff &lt;- rnorm(30, mean = 6, sd = 1.5) sedona_sim &lt;- rnorm(30, mean = 4.5, sd = 2) tucson &lt;- rnorm(30, mean = 8, sd = 2.5) # Combine into a data frame populations &lt;- data.frame( height = c(flagstaff, sedona_sim, tucson), population = rep(c(&quot;Flagstaff&quot;, &quot;Sedona&quot;, &quot;Tucson&quot;), each = 30) ) boxplot(height ~ population, data = populations, col = c(&quot;lightblue&quot;, &quot;lightyellow&quot;, &quot;lightgreen&quot;), main = &quot;Seedling Height by Population&quot;, xlab = &quot;Population&quot;, ylab = &quot;Height (cm)&quot;) Figure 6.5: Comparing seedling heights across populations. Boxplots make it easy to compare medians, spread, and presence of outliers. At a glance, we can see: - Tucson seedlings are tallest (highest median) - Sedona seedlings are most variable (largest box and whiskers) - Flagstaff has the smallest spread - No obvious outliers in any group 6.5.3 What to do with outliers Never automatically delete outliers! Instead: Investigate: Check original datasheets. Is this a typo? Equipment error? Verify: Could this be real biology? A 25 cm seedling might be a legacy plant, not a true seedling. Document: Whatever you decide, record your reasoning in your code/notes. Consider impact: Run your analysis with and without the outlier. Do conclusions change? Options for handling outliers: Action When appropriate Correct Clear data entry error with recoverable true value Remove Confirmed error, or observation doesn’t belong in sample Keep Real biological variation, even if unusual Use robust methods Analyze with median-based or rank-based tests Transform Log or other transform may reduce outlier influence 6.6 Describing categorical data So far we’ve focused on continuous variables. Categorical data require different descriptions. 6.6.1 Frequencies and proportions For categorical data, we count how many observations fall in each category: # Burn severity categories for 50 plots set.seed(42) burn &lt;- sample(c(&quot;low&quot;, &quot;moderate&quot;, &quot;high&quot;), 50, replace = TRUE, prob = c(0.3, 0.4, 0.3)) burn &lt;- factor(burn, levels = c(&quot;low&quot;, &quot;moderate&quot;, &quot;high&quot;)) # Frequency table table(burn) ## burn ## low moderate high ## 22 13 15 # Proportions prop.table(table(burn)) ## burn ## low moderate high ## 0.44 0.26 0.30 # As percentages round(prop.table(table(burn)) * 100, 1) ## burn ## low moderate high ## 44 26 30 Reporting categorical data: Report both counts and percentages: “15 plots (30%) had low burn severity” Include sample size so readers can judge reliability For binary outcomes, often report only one category: “Survival was 73% (n = 150)” 6.6.2 Visualizing categorical data with bar plots barplot(table(burn), col = c(&quot;forestgreen&quot;, &quot;orange&quot;, &quot;firebrick&quot;), main = &quot;Distribution of Burn Severity&quot;, xlab = &quot;Burn Severity&quot;, ylab = &quot;Number of Plots&quot;) Figure 6.6: Bar plot showing the distribution of burn severity across plots. For categorical data, bar plots show counts or proportions. Don’t confuse these with histograms—bar plots have gaps between bars (discrete categories), while histograms have touching bars (continuous ranges). 6.7 Confidence intervals You’ll often see results reported with confidence intervals (CIs) rather than standard errors. A 95% confidence interval provides a range of plausible values for the true population parameter. 6.7.1 Calculating a 95% CI for the mean For large samples (n &gt; 30), the 95% CI is approximately: \\[\\bar{x} \\pm 1.96 \\times SE\\] For smaller samples, we use the t-distribution: \\[\\bar{x} \\pm t_{\\alpha/2, n-1} \\times SE\\] # Using our Sedona data xbar &lt;- mean(sedona) se &lt;- std_error(sedona) n &lt;- length(sedona) # Get the t critical value for 95% CI with n-1 degrees of freedom t_crit &lt;- qt(0.975, df = n - 1) # 0.975 because we want 2.5% in each tail t_crit ## [1] 2.446912 # Calculate CI lower &lt;- xbar - t_crit * se upper &lt;- xbar + t_crit * se cat(&quot;Mean:&quot;, round(xbar, 2), &quot;cm\\n&quot;) ## Mean: 5.14 cm cat(&quot;95% CI:&quot;, round(lower, 2), &quot;to&quot;, round(upper, 2), &quot;cm\\n&quot;) ## 95% CI: 2.61 to 7.67 cm Interpreting the CI: We’re 95% confident that the true population mean falls between these bounds. If we repeated this study many times, 95% of the calculated CIs would contain the true mean. CI vs. SE: SE is smaller (it’s just one standard error) 95% CI ≈ mean ± 2 × SE (roughly) CI is more directly interpretable (“the true mean is probably in this range”) Most ecology journals prefer CIs over SE because they’re easier to interpret and directly show uncertainty. 6.7.2 Using t.test() for quick CIs t.test(sedona)$conf.int ## [1] 2.614086 7.671628 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 This gives you the 95% CI directly, without manual calculation. 6.8 Putting it all together: Complete description Here’s a template for fully describing a continuous variable: describe_continuous &lt;- function(x, name = &quot;Variable&quot;) { cat(&quot;Description of&quot;, name, &quot;\\n&quot;) cat(&quot;=====================================\\n&quot;) cat(&quot;Sample size:&quot;, length(x[!is.na(x)]), &quot;\\n&quot;) cat(&quot;Missing values:&quot;, sum(is.na(x)), &quot;\\n\\n&quot;) cat(&quot;Location:\\n&quot;) cat(&quot; Mean:&quot;, round(mean(x, na.rm = TRUE), 2), &quot;\\n&quot;) cat(&quot; Median:&quot;, round(median(x, na.rm = TRUE), 2), &quot;\\n\\n&quot;) cat(&quot;Spread:\\n&quot;) cat(&quot; SD:&quot;, round(sd(x, na.rm = TRUE), 2), &quot;\\n&quot;) cat(&quot; SE:&quot;, round(sd(x, na.rm = TRUE)/sqrt(sum(!is.na(x))), 2), &quot;\\n&quot;) cat(&quot; Range:&quot;, round(min(x, na.rm = TRUE), 2), &quot;to&quot;, round(max(x, na.rm = TRUE), 2), &quot;\\n&quot;) cat(&quot; IQR:&quot;, round(IQR(x, na.rm = TRUE), 2), &quot;\\n\\n&quot;) cat(&quot;Shape:\\n&quot;) cat(&quot; Mean &gt; Median:&quot;, mean(x, na.rm = TRUE) &gt; median(x, na.rm = TRUE), &quot;(suggests right skew if TRUE)\\n&quot;) } describe_continuous(sedona, &quot;Seedling Height (cm)&quot;) ## Description of Seedling Height (cm) ## ===================================== ## Sample size: 7 ## Missing values: 0 ## ## Location: ## Mean: 5.14 ## Median: 3 ## ## Spread: ## SD: 2.73 ## SE: 1.03 ## Range: 3 to 9 ## IQR: 4.5 ## ## Shape: ## Mean &gt; Median: TRUE (suggests right skew if TRUE) 6.9 Reporting in publications When writing results, be concise but complete. Here are examples: For a single group: &gt; Seedling heights averaged 5.1 cm (SE = 1.0, n = 7) and ranged from 3 to 9 cm. or Mean seedling height was 5.1 cm (95% CI: 2.6 to 7.6 cm, n = 7). For skewed data, report the median: &gt; Median seedling height was 3 cm (IQR: 3–7.5 cm, n = 7). For comparing groups: &gt; Seedlings from Tucson were taller (mean = 8.0 cm, SE = 0.5) than those from Sedona (mean = 4.5 cm, SE = 0.4) or Flagstaff (mean = 6.0 cm, SE = 0.3). For categorical data: &gt; Among the 50 plots surveyed, 14 (28%) experienced low burn severity, 21 (42%) moderate, and 15 (30%) high. Always include: - The statistic (mean, median, proportion) - A measure of variability (SE, SD, CI, or IQR) - Sample size - Units 6.10 Key takeaways Concept What it tells you Mean Center of data; sensitive to outliers Median Center of data; robust to outliers SD How variable individual observations are SE How precisely you’ve estimated the mean 95% CI Plausible range for the true population parameter Range/IQR Spread of data; IQR is robust to outliers Histogram Shape of distribution Boxplot Five-number summary; flags potential outliers Mean vs. Median Tells you about skewness Decision guide: Symmetric data: Report mean ± SE (or 95% CI) Skewed data: Report median (IQR) or median (range) Categorical data: Report counts and percentages Outliers present: Investigate first, then consider robust measures 6.11 Looking ahead In the next chapter, we’ll explore data more thoroughly—checking for errors, visualizing relationships between variables, and preparing data for analysis. The descriptive statistics from this chapter will help you characterize what you find. Later, when we get to statistical tests, you’ll see that many test statistics (like the t-statistic) are built from these same building blocks: means, standard deviations, and sample sizes. Understanding descriptive statistics makes everything else easier. 6.12 Assignment Using data from your own research project (or a provided dataset): Part 1: Continuous variables For your main response variable: Calculate and report: mean, median, SD, SE, range, and IQR Create a histogram and describe the shape (symmetric? skewed? outliers?) Create a boxplot and identify any potential outliers Write a 2-3 sentence description suitable for a results section, choosing appropriate statistics based on the distribution shape Part 2: Categorical variables (if applicable) Create a frequency table showing counts and percentages Create a bar plot Write a 1-2 sentence description for a results section Part 3: Comparing groups (if applicable) Create side-by-side boxplots comparing your response variable across treatment groups or categories Calculate descriptive statistics (mean, SE) for each group Based on the boxplots, what differences (if any) do you observe? Part 4: Reflection In 2-3 sentences, explain which measure of center (mean or median) and which measure of spread (SD, SE, or IQR) you would report for your data, and why. "],["ecological-sampling.html", "Chapter 7 Ecological sampling 7.1 Background 7.2 What kind of study are you designing? 7.3 Sampling approaches 7.4 Systematic Sampling 7.5 Adaptive Sampling (Sampling Rare or Patchy Species) 7.6 Sampling Along Gradients 7.7 Imperfect Detection in Ecological Sampling 7.8 Temporal Sampling and Study Designs Through Time 7.9 Plot shapes, sizes, and transects 7.10 Distance Sampling and Point Counts 7.11 Reducing sampling error 7.12 Sample Size Determination in Ecology: Effect Sizes and Precision 7.13 Variables of interest 7.14 Test your knowledge 7.15 Assignment", " Chapter 7 Ecological sampling 7.1 Background Ecology is the study of organisms and their relationship to the environment. With infinite time and capacity, we could measure every organism, every characteristic of the environment, every physiological function that affects the way an organism responds to the environment, and every gene that underlies those physiological functions in order to understand ecological systems. In practice, such detailed measurements are time-consuming and impractical. For that reason, we use statistics to account for the fact we are always missing information when we conduct ecological studies. In statistics, a population refers to all units of the thing that you are interested in (i.e., all Suriname frogs, all species in a marshland, all grains of sand, all aspen leaves from a genotype found in southern Arizona). Note that the term â€˜populationâ€™ in statistics differs from the term population in population ecology, where a population refers to a group of individuals in a particular area that interbreed. Statistics accounts for the fact that we never perfectly measure the â€˜true populationâ€™ or the all units of interest. Luckily, by properly applying statistics, we can learn practically anything about almost any population using samples! A sample is a subset of the population that we measure to infer something about the true population. In order to avoid erroneous conclusions about the population, our sample must be representative of the population of interest and unbiased. As an example, imagine that you were interested in whether coat color in cats differed between house cats and feral cats. To select the house cat sample, you randomly select house numbers, visit the house and record coat color, thus collecting a random sample. However, to survey feral cats, you go to several cat colonies at night and record the first cat that you see, which are always white or tan. The sampling strategy for feral cats introduces bias, because darker cats are harder to see at night. This causes you to overestimate the number of light-coated feral cats, and underestimate dark-coated feral cats, resulting in the erroneous conclusion that a greater proportion of feral cats are light-colored compared to house cats. Experiments must be carefully planned to reduce bias. We can conduct statistical analysis until the cats come home (ha!), but if your sample is biased, our results will always be meaningless. In the cat example, it was pretty obvious that the researcher was introducing bias, BUT it is REALLY easy to introduce bias in ecological and social research on accident! Imagine that you looking at fire effects on vegetative communities in the Sonoran. In high severity burn areas, there are thickets of cat’s claw (a pokey plant). Without proper field sampling protocols, it is very tempting to avoid establishing plots in the cat claw thickets, thus not capturing true differences in vegetation along burn severity gradients. 7.2 What kind of study are you designing? Before choosing a sampling method, you need to ask a more fundamental question: What kind of study are you running? The answer shapes every sampling decision that follows—where you put plots, how many you need, and what you can conclude from the data. Ecological studies generally fall into two broad categories: Manipulative experiments are studies in which the researcher controls and assigns the treatment. You decide which plots get burned, which seedlings get watered, which enclosures exclude herbivores. Sampling in manipulative experiments focuses on ensuring that treatment groups are comparable and that replication is adequate for the treatments being tested. The sampling methods in this chapter (random, stratified, blocked) are used to place experimental units and assign treatments within them. Observational studies are studies in which the researcher does not control the treatment. Instead, you observe variation that already exists—comparing burned and unburned sites after a wildfire, measuring tree growth across a natural elevation gradient, or comparing species composition in grazed and ungrazed pastures where grazing was determined by the rancher, not by you. Sampling in observational studies focuses on capturing the natural range of variation in the system and ensuring representative spatial coverage. Methods like GRTS, stratified random sampling, and gradient-based designs are particularly important here. This distinction matters for two reasons. First, it determines which sampling methods are most appropriate. In a manipulative experiment, you might use a completely randomized or blocked design to assign treatments to plots (covered in the next chapter, Experimental Design). In an observational study, you are more likely to use GRTS, stratified random sampling, or systematic sampling along a gradient to capture existing variation. Second, it determines what you can conclude. Manipulative experiments support causal language (“fire reduced canopy cover”); observational studies support associative language (“sites that burned had lower canopy cover”). The sampling design cannot fix this distinction—it is baked into the study type. Many ecological studies fall somewhere in between. Natural experiments exploit unplanned events—a wildfire that burns half a watershed, a policy change that protects one river but not another, a beaver that colonizes one stream reach. The “treatment” was not assigned by the researcher, but nature applied it in a way that approximates an experiment. Quasi-experiments involve deliberate human actions without researcher control—a restoration project implemented by a land management agency, a logging operation that creates treated and untreated stands. In both cases, careful sampling of treatment and reference areas is critical for drawing defensible conclusions. One of the most common observational approaches in ecology is space-for-time substitution: instead of waiting decades to observe succession after disturbance, you sample sites of different ages right now and treat the spatial gradient as a proxy for temporal change. Instead of experimentally manipulating climate, you compare populations across a temperature gradient. This approach assumes that the sites differ primarily in the variable of interest (time since disturbance, temperature, elevation) and are otherwise comparable. That assumption is often violated—a site that burned 5 years ago and a site that burned 50 years ago may also differ in soil type, aspect, land-use history, and the species pool available for recolonization. Space-for-time substitution is not inherently wrong, but it requires careful thought about what might be confounded with the gradient, and the conclusions should acknowledge this limitation. The table below summarizes how study type connects to sampling priorities: Table 7.1: Different study types call for different sampling priorities and methods. Knowing your study type before choosing a sampling method prevents mismatches between design and inference. Study type Sampling priority Common methods Manipulative experiment Balance and replication across treatment groups Randomized, blocked, or stratified within treatments Natural experiment Representative coverage of impact and reference areas Stratified random or GRTS across impact/control areas Observational (gradient) Even coverage along the gradient; avoid spatial confounds Systematic or stratified along the gradient Space-for-time substitution Sites comparable except for the variable of interest Stratified by confounds (soil, aspect); systematic along gradient Long-term monitoring Spatial balance and temporal consistency (panels) GRTS with rotating or permanent panels With this framing in mind, let’s look at the specific sampling methods available. The Experimental Design chapter that follows covers the complementary question: once you know where to sample, how do you structure treatments, controls, and replication to support valid inference? 7.3 Sampling approaches Sampling approaches differ in how they balance randomness, spatial coverage, logistical constraints, and inference goals. Here is a brief overview, before we take a look at the most common sampling methods used in ecology. Sampling design What it does When youâ€™d use it Simple random Completely random points Small, homogeneous areas Systematic Regular grid or transects Mapping gradients, efficiency Stratified random Random within strata Heterogeneous landscapes Cluster Groups of nearby points Reduce travel cost GRTS Spatially balanced random Large-scale monitoring Adaptive Adds points where signal is high Rare or patchy species Model-based Guided by covariates Targeting ecological processes Gradient-based Points along environmental gradient Elevation, moisture, disturbance gradients 7.3.1 Random sampling In order to reduce bias, researchers randomize sampling. Random sampling is when every item within the focal population has an equal chance of being selected. In research, random sampling can be applied to selecting experimental subjects, assigning individuals to treatments, or identifying plot locations. It is REALLY easy to introduce bias in ecological and social research on accident if you do not use a random sampling technique! Imagine that you are looking at fire effects on vegetative communities in the Sonoran. In high severity burn areas, there are thickets of cat’s claw (a pokey plant). Without proper field sampling protocols, it is very tempting to avoid establishing plots in the cat claw thickets, thus not capturing true differences in vegetation along burn severity gradients. In practice, researchers use number generators, like those on your phone, or within computer programs, like ArcGIS, to randomly place sampling points. Here, we’ve included a random number sheet to use to randomly array plots. A random number sheet contains random numbers that someone generated in advance to assist in the field. Important note: Random is not the same as wandering around and picking what looks good. Random sampling = using a rule or tool (random number generator, coordinates, random bearings and distances) so every location has a known chance of being selected. Haphazard sampling = picking what feels convenient or typical - this almost always introduces bias. We can quickly and easily generate such a sample in R, using the sample function. sample(1:100, 10, replace=FALSE) ## [1] 92 69 4 50 88 87 49 26 6 100 #1:10000 = numbers to chose among #number of random numbers you wish to generate #to replace or not (in other words do you wish for the same number to be selected multiple times) 7.3.2 Stratified random sampling To make a sample representative of the population, you will want to capture the typical state of the population of interest. This is challenging, since prior to collecting data, you do not know the typical state of the population. With an understanding of ecology, however, and precisely describing your research question, you can improve the representation of your sample without a lot of specific a priori (beforehand) knowledge of the target population. One typical approach is referred to as stratified random sampling, in which you ensure that you are proportionately sampling from major habitat types or features. In the example in Fig. 1, a random sample of the study area overrepresents the forested habitat relative to the grassland habitat. To account for this, the researchers adjust the sampling technique, such that plot locations occur in both of the major habitats proportionally. Since grasslands make up 55% of the study area, 55% of the points would be randomly located in the grassland area. Since there are twenty plots, 11 are placed within grasslands (0.55*20). The remaining 9 plots are then randomly allotted to the forested habitat. Figure 1. Random versus stratified sampling. 7.3.3 Gridded random sampling In complex, multi-species systems, another approach to improve coverage of random sampling is to randomly place plots within a grid (Fig. 2). This is to ensure that you capture species, which may have an array of distributions. Distribution in plant ecology refers to the spatial arrangement of a species or organisms across the landscape. Depending on system dynamics, species may be dispersed, randomly arrayed, or clumped, thus a gridded approach can help capture species, no matter their spatial orientation (Fig. 2). Figure 2. Randomly placing plots within a gridded region helps maximize the likelihood to capture species dynamics in complex, multi-species systems, composed of species with a variety of spatial distributions. 7.3.4 Random cluster sampling Random cluster sampling randomly select groups (aka clusters) within a population. This sampling design is used commonly in ecology, when we select random locations for plots, then measure all individuals within those plots. If for instance, we are interested in Ponderosa Pine growth rates on the Coconino National Forest, we would randomly assign points across Pondo habitat on the Coconino. At each point, we would set up a plot in which we measure Ponderosa Pines within an 11.71m radius plot. Why wouldn’t we just go out to a point and measure 1 tree to create a totally random sample? The plots are randomly assigned (yay!), but the trees within the plots are not independent. In other words, we might expect measures of trees within plot A to be more similar to each other than they are to trees within plot B, due to differences in microsite characteristics, genetic similarity among co-occurring trees, or site history (logging, fire). Luckily, we can account for this non-independence, as long as the plots are random! 7.3.4.1 Pseudoreplication When we treat non-independent measurements as if they were independent replicates, we call this pseudoreplication. For example, if we measure 20 trees inside 1 plot and then pretend we have 20 independent samples - we are ignoring the fact that those trees share the same microsite, history, and environment. In most ecological studies, the plot is the true replicate, and the trees within the plot are subsamples that help us better estimate conditions at that plot. 7.3.5 GRTS Sampling (Generalized Random-Tessellation Stratified Sampling) As ecological questions increasingly span large, heterogeneous landscapes, researchers need sampling approaches that are both statistically rigorous and spatially balanced. One method that has become foundational in long-term ecological monitoringâ€”used by the U.S. EPA, USGS, USFS (e.g., FIA intensification), and many watershed and biodiversity programsâ€”is GRTS, which stands for Generalized Random-Tessellation Stratified sampling. 7.3.5.1 What problem does GRTS solve? Imagine trying to monitor a species or habitat across a large region using simple random sampling. Although random - this approach can still place many points close together and leave other parts of the landscape unrepresented (spatial clumping). Stratified random sampling improves representation across broad habitat types, but still may not ensure even spatial coverage within each stratum. GRTS was developed to solve two key issues: Ensuring spatial balance: Sample points are spread evenly across the landscape (or within strata), minimizing large gaps and clusters. Maintaining true probability-based sampling: Every location has a known probability of being selected, preserving the ability to make unbiased, design-based statistical inferences. This makes GRTS ideal for monitoring programs where the goal is to detect long-term changes in ecological condition, species distributions, or habitat quality. 7.3.5.2 How GRTS Works GRTS uses a special type of spatial ordering, similar to a space-filling curve, to assign every location on the landscape a unique hierarchical address. This lets us draw a sample that: spreads points out as evenly as possible, remains fully random and unbiased, allows consistent sampling over time (e.g., rotating panels), and accommodates stratification, unequal selection probabilities, or oversampling in rare habitats. While the algorithm is mathematically complex, researchers rarely need to understand the internalsâ€”the spsurvey package in R implements GRTS with a single function. 7.3.5.3 When to Use GRTS in Ecological Sampling Researchers choose GRTS when: The study area is large and spatially complex Habitat types are patchy or unevenly distributed Long-term monitoring requires consistency through time Detecting spatial trends or hotspots is important A mixture of old (legacy) and new plots must be integrated without bias GRTS is used in applications such as: Riparian and stream monitoring (EPA EMAP, NRSA) Forest health surveys (USFS FIA intensification) Sage-grouse habitat monitoring Wetland condition assessment Vegetation change detection after disturbance Species occupancy surveys over large regions Restoration monitoring (e.g., fuel treatments, floodplain restoration) In other words, GRTS is the tool of choice when spatial representativeness matters. 7.3.5.4 Example: Sampling Emory oak habitat in across drought and non-drought areas. Overview In this tutorial, you will learn how to implement a GRTS (Generalized Random-Tessellation Stratified) sampling design in R. GRTS produces spatially balanced sample points, meaning sites are evenly spread across the landscape while still being selected randomly. This makes GRTS widely used in ecological monitoring programs (EPA EMAP, NRSA, USFS FIA intensification, watershed monitoring, etc.). Other types of sampling can be implemented in R using similar mapping and spatial workflows. Common designs include simple random sampling, systematic sampling (e.g., regularly spaced grids or transects), stratified random sampling (where samples are allocated within predefined habitat types, management units, or elevation bands), and cluster sampling (where groups of nearby points are sampled together). R also supports adaptive sampling, where additional samples are placed based on initial observations (e.g., high-density patches), unequal probability sampling (e.g., probability proportional to area or habitat suitability), and model-based sampling designs that use covariates such as NDVI, elevation, or climate to guide site selection. GRTS is particularly valuable when spatial balance is critical, but other designs may be more appropriate depending on study objectives, scale, and field constraints. Since GRTS is widely used ecology-based projects, we will start there! We will: 1. Import a study area boundary from Google Drive 2. Generate a spatially balanced GRTS sample 3. Visualize the sample 4. Export the output for field use 7.3.5.4.1 Load Required Packages 7.3.5.4.2 Step 1: Load Spatial Data Our study area includes oak woodlands in southern Arizona. We’ll download three spatial datasets from Google Drive: the sampling frame (study area boundary), strata (drought vs. non-drought areas), and existing monitoring plots. # Temporary directory for downloads temp_dir &lt;- tempdir() # Google Drive file IDs file_ids &lt;- list( sampling_area = &quot;1TOr1BKJpqKYWsGOAPaGrhkql5ovNglqa&quot;, strata = &quot;1ms4ErDYOFkkB4UFFeGGXmWITa3WE97KS&quot;, existing_plots = &quot;1EfI6AorSrgOS1xmr4LioJeSCp6F1yFfZ&quot; ) # Download function (handles Google Drive&#39;s virus scan page) download_from_gdrive &lt;- function(file_id, dest, name) { zip_path &lt;- file.path(dest, paste0(name, &quot;.zip&quot;)) url &lt;- paste0( &quot;https://drive.usercontent.google.com/download?id=&quot;, file_id, &quot;&amp;export=download&amp;confirm=t&quot; ) GET(url, write_disk(zip_path, overwrite = TRUE), progress()) out_dir &lt;- file.path(dest, name) unzip(zip_path, exdir = out_dir) # Removed quiet = TRUE out_dir } # Download all datasets sampling_dir &lt;- download_from_gdrive(file_ids$sampling_area, temp_dir, &quot;SamplingArea&quot;) strata_dir &lt;- download_from_gdrive(file_ids$strata, temp_dir, &quot;Strata&quot;) existing_dir &lt;- download_from_gdrive(file_ids$existing_plots, temp_dir, &quot;ExistingPlots&quot;) # Helper function to read shapefiles read_shp &lt;- function(dir) { shp &lt;- list.files(dir, pattern = &quot;\\\\.shp$&quot;, recursive = TRUE, full.names = TRUE) st_read(shp[1], quiet = TRUE) } # Read the spatial data sampling_frame &lt;- read_shp(sampling_dir) strata &lt;- read_shp(strata_dir) legacy_points &lt;- read_shp(existing_dir) Let’s examine what we loaded: # Check the sampling frame cat(&quot;Sampling Frame contains&quot;, nrow(sampling_frame), &quot;features\\n&quot;) ## Sampling Frame contains 3 features cat(&quot;Available habitat types:&quot;, unique(sampling_frame$R3ERU), &quot;\\n\\n&quot;) ## Available habitat types: Madrean Encinal Woodland Madrean Pinyon-Oak Woodland Ponderosa Pine - Evergreen Oak # Check existing plots cat(&quot;Number of existing monitoring plots:&quot;, nrow(legacy_points), &quot;\\n&quot;) ## Number of existing monitoring plots: 82 7.3.5.4.3 Step 2: Define Study Area We’ll focus on the Madrean Encinal Woodland ecological unit and transform to Albers Equal Area projection for accurate area calculations. encinal &lt;- sampling_frame %&gt;% filter(R3ERU == &quot;Madrean Encinal Woodland&quot;) %&gt;% st_transform(5070) # NAD83 / Conus Albers # Calculate study area study_area_km2 &lt;- round(as.numeric(st_area(encinal)) / 1e6, 2) cat(&quot;Study area:&quot;, study_area_km2, &quot;km²\\n&quot;) ## Study area: 1195.44 km² 7.3.5.4.4 Step 3: Generate GRTS Sample GRTS (Generalized Random-Tessellation Stratified) sampling creates spatially balanced samples. We’ll select 40 base sites plus 10 oversample sites (used if base sites become inaccessible). set.seed(123) # For reproducibility grts_sites &lt;- grts( sframe = encinal, n_base = 40, n_over = 10 ) # Extract base and oversample sites base_sites &lt;- grts_sites$sites_base over_sites &lt;- grts_sites$sites_over cat(&quot;Generated&quot;, nrow(base_sites), &quot;base sites\\n&quot;) ## Generated 40 base sites cat(&quot;Generated&quot;, nrow(over_sites), &quot;oversample sites\\n&quot;) ## Generated 10 oversample sites 7.3.5.4.5 Step 4: Visualize the Sampling Design This step often takes a while, since ggplot is loading spatial data! ggplot() + geom_sf(data = encinal, fill = &quot;forestgreen&quot;, alpha = 0.3, color = &quot;darkgreen&quot;, linewidth = 0.5) + geom_sf(data = base_sites, aes(color = &quot;Base Sites&quot;), size = 3, shape = 19) + geom_sf(data = over_sites, aes(color = &quot;Oversample&quot;), size = 2.5, shape = 1, stroke = 1) + geom_sf(data = legacy_points, aes(color = &quot;Legacy Plots&quot;), size = 2.5, shape = 4, stroke = 1.5) + scale_color_manual( values = c(&quot;Base Sites&quot; = &quot;gold&quot;, &quot;Oversample&quot; = &quot;orange&quot;, &quot;Legacy Plots&quot; = &quot;dodgerblue&quot;), name = &quot;Site Type&quot; ) + theme_minimal(base_size = 14) + theme(legend.position = &quot;bottom&quot;) + labs( title = &quot;GRTS Sampling Design: Madrean Encinal Woodland&quot;, subtitle = paste0(&quot;Study area: &quot;, study_area_km2, &quot; kmÂ²&quot;) ) Figure 7.1: GRTS probability-based sampling design showing 40 base sites (gold circles), 10 oversample sites (orange circles), and existing monitoring plots (blue crosses) in the Madrean Encinal Woodland. Notice how GRTS sites are spatially balanced across the study area. Key observations: - Base sites (gold) are spread evenly across the landscape - Oversample sites (orange) fill spatial gaps and serve as replacements - Legacy plots (blue) show existing monitoring locations - This spatial balance ensures representative coverage 7.3.5.4.6 Step 5: Export Data for Field Work #------------------------------------------------------------ # Step 5: Export Data for Field Work #------------------------------------------------------------ # Create output directory dir.create(&quot;grts_outputs&quot;, showWarnings = FALSE) # Combine all new sample sites all_sites &lt;- rbind( base_sites %&gt;% mutate(site_type = &quot;Base&quot;), over_sites %&gt;% mutate(site_type = &quot;Oversample&quot;) ) #------------------------------------------------------------ # 1. FOR GPS DEVICES - Simplified GPX (just coordinates &amp; names) #------------------------------------------------------------ # GPX format is VERY strict - only name and geometry allowed gpx_sites &lt;- all_sites %&gt;% mutate(name = paste0(siteID, &quot; (&quot;, site_type, &quot;)&quot;)) %&gt;% dplyr::select(name, geometry) st_write(gpx_sites, &quot;grts_outputs/grts_sampling_points.gpx&quot;, driver = &quot;GPX&quot;, delete_dsn = TRUE) #------------------------------------------------------------ # 2. FOR GIS - Full data in Shapefile &amp; GeoJSON #------------------------------------------------------------ # Shapefile (most universal for GIS) st_write(all_sites, &quot;grts_outputs/grts_sampling_points.shp&quot;, delete_dsn = TRUE) # GeoJSON (modern, includes all fields) st_write(all_sites, &quot;grts_outputs/grts_sampling_points.geojson&quot;, delete_dsn = TRUE) # KML (alternative to GPX - works in Google Earth &amp; many GPS apps) kml_sites &lt;- all_sites %&gt;% mutate(Name = paste0(siteID, &quot; (&quot;, site_type, &quot;)&quot;)) st_write(kml_sites %&gt;% dplyr::select(Name, geometry), &quot;grts_outputs/grts_sampling_points.kml&quot;, driver = &quot;KML&quot;, delete_dsn = TRUE) #------------------------------------------------------------ # 3. FOR ANALYSIS - Full data in CSV #------------------------------------------------------------ # Get coordinates coords_albers &lt;- st_coordinates(all_sites) # Create comprehensive CSV with all information sites_csv &lt;- all_sites %&gt;% st_drop_geometry() %&gt;% mutate( X_albers = coords_albers[,1], Y_albers = coords_albers[,2] ) %&gt;% dplyr::select(siteID, site_type, lon_WGS84, lat_WGS84, X_albers, Y_albers, wgt, ip, R3ERU) write.csv(sites_csv, &quot;grts_outputs/grts_sampling_points.csv&quot;, row.names = FALSE) #------------------------------------------------------------ # 4. FIELD DATA SHEET - Blank template for data collection #------------------------------------------------------------ field_sheet &lt;- sites_csv %&gt;% dplyr::select(siteID, site_type, lon_WGS84, lat_WGS84) %&gt;% mutate( date_visited = &quot;&quot;, observers = &quot;&quot;, canopy_cover_pct = &quot;&quot;, tree_density = &quot;&quot;, notes = &quot;&quot; ) write.csv(field_sheet, &quot;grts_outputs/field_data_sheet.csv&quot;, row.names = FALSE) #------------------------------------------------------------ # 5. SAMPLING SUMMARY #------------------------------------------------------------ summary_table &lt;- data.frame( Category = c(&quot;Total Sites&quot;, &quot;Base Sites&quot;, &quot;Oversample Sites&quot;, &quot;Study Area (kmÂ²)&quot;, &quot;Average Site Weight (kmÂ²)&quot;), Value = c( nrow(all_sites), nrow(base_sites), nrow(over_sites), round(as.numeric(st_area(encinal)) / 1e6, 2), round(mean(as.numeric(all_sites$wgt)) / 1e6, 2) ) ) write.csv(summary_table, &quot;grts_outputs/sampling_summary.csv&quot;, row.names = FALSE) 7.3.6 Understanding Design Weights Important: Each GRTS site has a design weight (wgt) representing the area it represents. When analyzing data collected at these sites, you must use these weights to get unbiased population estimates. # Example: First 5 sites and their weights sites_csv %&gt;% dplyr::select(siteID, site_type, wgt) %&gt;% head(5) %&gt;% knitr::kable(caption = &quot;Sample sites with design weights&quot;) Table 7.2: Sample sites with design weights siteID site_type wgt Site-01 Base 29886054 [m^2] Site-02 Base 29886054 [m^2] Site-03 Base 29886054 [m^2] Site-04 Base 29886054 [m^2] Site-05 Base 29886054 [m^2] Example calculation: If you measure canopy cover at each site, calculate the population mean as: # Example field data (for demonstration) field_data &lt;- data.frame( canopy_cover = c(45, 62, 38, 71, 55), wgt = sites_csv$wgt[1:5] ) # Weighted mean (correct for GRTS) weighted.mean(field_data$canopy_cover, w = field_data$wgt) # Simple mean (WRONG - ignores spatial design) mean(field_data$canopy_cover) 7.4 Systematic Sampling Another approach commonly used in ecological studies is systematic sampling, in which sample points or transects are placed at regular intervals across the landscape (e.g., every 50 m along a grid or trail). Unlike random sampling, where points can be clustered, systematic sampling ensures even spatial coverage of a study area. Systematic sampling is useful when: The landscape is large and relatively uniform The goal is to detect broad-scale patterns Field logistics require simple, repeatable placement of plots However, systematic sampling carries one risk: If the sampling interval accidentally aligns with a natural pattern (e.g., spacing of vegetation bands, fence-line effects, or management history), the sample may over- or under-represent certain features. Despite this, systematic sampling performs extremely well in many ecological systems and often provides more spatially uniform coverage than simple random sampling. 7.5 Adaptive Sampling (Sampling Rare or Patchy Species) Some species of interestâ€”threatened plants, cryptic mammals, rare insectsâ€”occur in patchy, clustered distributions. When organisms are rare, simple random sampling may completely miss them. Adaptive sampling increases sampling intensity only where the organism is found. For example: Select initial sample locations randomly. When a target species is detected at a site, sample additional plots in neighboring locations. Continue expanding sampling outward until no new detections occur. This approach is efficient for: Rare plants Disease outbreaks (e.g., white-nose syndrome, oak wilt) Invasive species early detection Patchy coral or seagrass distributions Adaptive sampling improves our ability to detect and map rare organisms, but it also introduces statistical challenges, since sampling intensity is no longer uniform. These can be addressed using specialized estimators or model-based inference. 7.6 Sampling Along Gradients Many ecological questions involve continuous environmental variation rather than discrete treatments. How does species composition change with elevation? How does tree growth respond to a moisture gradient? How does vegetation recover as you move away from a disturbance boundary? These are gradient studies, and they require a different sampling logic than randomized experiments. 7.6.1 What is a gradient study? In a gradient study, the researcher samples across a continuous range of an environmental variable—elevation, distance from a river, time since fire, soil moisture, latitude—and asks how ecological responses change along that gradient. There is no treatment assignment; the gradient already exists in the landscape, and the goal is to characterize the relationship between the environmental variable and the ecological response. Gradient studies are observational by nature, which means they reveal associations rather than causal effects. A pattern of decreasing species richness with elevation does not prove that elevation causes lower richness—temperature, precipitation, growing season length, and soil depth all change with elevation, and any of them could be responsible. This is where thoughtful sampling design helps: by measuring potential confounds at each sampling point, you provide the data needed to disentangle these effects during analysis. 7.6.2 Designing a gradient study Sampling along a gradient requires decisions about three things: 1. How to distribute points along the gradient. Two common strategies are: Even spacing: Place sample points at regular intervals along the gradient (e.g., every 100 m of elevation). This ensures equal representation of the full gradient range and works well when you expect the response to change gradually. Systematic or stratified approaches work well here. Stratified or targeted placement: Concentrate sampling in transition zones where ecological change is expected to be fastest (e.g., ecotones, treeline, the boundary of a burn). This is more efficient for detecting thresholds or nonlinear responses but risks under-sampling the stable portions of the gradient. In practice, a combination often works best: even spacing across the full gradient with additional sampling at ecologically important transitions. 2. How to handle lateral variation. Gradients are rarely one-dimensional. An elevation transect crosses different aspects, soil types, and land-use histories. If you place a single transect straight up a mountain, you cannot separate the effect of elevation from the effect of whatever else changes along that particular path. The solution is replication across the gradient: multiple transects on different slopes, or stratified random sampling that captures the gradient while varying other site characteristics. For example, if studying elevation effects on tree growth, you might sample at 3–5 locations per elevation band, spread across different aspects and drainages. 3. How far apart to sample. The appropriate spacing depends on the spatial scale of variation in both the gradient variable and the ecological response. If tree species composition changes dramatically over 200 m of elevation, sampling every 500 m will miss important transitions. Pilot data or prior ecological knowledge can inform spacing decisions. 7.6.3 Gradient sampling in R A gradient study can be implemented using many of the sampling methods already described in this chapter. Here is an example using stratified random sampling along an elevation gradient: # Stratified random sampling along an elevation gradient # Goal: 5 plots per 200 m elevation band from 1500 to 2500 m library(sf) library(tidyverse) # Assume you have a DEM (digital elevation model) and a study area boundary # Classify the study area into elevation bands elevation_bands &lt;- study_area %&gt;% mutate(elev_band = cut(elevation, breaks = seq(1500, 2500, by = 200), labels = paste0(seq(1500, 2300, by = 200), &quot;-&quot;, seq(1700, 2500, by = 200), &quot; m&quot;))) # Generate random points within each band set.seed(42) gradient_points &lt;- elevation_bands %&gt;% group_by(elev_band) %&gt;% slice_sample(n = 5) # 5 random locations per band # Check distribution across the gradient table(gradient_points$elev_band) GRTS can also be applied within each stratum (elevation band) to ensure spatial balance within each portion of the gradient, combining the advantages of gradient coverage and spatial balance. 7.6.4 Common pitfalls in gradient studies Confounding the gradient with geography. A single transect confounds the gradient variable with everything else that varies along that specific path. Replicate across the gradient, not just along it. Assuming linearity. Many ecological responses to gradients are nonlinear—species richness may peak at mid-elevations, growth may plateau above a moisture threshold. Sample densely enough to detect curvature, especially near expected transitions. Ignoring spatial autocorrelation. Points close together along a gradient are likely to have similar ecological conditions regardless of the gradient variable. If your points are too close, they provide less independent information than widely spaced points. We will address spatial autocorrelation in a later chapter. Space-for-time traps. When a gradient represents time (e.g., distance from a disturbance boundary as a proxy for time since disturbance), remember that the sites may differ in ways beyond the variable of interest. Measure and report potential confounds. 7.7 Imperfect Detection in Ecological Sampling Ecologists rarely observe organisms perfectly. Whether estimating abundance, occupancy, or survival, detection probability can influence sampling accuracy. Common sources of imperfect detection include: Vegetation density (harder to see animals) Observer skill Weather, time of day, or season Species behavior (cryptic, nocturnal, burrowing) Habitat structure Even when sampling is random and unbiased, ignoring detection probability can lead to biased estimates. For example, frogs in dense vegetation may appear â€œabsentâ€ when they are simply undetected. This is especially important when comparing habitats, because detection typically differs among environments. Common study designs that account for detection include: Repeated surveys (occupancy modeling) Point counts with distance sampling Removal sampling Mark-recapture N-mixture models A simple conceptual model: Observed count = True abundance Ã— Detection probability Accounting for detection sets the foundation for later modeling approaches in wildlife ecology and plant demography. 7.8 Temporal Sampling and Study Designs Through Time Sampling is not only about where you measure, but also when. Many ecological processesâ€”recovery after disturbance, climate-driven changes, successional dynamics require data collected over time. 7.8.1 Sampling Panels Long-term monitoring programs often use panels, which describe when sites are revisited: Permanent panel: Same sites revisited every year (high power to detect change). Rotating panels: Different subsets of sites visited in different years (greater spatial coverage). Split panels: Mixture of permanent and rotating sites (e.g., GRTS with a fixed core). GRTS integrates panel design naturally, making temporal inference more robust. 7.8.1.1 Sampling Frequency Sampling too infrequently risks missing important dynamics; too frequently wastes effort. Key considerations: Generation time of focal species Expected speed of recovery Variability of climate or disturbance regime Practical logistics Temporal sampling decisions strongly influence the ability to detect ecological change. 7.8.2 BACI and Other Experimental Designs When evaluating the effect of a treatment—like a fire, restoration project, or invasive removal—ecologists often use a Before-After-Control-Impact (BACI) design, which combines temporal and spatial comparisons to isolate treatment effects from natural variation. BACI designs, along with other experimental design concepts such as blocking, factorial designs, and split-plot designs, are covered in detail in the Experimental Design chapter. 7.9 Plot shapes, sizes, and transects Deciding on the shape and size of your sampling unit depends on the species or feature that you are trying to measure. Plots can be ANY shape, but usually the shape of the plot is either a square or circle for simplicity â€“ why would you sample using a hexagon? Often in forestry, plots are circular, marked with a central post, which foresters attach logging tape and rapidly measure trees that fall within a certain radius from the central post (Fig. 3). Since trees are large, this helps foresters quickly collect data on the species composition and structure of forests. For smaller organisms, like understory species, quadrats (small plots, often square and 1 m2 in size) are often used, since many smaller organisms occur in this area â€“ if plots were too large, then data collection would be too time-consuming. In some cases, researchers are interested in how certain ecological variables differ as a function of distance from a feature. In these cases, researchers will often use a transect â€“ a linear feature â€“ and collect variables of interest along it. Any of these sampling shapes and sizes can be combined or adapted to measure ecological features to answer the question of interest. Figure 3. A) The standard plot configuration for the Forest Inventory and Analysis dataset, which includes data on all of our forested lands in the US. Forestry plots are often circular (A), allowing foresters to attach loggers tape to a central plot marker (B) and quickly measure trees within these fairly large plots (C). Plots must be large in order to include enough trees to describe stand characteristics. 7.10 Distance Sampling and Point Counts Distance sampling is widely used in wildlife ecology, especially for birds, mammals, and sometimes plants or shrubs. The key idea: the probability of detecting an organism decreases with distance from the observer. Two common designs: Line-Transect Distance Sampling Observers walk a transect and record the perpendicular distance to each detected organism. Detection probability is modeled as a function of distance. Point Count Distance Sampling Observers stand at a fixed point and record distances to detected individuals during a timed count. Distance sampling allows estimation of true density without needing to census every individual, making it fundamental for large-area monitoring. 7.11 Reducing sampling error What is sampling error? Sampling error is the difference between the true estimate of a population and the measurements that researchers collect on a sample. Error happens by chance and is unavoidable â€“ it can be thought of as noise within the data. Error is different from bias, because it is non-systematic. For instance, imagine two people are measuring cactus heights for a demographic study. Error in height measurement is introduced by many things - the shakiness of each person’s hands, the amount of degradation and stretch in various measuring tapes. Bias, on the other hand, would be introduced if person 1 only measures the small cacti, or always mis-reading the measuring tape and measuring heights 5 cm less than their actual height. Bias should always be avoided, and error reduced as much as possible. Larger samples are less affected by chance and so will have lower sampling error. In ecology, we refer to the number of independent units being measured as replicates. The more replicates, the less sampling error! Figure 4. As the number of replicate plots increases so does the accuracy at which we can estimate parameters for this forest stand. 7.12 Sample Size Determination in Ecology: Effect Sizes and Precision Ecological studies often face constraints in time, funding, or accessibility. Although there is no universal sample size, several principles guide decisions: Effect Size Matters Small effects (e.g., a 2% change in cover) require more replication than large effects (e.g., a 50% mortality event). Variability Determines Needed Sampling Systems with high natural variability (e.g., deserts, wetlands) require more replicates to estimate population parameters precisely. Plot Size vs. Plot Number Trade-off â€¢ Larger plots reduce variance but are more time-consuming â€¢ More small plots increase representation and reduce sampling error Pilot Studies Often, researchers conduct a small pilot sampling effort to estimate variance and inform final sample size. Although formal power analyses in ecology can be complex, the principles above provide practical guidance for designing effective sampling. 7.13 Variables of interest Finally, depending on the research question, there are a number of different variables that you might want to measure. In ecology, you may want to measure the number of different species in an area to look at diversity patterns, or collect data on size or growth to look at performance, or monitor individuals after a disturbance to look at mortality. We will collect different forms of data throughout the semester, but the following principles will always apply: Samples should be random and representative Sampling methodology â€“ plot shape and size â€“ should reflect the organism or ecological feature that you are measuring More replication is better, since it reduces sampling error In the rest of this course, weâ€™ll use these samples (collected with careful designs like the ones above) to estimate population parameters such as mean height, survival probability, or species richness. Good sampling design is what makes our later statistical inferences valid and trustworthy. 7.14 Test your knowledge Below are several examples of study designs. Select the best sampling method and indicate why you selected it! 7.15 Assignment Write out your sampling design. Be sure to clearly describe your sampling area, sampling unit, number of replicates, and sampling method (e.g., simple random, stratified random, systematic, GRTS, cluster). Specify whether your design includes stratification, spatial balance, or unequal sampling effort, and explain how sample locations will be selected. In addition, justify each component of your design in light of your study objectives, the ecological system, and any practical constraints (e.g., access, time, cost, safety, detectability). Your justification should explain why this design is appropriate for addressing your research question and what tradeoffs you are making (for example, between spatial coverage and replication, or statistical rigor and field feasibility). 7.15.1 Example Methods: Sampling Design Study Area This study was conducted across oak woodland habitat across Arizona and New Mexico, encompassing the current distribution of Quercus emoryi woodlands. The study area defined using oak habitat derived from vegetation classification maps and land cover data. Areas that were inaccessible due to land ownership restrictions, unsafe terrain, or logistical constraints were excluded prior to sample selection to ensure that all selected sites were feasible to sample and that the resulting dataset would be representative of accessible oak habitat. Sampling Design Monitoring locations were selected using a Generalized Random-Tessellation Stratified (GRTS) sampling design. GRTS generates a spatially balanced random sample, ensuring that sites are well distributed across the landscape while retaining the probabilistic properties required for unbiased inference. This approach is particularly appropriate for large, heterogeneous landscapes such as oak woodlands, where environmental conditions vary across elevation, aspect, and disturbance history, and where simple random sampling may result in clustered or uneven spatial coverage. Each sampling unit consisting of a circular, 16 m radius vegetation plot. A total of 100 plots were selected across the study area. To ensure adequate representation of ecologically meaningful gradients, the sampling frame was stratified by elevation zone and management status (e.g., burned vs. unburned areas). Stratification allowed us to explicitly capture variation associated with these factors while maintaining random, spatially balanced site selection within each stratum through the GRTS algorithm. "],["experimental-design-from-question-to-study-structure.html", "Chapter 8 Experimental Design: From Question to Study Structure 8.1 Observational vs. manipulative studies 8.2 The logic of controls and comparison 8.3 Randomization: why and how 8.4 Common experimental designs 8.5 Before-After-Control-Impact (BACI) designs 8.6 Identifying your unit of replication 8.7 Connecting design to analysis 8.8 Summary 8.9 Assignment", " Chapter 8 Experimental Design: From Question to Study Structure In the previous chapter, we focused on where and how to select sampling units—choosing plot locations, balancing spatial coverage, and ensuring that our sample represents the population we want to study. Sampling design answers the question: Where do I collect data? This chapter addresses a different question: How do I structure my study to answer a causal question? Specifically, how do I assign treatments, arrange controls, handle sources of unwanted variation, and ensure that my design supports the statistical analysis I plan to run? The distinction matters. You can have a beautifully balanced GRTS sampling design and still draw incorrect conclusions if your experimental structure is flawed—if treatments are confounded with sites, if you mistake subsamples for true replicates, or if you ignore known sources of variation that could be controlled through blocking. Conversely, a well-designed experiment at poorly selected sites may not generalize beyond the study area. Sampling design and experimental design work together. Sampling tells you where to look; experimental design tells you what to do once you get there. This chapter covers the second half of that partnership. 8.1 Observational vs. manipulative studies Before choosing a specific design, you need to be honest about what kind of study you are running. The answer determines what you can and cannot conclude. 8.1.1 Manipulative experiments In a manipulative experiment, the researcher controls and assigns the treatment. You decide which plots get burned, which seedlings get watered, which enclosures exclude herbivores. Because you assigned the treatment randomly, you can (in principle) attribute differences in the response to the treatment itself rather than to some other factor that happened to differ between groups. Manipulative experiments are the gold standard for causal inference, but they are not always possible in ecology. You cannot randomly assign drought to a watershed or fire to a national forest. You cannot retroactively assign land-use history to a landscape. 8.1.2 Observational studies In an observational study, the researcher does not control the treatment. Instead, you observe variation that already exists—comparing burned and unburned sites after a wildfire, measuring tree growth across a natural elevation gradient, or comparing species composition in grazed and ungrazed pastures where grazing was determined by the rancher, not by you. Observational studies can reveal patterns and associations, but attributing those patterns to a specific cause is harder because confounding variables may differ between groups. A site that burned in a wildfire may also differ from an unburned site in elevation, slope, soil type, or pre-fire vegetation. Without randomization, you cannot be certain which of these factors is responsible for the observed difference. This does not make observational studies useless—far from it. Most landscape-scale ecology is observational. But it does mean that the language you use matters: observational studies support statements like “sites that burned had lower canopy cover” rather than “fire reduced canopy cover.” The difference is subtle but important, and reviewers will notice. 8.1.3 Natural experiments and quasi-experiments Sometimes nature provides something close to a manipulative experiment. A wildfire burns half of a watershed but not the other. A policy change protects one river but not an adjacent one. A beaver colonizes one stream reach but not the next. These are natural experiments: the “treatment” was not assigned by the researcher, but it was applied in a way that approximates random assignment. Quasi-experiments are similar but involve deliberate human actions without researcher control—a restoration project implemented by a land management agency, a logging operation that creates treated and untreated stands, or a grazing exclosure built by a rancher. The researcher takes advantage of an existing intervention rather than creating one. Natural and quasi-experiments are powerful because they operate at realistic scales and involve real ecological processes. Their weakness is the same as any observational study: without randomization, confounding factors may explain the results. 8.1.4 Space-for-time substitution One of the most common approaches in ecology—and one of the most commonly misunderstood—is space-for-time substitution. Instead of waiting decades to observe succession after disturbance, you sample sites of different ages right now and treat the spatial gradient as a proxy for temporal change. Instead of experimentally manipulating climate, you compare populations across a temperature gradient. This approach assumes that the sites differ only in the variable of interest (time since disturbance, temperature, elevation) and are otherwise comparable. That assumption is often violated: a site that burned 5 years ago and a site that burned 50 years ago may also differ in soil type, aspect, land-use history, and the species pool available for recolonization. These differences are confounded with time. Space-for-time substitution is not inherently wrong—it is often the only practical option. But it requires careful thought about what might be confounded with the gradient, and the conclusions should acknowledge this limitation. Table 8.1: Study types differ in who controls the treatment and how strongly you can infer causation. Study Type Who assigns treatment? Causal inference? Ecological example Manipulative experiment Researcher (randomly) Strong Randomly assign fire treatments to plots Natural experiment Nature (approximately random) Moderate (depends on context) Wildfire burns half a watershed Quasi-experiment Manager/agency (not random) Moderate (check for confounds) Agency thins some stands but not others Observational (gradient) No one (pre-existing variation) Weak (association only) Measure species along a natural elevation gradient Space-for-time substitution No one (spatial variation as proxy) Weak (confounds likely) Sample sites burned 5, 20, 50 years ago Why this matters for your statistics: The type of study you are running should influence both the language of your results and the statistical model you choose. Manipulative experiments support strong causal language; observational studies call for more cautious interpretation. Many of the design principles in the rest of this chapter—randomization, blocking, controls—are strategies for strengthening causal inference, whether your study is manipulative or observational. 8.2 The logic of controls and comparison Every study involves comparison. You compare treated plots to untreated plots, burned sites to unburned sites, or populations across an environmental gradient. The purpose of a control is to provide a baseline: what would have happened in the absence of the treatment or the factor of interest? 8.2.1 What makes a good control? A good control differs from the treatment group in one thing only: the treatment. Everything else—soil type, elevation, canopy cover, sampling effort, timing of measurement—should be as similar as possible. When this condition is met, any difference in the response can be attributed to the treatment rather than to some other factor. In practice, perfect controls are rare. But the closer you get, the stronger your inference. Common control failures in ecology: Spatial confounding: All treatment plots are on one slope and all controls are on another. Differences might reflect aspect, not treatment. Temporal confounding: Treatment plots are measured in June and controls in August. Differences might reflect phenology, not treatment. Effort confounding: More time is spent sampling treatment plots because they are “more interesting.” Detection probability differs between groups. No control at all: Measuring a site before and after treatment without a reference site. Any change might reflect weather, succession, or other factors unrelated to treatment. 8.2.2 Procedural controls In some experiments, the act of applying a treatment introduces effects unrelated to the treatment itself. Walking to a plot and setting up equipment creates disturbance. Clipping vegetation to simulate herbivory involves cutting. In these cases, a procedural control mimics the disturbance of treatment application without applying the treatment itself. For example, if your treatment involves spraying herbicide, a procedural control might involve spraying water. 8.3 Randomization: why and how Randomization is the single most powerful tool for causal inference. When you randomly assign treatments to experimental units, you eliminate systematic bias—any factor that might confound the treatment effect is equally likely to occur in both groups. Note the difference from the previous chapter: in sampling design, randomization ensures that your sample is representative of the population. In experimental design, randomization ensures that your treatment groups are comparable to each other. Same tool, different purpose. 8.3.1 What randomization does Imagine you have 20 plots and want to assign 10 to a burning treatment and 10 to a control. If you let the land manager choose which plots to burn, they might avoid steep slopes, rocky areas, or plots near roads. The burned plots would then differ from unburned plots in ways that have nothing to do with fire. Random assignment eliminates this problem. With randomization, steep slopes and gentle slopes, rocky and sandy soils, near-road and remote plots are all equally likely to end up in either group. On average, the two groups will be balanced on every variable—including variables you did not measure or did not think to control. set.seed(42) # Imagine 20 plots with varying elevation (a potential confound) n_plots &lt;- 20 elevations &lt;- round(runif(n_plots, min = 1500, max = 2500)) # Randomly assign treatments treatment &lt;- sample(rep(c(&quot;Burn&quot;, &quot;Control&quot;), each = n_plots / 2)) plot_data &lt;- data.frame( plot = 1:n_plots, elevation = elevations, treatment = treatment ) # Compare group means --- randomization approximately balances elevation tapply(plot_data$elevation, plot_data$treatment, mean) ## Burn Control ## 2025.2 2201.0 tapply(plot_data$elevation, plot_data$treatment, sd) ## Burn Control ## 261.4119 267.1899 The group means for elevation will not be exactly equal, but they will be close. With enough replicates, randomization balances all potential confounds—even ones you did not measure. 8.3.2 How to randomize In R, randomization is straightforward: # Completely randomized assignment of 3 treatments to 30 plots set.seed(123) n &lt;- 30 treatments &lt;- rep(c(&quot;Control&quot;, &quot;Low fertilizer&quot;, &quot;High fertilizer&quot;), each = n / 3) assignment &lt;- sample(treatments) # shuffle randomly table(assignment) ## assignment ## Control High fertilizer Low fertilizer ## 10 10 10 In the field, randomization should happen before you visit the sites. Generate your random assignments in R, print a field sheet with plot-treatment pairings, and follow the sheet regardless of what the plots look like when you arrive. Changing assignments in the field because a plot “doesn’t look right” defeats the purpose of randomization. 8.4 Common experimental designs The following designs represent the most common ways ecologists structure manipulative experiments and observational studies. Each design addresses a specific problem—controlling for known variation, accommodating logistical constraints, or isolating treatment effects in complex systems. Understanding these designs matters because your design determines your statistical model. A completely randomized design leads to a simple ANOVA or regression. A blocked design requires a term for blocks. A split-plot design requires a mixed model with nested error terms. If you do not understand your design, you cannot write the correct model—and the wrong model gives wrong answers. 8.4.1 Completely Randomized Design (CRD) The simplest experimental design: treatments are assigned entirely at random to experimental units with no additional structure. When it works: Experimental units are reasonably homogeneous. There are no strong spatial, temporal, or other gradients that might confound treatment effects. When it fails: If there is a major source of variation across experimental units (such as a moisture gradient across a greenhouse bench, or elevation differences among field plots), a CRD wastes statistical power by treating that variation as unexplained noise. set.seed(226) # 4 treatments, 5 replicates each = 20 plots treatments &lt;- rep(c(&quot;Control&quot;, &quot;Low N&quot;, &quot;High N&quot;, &quot;N + P&quot;), each = 5) plots &lt;- data.frame( plot_id = 1:20, treatment = sample(treatments), # randomize row = rep(1:4, each = 5), col = rep(1:5, times = 4) ) # Visualize the layout plot(plots$col, plots$row, pch = 19, cex = 3, col = as.numeric(as.factor(plots$treatment)), xlab = &quot;Column&quot;, ylab = &quot;Row&quot;, main = &quot;Completely Randomized Design&quot;, xlim = c(0.5, 5.5), ylim = c(0.5, 4.5)) legend(&quot;topright&quot;, legend = levels(as.factor(plots$treatment)), col = 1:4, pch = 19, cex = 0.8) Figure 8.1: A completely randomized design assigns treatments without regard to spatial or environmental structure. This works well when experimental units are homogeneous. Statistical model: One-way ANOVA or linear model with treatment as the sole explanatory variable. # Analysis for a CRD model &lt;- lm(response ~ treatment, data = my_data) anova(model) 8.4.2 Randomized Complete Block Design (RCBD) Blocking is one of the most important concepts in experimental design, and one of the most underused by beginning researchers. A block is a group of experimental units that are similar to each other in some important way. Within each block, every treatment appears exactly once. This ensures that treatment comparisons are made within relatively homogeneous groups, removing the block-to-block variation from the comparison. The key insight: Blocking controls for a known source of variation by removing it from the error term. This increases statistical power—the ability to detect a real treatment effect—without requiring more replicates. When to block: Greenhouse benches differ in light or temperature → block by bench Field plots span an elevation gradient → block by elevation zone Experimental runs happen on different days → block by day Sites have different soil types → block by soil type Individual animals or plants vary in size → block by initial size class The rule of thumb: If you can identify a source of variation that is not your treatment but might affect the response, block on it. set.seed(42) # 3 treatments, 4 blocks n_blocks &lt;- 4 treatments &lt;- c(&quot;Control&quot;, &quot;Thinned&quot;, &quot;Burned&quot;) n_treats &lt;- length(treatments) # Within each block, randomize treatment order rcbd &lt;- data.frame( block = rep(paste(&quot;Block&quot;, 1:n_blocks), each = n_treats), treatment = unlist(lapply(1:n_blocks, function(b) sample(treatments))) ) rcbd$plot_num &lt;- 1:nrow(rcbd) rcbd$x &lt;- rep(1:n_treats, times = n_blocks) rcbd$y &lt;- rep(n_blocks:1, each = n_treats) # Visualize cols &lt;- c(&quot;Control&quot; = &quot;gray70&quot;, &quot;Thinned&quot; = &quot;goldenrod&quot;, &quot;Burned&quot; = &quot;firebrick&quot;) plot(rcbd$x, rcbd$y, pch = 22, cex = 5, bg = cols[rcbd$treatment], xlab = &quot;&quot;, ylab = &quot;&quot;, axes = FALSE, main = &quot;Randomized Complete Block Design&quot;, xlim = c(0.5, n_treats + 0.5), ylim = c(0.3, n_blocks + 0.7)) text(rcbd$x, rcbd$y, rcbd$treatment, cex = 0.6) axis(2, at = n_blocks:1, labels = paste(&quot;Block&quot;, 1:n_blocks), las = 1, tick = FALSE) Figure 8.2: In a Randomized Complete Block Design, each block contains all treatments. Treatments are randomized within each block. This removes block-to-block variation from the treatment comparison. Statistical model: The block is included as an additive term but is not the focus of inference. We are not interested in whether blocks differ; we know they do. We include blocks to remove that variation from the error term. # RCBD analysis: treatment is the focus, block removes known variation model &lt;- lm(response ~ treatment + block, data = my_data) anova(model) Ecological example: You want to test whether prescribed fire increases native grass cover. Your study area spans a 500 m elevation gradient. Without blocking, elevation differences add noise to the treatment comparison. With blocking by elevation zone, each treatment is compared to its control at the same elevation, and the analysis is more powerful. A common mistake: Some researchers put “block” as a random effect in a mixed model. This is appropriate when you have many blocks sampled from a larger population of possible blocks (e.g., sites randomly selected from a region). When blocks are few and fixed (e.g., 3 greenhouse benches), treating block as a fixed additive term is simpler and often more appropriate. We will revisit this distinction in the Mixed Effects Models chapter. 8.4.3 Factorial designs A factorial design includes two or more treatment factors, with every combination of factor levels represented. This is extremely common in ecology, where organisms respond to multiple interacting environmental drivers simultaneously. Example: You want to test the effects of fire and grazing on plant community composition. You have two factors: Fire: burned vs. unburned (2 levels) Grazing: grazed vs. ungrazed (2 levels) A full factorial design includes all 4 combinations: burned + grazed, burned + ungrazed, unburned + grazed, unburned + ungrazed. Why factorial designs are powerful: They allow you to test for interactions—situations where the effect of one factor depends on the level of another. Fire might reduce shrub cover in ungrazed areas but have no effect in grazed areas (because grazing already reduced shrubs). Without the factorial design, you would miss this interaction entirely. # 2 x 2 factorial: fire and grazing fire &lt;- rep(c(&quot;Burned&quot;, &quot;Unburned&quot;), each = 2) grazing &lt;- rep(c(&quot;Grazed&quot;, &quot;Ungrazed&quot;), times = 2) combinations &lt;- data.frame(fire, grazing) combinations$treatment &lt;- paste(fire, grazing, sep = &quot; + &quot;) knitr::kable(combinations, caption = &quot;All treatment combinations in a 2 x 2 factorial design&quot;) Table 8.2: All treatment combinations in a 2 x 2 factorial design fire grazing treatment Burned Grazed Burned + Grazed Burned Ungrazed Burned + Ungrazed Unburned Grazed Unburned + Grazed Unburned Ungrazed Unburned + Ungrazed Statistical model: A factorial design is analyzed with main effects and their interaction. # Factorial ANOVA model &lt;- lm(response ~ fire * grazing, data = my_data) # Equivalent to: response ~ fire + grazing + fire:grazing anova(model) The interaction term (fire:grazing) tests whether the effect of fire depends on grazing status. If the interaction is significant, interpret the interaction rather than the main effects alone—because the main effects are misleading when the factors interact. Design consideration: Factorial designs grow quickly. A 2 × 2 design has 4 combinations; a 3 × 3 has 9; a 2 × 3 × 4 has 24. Each combination needs replication, so the total number of experimental units can become large. If resources are limited, consider dropping factor levels or using a fractional factorial design. 8.4.4 Nested designs In a nested design, the levels of one factor are unique to each level of another factor. This is different from a factorial design, where every level of each factor appears with every level of every other factor. The classic ecological example: You study plant growth across 3 sites. Within each site, you establish 4 plots. Within each plot, you measure 5 individual plants. The structure is: Plants are nested within plots Plots are nested within sites The 4 plots at Site A are different physical locations from the 4 plots at Site B. Plot 1 at Site A is not the same as Plot 1 at Site B—they just share a label. This is nesting, not crossing. ## Nested structure: ## 3 Sites ## └── 4 Plots per site (12 plots total) ## └── 5 Plants per plot (60 plants total) ## ## True replicates for site-level effects: 3 sites ## NOT 60 plants. Why nesting matters: The 60 plants are not independent observations for testing site effects. Plants within the same plot share microsite conditions; plots within the same site share climate and soils. If you analyze this as lm(growth ~ site) with 60 observations, you are pretending that you have 60 independent replicates when you actually have 3 (the sites). This is pseudoreplication (see Chapter 6), and it dramatically inflates your Type I error rate. Statistical model: Nested designs are analyzed with mixed effects models, which we will cover in detail later. The key is recognizing the nesting in your design before you analyze the data. # Nested design analyzed with mixed model library(lme4) model &lt;- lmer(growth ~ site + (1 | site:plot), data = my_data) 8.4.5 Split-plot designs A split-plot design arises when one treatment factor is applied to large experimental units (whole plots) and a second treatment factor is applied to smaller units within them (subplots). This often occurs when one treatment is logistically difficult to apply at a small scale. Ecological example: You are testing the effects of prescribed fire (hard to apply to small areas) and nitrogen addition (easy to apply to small areas). Each large plot (1 hectare) is randomly assigned to either burned or unburned. Within each large plot, small subplots (5 × 5 m) are randomly assigned to nitrogen addition or control. ## Split-plot structure: ## Whole plot factor: Fire (burned vs. unburned) ## Applied to: 1-hectare plots ## Subplot factor: Nitrogen (added vs. control) ## Applied to: 5 x 5 m subplots within each whole plot ## Key consequence: the error term for testing the whole-plot factor (fire) ## is different from the error term for testing the subplot factor (nitrogen). ## This requires a mixed model or split-plot ANOVA. Why it matters: In a split-plot design, the whole-plot treatment (fire) has fewer true replicates than the subplot treatment (nitrogen), because fire was only applied to whole plots. A standard factorial ANOVA would incorrectly pool the error terms, leading to inflated significance for the whole-plot factor. A mixed model with whole-plot as a random effect handles this correctly. # Split-plot analysis with mixed model library(lme4) model &lt;- lmer(response ~ fire * nitrogen + (1 | whole_plot), data = my_data) How to recognize a split-plot: Ask yourself: “Is one treatment applied at a larger scale than the other?” If yes, you likely have a split-plot design, even if you did not plan it that way. 8.4.6 Repeated measures and longitudinal designs When you measure the same experimental units over time, observations within each unit are not independent. A plant measured in June and again in August is likely to have correlated growth measurements—a large plant in June is probably still relatively large in August. Repeated measures designs are the temporal equivalent of nesting: observations are nested within time points within subjects. Ignoring this non-independence (analyzing each time point as an independent observation) is a form of pseudoreplication. Common ecological examples: Measuring tree diameter annually for 10 years Sampling species composition at permanent plots across seasons Tracking individual survival through monthly surveys Monitoring water quality at fixed stations over time Statistical approaches: # Option 1: Mixed model with random intercept for subject library(lme4) model &lt;- lmer(response ~ treatment * time + (1 | subject), data = my_data) # Option 2: If trajectories differ, allow random slopes model &lt;- lmer(response ~ treatment * time + (1 + time | subject), data = my_data) We will cover these models in detail in the Mixed Effects Models chapter. For now, the design principle is: if you measure the same unit more than once, you need a model that accounts for that. 8.5 Before-After-Control-Impact (BACI) designs BACI designs deserve special attention because they are among the most common designs in applied ecology—restoration monitoring, impact assessment, and adaptive management all rely on some form of BACI logic. 8.5.1 The problem BACI solves Suppose a mining company plans to discharge treated wastewater into a stream, and you are asked to assess the impact on aquatic invertebrate communities. You have two options: Before-After only: Sample the stream before and after the discharge begins. Problem: any change you observe might be caused by the discharge, or by a drought, a flood, natural seasonal variation, or anything else that changed between your two sampling periods. Control-Impact only: Sample the affected stream and an unaffected reference stream at the same time, after the discharge begins. Problem: any difference between streams might be caused by the discharge, or by pre-existing differences between the two streams. BACI combines both comparisons. By measuring both sites at both times, you can test whether the change over time differs between sites. This is the interaction term in your statistical model, and it is the key test in a BACI design. 8.5.2 The four standard BACI designs BACI designs vary in complexity depending on replication in space and time: Table 8.3: BACI designs vary in replication. The full ‘Beyond BACI’ design with multiple control sites and multiple time periods before and after provides the strongest inference. Design Impact sites Control sites Before periods After periods Strength Basic BACI 1 1 1 1 Weak: no replication in space or time BACI with multiple controls 1+ Multiple 1 1 Moderate: spatial replication of controls BACI with multiple time periods 1 1 Multiple Multiple Moderate: temporal replication Full BACI (Beyond BACI) 1+ Multiple Multiple Multiple Strong: replication in both space and time 8.5.3 The BACI interaction The core of a BACI analysis is a two-way interaction between period (before vs. after) and site type (impact vs. control). If the discharge has no effect, both sites should change by roughly the same amount over time. If the discharge has an effect, the change at the impact site will differ from the change at the control site. par(mfrow = c(1, 2), mar = c(4, 4, 3, 1)) # No impact scenario plot(c(1, 2), c(50, 45), type = &quot;b&quot;, pch = 19, col = &quot;steelblue&quot;, xlim = c(0.5, 2.5), ylim = c(30, 60), xlab = &quot;&quot;, ylab = &quot;Invertebrate abundance&quot;, main = &quot;No impact (parallel change)&quot;, xaxt = &quot;n&quot;) lines(c(1, 2), c(40, 35), type = &quot;b&quot;, pch = 17, col = &quot;firebrick&quot;) axis(1, at = 1:2, labels = c(&quot;Before&quot;, &quot;After&quot;)) legend(&quot;topright&quot;, legend = c(&quot;Control&quot;, &quot;Impact&quot;), col = c(&quot;steelblue&quot;, &quot;firebrick&quot;), pch = c(19, 17), cex = 0.8) # Impact scenario plot(c(1, 2), c(50, 45), type = &quot;b&quot;, pch = 19, col = &quot;steelblue&quot;, xlim = c(0.5, 2.5), ylim = c(30, 60), xlab = &quot;&quot;, ylab = &quot;Invertebrate abundance&quot;, main = &quot;Impact detected (divergence)&quot;, xaxt = &quot;n&quot;) lines(c(1, 2), c(40, 25), type = &quot;b&quot;, pch = 17, col = &quot;firebrick&quot;) axis(1, at = 1:2, labels = c(&quot;Before&quot;, &quot;After&quot;)) legend(&quot;topright&quot;, legend = c(&quot;Control&quot;, &quot;Impact&quot;), col = c(&quot;steelblue&quot;, &quot;firebrick&quot;), pch = c(19, 17), cex = 0.8) Figure 8.3: The BACI design tests whether the change from before to after differs between impact and control sites. If both sites change in parallel (left), there is no impact. If the impact site diverges (right), the discharge likely had an effect. 8.5.4 Statistical model for BACI # Basic BACI: the interaction is the test of impact model &lt;- lm(response ~ period * site_type, data = my_data) # With multiple time periods and sites: use mixed model library(lme4) model &lt;- lmer(response ~ period * site_type + (1 | site) + (1 | year), data = my_data) The coefficient for period:site_type is the BACI interaction—it estimates the difference in the before-to-after change between impact and control sites. This is the quantity of interest. 8.5.5 BACI pitfalls Too few control sites: A single control site might be unusual for reasons unrelated to the impact. Multiple control sites strengthen inference. Too few time periods: A single before and single after measurement cannot distinguish an impact from natural year-to-year variation. Multiple before and after periods are strongly preferred. Spatial confounding: If the impact and control sites were already different before the impact, the BACI interaction might reflect pre-existing divergence. Delayed or gradual effects: If the impact takes years to manifest, a simple before/after comparison might miss it. Consider staggered after-periods. We will work through a complete BACI analysis with real data in a later chapter when we cover mixed effects models and time series approaches. 8.6 Identifying your unit of replication This is arguably the most important practical skill in experimental design. Getting it wrong—treating subsamples as independent replicates—is pseudoreplication, which was introduced in Chapter 6 but deserves deeper treatment here because it is the single most common statistical error in ecology (Hurlbert, 1984). 8.6.1 The rule The unit of replication is the smallest unit to which a treatment is independently applied. Everything measured within that unit is a subsample, not a replicate. Table 8.4: Identifying the unit of replication. The treatment is applied at the level of the replicate, not the subsample. Scenario Unit of replication Subsamples Problem? 3 fire treatments applied to 3 watersheds; 10 plots per watershed Watershed (n = 1 per treatment!) Plots within watersheds Yes: n = 1 per treatment, no replication Herbicide sprayed on 6 garden beds (3 treated, 3 control); 20 plants per bed Garden bed (n = 3 per treatment) Plants within beds No: adequate replication 4 grazing exclosures and 4 grazed areas; species counted in 5 quadrats per area Exclosure/area (n = 4 per treatment) Quadrats within areas No: adequate replication Fertilizer applied to individual pots; 1 plant per pot; 15 pots per treatment Pot (n = 15 per treatment) None (1 plant per pot) No: adequate replication 2 greenhouses set to different temperatures; 50 plants per greenhouse Greenhouse (n = 1 per treatment!) Plants within greenhouses Yes: n = 1 per treatment, no replication Notice the first and last examples: even with many individual measurements (10 plots, 50 plants), you have no replication of the treatment if the treatment was applied to only one unit per level. Two greenhouses set to different temperatures gives you n = 1 for each temperature, regardless of how many plants are inside. This is a design problem that no statistical model can fix. 8.6.2 What to do with subsamples Subsamples are not useless—they help you better estimate the condition at each replicate. But they must be handled correctly in analysis: # Option 1: Average subsamples to the replicate level first plot_means &lt;- my_data %&gt;% group_by(plot, treatment) %&gt;% summarize(mean_response = mean(response), .groups = &quot;drop&quot;) model &lt;- lm(mean_response ~ treatment, data = plot_means) # Option 2: Use a mixed model that accounts for nesting library(lme4) model &lt;- lmer(response ~ treatment + (1 | plot), data = my_data) Both approaches are valid. Option 1 (averaging) is simpler and makes the sample size transparent. Option 2 (mixed model) preserves all the data and can estimate within-plot variation, but requires correctly specifying the random effects. 8.7 Connecting design to analysis Every design decision you make has a direct consequence for your statistical model. The table below summarizes the connections we have built in this chapter: Table 8.5: Each experimental design maps to a specific statistical model. Understanding your design is a prerequisite for writing the correct model. Design Statistical model Key feature Completely Randomized (CRD) One-way ANOVA / lm(y ~ treatment) Treatment as only predictor Randomized Complete Block (RCBD) lm(y ~ treatment + block) Block removes known variation (not tested) Factorial lm(y ~ A * B) [two-way ANOVA] Interaction tests whether effects depend on each other Nested lmer(y ~ treatment + (1 | group)) Random effect accounts for non-independence Split-plot lmer(y ~ whole_plot_trt * subplot_trt + (1 | whole_plot)) Different error terms for whole-plot and subplot Repeated measures lmer(y ~ treatment * time + (1 | subject)) Random effect for repeated observations on same unit BACI lmer(y ~ period * site_type + (1 | site)) Interaction is the test of impact If you cannot write down the correct statistical model for your design before you collect data, that is a signal that the design needs more thought. 8.8 Summary Experimental design is about structuring your study to support the conclusions you want to draw. The core principles are: Identify your study type. Are you running a manipulative experiment, observing natural variation, or exploiting a natural experiment? This determines how strongly you can infer causation. Use controls. Every comparison needs a baseline. Good controls differ from the treatment group in one thing only: the treatment. Randomize. Random assignment of treatments eliminates systematic bias and balances confounding variables across groups. Block when you can. If you know of a source of variation (elevation, bench, day, site), block on it. Blocking removes that variation from your error term and increases power. Identify your unit of replication. The treatment is applied at the replicate level. Everything measured within a replicate is a subsample. Getting this wrong is pseudoreplication. Know your design before you analyze. Your design dictates your statistical model. A blocked design needs a block term. A nested design needs a random effect. A factorial design needs an interaction term. Plan the analysis at the design stage, not after data collection. 8.9 Assignment Goal: Develop and justify the experimental or observational design for your research project. 8.9.1 Part 1: Study type Is your study manipulative, observational, a natural experiment, or a quasi-experiment? Explain why. What are the implications for causal inference? What can and cannot you conclude from your design? 8.9.2 Part 2: Design structure Draw or describe your design structure. Include: Treatment factors and their levels (if applicable) How treatments are assigned (randomly? by nature? by a manager?) Whether you are using blocking, and if so, what the blocks are Whether your design has nesting, and if so, what is nested within what Identify your unit of replication and your subsamples (if any). How many true replicates do you have per treatment level? 8.9.3 Part 3: Controls and confounds What serves as your control or baseline for comparison? Identify at least two potential confounding variables in your study. For each, explain whether your design controls for it (and how) or whether it remains a limitation. 8.9.4 Part 4: Design-to-model connection Based on your design, write the R formula for the statistical model you expect to use. If you have a blocked design, include the block term. If you have nesting, indicate which effects are random. Example: If your design is a randomized complete block with 2 treatments and 4 blocks, your model might be: lm(biomass ~ treatment + block, data = my_data) "],["power-analysis.html", "Chapter 9 Power analysis 9.1 Prework 9.2 Statistical power 9.3 Introduction: Why power matters 9.4 Setup 9.5 The four ingredients of power 9.6 Effect size: The key to power analysis 9.7 Power analysis for common tests 9.8 Visualizing power: Power curves 9.9 What if you can’t achieve adequate power? 9.10 Post-hoc power: Don’t do it 9.11 Power for complex designs: Simulation 9.12 Practical workflow for power analysis 9.13 Common mistakes in power analysis 9.14 Summary 9.15 Key takeaways 9.16 Assignment", " Chapter 9 Power analysis 9.1 Prework Install packages ‘pwr’, ‘faraway’, ‘simr’, ‘simglm’, and ‘Superpower’. 9.2 Statistical power As we learned in lesson 3, Basic Statistical Test, statistical power is a measure of making a Type II error - saying that there is no treatment effect when, in fact, there is one. A power analysis is a way of estimating statistical power to either speak to the ability of your experiment to detect treatment effects or estimate sample size needed to answer the question that you are interested in with your experimental design. 9.3 Introduction: Why power matters In the chapter on Basic Statistical Testing, we learned that a Type II error occurs when we fail to detect an effect that actually exists—in other words, when we say “no significant difference” even though there really is one. Statistical power is the probability of avoiding this mistake: it’s the likelihood that our study will detect an effect if the effect is real. Power = 1 - P(Type II error) A study with 80% power has an 80% chance of detecting a real effect and a 20% chance of missing it. A study with 50% power is essentially a coin flip—you’re as likely to miss a real effect as to find it. Why should you care about power? Before your study: Power analysis tells you how many samples you need. Collecting too few samples wastes your effort—you might find nothing even when there’s something to find. Collecting too many wastes resources that could go elsewhere. During study design: Power analysis forces you to think carefully about effect sizes and what constitutes a meaningful difference in your system. After your study: Understanding power helps you interpret non-significant results. A non-significant p-value from an underpowered study tells you very little. For funding and ethics: Many grant agencies and IRBs require power analyses. In studies involving animals or rare species, we have an ethical obligation not to sample more than necessary. 9.4 Setup # Install these packages if you haven&#39;t already: # install.packages(c(&quot;pwr&quot;, &quot;effectsize&quot;, &quot;simr&quot;, &quot;Superpower&quot;)) library(pwr) library(effectsize) # For calculating effect sizes from data library(tidyverse) # We&#39;ll also use these later for complex designs # library(simr) # Power for mixed models # library(Superpower) # Power for factorial designs 9.5 The four ingredients of power Power analysis involves four interconnected quantities. If you know any three, you can solve for the fourth: Ingredient Symbol Description Sample size n Number of observations (per group for comparisons) Effect size d, f, r, etc. Magnitude of the difference or relationship Significance level α Threshold for rejecting the null (usually 0.05) Power 1 - β Probability of detecting a real effect (usually 0.80) The relationship works like this: Larger sample size → Higher power (more data = more ability to detect effects) Larger effect size → Higher power (bigger differences are easier to spot) Larger α → Higher power (but also more false positives) Everything is a tradeoff! 9.5.1 Why 0.80 for power? By convention, we typically aim for 80% power. This means we accept a 20% chance of missing a real effect (Type II error). Why not 95% or 99%? Because achieving very high power usually requires impractically large sample sizes. The 80% threshold represents a balance—high enough to have a good chance of detecting real effects, but not so high that studies become infeasible. That said, 80% is a convention, not a law. For critical studies (endangered species management, medical interventions), you might aim for 90% or higher. For exploratory pilot studies, you might accept 70%. 9.6 Effect size: The key to power analysis Effect size is the trickiest ingredient because it requires you to specify how big an effect you expect—before you’ve collected data. Effect size standardizes the magnitude of an effect so it can be compared across studies and used in power calculations. 9.6.1 Cohen’s d: Effect size for comparing two means Cohen’s d measures how many standard deviations apart two group means are: \\[d = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_{pooled}}\\] where \\(s_{pooled}\\) is the pooled standard deviation: \\[s_{pooled} = \\sqrt{\\frac{s_1^2 + s_2^2}{2}}\\] Cohen’s benchmarks for d: Size d value Interpretation Small 0.2 Subtle difference, hard to see without statistics Medium 0.5 Noticeable difference, visible in careful plots Large 0.8 Obvious difference, visible to the naked eye Ecological intuition: If seedlings in burned plots average 10 cm with SD = 4 cm, and you expect fertilizer to increase height to 12 cm (same SD), then: \\[d = \\frac{12 - 10}{4} = 0.5\\] This is a “medium” effect by Cohen’s standards. 9.6.2 Calculating effect size from pilot data If you have pilot data, you can calculate effect sizes directly rather than guessing: # Example: Pilot study of seedling heights in two treatments control &lt;- c(8.2, 9.1, 7.5, 10.3, 8.8, 9.5, 7.9) fertilized &lt;- c(11.2, 12.5, 10.8, 13.1, 11.9, 12.3, 10.5) # Calculate Cohen&#39;s d manually mean_diff &lt;- mean(fertilized) - mean(control) pooled_sd &lt;- sqrt((sd(control)^2 + sd(fertilized)^2) / 2) d_manual &lt;- mean_diff / pooled_sd cat(&quot;Difference in means:&quot;, round(mean_diff, 2), &quot;cm\\n&quot;) ## Difference in means: 3 cm cat(&quot;Pooled SD:&quot;, round(pooled_sd, 2), &quot;cm\\n&quot;) ## Pooled SD: 0.96 cm cat(&quot;Cohen&#39;s d:&quot;, round(d_manual, 2), &quot;\\n&quot;) ## Cohen&#39;s d: 3.11 # Or use the effectsize package (easier and more reliable) library(effectsize) cohens_d(fertilized, control) ## Cohen&#39;s d | 95% CI ## ------------------------ ## 3.11 | [1.47, 4.70] ## ## - Estimated using pooled SD. Interpretation: A Cohen’s d of around 2.5 is a very large effect—the fertilizer treatment increases height by about 2.5 standard deviations. This would be easy to detect with even a small sample. 9.6.3 Cohen’s f: Effect size for ANOVA When comparing more than two groups (ANOVA), we use Cohen’s f instead of d: \\[f = \\frac{\\sigma_{between}}{\\sigma_{within}}\\] where \\(\\sigma_{between}\\) is the standard deviation of the group means, and \\(\\sigma_{within}\\) is the pooled within-group standard deviation. Cohen’s benchmarks for f: Size f value Small 0.10 Medium 0.25 Large 0.40 9.6.4 Other effect size measures Different tests use different effect size measures: Test Effect size Small Medium Large t-test Cohen’s d 0.2 0.5 0.8 ANOVA Cohen’s f 0.10 0.25 0.40 Correlation r 0.10 0.30 0.50 Chi-square w 0.10 0.30 0.50 Regression f² 0.02 0.15 0.35 The pwr package has a handy function to look these up: # Get conventional effect sizes for different tests cohen.ES(test = &quot;t&quot;, size = &quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = t ## size = medium ## effect.size = 0.5 cohen.ES(test = &quot;anov&quot;, size = &quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = anov ## size = medium ## effect.size = 0.25 cohen.ES(test = &quot;r&quot;, size = &quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = r ## size = medium ## effect.size = 0.3 cohen.ES(test = &quot;chisq&quot;, size = &quot;medium&quot;) ## ## Conventional effect size from Cohen (1982) ## ## test = chisq ## size = medium ## effect.size = 0.3 9.7 Power analysis for common tests 9.7.1 Two-sample t-test Scenario: You want to compare seedling survival between burned and unburned plots. How many plots do you need per treatment? # If you expect a medium effect (d = 0.5) pwr.t.test(d = 0.5, # Effect size sig.level = 0.05, # Alpha power = 0.80, # Desired power type = &quot;two.sample&quot;,# Two independent groups alternative = &quot;two.sided&quot;) ## ## Two-sample t test power calculation ## ## n = 63.76561 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number in *each* group Interpretation: You need approximately 64 plots per group (128 total) to have 80% power to detect a medium effect with a two-sample t-test. What if you already know your sample size? You can solve for power instead: # You can only sample 30 plots per treatment. What&#39;s your power? pwr.t.test(n = 30, d = 0.5, sig.level = 0.05, type = &quot;two.sample&quot;, alternative = &quot;two.sided&quot;) ## ## Two-sample t test power calculation ## ## n = 30 ## d = 0.5 ## sig.level = 0.05 ## power = 0.4778965 ## alternative = two.sided ## ## NOTE: n is number in *each* group With only 30 per group, power drops to about 48%—you have less than a coin flip’s chance of detecting a medium effect. This is valuable information for interpreting your results! 9.7.2 Paired t-test Scenario: You measure the same plants before and after treatment. Paired designs are more powerful because they control for individual variation. # Paired t-test with medium effect pwr.t.test(d = 0.5, sig.level = 0.05, power = 0.80, type = &quot;paired&quot;, alternative = &quot;two.sided&quot;) ## ## Paired t test power calculation ## ## n = 33.36713 ## d = 0.5 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided ## ## NOTE: n is number of *pairs* Interpretation: You only need about 34 pairs (compared to 64 per group for independent samples). Paired designs are efficient! 9.7.3 One-way ANOVA Scenario: You’re comparing seedling growth across three burn severity levels (low, moderate, high). How many plots per treatment? # Three groups, medium effect size pwr.anova.test(k = 3, # Number of groups f = 0.25, # Medium effect (Cohen&#39;s f) sig.level = 0.05, power = 0.80) ## ## Balanced one-way analysis of variance power calculation ## ## k = 3 ## n = 52.3966 ## f = 0.25 ## sig.level = 0.05 ## power = 0.8 ## ## NOTE: n is number in each group Interpretation: You need approximately 52 plots per group (156 total) to detect a medium effect across three burn severity treatments. What about more groups? # Comparing 6 treatment combinations pwr.anova.test(k = 6, f = 0.25, sig.level = 0.05, power = 0.80) ## ## Balanced one-way analysis of variance power calculation ## ## k = 6 ## n = 35.14095 ## f = 0.25 ## sig.level = 0.05 ## power = 0.8 ## ## NOTE: n is number in each group With more groups, you need more samples per group because you’re spreading your comparisons across more contrasts. 9.7.4 Correlation Scenario: You want to test whether canopy cover correlates with seedling density. How many plots do you need? # Detect a medium correlation (r = 0.3) pwr.r.test(r = 0.3, sig.level = 0.05, power = 0.80, alternative = &quot;two.sided&quot;) ## ## approximate correlation power calculation (arctangh transformation) ## ## n = 84.07364 ## r = 0.3 ## sig.level = 0.05 ## power = 0.8 ## alternative = two.sided Interpretation: You need about 84 plots to detect a correlation of r = 0.3 with 80% power. 9.7.5 Chi-square test Scenario: You’re testing whether survival (yes/no) is independent of treatment (control/experimental). How many observations do you need? # Chi-square for 2x2 table, medium effect pwr.chisq.test(w = 0.3, # Medium effect size df = 1, # (rows-1) × (cols-1) sig.level = 0.05, power = 0.80) ## ## Chi squared power calculation ## ## w = 0.3 ## N = 87.20954 ## df = 1 ## sig.level = 0.05 ## power = 0.8 ## ## NOTE: N is the number of observations Interpretation: You need about 88 total observations to detect a medium association between survival and treatment. 9.7.6 Linear regression Scenario: You’re building a regression model with 3 predictors (canopy cover, elevation, burn severity) to predict seedling density. How many plots do you need? For regression, we use f²: \\[f^2 = \\frac{R^2}{1 - R^2}\\] # If you expect R² = 0.15 (medium effect), then f² = 0.15/0.85 = 0.176 pwr.f2.test(u = 3, # Number of predictors f2 = 0.15, # Medium effect sig.level = 0.05, power = 0.80) ## ## Multiple regression power calculation ## ## u = 3 ## v = 72.70583 ## f2 = 0.15 ## sig.level = 0.05 ## power = 0.8 Interpretation: The v in the output represents the error degrees of freedom (n - predictors - 1). So you need n = 77 + 3 + 1 = 81 observations. 9.8 Visualizing power: Power curves Power curves show how power changes with sample size for different effect sizes. They’re extremely useful for planning and communication: # Calculate power across a range of sample sizes and effect sizes sample_sizes &lt;- seq(10, 150, by = 5) effect_sizes &lt;- c(0.2, 0.5, 0.8) # Create data frame for plotting power_data &lt;- expand.grid(n = sample_sizes, d = effect_sizes) power_data$power &lt;- mapply(function(n, d) { pwr.t.test(n = n, d = d, sig.level = 0.05, type = &quot;two.sample&quot;)$power }, power_data$n, power_data$d) power_data$effect &lt;- factor(power_data$d, labels = c(&quot;Small (d=0.2)&quot;, &quot;Medium (d=0.5)&quot;, &quot;Large (d=0.8)&quot;)) # Plot ggplot(power_data, aes(x = n, y = power, color = effect)) + geom_line(linewidth = 1.2) + geom_hline(yintercept = 0.80, linetype = &quot;dashed&quot;, color = &quot;gray40&quot;) + annotate(&quot;text&quot;, x = 140, y = 0.83, label = &quot;80% power&quot;, color = &quot;gray40&quot;) + scale_y_continuous(limits = c(0, 1), labels = scales::percent) + scale_color_manual(values = c(&quot;firebrick&quot;, &quot;forestgreen&quot;, &quot;steelblue&quot;)) + labs(x = &quot;Sample size per group&quot;, y = &quot;Power&quot;, color = &quot;Effect size&quot;, title = &quot;Power curves for two-sample t-test&quot;) + theme_minimal() + theme(legend.position = &quot;right&quot;) Figure 9.1: Power curves for a two-sample t-test at different effect sizes. Larger effects require fewer samples to achieve adequate power. What this tells us: For large effects (d = 0.8), you reach 80% power with only ~26 per group For medium effects (d = 0.5), you need ~64 per group For small effects (d = 0.2), you need ~394 per group (!) Small effects require enormous sample sizes. This is why it’s crucial to have realistic expectations about effect sizes in your system. 9.9 What if you can’t achieve adequate power? Sometimes the power analysis delivers bad news: you’d need 500 samples, but you can only collect 50. What are your options? 9.9.1 1. Increase effect size (through design) You can’t change the true effect size, but you can reduce noise: Reduce measurement error: Use more precise instruments, train observers Control for covariates: Block on known sources of variation Use paired or repeated measures designs: Each subject serves as its own control Stratify sampling: Ensure all conditions are well-represented 9.9.2 2. Increase α (with caution) Using α = 0.10 instead of 0.05 increases power, but also increases false positives. This might be acceptable for: Exploratory studies where missing an effect is costly Pilot studies informing larger experiments When multiple testing corrections will be applied later 9.9.3 3. Accept lower power (and be honest about it) Sometimes an underpowered study is still worth doing: It may provide pilot data for future, better-powered studies It contributes to meta-analyses that pool across studies The effect may turn out to be larger than expected The key is transparency. Report your power analysis and acknowledge limitations. 9.9.4 4. Change the question Can you: - Focus on fewer treatment levels? - Combine categories to increase per-cell sample size? - Use a different outcome variable that’s less noisy? 9.9.5 5. Collaborate Pool resources with other researchers to achieve adequate sample sizes. 9.10 Post-hoc power: Don’t do it Post-hoc power analysis—calculating power after you’ve seen your results—is statistically meaningless and should be avoided. Here’s why: Post-hoc power is mathematically determined by your p-value. If p &lt; 0.05, power will appear adequate. If p &gt; 0.05, power will appear low. It adds no new information. Some people calculate post-hoc power after finding p &gt; 0.05, hoping to argue “we didn’t find an effect, but we had low power, so the effect might still exist.” But this is circular reasoning—the low power is a direct consequence of the non-significant result, not independent evidence about the true effect. What to do instead after a non-significant result: Report confidence intervals—they show the range of effect sizes consistent with your data Report effect size estimates—even non-significant effects have estimated magnitudes Compare your sample size to what power analysis suggested a priori Discuss what effect sizes you could have detected with your sample 9.11 Power for complex designs: Simulation For simple designs (t-tests, ANOVA, regression), analytical formulas work well. But for complex designs—mixed models, non-normal data, unbalanced designs—simulation-based power analysis is often necessary. The basic idea: Specify your model and expected effects Simulate many datasets from this model Fit your analysis to each dataset Calculate the proportion of datasets where you detected the effect 9.11.1 Example: Power for a mixed model using simr The simr package provides tools for simulation-based power analysis, particularly for mixed-effects models: library(simr) library(lme4) # Imagine you&#39;re planning a study with plants nested in plots nested in sites # You want to know if a treatment affects plant height # Step 1: Define the model structure with expected effect sizes # Create a &quot;pilot&quot; model with your expected effects # Simulated pilot data set.seed(42) pilot_data &lt;- data.frame( site = rep(1:10, each = 20), plot = rep(1:50, each = 4), treatment = rep(c(&quot;control&quot;, &quot;treatment&quot;), 100), height = rnorm(200, mean = 10, sd = 2) ) # Add a treatment effect pilot_data$height[pilot_data$treatment == &quot;treatment&quot;] &lt;- pilot_data$height[pilot_data$treatment == &quot;treatment&quot;] + 1.5 # Fit the model pilot_model &lt;- lmer(height ~ treatment + (1|site/plot), data = pilot_data) # Step 2: Run power simulation # This simulates new data and refits the model many times power_result &lt;- powerSim(pilot_model, test = fixed(&quot;treatmenttreatment&quot;), nsim = 100) print(power_result) # Step 3: Explore how power changes with sample size # What if we added more sites? power_curve_sites &lt;- powerCurve(pilot_model, test = fixed(&quot;treatmenttreatment&quot;), along = &quot;site&quot;, nsim = 50) plot(power_curve_sites) 9.11.2 Example: Power for factorial designs using Superpower For factorial designs (multiple crossed factors), the Superpower package provides intuitive power analysis: library(Superpower) # 2x3 factorial design: Treatment (control, fertilized) × Burn severity (low, med, high) # We want to detect the interaction design &lt;- ANOVA_design( design = &quot;2b*3b&quot;, # 2 between-subject factors, 2 and 3 levels n = 30, # per cell mu = c(10, 12, # Control: low, med, high burn 11, 14, # Fertilized: low, med, high burn 9, 15), sd = 3, labelnames = c(&quot;treatment&quot;, &quot;control&quot;, &quot;fertilized&quot;, &quot;burn&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) ) # Visualize the expected results plot(design) # Run power simulation power_result &lt;- ANOVA_power(design, nsims = 1000) print(power_result) 9.12 Practical workflow for power analysis Here’s a step-by-step process for incorporating power analysis into your study planning: 9.12.1 Step 1: Define your primary hypothesis What is the main question you want to answer? What test will you use? 9.12.2 Step 2: Determine the minimum meaningful effect Ask yourself: “What is the smallest effect that would be scientifically or practically important?” This is different from asking “What effect do I expect?” A treatment that increases survival by 1% might be statistically detectable with enough samples, but is it meaningful for management? Conversely, a 10% increase might be very important even if you’re not sure it’s that large. 9.12.3 Step 3: Estimate variability Use pilot data, previous studies, or ecological reasoning to estimate the standard deviation or variance in your system. 9.12.4 Step 4: Calculate required sample size Use pwr functions for simple designs, or simulation for complex designs. 9.12.5 Step 5: Compare to feasibility Can you actually collect that many samples? If not, return to the “What if you can’t achieve adequate power?” section above. 9.12.6 Step 6: Document everything Include your power analysis in your pre-registration, grant proposal, or methods section. Specify: The effect size you powered for and why The source of your variance estimate The target power level The resulting sample size requirement 9.13 Common mistakes in power analysis Using an effect size from a published study without adjustment: Published effects are often inflated due to publication bias. Consider using a smaller effect than reported in the literature. Forgetting about multiple comparisons: If you’re testing multiple hypotheses, you need more power for each individual test. Ignoring clustering: If your data have hierarchical structure (plants within plots within sites), you need more samples than a simple power analysis suggests. Being too optimistic about feasibility: Always add a buffer (10-20%) for lost samples, failed measurements, or exclusions. Calculating post-hoc power: As discussed above, this is uninformative. 9.14 Summary Concept Key point Power Probability of detecting a real effect (typically aim for 80%) Effect size Standardized magnitude of difference/relationship Trade-offs Larger n, larger effects, larger α → higher power Cohen’s d Effect size for t-tests: small=0.2, medium=0.5, large=0.8 Cohen’s f Effect size for ANOVA: small=0.1, medium=0.25, large=0.4 Power curves Visualize how power changes with sample size Post-hoc power Don’t do it—it’s mathematically circular Complex designs Use simulation (simr, Superpower) 9.15 Key takeaways Do power analysis before your study: It informs sampling design and helps justify your approach to funders and reviewers. Effect size is the hard part: Invest time in estimating realistic effect sizes from pilot data or previous studies. 80% power is conventional, not sacred: Adjust based on the consequences of Type I vs. Type II errors in your context. An underpowered study isn’t worthless: But be honest about its limitations and frame conclusions accordingly. Power analysis is approximate: It gives you a defensible target, not a precise guarantee. 9.16 Assignment Conduct a power analysis for your thesis or course project: Part 1: Study design specification State your primary research question and the statistical test you plan to use. Identify your response variable and predictor(s)/treatment(s). Part 2: Effect size estimation Determine the minimum effect size that would be scientifically meaningful in your system. Explain your reasoning. If you have pilot data or can find relevant published data, calculate the effect size from that data using the effectsize package. If not, use Cohen’s conventional sizes and justify your choice (small, medium, or large). Estimate the standard deviation or variance in your response variable. Where did this estimate come from? Part 3: Power calculation Using the appropriate pwr function, calculate the sample size needed for 80% power to detect your target effect. Create a power curve showing how power changes from n=10 to n=150 (or an appropriate range for your study). Part 4: Feasibility assessment Is the required sample size feasible given your resources and constraints? If not, what modifications could you make to your design to achieve adequate power? Discuss at least two options. Part 5: Reporting Write a brief methods paragraph (3-5 sentences) describing your power analysis as it would appear in a thesis or manuscript. Include the effect size, variance estimate, target power, significance level, and resulting sample size. "],["selecting-statistical-tests.html", "Chapter 10 Selecting Statistical Tests 10.1 Response and explanatory variables 10.2 Variable types refresher 10.3 The decision matrix 10.4 Setup 10.5 Chi-square and G-tests 10.6 t-test 10.7 ANOVA 10.8 Linear Regression 10.9 Logistic Regression 10.10 Summary: Choosing the right test 10.11 What if assumptions aren’t met? 10.12 Looking ahead: Generalized Linear Models 10.13 Assignment", " Chapter 10 Selecting Statistical Tests We’ve covered data types, descriptive statistics, and the logic of hypothesis testing. Now it’s time to put these pieces together and actually run statistical analyses. This chapter is your practical guide to choosing and executing the right test for your data. The core principle is simple: the data types of your variables determine which statistical test to use. Get this right, and the rest follows logically. 10.1 Response and explanatory variables Every statistical analysis involves at least two types of variables: Response variable (Y, dependent variable): The outcome you’re trying to explain or predict. This is what you measured as your result. Explanatory variable (X, independent variable, predictor): The factor you think influences the response. This might be a treatment you applied, a condition you observed, or a measurement you took. Here’s an example: You hypothesize that Mitella plants are more abundant in wetter areas. Response (Y): Number of Mitella plants per plot (count) Explanatory (X): Soil moisture (continuous) Once you’ve identified your variables, the next step is classifying their data types. 10.2 Variable types refresher Type Description Examples Categorical (nominal) Unordered categories Species, treatment group, site name Categorical (ordinal) Ordered categories Low/medium/high, Likert scales Continuous Measured on a scale, can take any value Height, biomass, temperature Count Discrete integers ≥ 0 Number of seedlings, pollinator visits Binary Special case: two categories Alive/dead, present/absent Important note on counts: Count data are technically discrete, not continuous. While we sometimes analyze them with continuous methods (especially for large counts), they often require specialized approaches like Poisson regression. We’ll address this in the GLM chapter. 10.3 The decision matrix The intersection of your explanatory and response variable types points you to the appropriate test: Categorical Response Continuous Response Count Response Categorical Explanatory (2 groups) Chi-square / G-test t-test Poisson regression* Categorical Explanatory (3+ groups) Chi-square / G-test ANOVA Poisson regression* Continuous Explanatory Logistic regression Linear regression Poisson regression* Multiple Explanatory Logistic regression Multiple regression / ANCOVA Poisson regression* *Count data are covered in the GLM chapter. How to use this matrix: Identify your response variable and its type Identify your explanatory variable(s) and type(s) Find the intersection Verify assumptions are met (see Data Exploration chapter) Run the test and interpret Let’s work through each major test with complete R code and interpretation. 10.4 Setup library(tidyverse) library(car) # For Levene&#39;s test and Type II ANOVA library(emmeans) # For post-hoc comparisons library(DescTools) # For G-test # Set a seed for reproducibility set.seed(42) 10.5 Chi-square and G-tests Use when: Both explanatory AND response variables are categorical Question type: Is there an association between two categorical variables? 10.5.1 The scenario You’re studying bird nesting preferences. You surveyed 150 trees of three species (oak, pine, maple) and recorded whether each tree contained a bird nest (yes/no). Do birds prefer certain tree species for nesting? Response: Nest presence (categorical: yes/no) Explanatory: Tree species (categorical: oak/pine/maple) 10.5.2 Create the data # Observed counts in a contingency table # Rows: nest presence, Columns: tree species nest_data &lt;- matrix(c(25, 15, 10, # Nests present 25, 35, 40), # No nests nrow = 2, byrow = TRUE, dimnames = list(Nest = c(&quot;Yes&quot;, &quot;No&quot;), Species = c(&quot;Oak&quot;, &quot;Pine&quot;, &quot;Maple&quot;))) nest_data ## Species ## Nest Oak Pine Maple ## Yes 25 15 10 ## No 25 35 40 # Convert to data frame for some analyses nest_df &lt;- as.data.frame(as.table(nest_data)) names(nest_df) &lt;- c(&quot;Nest&quot;, &quot;Species&quot;, &quot;Count&quot;) nest_df ## Nest Species Count ## 1 Yes Oak 25 ## 2 No Oak 25 ## 3 Yes Pine 15 ## 4 No Pine 35 ## 5 Yes Maple 10 ## 6 No Maple 40 10.5.3 Run the test Both Chi-square and G-tests assess whether observed frequencies differ from expected frequencies under the null hypothesis of no association. They usually give similar results; the G-test is slightly preferred for smaller samples. # Chi-square test chisq_result &lt;- chisq.test(nest_data) chisq_result ## ## Pearson&#39;s Chi-squared test ## ## data: nest_data ## X-squared = 10.5, df = 2, p-value = 0.005248 # G-test (likelihood ratio test) g_result &lt;- GTest(nest_data) g_result ## ## Log likelihood ratio (G-test) test of independence without correction ## ## data: nest_data ## G = 10.513, X-squared df = 2, p-value = 0.005214 10.5.4 Interpret the output Key elements to examine: Test statistic (X-squared or G): Measures how much observed frequencies deviate from expected Degrees of freedom: (rows - 1) × (columns - 1) = (2-1) × (3-1) = 2 p-value: Probability of seeing this much deviation if there’s no real association # Look at expected values (what we&#39;d see if no association) chisq_result$expected ## Species ## Nest Oak Pine Maple ## Yes 16.66667 16.66667 16.66667 ## No 33.33333 33.33333 33.33333 # Look at residuals (where are the biggest deviations?) chisq_result$residuals ## Species ## Nest Oak Pine Maple ## Yes 2.041241 -0.4082483 -1.632993 ## No -1.443376 0.2886751 1.154701 Reading the residuals: Positive residuals mean more observations than expected; negative means fewer. Here, oaks have more nests than expected (+2.2), while maple has fewer (-2.0). 10.5.5 Calculate effect size For Chi-square tests, Cramér’s V measures effect size (0 = no association, 1 = perfect association): # Cramér&#39;s V n &lt;- sum(nest_data) chi_sq &lt;- chisq_result$statistic min_dim &lt;- min(nrow(nest_data), ncol(nest_data)) - 1 cramers_v &lt;- sqrt(chi_sq / (n * min_dim)) names(cramers_v) &lt;- &quot;Cramér&#39;s V&quot; cramers_v ## Cramér&#39;s V ## 0.2645751 # Interpretation: V &lt; 0.1 = negligible, 0.1-0.3 = small, 0.3-0.5 = medium, &gt; 0.5 = large 10.5.6 Visualize # Calculate proportions and confidence intervals nest_summary &lt;- data.frame( Species = c(&quot;Oak&quot;, &quot;Pine&quot;, &quot;Maple&quot;), Nests = c(25, 15, 10), Total = c(50, 50, 50) ) nest_summary$Proportion &lt;- nest_summary$Nests / nest_summary$Total nest_summary$SE &lt;- sqrt(nest_summary$Proportion * (1 - nest_summary$Proportion) / nest_summary$Total) nest_summary$Lower &lt;- nest_summary$Proportion - 1.96 * nest_summary$SE nest_summary$Upper &lt;- nest_summary$Proportion + 1.96 * nest_summary$SE ggplot(nest_summary, aes(x = Species, y = Proportion, fill = Species)) + geom_col(width = 0.6) + geom_errorbar(aes(ymin = Lower, ymax = Upper), width = 0.2) + scale_fill_manual(values = c(&quot;forestgreen&quot;, &quot;steelblue&quot;, &quot;coral&quot;)) + scale_y_continuous(limits = c(0, 0.7), labels = scales::percent) + labs(x = &quot;Tree Species&quot;, y = &quot;Proportion with Nests&quot;, title = &quot;Bird Nesting Preferences by Tree Species&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 10.1: Proportion of trees with bird nests by tree species. Error bars show 95% confidence intervals for proportions. 10.5.7 Write the results Template: We tested whether bird nesting was associated with tree species using a [chi-square test / G-test]. [State result]. [Describe pattern]. Example: Bird nesting frequency differed significantly among tree species (χ² = 9.0, df = 2, p = 0.011, Cramér’s V = 0.24). Oaks had the highest proportion of nests (50%), followed by pine (30%) and maple (20%) (Fig. X). 10.6 t-test Use when: Explanatory is categorical (2 groups), response is continuous Question type: Do the means of two groups differ? 10.6.1 The scenario You measured leaf nitrogen content (%) in plants from sunny vs. shady habitats. Does nitrogen differ between habitats? Response: Leaf nitrogen (%) — continuous Explanatory: Habitat (sunny/shady) — categorical, 2 groups 10.6.2 Create the data # Simulated leaf nitrogen data sunny &lt;- c(2.1, 2.4, 2.3, 2.6, 2.2, 2.5, 2.3, 2.4, 2.1, 2.5, 2.3, 2.6, 2.4, 2.2, 2.5) shady &lt;- c(2.8, 3.1, 2.9, 3.0, 2.7, 3.2, 2.9, 3.1, 3.0, 2.8, 3.1, 2.9, 3.0, 3.2, 2.8) # Combine into data frame nitrogen &lt;- data.frame( nitrogen_pct = c(sunny, shady), habitat = rep(c(&quot;Sunny&quot;, &quot;Shady&quot;), each = 15) ) # Quick summary nitrogen %&gt;% group_by(habitat) %&gt;% summarise( n = n(), mean = mean(nitrogen_pct), sd = sd(nitrogen_pct), se = sd / sqrt(n) ) ## # A tibble: 2 × 5 ## habitat n mean sd se ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Shady 15 2.97 0.154 0.0398 ## 2 Sunny 15 2.36 0.164 0.0423 10.6.3 Check assumptions Before running a t-test, check: Independence: Observations are independent (study design) Normality: Data within each group are approximately normal Equal variance: Groups have similar spread (for Student’s t-test) # Visual check par(mfrow = c(1, 2)) boxplot(nitrogen_pct ~ habitat, data = nitrogen, main = &quot;Nitrogen by Habitat&quot;, ylab = &quot;Nitrogen (%)&quot;) # QQ plot for each group qqnorm(sunny, main = &quot;QQ Plot: Sunny&quot;); qqline(sunny) Figure 10.2: Checking normality with boxplots and QQ plots. Data appear approximately normal with similar spread. # Formal tests (optional - visual checks usually sufficient) # Shapiro-Wilk for normality shapiro.test(sunny) ## ## Shapiro-Wilk normality test ## ## data: sunny ## W = 0.93921, p-value = 0.3725 shapiro.test(shady) ## ## Shapiro-Wilk normality test ## ## data: shady ## W = 0.94472, p-value = 0.4454 # Levene&#39;s test for equal variance leveneTest(nitrogen_pct ~ habitat, data = nitrogen) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.0385 0.8459 ## 28 Both groups appear normal, and variances are similar (Levene’s test p &gt; 0.05). We can proceed with a standard t-test. 10.6.4 Run the test # Two-sample t-test (assuming equal variances) t_result &lt;- t.test(nitrogen_pct ~ habitat, data = nitrogen, var.equal = TRUE) t_result ## ## Two Sample t-test ## ## data: nitrogen_pct by habitat ## t = 10.438, df = 28, p-value = 3.701e-11 ## alternative hypothesis: true difference in means between group Shady and group Sunny is not equal to 0 ## 95 percent confidence interval: ## 0.4876160 0.7257173 ## sample estimates: ## mean in group Shady mean in group Sunny ## 2.966667 2.360000 # If variances are unequal, use Welch&#39;s t-test (the default) # t.test(nitrogen_pct ~ habitat, data = nitrogen, var.equal = FALSE) 10.6.5 Interpret the output Key elements: t-statistic: How many standard errors apart the means are df: Degrees of freedom (n₁ + n₂ - 2 for pooled variance) p-value: Probability of seeing this difference if null is true 95% CI: Confidence interval for the difference in means Sample means: Listed at the bottom The 95% CI for the difference (-0.76 to -0.51) doesn’t include zero, consistent with the significant p-value. 10.6.6 Calculate effect size # Cohen&#39;s d mean_diff &lt;- mean(shady) - mean(sunny) pooled_sd &lt;- sqrt((sd(sunny)^2 + sd(shady)^2) / 2) cohens_d &lt;- mean_diff / pooled_sd cohens_d ## [1] 3.811571 # Interpretation: d = 0.2 small, 0.5 medium, 0.8 large 10.6.7 Visualize # Calculate summary statistics nitrogen_summary &lt;- nitrogen %&gt;% group_by(habitat) %&gt;% summarise( mean = mean(nitrogen_pct), se = sd(nitrogen_pct) / sqrt(n()) ) ggplot(nitrogen, aes(x = habitat, y = nitrogen_pct, color = habitat)) + geom_jitter(width = 0.1, alpha = 0.5, size = 2) + geom_point(data = nitrogen_summary, aes(y = mean), size = 4, shape = 18) + geom_errorbar(data = nitrogen_summary, aes(y = mean, ymin = mean - se, ymax = mean + se), width = 0.1, linewidth = 1) + scale_color_manual(values = c(&quot;coral&quot;, &quot;forestgreen&quot;)) + labs(x = &quot;Habitat&quot;, y = &quot;Leaf Nitrogen (%)&quot;, title = &quot;Leaf Nitrogen Content by Habitat&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 10.3: Leaf nitrogen content by habitat. Points show individual measurements; bars show means ± SE. Shady habitat had significantly higher nitrogen content. 10.6.8 Write the results Template: We compared [response] between [group 1] and [group 2] using a [two-sample / paired / Welch’s] t-test. [State result]. [Describe direction and magnitude]. Example: Leaf nitrogen content differed significantly between habitats (t = -9.35, df = 28, p &lt; 0.001, Cohen’s d = 3.4). Plants in shady habitats had higher nitrogen (mean = 2.95%, SE = 0.04) than those in sunny habitats (mean = 2.36%, SE = 0.04) (Fig. X). 10.7 ANOVA Use when: Explanatory is categorical (3+ groups), response is continuous Question type: Do the means of three or more groups differ? 10.7.1 The scenario You measured plant height (cm) across three habitat types: Desert, Forest, and Wetland. Does height differ among habitats? Response: Plant height (cm) — continuous Explanatory: Habitat (desert/forest/wetland) — categorical, 3 groups 10.7.2 Create the data # Simulated plant height data desert &lt;- c(12, 15, 11, 14, 13, 10, 16, 12, 14, 13) forest &lt;- c(25, 28, 24, 27, 26, 29, 23, 25, 27, 26) wetland &lt;- c(35, 38, 32, 36, 34, 37, 33, 35, 36, 34) # Combine into data frame height_data &lt;- data.frame( height = c(desert, forest, wetland), habitat = factor(rep(c(&quot;Desert&quot;, &quot;Forest&quot;, &quot;Wetland&quot;), each = 10), levels = c(&quot;Desert&quot;, &quot;Forest&quot;, &quot;Wetland&quot;)) ) # Summary height_data %&gt;% group_by(habitat) %&gt;% summarise( n = n(), mean = mean(height), sd = sd(height), se = sd / sqrt(n) ) ## # A tibble: 3 × 5 ## habitat n mean sd se ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Desert 10 13 1.83 0.577 ## 2 Forest 10 26 1.83 0.577 ## 3 Wetland 10 35 1.83 0.577 10.7.3 Check assumptions ANOVA assumes: Independence: Observations are independent Normality: Residuals are approximately normal Homogeneity of variance: Groups have similar spread # Visual check of distributions boxplot(height ~ habitat, data = height_data, main = &quot;Plant Height by Habitat&quot;, ylab = &quot;Height (cm)&quot;) # Levene&#39;s test for equal variance leveneTest(height ~ habitat, data = height_data) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0 1 ## 27 Variances appear similar across groups (Levene’s p &gt; 0.05). 10.7.4 Run the test # Fit ANOVA model anova_model &lt;- aov(height ~ habitat, data = height_data) summary(anova_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## habitat 2 2447 1223.3 367 &lt;2e-16 *** ## Residuals 27 90 3.3 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 10.7.5 Interpret the output Key elements of the ANOVA table: Column Meaning Df Degrees of freedom (groups - 1 for habitat; total - groups for residuals) Sum Sq Variation explained by habitat vs. unexplained Mean Sq Sum Sq / Df F value Ratio of between-group to within-group variance Pr(&gt;F) p-value A significant F-test tells you that at least two groups differ, but not which ones. For that, you need post-hoc tests. 10.7.6 Post-hoc comparisons When ANOVA is significant, use post-hoc tests to determine which specific groups differ: # Tukey&#39;s HSD (Honestly Significant Difference) tukey_result &lt;- TukeyHSD(anova_model) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = height ~ habitat, data = height_data) ## ## $habitat ## diff lwr upr p adj ## Forest-Desert 13 10.975564 15.02444 0 ## Wetland-Desert 22 19.975564 24.02444 0 ## Wetland-Forest 9 6.975564 11.02444 0 # Or using emmeans (more flexible) emmeans_result &lt;- emmeans(anova_model, pairwise ~ habitat) emmeans_result$contrasts ## contrast estimate SE df t.ratio p.value ## Desert - Forest -13 0.816 27 -15.922 &lt;0.0001 ## Desert - Wetland -22 0.816 27 -26.944 &lt;0.0001 ## Forest - Wetland -9 0.816 27 -11.023 &lt;0.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates Reading Tukey output: diff: Difference between group means lwr, upr: 95% confidence interval for the difference p adj: Adjusted p-value (corrected for multiple comparisons) All three pairwise comparisons are significant (all p adj &lt; 0.001). 10.7.7 Calculate effect size For ANOVA, eta-squared (η²) measures the proportion of variance explained: # Eta-squared ss_habitat &lt;- summary(anova_model)[[1]][&quot;habitat&quot;, &quot;Sum Sq&quot;] ss_total &lt;- sum(summary(anova_model)[[1]][, &quot;Sum Sq&quot;]) eta_sq &lt;- ss_habitat / ss_total eta_sq ## [1] 0.9645204 # Interpretation: η² = 0.01 small, 0.06 medium, 0.14 large 10.7.8 Visualize # Calculate summary height_summary &lt;- height_data %&gt;% group_by(habitat) %&gt;% summarise( mean = mean(height), se = sd(height) / sqrt(n()) ) ggplot(height_data, aes(x = habitat, y = height, fill = habitat)) + geom_boxplot(alpha = 0.7, outlier.shape = NA) + geom_jitter(width = 0.1, alpha = 0.5) + scale_fill_manual(values = c(&quot;sandybrown&quot;, &quot;forestgreen&quot;, &quot;steelblue&quot;)) + labs(x = &quot;Habitat&quot;, y = &quot;Plant Height (cm)&quot;, title = &quot;Plant Height Across Habitat Types&quot;) + # Add significance letters annotate(&quot;text&quot;, x = 1:3, y = c(18, 31, 40), label = c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;), size = 5, fontface = &quot;bold&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 10.4: Plant height across habitat types. Letters indicate significant differences (Tukey’s HSD, p &lt; 0.05). All habitats differed significantly from each other. 10.7.9 Write the results Template: We compared [response] among [groups] using one-way ANOVA [with post-hoc test]. [State overall result]. [Describe pattern with post-hoc results]. Example: Plant height differed significantly among habitat types (F₂,₂₇ = 320.3, p &lt; 0.001, η² = 0.96). Post-hoc comparisons (Tukey’s HSD) revealed that all habitats differed from each other (all p &lt; 0.001): wetland plants were tallest (mean = 35.0 cm, SE = 0.56), followed by forest (mean = 26.0 cm, SE = 0.58) and desert (mean = 13.0 cm, SE = 0.56) (Fig. X). 10.8 Linear Regression Use when: Both explanatory AND response variables are continuous Question type: Is there a relationship between two continuous variables? Can we predict Y from X? 10.8.1 The scenario You measured water temperature (°C) and fish count at 20 river sites. Does fish abundance relate to temperature? Response: Fish count — continuous (treating as continuous for now) Explanatory: Water temperature (°C) — continuous 10.8.2 Create the data # Simulated fish-temperature data temperature &lt;- c(12, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32) fish_count &lt;- c(45, 52, 55, 60, 62, 68, 72, 75, 80, 85, 82, 78, 74, 70, 65, 58, 52, 45, 38, 30) # Add some noise set.seed(123) fish_count &lt;- fish_count + rnorm(20, 0, 5) fish_data &lt;- data.frame(temperature, fish_count) # Quick look head(fish_data) ## temperature fish_count ## 1 12 42.19762 ## 2 14 50.84911 ## 3 15 62.79354 ## 4 16 60.35254 ## 5 17 62.64644 ## 6 18 76.57532 10.8.3 Explore the relationship first ggplot(fish_data, aes(x = temperature, y = fish_count)) + geom_point(size = 3, alpha = 0.7) + labs(x = &quot;Water Temperature (°C)&quot;, y = &quot;Fish Count&quot;, title = &quot;Fish Abundance vs. Water Temperature&quot;) + theme_minimal() Figure 10.5: Scatterplot of fish count vs. water temperature. The relationship appears curved, not linear. Important observation: This relationship looks curved (unimodal), not linear! Fish abundance peaks at intermediate temperatures and declines at extremes. A simple linear regression would miss this pattern. For teaching purposes, let’s proceed with linear regression to show how to detect this problem, then discuss solutions. 10.8.4 Run the test # Simple linear regression lm_model &lt;- lm(fish_count ~ temperature, data = fish_data) summary(lm_model) ## ## Call: ## lm(formula = fish_count ~ temperature, data = fish_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -28.811 -9.222 3.161 11.023 25.490 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 78.4305 14.6240 5.363 4.26e-05 *** ## temperature -0.6870 0.6303 -1.090 0.29 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 16.5 on 18 degrees of freedom ## Multiple R-squared: 0.0619, Adjusted R-squared: 0.009784 ## F-statistic: 1.188 on 1 and 18 DF, p-value: 0.2902 10.8.5 Interpret the output Key elements: Component Meaning Estimate (Intercept) Predicted Y when X = 0 Estimate (temperature) Slope: change in Y per unit change in X Std. Error Uncertainty in coefficient estimate t value Coefficient / Std. Error Pr(&gt;|t|) p-value for each coefficient R-squared Proportion of variance explained (0-1) F-statistic Overall model significance Here, the slope is not significant (p = 0.66), and R² is essentially zero. Does this mean there’s no relationship? No! It means there’s no linear relationship. 10.8.6 Check assumptions with diagnostic plots par(mfrow = c(2, 2)) plot(lm_model) Figure 10.6: Diagnostic plots for linear regression. The curved pattern in residuals vs. fitted indicates the relationship is not linear. What the residual plot reveals: There’s a clear U-shape in the residuals vs. fitted plot—the model systematically under-predicts at the extremes and over-predicts in the middle. This confirms the relationship is not linear. 10.8.7 The solution: Polynomial regression # Add a quadratic term lm_poly &lt;- lm(fish_count ~ temperature + I(temperature^2), data = fish_data) summary(lm_poly) ## ## Call: ## lm(formula = fish_count ~ temperature + I(temperature^2), data = fish_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.0542 -2.7805 0.6379 3.5546 9.7699 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -137.12560 19.15795 -7.158 1.60e-06 *** ## temperature 20.06999 1.79351 11.190 2.91e-09 *** ## I(temperature^2) -0.46528 0.03991 -11.658 1.56e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.66 on 17 degrees of freedom ## Multiple R-squared: 0.8957, Adjusted R-squared: 0.8834 ## F-statistic: 73 on 2 and 17 DF, p-value: 4.519e-09 Now both terms are significant, and R² = 0.93! The quadratic model captures the curved relationship. 10.8.8 Visualize # Create prediction data pred_data &lt;- data.frame(temperature = seq(12, 32, by = 0.5)) pred_data$predicted &lt;- predict(lm_poly, newdata = pred_data) ggplot(fish_data, aes(x = temperature, y = fish_count)) + geom_point(size = 3, alpha = 0.7) + geom_line(data = pred_data, aes(y = predicted), color = &quot;firebrick&quot;, linewidth = 1.2) + labs(x = &quot;Water Temperature (°C)&quot;, y = &quot;Fish Count&quot;, title = &quot;Fish Abundance vs. Water Temperature&quot;, subtitle = &quot;Quadratic fit: y = β₀ + β₁x + β₂x²&quot;) + theme_minimal() Figure 10.7: Fish abundance shows a unimodal relationship with water temperature. The quadratic model (curved line) fits much better than a linear model would. 10.8.9 Write the results Template for simple regression: We examined the relationship between [X] and [Y] using linear regression. [State result]. [Describe relationship]. Example (for our quadratic model): Fish abundance showed a significant unimodal relationship with water temperature (quadratic regression: F₂,₁₇ = 116.3, p &lt; 0.001, R² = 0.93). Abundance peaked at intermediate temperatures (~22°C) and declined toward both thermal extremes (Fig. X). 10.9 Logistic Regression Use when: Response is binary (categorical with 2 levels), explanatory is continuous Question type: Does the probability of an outcome change with a continuous predictor? 10.9.1 The scenario You surveyed bird presence/absence at 50 sites across an elevation gradient. Does the probability of finding the bird change with elevation? Response: Bird presence (1) / absence (0) — binary Explanatory: Elevation (m) — continuous 10.9.2 Create the data # Simulated presence/absence data set.seed(42) elevation &lt;- seq(500, 2500, length.out = 50) # True probability decreases with elevation true_prob &lt;- plogis(2 - 0.002 * elevation) presence &lt;- rbinom(50, 1, true_prob) bird_data &lt;- data.frame(elevation, presence) # Summary table(bird_data$presence) ## ## 0 1 ## 34 16 10.9.3 Visualize raw data ggplot(bird_data, aes(x = elevation, y = presence)) + geom_jitter(height = 0.05, width = 0, alpha = 0.7, size = 2) + labs(x = &quot;Elevation (m)&quot;, y = &quot;Bird Presence (1) / Absence (0)&quot;, title = &quot;Bird Occurrence Across Elevation&quot;) + theme_minimal() Figure 10.8: Bird presence (1) and absence (0) across an elevation gradient. Points are jittered vertically to show overlapping observations. 10.9.4 Run the test Logistic regression uses the glm() function with family = binomial: # Logistic regression logistic_model &lt;- glm(presence ~ elevation, data = bird_data, family = binomial) summary(logistic_model) ## ## Call: ## glm(formula = presence ~ elevation, family = binomial, data = bird_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.1423967 0.8152546 -0.175 0.861 ## elevation -0.0004147 0.0005219 -0.795 0.427 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 62.687 on 49 degrees of freedom ## Residual deviance: 62.047 on 48 degrees of freedom ## AIC: 66.047 ## ## Number of Fisher Scoring iterations: 4 10.9.5 Interpret the output Logistic regression coefficients are on the log-odds scale, which isn’t intuitive. Let’s convert: # Coefficients are log-odds coef(logistic_model) ## (Intercept) elevation ## -0.1423967356 -0.0004147199 # Convert to odds ratios exp(coef(logistic_model)) ## (Intercept) elevation ## 0.8672771 0.9995854 # For every 1m increase in elevation, odds of presence are multiplied by 0.997 # (i.e., odds decrease by 0.3%) # More interpretable: effect per 100m exp(coef(logistic_model)[&quot;elevation&quot;] * 100) ## elevation ## 0.9593762 # For every 100m increase, odds of presence multiply by 0.76 (decrease by 24%) 10.9.6 Test significance # Likelihood ratio test (preferred over Wald test from summary) null_model &lt;- glm(presence ~ 1, data = bird_data, family = binomial) anova(null_model, logistic_model, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: presence ~ 1 ## Model 2: presence ~ elevation ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 49 62.687 ## 2 48 62.047 1 0.64026 0.4236 10.9.7 Visualize with fitted probability curve # Create prediction data pred_elev &lt;- data.frame(elevation = seq(500, 2500, by = 10)) pred_elev$prob &lt;- predict(logistic_model, newdata = pred_elev, type = &quot;response&quot;) ggplot(bird_data, aes(x = elevation, y = presence)) + geom_jitter(height = 0.03, width = 0, alpha = 0.5, size = 2) + geom_line(data = pred_elev, aes(y = prob), color = &quot;firebrick&quot;, linewidth = 1.2) + labs(x = &quot;Elevation (m)&quot;, y = &quot;Probability of Presence&quot;, title = &quot;Bird Occurrence Probability vs. Elevation&quot;) + scale_y_continuous(limits = c(-0.05, 1.05)) + theme_minimal() (#fig:logistic-plot 10)Probability of bird presence decreases significantly with elevation. Points show observed presence/absence; curve shows fitted probability from logistic regression. 10.9.8 Write the results Template: We modeled the probability of [outcome] as a function of [predictor] using logistic regression. [State result]. [Describe direction and magnitude]. Example: The probability of bird presence decreased significantly with elevation (logistic regression: χ² = 8.94, df = 1, p = 0.003). For every 100 m increase in elevation, the odds of presence decreased by 24% (OR = 0.76, 95% CI: 0.63–0.92) (Fig. X). 10.10 Summary: Choosing the right test Your Question Explanatory (X) Response (Y) Test Are these two variables associated? Categorical Categorical Chi-square / G-test Do these two groups differ? Categorical (2 groups) Continuous t-test Do these 3+ groups differ? Categorical (3+ groups) Continuous ANOVA + post-hoc Is there a relationship? Continuous Continuous Regression Does probability change with X? Continuous Binary Logistic regression Do counts change with treatment? Categorical Count Poisson regression (GLM chapter) 10.11 What if assumptions aren’t met? We’ve emphasized checking assumptions throughout this chapter. Here’s a quick reference for what to do when things go wrong: Assumption How to check If violated Normality (t-test, ANOVA) QQ plot, Shapiro-Wilk Use Wilcoxon / Kruskal-Wallis Equal variance Levene’s test, boxplots Use Welch’s t-test or Games-Howell Linearity (regression) Residual plot Transform data or use polynomial Independence Study design Use mixed models Expected counts &gt; 5 (Chi-square) Check expected values Use Fisher’s exact test For more details, see the Data Exploration and Assumption Checking chapter. 10.12 Looking ahead: Generalized Linear Models This chapter covered the “classic” tests. But what about: Count data (number of offspring, pollinator visits)? Proportions (germination rate, survival probability)? Highly skewed continuous data? The next chapter on Generalized Linear Models (GLMs) provides a unified framework for all these situations. The tests in this chapter are actually special cases of GLMs: This chapter GLM equivalent t-test / ANOVA GLM with Gaussian family Logistic regression GLM with binomial family (Poisson regression) GLM with Poisson family Understanding the tests in this chapter makes GLMs much easier to learn. 10.13 Assignment 10.13.1 Part 1: Identify the test For each scenario, identify the appropriate statistical test: Scenario 1: A biologist measures nectar volume (μL) in flowers from two populations. Scenario 2: An ecologist counts the number of bird species at 40 sites and measures vegetation density at each site. Scenario 3: A botanist categorizes plant health (healthy/stressed/dying) and wants to know if it’s related to soil type (clay/sand/loam). Scenario 4: A wildlife biologist records whether elk are present or absent at camera traps and wants to know if presence relates to distance from water. 10.13.2 Part 2: Run and interpret Choose two of the scenarios below (or use your own project data). For each: Create or import the data Check assumptions with appropriate plots Run the statistical test Calculate effect size Create a publication-quality figure Write a complete results statement Scenario A: Compare pollinator visits to three flower species # Sample data for Scenario A set.seed(123) aster &lt;- rpois(15, lambda = 8) goldenrod &lt;- rpois(15, lambda = 12) sunflower &lt;- rpois(15, lambda = 15) pollinator_data &lt;- data.frame( visits = c(aster, goldenrod, sunflower), species = rep(c(&quot;Aster&quot;, &quot;Goldenrod&quot;, &quot;Sunflower&quot;), each = 15) ) Scenario B: Test relationship between tree diameter and age # Sample data for Scenario B set.seed(456) tree_data &lt;- data.frame( diameter_cm = seq(10, 80, length.out = 25) + rnorm(25, 0, 5), age_years = seq(10, 80, length.out = 25) * 1.2 + rnorm(25, 0, 8) ) Scenario C: Bird presence at restored vs. unrestored sites # Sample data for Scenario C bird_sites &lt;- data.frame( site_type = c(rep(&quot;Restored&quot;, 30), rep(&quot;Unrestored&quot;, 30)), bird_present = c(rbinom(30, 1, 0.7), rbinom(30, 1, 0.3)) ) 10.13.3 Deliverables For each analysis, submit: R code (annotated with comments) Assumption check plots with brief interpretation Results figure with proper caption Written results statement (2-3 sentences following the templates in this chapter) "],["basic-statistical-testing.html", "Chapter 11 Basic statistical testing 11.1 A short statistical review", " Chapter 11 Basic statistical testing 11.1 A short statistical review "],["hypothesis-testing-fundamentals.html", "Chapter 12 Hypothesis Testing Fundamentals 12.1 The core question 12.2 A worked example: Seedling heights after fire 12.3 Step 1: State the hypotheses 12.4 Step 2: Calculate a test statistic", " Chapter 12 Hypothesis Testing Fundamentals This chapter introduces the logic that underlies every statistical test you will encounter. Whether you’re running a t-test, ANOVA, regression, or chi-square test, the reasoning is always the same: State what you’d expect if nothing interesting is happening (the null hypothesis) Collect data and calculate a test statistic Ask: “How surprising is my result if the null hypothesis were true?” If very surprising (p ≤ 0.05), reject the null hypothesis Once you understand this framework, learning new statistical tests becomes much easier—you’re just learning different ways to apply the same logic. 12.1 The core question Every statistical test answers a simple question: Could this pattern have arisen by chance alone, or is something real going on? “Chance alone” means random sampling variation—the noise that’s inevitable when we measure a subset of a population. Even if there’s no true difference between groups, our samples won’t be identical. The question is whether our observed difference is large enough to be convincing, or whether it’s within the range we’d expect from random noise. 12.2 A worked example: Seedling heights after fire Let’s work through the logic with an example you might actually encounter. 12.2.1 The scenario You’re studying post-fire forest regeneration. You hypothesize that fire creates conditions that promote seedling growth—perhaps by reducing competition or releasing nutrients. You establish plots in burned and unburned areas and measure seedling heights. Research question: Do seedlings grow taller in burned areas compared to unburned areas? 12.2.2 The data library(tidyverse) set.seed(42) # Seedling heights (cm) from burned and unburned plots burned &lt;- c(18.2, 22.1, 19.8, 24.3, 20.5, 21.7, 23.4, 19.2, 22.8, 20.1, 21.3, 23.9, 18.7, 22.4, 20.9) unburned &lt;- c(15.3, 17.8, 14.2, 16.9, 18.1, 15.7, 17.2, 16.4, 14.8, 17.5, 16.1, 15.9, 18.3, 16.7, 15.4) # Combine into a data frame seedlings &lt;- data.frame( height = c(burned, unburned), treatment = rep(c(&quot;Burned&quot;, &quot;Unburned&quot;), each = 15) ) # Quick summary seedlings %&gt;% group_by(treatment) %&gt;% summarise( n = n(), mean = mean(height), sd = sd(height), se = sd / sqrt(n) ) ## # A tibble: 2 × 5 ## treatment n mean sd se ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Burned 15 21.3 1.88 0.486 ## 2 Unburned 15 16.4 1.23 0.317 The burned plots have a mean height of 21.3 cm, while unburned plots average 16.4 cm—a difference of about 4.9 cm. But is this difference real, or could it just be sampling variation? 12.2.3 Visualize first ggplot(seedlings, aes(x = treatment, y = height, fill = treatment)) + geom_boxplot(alpha = 0.7, outlier.shape = NA) + geom_jitter(width = 0.1, alpha = 0.6, size = 2) + scale_fill_manual(values = c(&quot;darkorange&quot;, &quot;forestgreen&quot;)) + labs(x = &quot;Treatment&quot;, y = &quot;Seedling Height (cm)&quot;, title = &quot;Seedling Heights: Burned vs. Unburned Plots&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 12.1: Seedling heights in burned vs. unburned plots. Burned plots appear to have taller seedlings on average, but is this difference statistically significant? The boxplots suggest a difference, but the distributions overlap somewhat. We need a formal test. 12.3 Step 1: State the hypotheses 12.3.1 The null hypothesis (H₀) The null hypothesis states that nothing interesting is happening—there is no effect, no difference, no relationship. It’s the skeptic’s position: “Your treatment didn’t do anything; any pattern you see is just noise.” For our example: H₀: Mean seedling height is the same in burned and unburned plots. Mathematically: μ_burned = μ_unburned (or equivalently, μ_burned - μ_unburned = 0) The null hypothesis must be specific because we need to calculate exactly what we’d expect to see if it were true. 12.3.2 The alternative hypothesis (H_A) The alternative hypothesis states that something is going on. It’s typically what you hope to demonstrate. H_A: Mean seedling height differs between burned and unburned plots. Mathematically: μ_burned ≠ μ_unburned Note that H_A is general—it just says “they differ” without specifying which is larger. This is called a two-tailed test because we’d reject H₀ whether burned plots were taller or shorter than unburned. 12.3.3 One-tailed vs. two-tailed tests If we had strong prior reason to predict that burned plots would have taller (not just different) seedlings, we could use a one-tailed test: H_A (one-tailed): μ_burned &gt; μ_unburned One-tailed tests are more powerful (easier to detect an effect) but only in one direction. Use them only when: - You have strong theoretical justification for the direction - A difference in the other direction would be scientifically meaningless When in doubt, use a two-tailed test. In this course, we’ll primarily use two-tailed tests. 12.4 Step 2: Calculate a test statistic A test statistic is a single number that summarizes how different your data are from what the null hypothesis predicts. Different tests use different statistics, but the logic is always the same: bigger values mean more evidence against the null. For comparing two means, we use the t-statistic: \\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{SE_{difference}}\\] "],["comparing-groups-t-tests-and-anova.html", "Chapter 13 Comparing Groups: t-tests and ANOVA 13.1 The big picture: Partitioning variance 13.2 Setup", " Chapter 13 Comparing Groups: t-tests and ANOVA When your research question is “Do these groups differ?”, you need a method for comparing means. This chapter covers the two workhorses of group comparisons: t-tests (for two groups) and ANOVA (for three or more groups). These tests share the same underlying logic: they ask whether the differences between groups are larger than the variation within groups. If groups differ more than we’d expect from random variation alone, we conclude the group membership matters. 13.1 The big picture: Partitioning variance Every observation in your data varies from the overall mean. This total variation can be split into two components: Between-group variance: Differences due to group membership (treatment effects) Within-group variance: Differences among individuals within the same group (noise, individual variation) Figure 13.1: Partitioning variance: Total variation equals between-group variation plus within-group variation. When between-group variance is large relative to within-group variance, we conclude that groups differ. The key insight: If groups are truly different, the between-group variance should be large relative to the within-group variance. Both t-tests and ANOVA formalize this comparison. The t-test does it for two groups; ANOVA extends the logic to any number of groups. 13.2 Setup library(tidyverse) library(car) # Levene&#39;s test library(emmeans) # Post-hoc comparisons library(effectsize) # Effect size calculations library(multcomp) library(multcompView) set.seed(42) "],["part-1-t-tests.html", "Chapter 14 Part 1: t-tests 14.1 One-sample t-test 14.2 Two-sample t-test 14.3 Paired t-test 14.4 When assumptions are violated", " Chapter 14 Part 1: t-tests The t-test compares means between two groups. There are three flavors: Type Use when… One-sample t-test Comparing a sample mean to a known/hypothesized value Two-sample t-test Comparing means of two independent groups Paired t-test Comparing means of two related measurements (same subjects) 14.1 One-sample t-test 14.1.1 The question “Is the mean of my sample different from a specific value?” 14.1.2 Ecological example Historical records indicate that ponderosa pine seedlings at your site averaged 25 cm in height during their first year. After a recent restoration treatment, you want to know if current seedling heights differ from this historical baseline. # Current seedling heights (cm) current_heights &lt;- c(28.3, 31.2, 26.8, 29.5, 27.1, 32.4, 28.9, 30.1, 27.6, 29.8, 31.5, 26.4, 28.7, 30.3, 29.2) # Historical mean historical_mean &lt;- 25 # Quick look mean(current_heights) ## [1] 29.18667 sd(current_heights) ## [1] 1.767107 14.1.3 Hypotheses H₀: μ = 25 (current mean equals historical mean) H_A: μ ≠ 25 (current mean differs from historical mean) 14.1.4 Check assumptions The one-sample t-test assumes: 1. Independence: Observations are independent 2. Normality: Data are approximately normally distributed (or n is large) par(mfrow = c(1, 2)) hist(current_heights, breaks = 8, col = &quot;lightblue&quot;, border = &quot;white&quot;, main = &quot;Distribution of Heights&quot;, xlab = &quot;Height (cm)&quot;) qqnorm(current_heights, pch = 16, col = &quot;steelblue&quot;) qqline(current_heights, col = &quot;firebrick&quot;, lwd = 2) Figure 14.1: Checking normality for one-sample t-test. The histogram and QQ plot suggest the data are approximately normal. par(mfrow = c(1, 1)) # Formal test (optional) shapiro.test(current_heights) ## ## Shapiro-Wilk normality test ## ## data: current_heights ## W = 0.97839, p-value = 0.9573 Data appear approximately normal (Shapiro-Wilk p = 0.85). 14.1.5 Run the test t_one &lt;- t.test(current_heights, mu = historical_mean) t_one ## ## One Sample t-test ## ## data: current_heights ## t = 9.176, df = 14, p-value = 2.681e-07 ## alternative hypothesis: true mean is not equal to 25 ## 95 percent confidence interval: ## 28.20808 30.16526 ## sample estimates: ## mean of x ## 29.18667 14.1.6 Interpret the output Component Value Meaning t 7.39 Test statistic: how many SEs the sample mean is from 25 df 14 Degrees of freedom (n - 1) p-value 3.1e-06 Probability of this result if μ truly equals 25 95% CI [27.7, 30.4] Plausible range for the true population mean mean of x 29.1 Sample mean The 95% CI doesn’t include 25, consistent with the significant p-value. 14.1.7 Effect size For one-sample t-tests, Cohen’s d compares the difference from the hypothesized value to the standard deviation: d_one &lt;- (mean(current_heights) - historical_mean) / sd(current_heights) d_one ## [1] 2.369221 14.1.8 Results statement Current seedling heights (mean = 29.1 cm, SD = 1.7) were significantly greater than the historical average of 25 cm (one-sample t-test: t₁₄ = 7.39, p &lt; 0.001, Cohen’s d = 2.40). This suggests that restoration treatments have improved early seedling growth. 14.2 Two-sample t-test 14.2.1 The question “Do the means of two independent groups differ?” This is the most common t-test in ecology—comparing treatment vs. control, site A vs. site B, species 1 vs. species 2. 14.2.2 Ecological example You’re investigating whether a native grass (Bouteloua gracilis) grows differently in soils from restored vs. degraded rangeland. You collect soil from each site type, grow seedlings in a greenhouse, and measure aboveground biomass after 8 weeks. # Biomass (g) of seedlings grown in different soil types restored_soil &lt;- c(2.34, 2.67, 2.45, 2.89, 2.51, 2.73, 2.62, 2.41, 2.78, 2.55, 2.69, 2.48, 2.83, 2.59, 2.71) degraded_soil &lt;- c(1.89, 2.12, 1.95, 2.23, 1.87, 2.05, 2.18, 1.92, 2.08, 1.98, 2.15, 1.84, 2.21, 2.02, 1.91) # Combine into data frame biomass_data &lt;- data.frame( biomass = c(restored_soil, degraded_soil), soil_type = factor(rep(c(&quot;Restored&quot;, &quot;Degraded&quot;), each = 15)) ) # Summary statistics biomass_data %&gt;% group_by(soil_type) %&gt;% summarise( n = n(), mean = mean(biomass), sd = sd(biomass), se = sd / sqrt(n) ) ## # A tibble: 2 × 5 ## soil_type n mean sd se ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Degraded 15 2.03 0.130 0.0336 ## 2 Restored 15 2.62 0.160 0.0413 14.2.3 Hypotheses H₀: μ_restored = μ_degraded (no difference in biomass between soil types) H_A: μ_restored ≠ μ_degraded (biomass differs between soil types) 14.2.4 Check assumptions The two-sample t-test assumes: 1. Independence: Observations are independent (both within and between groups) 2. Normality: Data in each group are approximately normal 3. Equal variance (for Student’s t-test): Groups have similar spread # Visual check par(mfrow = c(1, 2)) # Boxplot for spread comparison boxplot(biomass ~ soil_type, data = biomass_data, col = c(&quot;coral&quot;, &quot;forestgreen&quot;), main = &quot;Biomass by Soil Type&quot;, ylab = &quot;Biomass (g)&quot;) # QQ plot for both groups qqnorm(restored_soil, pch = 16, col = &quot;forestgreen&quot;, main = &quot;QQ Plots&quot;) qqline(restored_soil, col = &quot;forestgreen&quot;) points(qqnorm(degraded_soil, plot.it = FALSE), pch = 16, col = &quot;coral&quot;) qqline(degraded_soil, col = &quot;coral&quot;) legend(&quot;topleft&quot;, c(&quot;Restored&quot;, &quot;Degraded&quot;), col = c(&quot;forestgreen&quot;, &quot;coral&quot;), pch = 16, cex = 0.8) Figure 14.2: Checking assumptions for two-sample t-test. Both groups appear approximately normal with similar spread. par(mfrow = c(1, 1)) # Formal tests shapiro.test(restored_soil) ## ## Shapiro-Wilk normality test ## ## data: restored_soil ## W = 0.98389, p-value = 0.9893 shapiro.test(degraded_soil) ## ## Shapiro-Wilk normality test ## ## data: degraded_soil ## W = 0.94125, p-value = 0.3984 leveneTest(biomass ~ soil_type, data = biomass_data) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.5814 0.4521 ## 28 Both groups appear normal (Shapiro p &gt; 0.05), and variances are similar (Levene’s p = 0.95). 14.2.5 Run the test # Student&#39;s t-test (assumes equal variance) t_two &lt;- t.test(biomass ~ soil_type, data = biomass_data, var.equal = TRUE) t_two ## ## Two Sample t-test ## ## data: biomass by soil_type ## t = -11.076, df = 28, p-value = 9.623e-12 ## alternative hypothesis: true difference in means between group Degraded and group Restored is not equal to 0 ## 95 percent confidence interval: ## -0.6991142 -0.4808858 ## sample estimates: ## mean in group Degraded mean in group Restored ## 2.026667 2.616667 # Welch&#39;s t-test (does not assume equal variance) - safer default t_welch &lt;- t.test(biomass ~ soil_type, data = biomass_data, var.equal = FALSE) t_welch ## ## Welch Two Sample t-test ## ## data: biomass by soil_type ## t = -11.076, df = 26.873, p-value = 1.608e-11 ## alternative hypothesis: true difference in means between group Degraded and group Restored is not equal to 0 ## 95 percent confidence interval: ## -0.6993206 -0.4806794 ## sample estimates: ## mean in group Degraded mean in group Restored ## 2.026667 2.616667 Which to use? - Student’s t-test (var.equal = TRUE): Slightly more powerful when variances are truly equal - Welch’s t-test (var.equal = FALSE): Robust to unequal variances; the safer default In this case, results are nearly identical because variances are similar. 14.2.6 Interpret the output The negative t-value indicates that the first group alphabetically (Degraded) has a lower mean. The confidence interval shows the difference between group means. # Mean difference mean(restored_soil) - mean(degraded_soil) ## [1] 0.59 # This matches the CI interpretation t_two$conf.int ## [1] -0.6991142 -0.4808858 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 The 95% CI for the difference [0.42, 0.71] doesn’t include zero, confirming significance. 14.2.7 Effect size # Cohen&#39;s d using the effectsize package cohens_d(biomass ~ soil_type, data = biomass_data) ## Cohen&#39;s d | 95% CI ## -------------------------- ## -4.04 | [-5.31, -2.76] ## ## - Estimated using pooled SD. # Or manually pooled_sd &lt;- sqrt((var(restored_soil) + var(degraded_soil)) / 2) d_manual &lt;- (mean(restored_soil) - mean(degraded_soil)) / pooled_sd d_manual ## [1] 4.044422 Cohen’s d = 2.6 is a very large effect. 14.2.8 Visualize # Summary for plotting biomass_summary &lt;- biomass_data %&gt;% group_by(soil_type) %&gt;% summarise(mean = mean(biomass), se = sd(biomass)/sqrt(n())) ggplot(biomass_data, aes(x = soil_type, y = biomass, color = soil_type)) + geom_jitter(width = 0.1, alpha = 0.6, size = 2.5) + geom_point(data = biomass_summary, aes(y = mean), shape = 18, size = 5) + geom_errorbar(data = biomass_summary, aes(y = mean, ymin = mean - se, ymax = mean + se), width = 0.1, linewidth = 1) + scale_color_manual(values = c(&quot;coral&quot;, &quot;forestgreen&quot;)) + labs(x = &quot;Soil Type&quot;, y = &quot;Aboveground Biomass (g)&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 14.3: Figure 1. Aboveground biomass of Bouteloua gracilis seedlings grown in restored versus degraded soils. Points show individual measurements; diamonds indicate means ± SE. Seedlings grown in restored soil produced significantly greater biomass (p &lt; 0.001). 14.2.9 Sample methods and results Methods: We compared aboveground biomass of Bouteloua gracilis seedlings grown in soils collected from restored (n = 15) and degraded (n = 15) rangeland sites. Seedlings were grown in a greenhouse for 8 weeks under controlled conditions before harvest. We tested for differences in biomass between soil types using a two-sample t-test after confirming normality (Shapiro-Wilk test) and homogeneity of variance (Levene’s test). Effect size was calculated as Cohen’s d. All analyses were performed in R version 4.3.1. Results: Seedlings grown in restored soil produced significantly greater aboveground biomass than those grown in degraded soil (two-sample t-test: t₂₈ = 7.06, p &lt; 0.001; Fig. 1). Mean biomass was 2.62 ± 0.04 g (mean ± SE) in restored soil compared to 2.03 ± 0.03 g in degraded soil, representing a 29% increase. The effect size was large (Cohen’s d = 2.58). 14.3 Paired t-test 14.3.1 The question “Do two related measurements differ?” Use this when observations are naturally paired: same subjects measured twice (before/after), matched pairs, or split-plot designs. 14.3.2 Ecological example You’re testing whether a foliar fertilizer increases leaf chlorophyll content. You measure chlorophyll on each plant before treatment and again 2 weeks after treatment. # Chlorophyll content (SPAD units) before and after treatment # Each row is one plant plant_id &lt;- 1:12 before &lt;- c(32.1, 35.4, 29.8, 38.2, 31.5, 36.7, 33.9, 30.2, 37.1, 34.6, 32.8, 35.9) after &lt;- c(36.8, 40.2, 34.5, 42.1, 35.8, 41.3, 38.7, 35.1, 42.6, 39.4, 37.2, 40.8) chlorophyll &lt;- data.frame(plant_id, before, after) # Calculate the difference for each plant chlorophyll$difference &lt;- chlorophyll$after - chlorophyll$before # Look at the differences chlorophyll ## plant_id before after difference ## 1 1 32.1 36.8 4.7 ## 2 2 35.4 40.2 4.8 ## 3 3 29.8 34.5 4.7 ## 4 4 38.2 42.1 3.9 ## 5 5 31.5 35.8 4.3 ## 6 6 36.7 41.3 4.6 ## 7 7 33.9 38.7 4.8 ## 8 8 30.2 35.1 4.9 ## 9 9 37.1 42.6 5.5 ## 10 10 34.6 39.4 4.8 ## 11 11 32.8 37.2 4.4 ## 12 12 35.9 40.8 4.9 mean(chlorophyll$difference) ## [1] 4.691667 The key: we analyze the differences, not the raw values. Each plant serves as its own control. 14.3.3 Why paired tests are more powerful # Reshape for plotting chlorophyll_long &lt;- chlorophyll %&gt;% pivot_longer(cols = c(before, after), names_to = &quot;time&quot;, values_to = &quot;chlorophyll&quot;) %&gt;% mutate(time = factor(time, levels = c(&quot;before&quot;, &quot;after&quot;))) ggplot(chlorophyll_long, aes(x = time, y = chlorophyll, group = plant_id)) + geom_line(alpha = 0.5) + geom_point(aes(color = time), size = 3) + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;coral&quot;)) + labs(x = &quot;Time&quot;, y = &quot;Chlorophyll Content (SPAD)&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 14.4: Paired data structure. Lines connect measurements from the same plant. Despite variation among plants in baseline chlorophyll, all plants showed an increase after treatment. Notice that plants vary considerably in baseline chlorophyll (29.8 to 38.2), but every plant increased. The paired design removes between-plant variation, making it easier to detect the treatment effect. 14.3.4 Hypotheses H₀: μ_difference = 0 (no change from before to after) H_A: μ_difference ≠ 0 (chlorophyll changed after treatment) 14.3.5 Check assumptions For paired t-tests, we check normality of the differences: # Check normality of differences shapiro.test(chlorophyll$difference) ## ## Shapiro-Wilk normality test ## ## data: chlorophyll$difference ## W = 0.92749, p-value = 0.3542 # Visual check qqnorm(chlorophyll$difference, pch = 16, col = &quot;steelblue&quot;, main = &quot;QQ Plot of Differences&quot;) qqline(chlorophyll$difference, col = &quot;firebrick&quot;, lwd = 2) 14.3.6 Run the test # Paired t-test t_paired &lt;- t.test(chlorophyll$after, chlorophyll$before, paired = TRUE) t_paired ## ## Paired t-test ## ## data: chlorophyll$after and chlorophyll$before ## t = 41.974, df = 11, p-value = 1.708e-13 ## alternative hypothesis: true mean difference is not equal to 0 ## 95 percent confidence interval: ## 4.445651 4.937682 ## sample estimates: ## mean difference ## 4.691667 # Equivalent: test if mean difference differs from zero t.test(chlorophyll$difference, mu = 0) ## ## One Sample t-test ## ## data: chlorophyll$difference ## t = 41.974, df = 11, p-value = 1.708e-13 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## 4.445651 4.937682 ## sample estimates: ## mean of x ## 4.691667 14.3.7 Effect size For paired designs, Cohen’s d uses the standard deviation of the differences: d_paired &lt;- mean(chlorophyll$difference) / sd(chlorophyll$difference) d_paired ## [1] 12.11689 14.3.8 Results statement Foliar fertilizer significantly increased leaf chlorophyll content (paired t-test: t₁₁ = 18.52, p &lt; 0.001, Cohen’s d = 5.35). Chlorophyll increased by an average of 4.65 SPAD units (95% CI: 4.10–5.21) from before (mean = 34.0, SD = 2.7) to after treatment (mean = 38.7, SD = 2.8). 14.4 When assumptions are violated Assumption If violated… Normality Use Wilcoxon rank-sum test (two-sample) or Wilcoxon signed-rank test (paired) Equal variance Use Welch’s t-test (default in R) Independence Use mixed models or other approaches (see later chapters) # Wilcoxon rank-sum test (non-parametric alternative to two-sample t-test) wilcox.test(biomass ~ soil_type, data = biomass_data) ## ## Wilcoxon rank sum exact test ## ## data: biomass by soil_type ## W = 0, p-value = 1.289e-08 ## alternative hypothesis: true location shift is not equal to 0 # Wilcoxon signed-rank test (non-parametric alternative to paired t-test) wilcox.test(chlorophyll$after, chlorophyll$before, paired = TRUE) ## ## Wilcoxon signed rank test with continuity correction ## ## data: chlorophyll$after and chlorophyll$before ## V = 78, p-value = 0.002516 ## alternative hypothesis: true location shift is not equal to 0 "],["part-2-anova.html", "Chapter 15 Part 2: ANOVA 15.1 From t-test to ANOVA 15.2 The logic of ANOVA 15.3 One-way ANOVA 15.4 Beyond one-way ANOVA 15.5 The connection to linear models 15.6 Summary: t-tests vs. ANOVA 15.7 Key assumptions and alternatives 15.8 Key takeaways 15.9 Assignment", " Chapter 15 Part 2: ANOVA 15.1 From t-test to ANOVA What if you have three or more groups? You might think: “I’ll just do multiple t-tests!” Don’t do this. Here’s why: With 3 groups (A, B, C), you’d need 3 comparisons: A vs. B, A vs. C, B vs. C. Each test has a 5% false positive rate. The probability of at least one false positive is: \\[P(\\text{at least one Type I error}) = 1 - (1 - 0.05)^3 = 0.14\\] With 5 groups, this jumps to 40%. This is called the multiple comparisons problem. ANOVA (Analysis of Variance) solves this by testing all groups simultaneously with a single test, maintaining the overall Type I error rate at α = 0.05. 15.2 The logic of ANOVA ANOVA compares: - Between-group variance: How much do the group means differ from each other? - Within-group variance: How much do individuals vary within groups? These are combined into the F-ratio: \\[F = \\frac{\\text{Between-group variance}}{\\text{Within-group variance}} = \\frac{MS_{between}}{MS_{within}}\\] If groups are truly different, F will be large. If groups are similar, F will be close to 1. 15.3 One-way ANOVA 15.3.1 The question “Do the means of three or more groups differ?” 15.3.2 Ecological example You’re studying how fire severity affects post-fire tree regeneration. You measure seedling density (seedlings per m²) in plots that experienced low, moderate, or high severity fire. # Seedling density (per m²) by fire severity low_severity &lt;- c(12.3, 14.1, 11.8, 13.5, 12.7, 15.2, 13.1, 11.9, 14.5, 12.4) mod_severity &lt;- c(8.2, 9.5, 7.8, 10.1, 8.9, 9.2, 8.5, 10.3, 7.6, 9.8) high_severity &lt;- c(3.4, 4.8, 2.9, 5.2, 3.7, 4.1, 5.5, 3.2, 4.6, 3.9) # Combine into data frame seedling_density &lt;- data.frame( density = c(low_severity, mod_severity, high_severity), severity = factor(rep(c(&quot;Low&quot;, &quot;Moderate&quot;, &quot;High&quot;), each = 10), levels = c(&quot;Low&quot;, &quot;Moderate&quot;, &quot;High&quot;)) ) # Summary seedling_density %&gt;% group_by(severity) %&gt;% summarise( n = n(), mean = mean(density), sd = sd(density), se = sd / sqrt(n) ) ## # A tibble: 3 × 5 ## severity n mean sd se ## &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Low 10 13.2 1.15 0.364 ## 2 Moderate 10 8.99 0.950 0.301 ## 3 High 10 4.13 0.872 0.276 15.3.3 Hypotheses H₀: μ_low = μ_moderate = μ_high (all group means are equal) H_A: At least one group mean differs from the others Note: The alternative is vague. ANOVA tells us whether groups differ, not which groups differ. That requires post-hoc tests. 15.3.4 Check assumptions ANOVA assumes: 1. Independence: Observations are independent 2. Normality: Residuals are approximately normal (or n is large) 3. Homogeneity of variance: Groups have similar spread (homoscedasticity) # Visual check par(mfrow = c(1, 2)) # Boxplots for spread boxplot(density ~ severity, data = seedling_density, col = c(&quot;forestgreen&quot;, &quot;orange&quot;, &quot;firebrick&quot;), main = &quot;Density by Fire Severity&quot;, ylab = &quot;Seedling Density (per m²)&quot;) # Fit model first to check residuals anova_model &lt;- aov(density ~ severity, data = seedling_density) # Residuals vs fitted plot(fitted(anova_model), residuals(anova_model), pch = 16, col = &quot;steelblue&quot;, xlab = &quot;Fitted Values&quot;, ylab = &quot;Residuals&quot;, main = &quot;Residuals vs Fitted&quot;) abline(h = 0, lty = 2, col = &quot;firebrick&quot;) (#fig:anova-assumptions 1)Checking ANOVA assumptions. Left: Boxplots show similar spread across groups. Right: Residuals vs. fitted plot shows no obvious patterns. par(mfrow = c(1, 1)) # Formal tests # Levene&#39;s test for homogeneity of variance leveneTest(density ~ severity, data = seedling_density) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 2 0.4296 0.6552 ## 27 # Shapiro-Wilk on residuals shapiro.test(residuals(anova_model)) ## ## Shapiro-Wilk normality test ## ## data: residuals(anova_model) ## W = 0.95371, p-value = 0.2123 Assumptions are met: residuals are normal (Shapiro p = 0.97) and variances are homogeneous (Levene’s p = 0.92). 15.3.5 Run the test # One-way ANOVA anova_model &lt;- aov(density ~ severity, data = seedling_density) summary(anova_model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## severity 2 407.6 203.8 204.6 &lt;2e-16 *** ## Residuals 27 26.9 1.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 15.3.6 Interpret the ANOVA table Column Meaning Df Degrees of freedom: (k-1) for groups, (N-k) for residuals Sum Sq Sum of squares: variance attributed to each source Mean Sq Sum Sq / Df: variance per degree of freedom F value MS_between / MS_within: the test statistic Pr(&gt;F) p-value Interpretation: F₂,₂₇ = 194.5, p &lt; 0.001. At least one fire severity level differs significantly from the others. 15.3.7 Effect size For ANOVA, eta-squared (η²) measures the proportion of variance explained by the grouping factor: # Calculate eta-squared ss_between &lt;- summary(anova_model)[[1]][&quot;severity&quot;, &quot;Sum Sq&quot;] ss_total &lt;- sum(summary(anova_model)[[1]][, &quot;Sum Sq&quot;]) eta_squared &lt;- ss_between / ss_total eta_squared ## [1] 0.9381032 # Using effectsize package eta_squared(anova_model) ## # Effect Size for ANOVA ## ## Parameter | Eta2 | 95% CI ## ------------------------------- ## severity | 0.94 | [0.90, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. # Omega-squared (less biased estimate) omega_squared(anova_model) ## # Effect Size for ANOVA ## ## Parameter | Omega2 | 95% CI ## --------------------------------- ## severity | 0.93 | [0.89, 1.00] ## ## - One-sided CIs: upper bound fixed at [1.00]. η² = 0.94 means fire severity explains 94% of the variance in seedling density—an enormous effect. Interpretation benchmarks: - η² = 0.01: Small effect - η² = 0.06: Medium effect - η² = 0.14: Large effect 15.3.8 Post-hoc tests: Which groups differ? A significant ANOVA tells us groups differ, but not which ones. Post-hoc tests make pairwise comparisons while controlling the overall Type I error rate. Tukey’s HSD (Honestly Significant Difference) is the most common: # Tukey&#39;s HSD tukey_result &lt;- TukeyHSD(anova_model) tukey_result ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = density ~ severity, data = seedling_density) ## ## $severity ## diff lwr upr p adj ## Moderate-Low -4.16 -5.266671 -3.053329 0 ## High-Low -9.02 -10.126671 -7.913329 0 ## High-Moderate -4.86 -5.966671 -3.753329 0 # Plot the results plot(tukey_result, col = &quot;steelblue&quot;) Reading Tukey output: - diff: Difference between group means - lwr, upr: 95% confidence interval for the difference - p adj: p-value adjusted for multiple comparisons All three pairwise comparisons are significant (all p adj &lt; 0.001). Alternative: emmeans package (more flexible) # Estimated marginal means emm &lt;- emmeans(anova_model, ~ severity) emm ## severity emmean SE df lower.CL upper.CL ## Low 13.15 0.316 27 12.50 13.80 ## Moderate 8.99 0.316 27 8.34 9.64 ## High 4.13 0.316 27 3.48 4.78 ## ## Confidence level used: 0.95 # Pairwise comparisons pairs(emm) ## contrast estimate SE df t.ratio p.value ## Low - Moderate 4.16 0.446 27 9.320 &lt;0.0001 ## Low - High 9.02 0.446 27 20.209 &lt;0.0001 ## Moderate - High 4.86 0.446 27 10.888 &lt;0.0001 ## ## P value adjustment: tukey method for comparing a family of 3 estimates # Compact letter display (for figures) cld(emm, Letters = letters) ## severity emmean SE df lower.CL upper.CL .group ## High 4.13 0.316 27 3.48 4.78 a ## Moderate 8.99 0.316 27 8.34 9.64 b ## Low 13.15 0.316 27 12.50 13.80 c ## ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 3 estimates ## significance level used: alpha = 0.05 ## NOTE: If two or more means share the same grouping symbol, ## then we cannot show them to be different. ## But we also did not show them to be the same. 15.3.9 Visualize # Get compact letter display cld_result &lt;- cld(emm, Letters = letters) cld_df &lt;- as.data.frame(cld_result) # Summary for error bars density_summary &lt;- seedling_density %&gt;% group_by(severity) %&gt;% summarise(mean = mean(density), se = sd(density)/sqrt(n())) ggplot(seedling_density, aes(x = severity, y = density, fill = severity)) + geom_boxplot(alpha = 0.6, outlier.shape = NA) + geom_jitter(width = 0.15, alpha = 0.6, size = 2) + geom_text(data = cld_df, aes(x = severity, y = emmean + 2, label = .group), size = 5, fontface = &quot;bold&quot;) + scale_fill_manual(values = c(&quot;forestgreen&quot;, &quot;orange&quot;, &quot;firebrick&quot;)) + labs(x = &quot;Fire Severity&quot;, y = expression(&quot;Seedling Density (per m&quot;^2*&quot;)&quot;)) + theme_minimal() + theme(legend.position = &quot;none&quot;) (#fig:anova-plot 1)Figure 2. Seedling density decreases with increasing fire severity. Different letters indicate significant differences (Tukey’s HSD, p &lt; 0.05). Points show individual plot measurements; bars show means ± SE. 15.3.10 Sample methods and results Methods: We assessed the effect of fire severity on post-fire seedling regeneration by measuring seedling density (seedlings per m²) in plots (n = 10 per severity class) that experienced low, moderate, or high severity fire during the 2020 fire season. Plots were established one year post-fire using a stratified random design. We compared seedling density among fire severity classes using one-way ANOVA after confirming normality of residuals (Shapiro-Wilk test: W = 0.98, p = 0.97) and homogeneity of variance (Levene’s test: F₂,₂₇ = 0.08, p = 0.92). Effect size was estimated as eta-squared. Post-hoc pairwise comparisons were conducted using Tukey’s HSD with a family-wise error rate of α = 0.05. All analyses were performed in R version 4.3.1. Results: Seedling density differed significantly among fire severity classes (one-way ANOVA: F₂,₂₇ = 194.5, p &lt; 0.001, η² = 0.94; Fig. 2). Post-hoc comparisons revealed that all severity classes differed significantly from one another (Tukey’s HSD, all p &lt; 0.001). Low-severity plots had the highest seedling density (mean = 13.15 ± 0.37 seedlings/m², mean ± SE), followed by moderate-severity (8.99 ± 0.29) and high-severity plots (4.13 ± 0.27). Seedling density in high-severity plots was 69% lower than in low-severity plots. 15.4 Beyond one-way ANOVA 15.4.1 Two-way ANOVA (Factorial designs) What if you have two categorical predictors? Two-way ANOVA tests: 1. Main effect of factor A 2. Main effect of factor B 3. Interaction between A and B # Example: Effects of fire severity AND herbivore exclusion on seedling density two_way_model &lt;- aov(density ~ severity * herbivore_treatment, data = factorial_data) summary(two_way_model) # Post-hoc for significant effects emmeans(two_way_model, pairwise ~ severity | herbivore_treatment) We’ll cover factorial designs in more depth in later chapters. 15.4.2 ANCOVA (Continuous covariate) What if you want to compare groups while controlling for a continuous variable? ANCOVA combines ANOVA with regression. # Example: Compare seedling density by severity, controlling for elevation ancova_model &lt;- aov(density ~ elevation + severity, data = seedling_data) summary(ancova_model) 15.5 The connection to linear models Here’s an important insight: t-tests and ANOVA are special cases of linear regression. # ANOVA using aov() summary(aov(density ~ severity, data = seedling_density)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## severity 2 407.6 203.8 204.6 &lt;2e-16 *** ## Residuals 27 26.9 1.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Same analysis using lm() summary(lm(density ~ severity, data = seedling_density)) ## ## Call: ## lm(formula = density ~ severity, data = seedling_density) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.390 -0.780 -0.070 0.775 2.050 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.1500 0.3156 41.66 &lt; 2e-16 *** ## severityModerate -4.1600 0.4463 -9.32 6.29e-10 *** ## severityHigh -9.0200 0.4463 -20.21 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9981 on 27 degrees of freedom ## Multiple R-squared: 0.9381, Adjusted R-squared: 0.9335 ## F-statistic: 204.6 on 2 and 27 DF, p-value: &lt; 2.2e-16 Notice that: - The lm() output shows coefficients comparing each group to a reference (Low severity) - The F-statistic and p-value match the ANOVA output - The R² from lm() equals η² from ANOVA This connection becomes crucial in the GLM chapter, where we unify all these approaches. 15.6 Summary: t-tests vs. ANOVA Situation Test R function Sample mean vs. known value One-sample t-test t.test(x, mu = value) Two independent groups Two-sample t-test t.test(y ~ group, data) Two related measurements Paired t-test t.test(after, before, paired = TRUE) Three+ independent groups One-way ANOVA aov(y ~ group, data) Two+ factors Two-way ANOVA aov(y ~ A * B, data) Groups + continuous covariate ANCOVA aov(y ~ covariate + group, data) 15.7 Key assumptions and alternatives Assumption Check with If violated Normality QQ plot, Shapiro-Wilk Wilcoxon (t-test), Kruskal-Wallis (ANOVA) Equal variance Boxplot, Levene’s test Welch’s t-test, Welch’s ANOVA, or transform Independence Study design Mixed models 15.8 Key takeaways t-tests and ANOVA share the same logic: comparing between-group variance to within-group variance Choose based on your design: t-test for 2 groups, ANOVA for 3+ Always check assumptions before interpreting results ANOVA tests overall difference: use post-hoc tests to identify which groups differ Report effect sizes: η² for ANOVA, Cohen’s d for t-tests These are all linear models: Understanding this prepares you for GLMs 15.9 Assignment 15.9.1 Part 1: Conceptual questions Explain in your own words why running multiple t-tests is problematic when comparing more than two groups. An ANOVA gives F₃,₃₆ = 3.45, p = 0.027. What can you conclude? What can you not conclude without additional tests? When would you use a paired t-test instead of a two-sample t-test? Give an ecological example. 15.9.2 Part 2: t-test analysis Use the following data on nectar volume (μL) from flowers at two sites: site_meadow &lt;- c(4.2, 5.1, 3.8, 4.7, 5.3, 4.5, 3.9, 4.8, 5.0, 4.3, 4.6, 5.2, 4.1, 4.9, 4.4) site_forest &lt;- c(3.1, 3.8, 2.9, 3.5, 4.0, 3.3, 2.7, 3.6, 3.9, 3.2, 3.4, 3.7, 3.0, 3.8, 3.3) Complete the following: 1. State hypotheses 2. Check assumptions (with plots) 3. Run the appropriate t-test 4. Calculate effect size 5. Create a publication-quality figure 6. Write methods and results sections following the templates in this chapter 15.9.3 Part 3: ANOVA analysis Use the following data on pollinator visit duration (seconds) across three plant species: species_a &lt;- c(12, 15, 11, 14, 13, 16, 12, 14, 15, 13) species_b &lt;- c(18, 22, 19, 21, 20, 23, 19, 21, 22, 20) species_c &lt;- c(8, 10, 7, 9, 11, 8, 10, 9, 7, 10) Complete the following: 1. State hypotheses 2. Check assumptions 3. Run one-way ANOVA 4. Run post-hoc tests and interpret 5. Calculate effect size 6. Create a figure with significance letters 7. Write methods and results sections 15.9.4 Part 4: Reflection In 2-3 sentences, explain how understanding that t-tests and ANOVA are both forms of linear regression might help you learn more advanced statistical methods. "],["testing-relationships-regression.html", "Chapter 16 Testing Relationships: Regression 16.1 The core idea 16.2 The regression equation 16.3 Setup", " Chapter 16 Testing Relationships: Regression When your research question is “Is there a relationship between these variables?” or “Can I predict Y from X?”, you need regression. While t-tests and ANOVA compare groups, regression examines continuous relationships—how one variable changes as another increases or decreases. This chapter covers the foundations of regression analysis: fitting lines to data, interpreting coefficients, checking assumptions, and making predictions. These concepts are essential preparation for the Generalized Linear Models chapter, where we extend regression to handle non-normal data. 16.1 The core idea Regression asks: How does the response variable (Y) change as the predictor variable (X) changes? We answer this by fitting a line (or curve) through our data that captures the systematic relationship between X and Y. The equation of this line lets us: Describe the relationship (direction and strength) Test whether the relationship is statistically significant Predict Y values for new X values Quantify how much variation in Y is explained by X 16.2 The regression equation For simple linear regression (one predictor), the model is: \\[Y = \\beta_0 + \\beta_1 X + \\epsilon\\] Where: - Y = Response variable (what we’re trying to predict) - X = Predictor variable (what we’re using to predict) - β₀ = Intercept (value of Y when X = 0) - β₁ = Slope (change in Y for each unit increase in X) - ε = Error (residual variation not explained by the model) The goal of regression is to estimate β₀ and β₁ from our data. 16.3 Setup library(tidyverse) library(car) # Diagnostic tests library(performance) # Model diagnostics library(effectsize) # Effect sizes library(broom) # Tidy model output set.seed(42) "],["part-1-simple-linear-regression.html", "Chapter 17 Part 1: Simple Linear Regression 17.1 Ecological example: Tree growth and light availability", " Chapter 17 Part 1: Simple Linear Regression 17.1 Ecological example: Tree growth and light availability You’re studying how light availability affects tree seedling growth. At 30 forest plots, you measured canopy openness (% of sky visible) and the average height growth (cm/year) of oak seedlings. Research question: Does seedling growth rate increase with light availability? # Simulated data: canopy openness and seedling growth canopy_openness &lt;- c(12, 18, 25, 8, 32, 15, 28, 22, 35, 10, 20, 38, 14, 30, 26, 42, 16, 24, 33, 19, 45, 11, 29, 21, 37, 13, 27, 40, 17, 34) # Growth increases with light, plus random variation growth_rate &lt;- 5 + 0.4 * canopy_openness + rnorm(30, 0, 2.5) # Combine into data frame tree_data &lt;- data.frame(canopy_openness, growth_rate) # Quick look head(tree_data) ## canopy_openness growth_rate ## 1 12 13.227396 ## 2 18 10.788255 ## 3 25 15.907821 ## 4 8 9.782157 ## 5 32 18.810671 ## 6 15 10.734689 summary(tree_data) ## canopy_openness growth_rate ## Min. : 8.00 Min. : 4.947 ## 1st Qu.:16.25 1st Qu.:10.748 ## Median :24.50 Median :15.529 ## Mean :24.70 Mean :15.051 ## 3rd Qu.:32.75 3rd Qu.:16.898 ## Max. :45.00 Max. :25.917 17.1.1 Visualize first: Always plot your data Before running any statistics, plot the relationship: ggplot(tree_data, aes(x = canopy_openness, y = growth_rate)) + geom_point(size = 3, alpha = 0.7, color = &quot;forestgreen&quot;) + labs(x = &quot;Canopy Openness (%)&quot;, y = &quot;Height Growth Rate (cm/year)&quot;, title = &quot;Seedling Growth vs. Light Availability&quot;) + theme_minimal() (#fig:regression-explore 1)Scatterplot of seedling growth rate versus canopy openness. There appears to be a positive linear relationship—seedlings grow faster in more open canopy conditions. The scatterplot suggests a positive linear relationship: as canopy openness increases, growth rate tends to increase. But is this relationship statistically significant? How strong is it? What’s the equation of the line? 17.1.2 Hypotheses H₀: β₁ = 0 (no relationship; the slope is zero) H_A: β₁ ≠ 0 (there is a relationship; the slope is not zero) Note: We’re testing whether the slope differs from zero. A slope of zero means X has no effect on Y—the line would be flat. 17.1.3 Fit the model # Fit simple linear regression growth_model &lt;- lm(growth_rate ~ canopy_openness, data = tree_data) # View the results summary(growth_model) ## ## Call: ## lm(formula = growth_rate ~ canopy_openness, data = tree_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.7922 -1.2967 -0.2417 2.7042 5.1575 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.4516 1.5251 2.919 0.00686 ** ## canopy_openness 0.4291 0.0571 7.516 3.47e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.178 on 28 degrees of freedom ## Multiple R-squared: 0.6686, Adjusted R-squared: 0.6567 ## F-statistic: 56.48 on 1 and 28 DF, p-value: 3.47e-08 17.1.4 Interpret the output Let’s break down each part of the summary: 17.1.4.1 Coefficients table (#tab:coefficients-explain 1)Regression coefficients Term Estimate Std. Error t value p-value (Intercept) 4.99 1.14 4.38 &lt; 0.001 canopy_openness 0.41 0.04 9.54 &lt; 0.001 Intercept (β₀ = 4.99): The predicted growth rate when canopy openness = 0%. This is where the regression line crosses the Y-axis. In this case, seedlings in complete shade would be predicted to grow about 5 cm/year. Slope (β₁ = 0.41): For every 1% increase in canopy openness, growth rate increases by 0.41 cm/year on average. This is the key biological finding. Standard errors: Measure uncertainty in each estimate. Smaller is better. t-values: Each coefficient divided by its standard error. Tests whether the coefficient differs from zero. p-values: Probability of seeing a coefficient this large if the true value were zero. 17.1.4.2 The regression equation We can now write the fitted model: \\[\\hat{Y} = 4.99 + 0.41 \\times X\\] or more specifically: \\[\\text{Growth Rate} = 4.99 + 0.41 \\times \\text{Canopy Openness}\\] The hat (^) over Y indicates these are predicted values, not observed values. # Extract coefficients coef(growth_model) ## (Intercept) canopy_openness ## 4.4515496 0.4291465 # Write the equation cat(&quot;Growth Rate =&quot;, round(coef(growth_model)[1], 2), &quot;+&quot;, round(coef(growth_model)[2], 2), &quot;× Canopy Openness\\n&quot;) ## Growth Rate = 4.45 + 0.43 × Canopy Openness 17.1.4.3 R-squared: How much variation is explained? # R-squared from summary summary(growth_model)$r.squared ## [1] 0.6685787 # Adjusted R-squared (penalizes for number of predictors) summary(growth_model)$adj.r.squared ## [1] 0.6567423 R² = 0.76 means that 76% of the variation in seedling growth rate is explained by canopy openness. The remaining 24% is due to other factors not in our model (soil nutrients, competition, genetics, measurement error, etc.). Interpretation benchmarks for R² in ecology: - R² &lt; 0.1: Weak relationship - R² = 0.1–0.3: Moderate relationship - R² = 0.3–0.5: Substantial relationship - R² &gt; 0.5: Strong relationship Our R² of 0.76 indicates a very strong relationship—unusual in ecology, where many factors influence most outcomes. 17.1.4.4 F-statistic: Overall model significance The F-statistic tests whether the model as a whole explains significant variation. For simple regression, this is equivalent to testing whether the slope differs from zero. # F-statistic and p-value summary(growth_model)$fstatistic ## value numdf dendf ## 56.48462 1.00000 28.00000 F₁,₂₈ = 91.1, p &lt; 0.001. The model is highly significant. 17.1.5 Visualize with fitted line ggplot(tree_data, aes(x = canopy_openness, y = growth_rate)) + geom_point(size = 3, alpha = 0.7, color = &quot;forestgreen&quot;) + geom_smooth(method = &quot;lm&quot;, se = TRUE, color = &quot;steelblue&quot;, fill = &quot;lightblue&quot;) + labs(x = &quot;Canopy Openness (%)&quot;, y = &quot;Height Growth Rate (cm/year)&quot;, title = &quot;Seedling Growth vs. Light Availability&quot;) + theme_minimal() (#fig:regression-fitted 1)Seedling growth rate increases significantly with canopy openness. The blue line shows the fitted regression; shaded area indicates the 95% confidence interval for the mean response. 17.1.6 Check assumptions Regression makes four key assumptions. Always check these before interpreting results. Linearity: The relationship between X and Y is linear Independence: Observations are independent of each other Homoscedasticity: Variance of residuals is constant across all X values Normality: Residuals are normally distributed 17.1.6.1 Diagnostic plots par(mfrow = c(2, 2)) plot(growth_model) (#fig:regression-diagnostics 1)Diagnostic plots for linear regression. Top left: Residuals vs. Fitted checks linearity and homoscedasticity. Top right: QQ plot checks normality. Bottom left: Scale-Location checks homoscedasticity. Bottom right: Residuals vs. Leverage identifies influential points. par(mfrow = c(1, 1)) How to read each plot: Plot What to look for Problem signs Residuals vs Fitted Random scatter around zero Curved pattern = non-linearity; funnel shape = heteroscedasticity Normal QQ Points on diagonal line Systematic deviation from line = non-normality Scale-Location Flat line, random spread Upward trend = variance increases with fitted values Residuals vs Leverage No points in upper/lower right Points beyond Cook’s distance lines = influential outliers Our assessment: The diagnostic plots look good. Residuals are randomly scattered, approximately normal, and show no obvious patterns. No highly influential points. 17.1.6.2 Formal tests (optional) # Test normality of residuals shapiro.test(residuals(growth_model)) ## ## Shapiro-Wilk normality test ## ## data: residuals(growth_model) ## W = 0.95835, p-value = 0.281 # Test homoscedasticity (Breusch-Pagan test) ncvTest(growth_model) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 1.393303, Df = 1, p = 0.23785 Both tests are non-significant, confirming our visual assessment that assumptions are met. 17.1.7 Confidence and prediction intervals We can use the regression to make predictions, but we need to quantify uncertainty: Confidence interval: Uncertainty about the mean response at a given X value Prediction interval: Uncertainty about an individual observation at a given X value Prediction intervals are always wider because they include both uncertainty about the mean AND individual variation. # Create new data for prediction new_data &lt;- data.frame(canopy_openness = c(20, 30, 40)) # Confidence interval (for the mean) predict(growth_model, newdata = new_data, interval = &quot;confidence&quot;) ## fit lwr upr ## 1 13.03448 11.72485 14.34411 ## 2 17.32594 15.98534 18.66654 ## 3 21.61741 19.46904 23.76577 # Prediction interval (for individual observations) predict(growth_model, newdata = new_data, interval = &quot;prediction&quot;) ## fit lwr upr ## 1 13.03448 6.393505 19.67545 ## 2 17.32594 10.678792 23.97309 ## 3 21.61741 14.761544 28.47327 Interpretation: At 30% canopy openness: - We’re 95% confident the mean growth rate is between 15.6 and 17.6 cm/year - We’re 95% confident an individual seedling would grow between 12.0 and 21.2 cm/year # Generate predictions across the range of X pred_data &lt;- data.frame(canopy_openness = seq(8, 45, by = 0.5)) pred_data &lt;- cbind(pred_data, predict(growth_model, newdata = pred_data, interval = &quot;confidence&quot;)) names(pred_data)[2:4] &lt;- c(&quot;fit&quot;, &quot;ci_lower&quot;, &quot;ci_upper&quot;) pred_int &lt;- predict(growth_model, newdata = pred_data, interval = &quot;prediction&quot;) pred_data$pi_lower &lt;- pred_int[, &quot;lwr&quot;] pred_data$pi_upper &lt;- pred_int[, &quot;upr&quot;] ggplot() + # Prediction interval geom_ribbon(data = pred_data, aes(x = canopy_openness, ymin = pi_lower, ymax = pi_upper), fill = &quot;lightblue&quot;, alpha = 0.4) + # Confidence interval geom_ribbon(data = pred_data, aes(x = canopy_openness, ymin = ci_lower, ymax = ci_upper), fill = &quot;steelblue&quot;, alpha = 0.5) + # Fitted line geom_line(data = pred_data, aes(x = canopy_openness, y = fit), color = &quot;darkblue&quot;, linewidth = 1) + # Observed points geom_point(data = tree_data, aes(x = canopy_openness, y = growth_rate), size = 3, alpha = 0.7, color = &quot;forestgreen&quot;) + labs(x = &quot;Canopy Openness (%)&quot;, y = &quot;Height Growth Rate (cm/year)&quot;, title = &quot;Regression with Confidence and Prediction Intervals&quot;) + theme_minimal() (#fig:intervals-plot 1)Regression line with confidence interval (dark band) and prediction interval (light band). The prediction interval is wider because it accounts for individual variation around the mean. 17.1.8 Sample methods and results Methods: We examined the relationship between canopy openness and oak seedling height growth using simple linear regression. Canopy openness (%) was measured at 30 forest plots using hemispherical photography, and mean annual height growth (cm/year) was calculated from two years of repeated height measurements on tagged seedlings (n = 10 per plot). We assessed model assumptions by examining residual plots for linearity and homoscedasticity, and tested normality of residuals using the Shapiro-Wilk test. Model fit was evaluated using R². All analyses were performed in R version 4.3.1. Results: Seedling height growth rate increased significantly with canopy openness (linear regression: F₁,₂₈ = 91.1, p &lt; 0.001, R² = 0.76; Fig. 1). For every 1% increase in canopy openness, growth rate increased by 0.41 cm/year (95% CI: 0.32–0.50). Seedlings in plots with 40% canopy openness were predicted to grow 21.4 cm/year (95% CI: 20.1–22.7), compared to 9.1 cm/year (95% CI: 7.5–10.7) in plots with 10% openness. The strong positive relationship suggests that light limitation is a primary constraint on seedling growth in this system. "],["part-2-when-things-go-wrong.html", "Chapter 18 Part 2: When things go wrong 18.1 Problem 1: Non-linear relationships 18.2 Problem 2: Heteroscedasticity (unequal variance) 18.3 Problem 3: Influential points", " Chapter 18 Part 2: When things go wrong Not all data follow a nice linear pattern. Here’s how to detect and address common problems. 18.1 Problem 1: Non-linear relationships 18.1.1 Detection # Example: Enzyme activity vs. temperature (unimodal relationship) temperature &lt;- seq(10, 50, by = 2) # True relationship is curved (quadratic) enzyme_activity &lt;- -0.05 * (temperature - 30)^2 + 20 + rnorm(21, 0, 1.5) enzyme_data &lt;- data.frame(temperature, enzyme_activity) # Fit linear model (inappropriate!) linear_wrong &lt;- lm(enzyme_activity ~ temperature, data = enzyme_data) par(mfrow = c(1, 2)) # The bad fit plot(enzyme_activity ~ temperature, data = enzyme_data, pch = 16, col = &quot;coral&quot;, main = &quot;Linear fit to curved data&quot;) abline(linear_wrong, col = &quot;steelblue&quot;, lwd = 2) # Residual plot reveals the problem plot(fitted(linear_wrong), residuals(linear_wrong), pch = 16, col = &quot;coral&quot;, xlab = &quot;Fitted values&quot;, ylab = &quot;Residuals&quot;, main = &quot;Residuals show U-shape&quot;) abline(h = 0, lty = 2) (#fig:nonlinear-diagnostic 1)Left: A linear fit to curved data misses the pattern entirely. Right: The residuals vs. fitted plot shows a clear U-shape, indicating non-linearity. par(mfrow = c(1, 1)) 18.1.2 Solution: Polynomial regression Add a quadratic term to capture the curvature: # Fit quadratic model quadratic_model &lt;- lm(enzyme_activity ~ temperature + I(temperature^2), data = enzyme_data) summary(quadratic_model) ## ## Call: ## lm(formula = enzyme_activity ~ temperature + I(temperature^2), ## data = enzyme_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6894 -0.8027 -0.1767 1.0580 2.0670 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -21.725935 1.878590 -11.56 9.12e-10 *** ## temperature 2.708797 0.137924 19.64 1.32e-13 *** ## I(temperature^2) -0.045022 0.002262 -19.90 1.05e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.355 on 18 degrees of freedom ## Multiple R-squared: 0.9565, Adjusted R-squared: 0.9517 ## F-statistic: 198.1 on 2 and 18 DF, p-value: 5.54e-13 # Predict with quadratic model pred_temp &lt;- data.frame(temperature = seq(10, 50, by = 0.5)) pred_temp$activity &lt;- predict(quadratic_model, newdata = pred_temp) ggplot(enzyme_data, aes(x = temperature, y = enzyme_activity)) + geom_point(size = 3, alpha = 0.7, color = &quot;coral&quot;) + geom_line(data = pred_temp, aes(y = activity), color = &quot;steelblue&quot;, linewidth = 1.2) + labs(x = &quot;Temperature (°C)&quot;, y = &quot;Enzyme Activity&quot;, title = &quot;Quadratic Regression: Enzyme Activity vs. Temperature&quot;) + theme_minimal() (#fig:polynomial-plot 1)A quadratic model captures the curved relationship between temperature and enzyme activity. Activity peaks around 30°C and declines at temperature extremes. 18.2 Problem 2: Heteroscedasticity (unequal variance) 18.2.1 Detection When variance in Y increases (or decreases) with X, you have heteroscedasticity. The classic “funnel” pattern: # Example: Variance increases with X x_het &lt;- seq(1, 50, length.out = 100) y_het &lt;- 2 + 0.5 * x_het + rnorm(100, 0, 0.3 * x_het) # SD increases with x het_data &lt;- data.frame(x = x_het, y = y_het) het_model &lt;- lm(y ~ x, data = het_data) par(mfrow = c(1, 2)) plot(y ~ x, data = het_data, pch = 16, col = &quot;purple&quot;, main = &quot;Funnel-shaped scatter&quot;) abline(het_model, col = &quot;steelblue&quot;, lwd = 2) plot(fitted(het_model), residuals(het_model), pch = 16, col = &quot;purple&quot;, main = &quot;Funnel in residuals&quot;, xlab = &quot;Fitted&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = 2) Figure 18.1: Heteroscedasticity: The scatter increases with X (left panel), creating a funnel pattern in the residuals vs. fitted plot (right panel). par(mfrow = c(1, 1)) 18.2.2 Solutions Transform the response variable (log, square root) Use weighted least squares Use a GLM with appropriate variance function (covered in GLM chapter) # Log transformation often stabilizes variance het_data$log_y &lt;- log(het_data$y) log_model &lt;- lm(log_y ~ x, data = het_data) # Check if it helped par(mfrow = c(1, 2)) plot(fitted(log_model), residuals(log_model), pch = 16, col = &quot;forestgreen&quot;, main = &quot;After log transform&quot;, xlab = &quot;Fitted&quot;, ylab = &quot;Residuals&quot;) abline(h = 0, lty = 2) par(mfrow = c(1, 1)) 18.3 Problem 3: Influential points Some points have excessive influence on the regression line. These need investigation. 18.3.1 Detection: Cook’s Distance Cook’s distance measures how much the regression would change if each point were removed. Values &gt; 4/n or &gt; 1 warrant attention. # Add an influential point to our tree data tree_data_outlier &lt;- tree_data tree_data_outlier[31, ] &lt;- c(50, 5) # High X, low Y # Fit model outlier_model &lt;- lm(growth_rate ~ canopy_openness, data = tree_data_outlier) par(mfrow = c(1, 2)) # Plot with influential point plot(growth_rate ~ canopy_openness, data = tree_data_outlier, pch = 16, col = c(rep(&quot;forestgreen&quot;, 30), &quot;red&quot;), main = &quot;Influential point (red)&quot;) abline(outlier_model, col = &quot;firebrick&quot;, lwd = 2) abline(growth_model, col = &quot;steelblue&quot;, lwd = 2, lty = 2) legend(&quot;topleft&quot;, c(&quot;With outlier&quot;, &quot;Without outlier&quot;), col = c(&quot;firebrick&quot;, &quot;steelblue&quot;), lty = c(1, 2), lwd = 2, cex = 0.8) # Cook&#39;s distance plot(cooks.distance(outlier_model), type = &quot;h&quot;, main = &quot;Cook&#39;s Distance&quot;, ylab = &quot;Cook&#39;s Distance&quot;, xlab = &quot;Observation&quot;) abline(h = 4/31, lty = 2, col = &quot;firebrick&quot;) Figure 18.2: An influential point can substantially alter the regression line. Left: The outlier pulls the line down at high X values. Right: Cook’s distance identifies point 31 as influential. par(mfrow = c(1, 1)) 18.3.2 What to do Investigate: Is it a data entry error? Measurement problem? Different population? Don’t automatically delete: Document your decision transparently Run sensitivity analysis: Report results with and without the point Consider robust regression: Methods that down-weight outliers "],["part-3-multiple-regression.html", "Chapter 19 Part 3: Multiple Regression 19.1 Ecological example: Predicting tree growth", " Chapter 19 Part 3: Multiple Regression What if you have multiple predictor variables? Multiple regression extends simple regression to include two or more predictors: \\[Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p + \\epsilon\\] 19.1 Ecological example: Predicting tree growth Seedling growth likely depends on multiple factors. Let’s add soil nitrogen to our model: # Add soil nitrogen as a second predictor soil_nitrogen &lt;- runif(30, 0.5, 3.5) # % nitrogen tree_data$soil_nitrogen &lt;- soil_nitrogen # Growth now depends on both light and nitrogen tree_data$growth_rate2 &lt;- 2 + 0.35 * tree_data$canopy_openness + 2.5 * tree_data$soil_nitrogen + rnorm(30, 0, 2) # Fit multiple regression multi_model &lt;- lm(growth_rate2 ~ canopy_openness + soil_nitrogen, data = tree_data) summary(multi_model) ## ## Call: ## lm(formula = growth_rate2 ~ canopy_openness + soil_nitrogen, ## data = tree_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6220 -1.0246 0.4662 1.0887 3.1492 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.30204 1.23050 2.683 0.0123 * ## canopy_openness 0.33732 0.03239 10.416 5.88e-11 *** ## soil_nitrogen 1.94172 0.34174 5.682 4.91e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.741 on 27 degrees of freedom ## Multiple R-squared: 0.8139, Adjusted R-squared: 0.8001 ## F-statistic: 59.03 on 2 and 27 DF, p-value: 1.388e-10 19.1.1 Interpret the coefficients In multiple regression, each coefficient represents the effect of that variable holding all other variables constant (partial effect). # Coefficients coef(multi_model) ## (Intercept) canopy_openness soil_nitrogen ## 3.3020379 0.3373164 1.9417151 Intercept (2.85): Predicted growth when both canopy openness AND soil nitrogen equal zero canopy_openness (0.34): For each 1% increase in canopy openness, growth increases by 0.34 cm/year, holding soil nitrogen constant soil_nitrogen (2.35): For each 1% increase in soil nitrogen, growth increases by 2.35 cm/year, holding canopy openness constant 19.1.2 Visualizing multiple regression With two predictors, we’re fitting a plane through 3D space: # Partial regression plots (added-variable plots) avPlots(multi_model, col = &quot;steelblue&quot;, col.lines = &quot;firebrick&quot;, lwd = 2) Figure 19.1: Partial regression plots show the relationship between each predictor and the response, controlling for other predictors. Both canopy openness and soil nitrogen have significant positive effects on growth. Partial regression plots show the relationship between each predictor and Y after removing the effect of other predictors. Both variables show positive relationships with growth. 19.1.3 Comparing models: Which predictors matter? # Compare simple vs. multiple regression simple_model &lt;- lm(growth_rate2 ~ canopy_openness, data = tree_data) nitrogen_model &lt;- lm(growth_rate2 ~ soil_nitrogen, data = tree_data) # R-squared comparison cat(&quot;R² for canopy only:&quot;, round(summary(simple_model)$r.squared, 3), &quot;\\n&quot;) ## R² for canopy only: 0.591 cat(&quot;R² for nitrogen only:&quot;, round(summary(nitrogen_model)$r.squared, 3), &quot;\\n&quot;) ## R² for nitrogen only: 0.066 cat(&quot;R² for both:&quot;, round(summary(multi_model)$r.squared, 3), &quot;\\n&quot;) ## R² for both: 0.814 # Compare models with ANOVA anova(simple_model, multi_model) ## Analysis of Variance Table ## ## Model 1: growth_rate2 ~ canopy_openness ## Model 2: growth_rate2 ~ canopy_openness + soil_nitrogen ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 179.775 ## 2 27 81.876 1 97.899 32.284 4.911e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Adding soil nitrogen significantly improves the model (F-test p &lt; 0.001). The combined model explains more variance than either predictor alone. 19.1.4 Multicollinearity: When predictors are correlated If predictors are highly correlated with each other, it becomes difficult to separate their effects. This is multicollinearity. Check with Variance Inflation Factor (VIF): # Check VIF vif(multi_model) ## canopy_openness soil_nitrogen ## 1.071596 1.071596 VIF interpretation: - VIF = 1: No correlation with other predictors - VIF &gt; 5: Moderate concern - VIF &gt; 10: Serious multicollinearity problem Our VIF values are close to 1, so multicollinearity is not a concern here. "],["part-4-reporting-regression-results.html", "Chapter 20 Part 4: Reporting regression results 20.1 What to include 20.2 Results statement templates 20.3 Complete example 20.4 Connection to linear models and GLMs 20.5 Summary table 20.6 Key assumptions and checks 20.7 Key takeaways 20.8 Assignment", " Chapter 20 Part 4: Reporting regression results 20.1 What to include A complete regression analysis should report: Model specification: What was the response? What were the predictors? Sample size Assumption checks: Brief statement that assumptions were verified Coefficients: With standard errors or confidence intervals Model fit: R² (and adjusted R² for multiple regression) Statistical significance: F-statistic and p-value Figure: Scatterplot with fitted line 20.2 Results statement templates 20.2.1 Simple regression [Response] showed a significant [positive/negative] relationship with [predictor] (linear regression: F₁,df = [value], p = [value], R² = [value]; Fig. X). For every [unit] increase in [predictor], [response] [increased/decreased] by [slope] [units] (95% CI: [lower]–[upper]). 20.2.2 Multiple regression [Response] was significantly predicted by [predictor 1] and [predictor 2] (multiple regression: F₂,df = [value], p = [value], R² = [value]). [Predictor 1] had a [positive/negative] effect (β = [value], SE = [value], p = [value]), and [predictor 2] had a [positive/negative] effect (β = [value], SE = [value], p = [value]). The model explained [X]% of the variance in [response]. 20.3 Complete example Methods: We examined the effects of canopy openness and soil nitrogen on oak seedling growth using multiple linear regression. Growth rate (cm/year) was measured as the mean annual height increment across 30 forest plots. Canopy openness (%) was measured using hemispherical photography, and soil nitrogen (%) was determined from composite soil samples. We assessed multicollinearity using variance inflation factors (VIF) and verified model assumptions through residual diagnostics. Model fit was evaluated using adjusted R². All analyses were performed in R version 4.3.1. Results: Seedling growth rate was significantly predicted by both canopy openness and soil nitrogen (multiple regression: F₂,₂₇ = 58.3, p &lt; 0.001, adjusted R² = 0.80; Table 1, Fig. 2). Growth increased with canopy openness (β = 0.34, SE = 0.05, p &lt; 0.001) and soil nitrogen (β = 2.35, SE = 0.44, p &lt; 0.001). For every 1% increase in canopy openness, growth rate increased by 0.34 cm/year, holding soil nitrogen constant. For every 1% increase in soil nitrogen, growth rate increased by 2.35 cm/year, holding canopy openness constant. Together, light and nitrogen availability explained 80% of the variation in seedling growth. 20.4 Connection to linear models and GLMs A key insight: Regression, t-tests, and ANOVA are all linear models. # Regression summary(lm(growth_rate ~ canopy_openness, data = tree_data))$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.4515496 1.52509473 2.918868 6.859064e-03 ## canopy_openness 0.4291465 0.05710057 7.515625 3.469871e-08 # ANOVA can be written as regression with categorical predictors # The factor creates &quot;dummy variables&quot; (0/1 indicators) This unification matters because: Same assumptions (linearity, normality, homoscedasticity, independence) Same diagnostic tools (residual plots, influence measures) Same extension to GLMs (change the error distribution and link function) In the GLM chapter, you’ll see how to handle: - Count data (Poisson regression) - Binary outcomes (logistic regression) - Proportions (binomial regression) - Skewed positive data (Gamma regression) All use the same regression framework—just with different assumptions about the errors. 20.5 Summary table Situation Model R syntax One continuous predictor Simple linear regression lm(y ~ x) Multiple continuous predictors Multiple regression lm(y ~ x1 + x2) Continuous + categorical predictors ANCOVA lm(y ~ x + factor) Interaction between predictors Interaction model lm(y ~ x1 * x2) Curved relationship Polynomial regression lm(y ~ x + I(x^2)) 20.6 Key assumptions and checks Assumption Check with If violated Linearity Residuals vs. fitted plot Transform X, add polynomial, or use GAM Normality QQ plot, Shapiro-Wilk Transform Y or use GLM Homoscedasticity Scale-location plot, ncvTest Transform Y, use weighted regression, or GLM Independence Study design Use mixed models No influential outliers Cook’s distance Investigate, run sensitivity analysis No multicollinearity VIF Remove redundant predictors, use PCA 20.7 Key takeaways Regression describes relationships between continuous variables and enables prediction R² quantifies explanatory power — how much variation is explained by the model Always visualize first — plot your data before running statistics Always check assumptions — diagnostic plots reveal problems that summary statistics miss Multiple regression controls for confounders — each coefficient is a partial effect Regression is a linear model — the same framework underlies t-tests, ANOVA, and GLMs 20.8 Assignment 20.8.1 Part 1: Conceptual questions Explain the difference between a confidence interval and a prediction interval. When would you use each? You fit a regression and get R² = 0.95 with p &lt; 0.001. Your colleague says “Canopy openness explains 95% of seedling growth, so nothing else matters.” What’s wrong with this interpretation? In multiple regression, the coefficient for soil nitrogen is 2.35. What does this number mean? Be specific about what’s being held constant. 20.8.2 Part 2: Simple regression analysis Analyze the relationship between precipitation and plant cover: # Annual precipitation (mm) and vegetation cover (%) precipitation &lt;- c(150, 200, 250, 180, 320, 280, 220, 350, 190, 270, 300, 175, 230, 290, 340, 210, 260, 195, 310, 240) vegetation_cover &lt;- c(12, 18, 28, 15, 52, 42, 22, 58, 17, 38, 48, 14, 25, 45, 55, 20, 35, 16, 50, 30) Complete the following: 1. Create a scatterplot and describe the relationship 2. State hypotheses 3. Fit a linear regression model 4. Check all assumptions with appropriate plots 5. Interpret the slope, intercept, and R² 6. Calculate and interpret the 95% CI for the slope 7. Predict vegetation cover at 275 mm precipitation (with confidence interval) 8. Create a publication-quality figure 9. Write complete methods and results sections 20.8.3 Part 3: Multiple regression (optional challenge) Add a second predictor (temperature) to the above model: # Add mean annual temperature temperature &lt;- c(18, 22, 20, 19, 24, 23, 21, 25, 19, 22, 24, 18, 20, 23, 25, 20, 22, 19, 24, 21) Fit a model with both precipitation and temperature as predictors Check for multicollinearity (VIF) Compare R² between the simple and multiple regression models Interpret both coefficients Write a results statement for the multiple regression 20.8.4 Part 4: Reflection In 2-3 sentences, explain why visualizing data before running a regression is essential. What problems might you catch that wouldn’t be obvious from the statistical output alone? "],["analyzing-categorical-data-chi-square-and-related-tests.html", "Chapter 21 Analyzing Categorical Data: Chi-square and Related Tests 21.1 The core question 21.2 Types of chi-square tests 21.3 Setup", " Chapter 21 Analyzing Categorical Data: Chi-square and Related Tests When both your response and explanatory variables are categorical, you need methods designed for counts and proportions. This chapter covers the chi-square family of tests—tools for asking whether categorical variables are associated with each other, or whether observed frequencies match expected patterns. These tests are fundamentally different from t-tests and regression: instead of comparing means or fitting lines, we compare observed counts to expected counts. If what we observe differs substantially from what we’d expect under the null hypothesis, we conclude there’s an association. 21.1 The core question Chi-square tests ask: Are these categorical variables independent, or are they associated? Independence means that knowing one variable tells you nothing about the other. Association means that the categories of one variable are linked to the categories of the other. Example: Is bird nest success (successful/failed) independent of habitat type (forest/grassland/wetland)? If independent, the proportion of successful nests should be similar across all habitats. If associated, success rates differ by habitat. 21.2 Types of chi-square tests Test Question Data structure Chi-square test of independence Are two categorical variables associated? Contingency table (rows × columns) Chi-square goodness of fit Do observed frequencies match expected frequencies? One categorical variable G-test Same as chi-square (alternative calculation) Either structure Fisher’s exact test Same as independence test (for small samples) 2×2 or small tables 21.3 Setup library(tidyverse) library(DescTools) # For G-test, Cramér&#39;s V library(vcd) # For mosaic plots library(epitools) # For odds ratios set.seed(42) "],["part-1-chi-square-test-of-independence.html", "Chapter 22 Part 1: Chi-square Test of Independence 22.1 The question 22.2 Ecological example: Nest success and habitat type", " Chapter 22 Part 1: Chi-square Test of Independence 22.1 The question “Are these two categorical variables associated, or are they independent?” 22.2 Ecological example: Nest success and habitat type You’re studying songbird nest success across three habitat types. You monitored 180 nests and recorded whether each nest successfully fledged young (yes/no) and the habitat type (forest/grassland/wetland). Research question: Does nest success depend on habitat type? # Create the contingency table # Rows: Outcome (Success/Failure) # Columns: Habitat type nest_table &lt;- matrix(c(35, 25, 18, # Successful nests 15, 35, 52), # Failed nests nrow = 2, byrow = TRUE, dimnames = list( Outcome = c(&quot;Success&quot;, &quot;Failure&quot;), Habitat = c(&quot;Forest&quot;, &quot;Grassland&quot;, &quot;Wetland&quot;))) nest_table ## Habitat ## Outcome Forest Grassland Wetland ## Success 35 25 18 ## Failure 15 35 52 # Convert to data frame for some visualizations nest_df &lt;- as.data.frame(as.table(nest_table)) names(nest_df) &lt;- c(&quot;Outcome&quot;, &quot;Habitat&quot;, &quot;Count&quot;) nest_df ## Outcome Habitat Count ## 1 Success Forest 35 ## 2 Failure Forest 15 ## 3 Success Grassland 25 ## 4 Failure Grassland 35 ## 5 Success Wetland 18 ## 6 Failure Wetland 52 Let’s calculate the success rates in each habitat: # Success rate by habitat success_rates &lt;- data.frame( Habitat = c(&quot;Forest&quot;, &quot;Grassland&quot;, &quot;Wetland&quot;), Success = c(35, 25, 18), Total = c(50, 60, 70), Rate = c(35/50, 25/60, 18/70) ) success_rates$Percent &lt;- round(success_rates$Rate * 100, 1) success_rates ## Habitat Success Total Rate Percent ## 1 Forest 35 50 0.7000000 70.0 ## 2 Grassland 25 60 0.4166667 41.7 ## 3 Wetland 18 70 0.2571429 25.7 Forest has 70% nest success, grassland 42%, and wetland only 26%. These look different—but could this pattern arise by chance? 22.2.1 Hypotheses H₀: Nest success is independent of habitat type (success rates are equal across habitats) H_A: Nest success is associated with habitat type (success rates differ among habitats) 22.2.2 The logic: Observed vs. expected Chi-square tests compare what we observed to what we’d expect if the null hypothesis were true. Under independence, we expect the success rate to be the same in all habitats. The overall success rate is: total_success &lt;- sum(nest_table[&quot;Success&quot;, ]) total_nests &lt;- sum(nest_table) overall_rate &lt;- total_success / total_nests overall_rate ## [1] 0.4333333 Overall, 43.3% of nests succeeded. If habitat doesn’t matter, we’d expect 43.3% success in forest, grassland, AND wetland. Expected counts are calculated as: \\[E_{ij} = \\frac{(\\text{Row total}_i) \\times (\\text{Column total}_j)}{\\text{Grand total}}\\] # Calculate expected counts manually row_totals &lt;- rowSums(nest_table) col_totals &lt;- colSums(nest_table) grand_total &lt;- sum(nest_table) expected &lt;- outer(row_totals, col_totals) / grand_total expected ## Forest Grassland Wetland ## Success 21.66667 26 30.33333 ## Failure 28.33333 34 39.66667 Compare observed to expected: - Forest: Observed 35 successes, expected 21.7 (more than expected!) - Wetland: Observed 18 successes, expected 30.3 (fewer than expected!) 22.2.3 The chi-square statistic The chi-square statistic measures the total discrepancy between observed and expected: \\[\\chi^2 = \\sum \\frac{(O - E)^2}{E}\\] Large values indicate observed counts deviate substantially from expected. # Calculate chi-square manually chi_sq_manual &lt;- sum((nest_table - expected)^2 / expected) chi_sq_manual ## [1] 23.3969 22.2.4 Run the test # Chi-square test of independence chi_result &lt;- chisq.test(nest_table) chi_result ## ## Pearson&#39;s Chi-squared test ## ## data: nest_table ## X-squared = 23.397, df = 2, p-value = 8.307e-06 22.2.5 Interpret the output Component Value Meaning X-squared 19.73 Chi-square statistic df 2 Degrees of freedom: (rows-1) × (cols-1) = (2-1) × (3-1) p-value 5.2e-05 Probability of this result if variables were independent Conclusion: p &lt; 0.001, so we reject H₀. Nest success is significantly associated with habitat type. 22.2.6 Examine the pattern: Which cells drive the result? The overall test tells us there’s an association, but not where. Residuals show which cells deviate most from expected: # Standardized residuals (Pearson residuals) chi_result$residuals ## Habitat ## Outcome Forest Grassland Wetland ## Success 2.864459 -0.1961161 -2.239342 ## Failure -2.504897 0.1714986 1.958248 # Standardized residuals &gt; |2| indicate significant deviation Reading residuals: - Positive residual: More observations than expected - Negative residual: Fewer observations than expected - |Residual| &gt; 2: This cell contributes substantially to the chi-square Interpretation: - Forest has more successful nests than expected (+2.9) and fewer failures (-2.4) - Wetland has fewer successful nests than expected (-2.2) and more failures (+1.9) - Grassland is close to expected 22.2.7 Check assumptions Chi-square tests assume: Independence: Observations are independent (each nest counted once) Expected cell counts ≥ 5: All expected values should be at least 5 # Check expected counts chi_result$expected ## Habitat ## Outcome Forest Grassland Wetland ## Success 21.66667 26 30.33333 ## Failure 28.33333 34 39.66667 All expected counts are &gt; 5, so the assumption is met. If expected counts are too small: Use Fisher’s exact test (covered below) or combine categories. 22.2.8 Effect size: Cramér’s V Statistical significance doesn’t tell us how strong the association is. Cramér’s V measures effect size for chi-square tests: \\[V = \\sqrt{\\frac{\\chi^2}{n \\times (k - 1)}}\\] where k is the minimum of rows or columns. # Cramér&#39;s V CramerV(nest_table) ## [1] 0.3605312 # Or manually n &lt;- sum(nest_table) k &lt;- min(nrow(nest_table), ncol(nest_table)) cramers_v &lt;- sqrt(chi_result$statistic / (n * (k - 1))) names(cramers_v) &lt;- &quot;Cramér&#39;s V&quot; cramers_v ## Cramér&#39;s V ## 0.3605312 Interpretation of Cramér’s V: - V &lt; 0.1: Negligible association - V = 0.1–0.3: Small association - V = 0.3–0.5: Medium association - V &gt; 0.5: Large association Our V = 0.33 indicates a medium-strength association between habitat and nest success. 22.2.9 Visualize 22.2.9.1 Bar plot ggplot(success_rates, aes(x = Habitat, y = Percent, fill = Habitat)) + geom_col(width = 0.6) + geom_text(aes(label = paste0(Percent, &quot;%&quot;)), vjust = -0.5, size = 4) + scale_fill_manual(values = c(&quot;forestgreen&quot;, &quot;goldenrod&quot;, &quot;steelblue&quot;)) + scale_y_continuous(limits = c(0, 85), expand = c(0, 0)) + labs(x = &quot;Habitat Type&quot;, y = &quot;Nest Success Rate (%)&quot;, title = &quot;Nest Success by Habitat&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 22.1: Figure 1. Nest success rates differ significantly among habitat types (χ² = 19.7, p &lt; 0.001). Forest had the highest success rate (70%), while wetland had the lowest (26%). 22.2.9.2 Mosaic plot Mosaic plots show both proportions AND sample sizes: # Using vcd package mosaic(nest_table, shade = TRUE, legend = TRUE, main = &quot;Nest Success by Habitat&quot;, labeling = labeling_border(rot_labels = c(0, 0, 0, 0))) Figure 22.2: Mosaic plot of nest success by habitat. Tile width represents sample size (more nests monitored in wetland); tile height represents proportion in each outcome category. Shading indicates residuals (blue = more than expected, red = fewer). 22.2.10 Sample methods and results Methods: We tested whether nest success was associated with habitat type using a chi-square test of independence. We monitored nests (n = 180) across three habitat types (forest, n = 50; grassland, n = 60; wetland, n = 70) and recorded whether each nest successfully fledged at least one young. We verified that all expected cell counts exceeded 5 and calculated Cramér’s V as a measure of effect size. Standardized residuals were examined to identify which habitat types deviated from expected patterns. All analyses were performed in R version 4.3.1. Results: Nest success was significantly associated with habitat type (χ² = 19.7, df = 2, p &lt; 0.001, Cramér’s V = 0.33; Fig. 1). Forest had the highest nest success rate (70%), followed by grassland (42%) and wetland (26%). Standardized residuals indicated that forest had significantly more successful nests than expected (residual = +2.9), while wetland had significantly fewer (residual = −2.2). These results suggest that habitat type is an important predictor of reproductive success in this system. "],["part-2-chi-square-goodness-of-fit.html", "Chapter 23 Part 2: Chi-square Goodness of Fit 23.1 The question 23.2 Ecological example: Seed dispersal distances", " Chapter 23 Part 2: Chi-square Goodness of Fit 23.1 The question “Do observed frequencies match expected frequencies based on theory, prior data, or hypothesized proportions?” This test involves one categorical variable and compares observed counts to specified expected proportions. 23.2 Ecological example: Seed dispersal distances You’re studying seed dispersal by wind. Based on a dispersal kernel model, you predict that seeds should land in four distance classes with the following proportions: 0–5m (50%), 5–10m (30%), 10–20m (15%), &gt;20m (5%). You collected 200 seeds and recorded their distances: # Observed counts observed &lt;- c(120, 48, 24, 8) distance_class &lt;- c(&quot;0-5m&quot;, &quot;5-10m&quot;, &quot;10-20m&quot;, &quot;&gt;20m&quot;) names(observed) &lt;- distance_class # Expected proportions (from model) expected_prop &lt;- c(0.50, 0.30, 0.15, 0.05) observed ## 0-5m 5-10m 10-20m &gt;20m ## 120 48 24 8 Research question: Do observed dispersal distances match the predicted distribution? 23.2.1 Hypotheses H₀: Observed frequencies match expected proportions (model is correct) H_A: Observed frequencies differ from expected proportions (model needs revision) 23.2.2 Run the test # Chi-square goodness of fit gof_result &lt;- chisq.test(observed, p = expected_prop) gof_result ## ## Chi-squared test for given probabilities ## ## data: observed ## X-squared = 8, df = 3, p-value = 0.04601 # Compare observed to expected data.frame( Distance = distance_class, Observed = observed, Expected = gof_result$expected, Residual = round(gof_result$residuals, 2) ) ## Distance Observed Expected Residual ## 0-5m 0-5m 120 100 2.00 ## 5-10m 5-10m 48 60 -1.55 ## 10-20m 10-20m 24 30 -1.10 ## &gt;20m &gt;20m 8 10 -0.63 23.2.3 Interpret χ² = 9.6, df = 3, p = 0.022 The observed distribution differs significantly from the model prediction. Looking at residuals: - 0–5m: Observed 120, expected 100 (+2.0 residual) — more short-distance dispersal than predicted - 5–10m: Observed 48, expected 60 (−1.5 residual) — somewhat fewer - 10–20m: Observed 24, expected 30 (−1.1 residual) — somewhat fewer - &gt;20m: Observed 8, expected 10 (−0.6 residual) — close to expected 23.2.4 Visualize gof_df &lt;- data.frame( Distance = factor(distance_class, levels = distance_class), Observed = observed, Expected = gof_result$expected ) ggplot(gof_df, aes(x = Distance)) + geom_col(aes(y = Observed), fill = &quot;steelblue&quot;, alpha = 0.7, width = 0.6) + geom_point(aes(y = Expected), color = &quot;firebrick&quot;, size = 4) + geom_line(aes(y = Expected, group = 1), color = &quot;firebrick&quot;, linewidth = 1) + labs(x = &quot;Distance Class&quot;, y = &quot;Number of Seeds&quot;, title = &quot;Observed vs. Expected Seed Dispersal&quot;, subtitle = &quot;Bars = observed, points/line = expected from model&quot;) + theme_minimal() Figure 23.1: Observed seed dispersal distances compared to model predictions. Observed frequencies (bars) deviate significantly from expected (line), with more short-distance dispersal than predicted. 23.2.5 Results statement Observed seed dispersal distances differed significantly from model predictions (χ² = 9.6, df = 3, p = 0.022). Seeds were more concentrated in the 0–5 m class than expected (observed: 60%, expected: 50%), suggesting the dispersal kernel may underestimate short-distance dispersal. "],["part-3-alternative-tests.html", "Chapter 24 Part 3: Alternative Tests 24.1 G-test (Likelihood Ratio Test) 24.2 Fisher’s Exact Test", " Chapter 24 Part 3: Alternative Tests 24.1 G-test (Likelihood Ratio Test) The G-test is an alternative to chi-square that uses the log-likelihood ratio: \\[G = 2 \\sum O \\times \\ln\\left(\\frac{O}{E}\\right)\\] G-tests and chi-square tests usually give similar results. G-tests are slightly preferred for: - Smaller sample sizes - When you want to decompose complex tables - Consistency with other likelihood-based methods # G-test for our nest data GTest(nest_table) ## ## Log likelihood ratio (G-test) test of independence without correction ## ## data: nest_table ## G = 23.927, X-squared df = 2, p-value = 6.372e-06 # Compare to chi-square chisq.test(nest_table) ## ## Pearson&#39;s Chi-squared test ## ## data: nest_table ## X-squared = 23.397, df = 2, p-value = 8.307e-06 Results are very similar: both highly significant. 24.2 Fisher’s Exact Test When expected cell counts are small (&lt; 5), chi-square approximations may be unreliable. Fisher’s exact test calculates exact probabilities without relying on approximations. 24.2.1 Example: Rare species occurrence You’re studying whether a rare orchid is associated with a specific mycorrhizal fungus. You surveyed 25 sites: # Small sample contingency table orchid_table &lt;- matrix(c(8, 2, # Fungus present: orchid present/absent 3, 12), # Fungus absent: orchid present/absent nrow = 2, byrow = TRUE, dimnames = list( Fungus = c(&quot;Present&quot;, &quot;Absent&quot;), Orchid = c(&quot;Present&quot;, &quot;Absent&quot;))) orchid_table ## Orchid ## Fungus Present Absent ## Present 8 2 ## Absent 3 12 # Check expected counts chisq.test(orchid_table)$expected ## Orchid ## Fungus Present Absent ## Present 4.4 5.6 ## Absent 6.6 8.4 Some expected counts are below 5, so chi-square may not be reliable. # Fisher&#39;s exact test fisher.test(orchid_table) ## ## Fisher&#39;s Exact Test for Count Data ## ## data: orchid_table ## p-value = 0.005139 ## alternative hypothesis: true odds ratio is not equal to 1 ## 95 percent confidence interval: ## 1.65093 202.72675 ## sample estimates: ## odds ratio ## 13.77195 24.2.2 Interpret Fisher’s output Component Value Meaning p-value 0.0028 Exact probability of this (or more extreme) association odds ratio 14.1 Odds of orchid presence are 14× higher when fungus is present 95% CI [2.0, 144.7] Confidence interval for odds ratio Conclusion: Orchid presence is significantly associated with fungus presence (p = 0.003). The wide confidence interval reflects the small sample size. "],["part-4-22-tables-and-odds-ratios.html", "Chapter 25 Part 4: 2×2 Tables and Odds Ratios 25.1 Understanding odds and odds ratios", " Chapter 25 Part 4: 2×2 Tables and Odds Ratios For 2×2 contingency tables, the odds ratio is a particularly useful measure of effect size. 25.1 Understanding odds and odds ratios Odds = probability of event / probability of non-event = p / (1-p) Odds ratio = odds in group 1 / odds in group 2 # Our orchid-fungus table orchid_table ## Orchid ## Fungus Present Absent ## Present 8 2 ## Absent 3 12 # Calculate odds ratio manually # Odds of orchid when fungus present: 8/2 = 4 # Odds of orchid when fungus absent: 3/12 = 0.25 # Odds ratio: 4 / 0.25 = 16 # Using epitools oddsratio(orchid_table) ## Odds ratio | 95% CI ## --------------------------- ## 16 | [2.16, 118.27] Interpretation of odds ratios: - OR = 1: No association (odds are equal) - OR &gt; 1: Odds are higher in the first group (positive association) - OR &lt; 1: Odds are lower in the first group (negative association) Our OR = 16 means orchids are 16 times more likely (in terms of odds) to be present when the fungus is present than when absent. 25.1.1 Visualize 2×2 tables orchid_df &lt;- as.data.frame(as.table(orchid_table)) names(orchid_df) &lt;- c(&quot;Fungus&quot;, &quot;Orchid&quot;, &quot;Count&quot;) # Calculate proportions orchid_summary &lt;- orchid_df %&gt;% group_by(Fungus) %&gt;% mutate(Total = sum(Count), Proportion = Count / Total) %&gt;% filter(Orchid == &quot;Present&quot;) ggplot(orchid_summary, aes(x = Fungus, y = Proportion * 100, fill = Fungus)) + geom_col(width = 0.5) + geom_text(aes(label = paste0(round(Proportion * 100), &quot;%&quot;)), vjust = -0.5, size = 5) + scale_fill_manual(values = c(&quot;coral&quot;, &quot;forestgreen&quot;)) + scale_y_continuous(limits = c(0, 100), expand = c(0, 0)) + labs(x = &quot;Mycorrhizal Fungus&quot;, y = &quot;Orchid Occurrence (%)&quot;, title = &quot;Orchid-Fungus Association&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) Figure 25.1: Association between mycorrhizal fungus presence and orchid occurrence. Sites with the fungus present have much higher orchid occurrence (80%) than sites without (20%). "],["part-5-common-pitfalls-and-solutions.html", "Chapter 26 Part 5: Common Pitfalls and Solutions 26.1 Pitfall 1: Low expected counts 26.2 Pitfall 2: Non-independent observations 26.3 Pitfall 3: Confusing proportions with counts 26.4 Pitfall 4: Testing the wrong hypothesis", " Chapter 26 Part 5: Common Pitfalls and Solutions 26.1 Pitfall 1: Low expected counts Problem: Chi-square approximation fails when expected counts &lt; 5. Solutions: 1. Use Fisher’s exact test (for 2×2 tables) 2. Combine categories to increase counts 3. Use exact tests or simulation-based p-values # Simulation-based p-value (works for any table size) chisq.test(orchid_table, simulate.p.value = TRUE, B = 10000) ## ## Pearson&#39;s Chi-squared test with simulated p-value (based on 10000 replicates) ## ## data: orchid_table ## X-squared = 8.7662, df = NA, p-value = 0.005499 26.2 Pitfall 2: Non-independent observations Problem: Same individual counted multiple times, or hierarchical data structure. Example: Counting behaviors from 10 birds, with 50 observations per bird. You have 500 observations, but only 10 independent units. Solutions: 1. Use individual as the sampling unit (summarize per individual first) 2. Use mixed models with individual as random effect 3. Use generalized estimating equations (GEE) 26.3 Pitfall 3: Confusing proportions with counts Problem: Chi-square tests require counts, not proportions or percentages. Wrong: # DON&#39;T DO THIS - proportions instead of counts wrong_data &lt;- c(0.70, 0.42, 0.26) # Success rates chisq.test(wrong_data) Right: # DO THIS - actual counts right_data &lt;- matrix(c(35, 25, 18, 15, 35, 52), nrow = 2) chisq.test(right_data) 26.4 Pitfall 4: Testing the wrong hypothesis Chi-square test of independence asks: “Are the variables associated?” It does NOT ask: - “Which groups differ?” (need post-hoc tests or pairwise comparisons) - “How much do they differ?” (need effect sizes like Cramér’s V or odds ratio) - “Is the association causal?” (observational data cannot establish causation) "],["part-6-connection-to-glms.html", "Chapter 27 Part 6: Connection to GLMs", " Chapter 27 Part 6: Connection to GLMs Chi-square tests for 2×2 tables are closely related to logistic regression. Both ask whether a binary outcome depends on a categorical predictor. # Our nest success data as individual observations set.seed(123) nest_individual &lt;- data.frame( habitat = rep(c(&quot;Forest&quot;, &quot;Grassland&quot;, &quot;Wetland&quot;), c(50, 60, 70)), success = c(rep(c(1, 0), c(35, 15)), # Forest rep(c(1, 0), c(25, 35)), # Grassland rep(c(1, 0), c(18, 52))) # Wetland ) # Logistic regression logistic_model &lt;- glm(success ~ habitat, data = nest_individual, family = binomial) summary(logistic_model) ## ## Call: ## glm(formula = success ~ habitat, family = binomial, data = nest_individual) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.8473 0.3086 2.746 0.00604 ** ## habitatGrassland -1.1838 0.4047 -2.925 0.00345 ** ## habitatWetland -1.9082 0.4123 -4.628 3.7e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 246.32 on 179 degrees of freedom ## Residual deviance: 222.40 on 177 degrees of freedom ## AIC: 228.4 ## ## Number of Fisher Scoring iterations: 4 # Compare to chi-square # Likelihood ratio test anova(logistic_model, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: success ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 179 246.32 ## habitat 2 23.927 177 222.40 6.372e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The logistic regression gives essentially the same result as the chi-square test, but with more flexibility: - Can include continuous predictors - Can include multiple predictors - Provides coefficients (log-odds) for each level We’ll explore logistic regression fully in the GLM chapter. "],["part-7-summary.html", "Chapter 28 Part 7: Summary 28.1 Which test to use? 28.2 Assumptions checklist 28.3 Reporting checklist 28.4 Sample methods and results templates 28.5 Key takeaways 28.6 Assignment", " Chapter 28 Part 7: Summary 28.1 Which test to use? Situation Test R function Two categorical variables (adequate sample) Chi-square test of independence chisq.test(table) Two categorical variables (small expected counts) Fisher’s exact test fisher.test(table) One categorical variable vs. expected proportions Chi-square goodness of fit chisq.test(x, p = expected) Alternative to chi-square G-test GTest(table) (DescTools) 2×2 table effect size Odds ratio oddsratio(table) (epitools) Any table effect size Cramér’s V CramerV(table) (DescTools) 28.2 Assumptions checklist Assumption Check If violated Independence Study design Analyze at appropriate level Expected counts ≥ 5 chisq.test()$expected Use Fisher’s exact or combine categories Random sampling Study design Be cautious about generalization 28.3 Reporting checklist Include: 1. Sample size (total and per category) 2. Test used and why (chi-square, G-test, or Fisher’s) 3. Test statistic, df, and p-value 4. Effect size (Cramér’s V or odds ratio) 5. Description of the pattern (which categories differ?) 6. Figure showing proportions or counts 28.4 Sample methods and results templates 28.4.1 Chi-square test of independence Methods: We tested whether [variable 1] was associated with [variable 2] using a chi-square test of independence. We recorded [variable 1] (categories: …) and [variable 2] (categories: …) for [n] observations. We verified that all expected cell counts exceeded 5 and calculated Cramér’s V as a measure of effect size. [Alternative: We used Fisher’s exact test because some expected counts were below 5.] Analyses were performed in R version 4.3.1. Results: [Variable 1] was significantly associated with [variable 2] (χ² = [value], df = [value], p = [value], Cramér’s V = [value]; Fig. X). [Describe pattern: which categories showed higher/lower proportions than expected]. Standardized residuals indicated that [specific cells with |residual| &gt; 2] deviated significantly from expected values. 28.4.2 Goodness of fit Methods: We tested whether observed frequencies of [variable] matched expected proportions using a chi-square goodness-of-fit test. Expected proportions were [source: theory/prior study/hypothesis]. Results: Observed frequencies differed significantly from expected proportions (χ² = [value], df = [value], p = [value]). [Describe pattern: which categories were over/under-represented]. 28.4.3 Fisher’s exact test with odds ratio Methods: We tested whether [binary outcome] was associated with [binary predictor] using Fisher’s exact test due to low expected cell counts. We calculated the odds ratio with 95% confidence interval. Results: [Outcome] was significantly associated with [predictor] (Fisher’s exact test: p = [value]). The odds of [outcome] were [X] times higher when [predictor condition] (OR = [value], 95% CI: [lower]–[upper]). 28.5 Key takeaways Chi-square tests compare observed to expected counts — they ask whether patterns deviate from what chance would predict Test of independence for two categorical variables; goodness of fit for one variable against expected proportions Always check expected cell counts — use Fisher’s exact test if any are below 5 Effect sizes matter: Cramér’s V (general) or odds ratio (2×2 tables) quantify association strength Residuals reveal the pattern — they show which cells drive significant results Chi-square connects to logistic regression — both analyze categorical outcomes, but GLMs offer more flexibility 28.6 Assignment 28.6.1 Part 1: Conceptual questions Explain the difference between a chi-square test of independence and a chi-square goodness-of-fit test. When would you use each? You run a chi-square test and get p = 0.03 with Cramér’s V = 0.08. What do you conclude? Is this result scientifically meaningful? Your contingency table has an expected count of 3.2 in one cell. What should you do? 28.6.2 Part 2: Test of independence A botanist studied whether flower color polymorphism (white/pink/purple) varies between two populations: # Flower color counts by population flower_table &lt;- matrix(c(45, 30, 25, # Population A 20, 35, 45), # Population B nrow = 2, byrow = TRUE, dimnames = list( Population = c(&quot;Pop_A&quot;, &quot;Pop_B&quot;), Color = c(&quot;White&quot;, &quot;Pink&quot;, &quot;Purple&quot;))) flower_table ## Color ## Population White Pink Purple ## Pop_A 45 30 25 ## Pop_B 20 35 45 Complete the following: 1. State null and alternative hypotheses 2. Check expected cell counts 3. Run the chi-square test 4. Calculate Cramér’s V 5. Examine residuals to identify which cells drive the result 6. Create a publication-quality figure 7. Write complete methods and results sections 28.6.3 Part 3: Goodness of fit According to Mendelian genetics, a cross should produce offspring in a 9:3:3:1 phenotypic ratio. A researcher observed the following counts: # Observed offspring phenotypes observed_phenotypes &lt;- c(315, 108, 101, 32) names(observed_phenotypes) &lt;- c(&quot;A_B_&quot;, &quot;A_bb&quot;, &quot;aaB_&quot;, &quot;aabb&quot;) observed_phenotypes ## A_B_ A_bb aaB_ aabb ## 315 108 101 32 # Expected ratio expected_ratio &lt;- c(9, 3, 3, 1) / 16 State hypotheses Run the goodness-of-fit test Compare observed to expected counts Interpret: Do the data support Mendelian expectations? Write a results statement 28.6.4 Part 4: Small sample analysis An ecologist studied predation on marked prey items: # Predation by prey coloration (small sample) predation_table &lt;- matrix(c(2, 8, # Cryptic: eaten/survived 9, 3), # Conspicuous: eaten/survived nrow = 2, byrow = TRUE, dimnames = list( Coloration = c(&quot;Cryptic&quot;, &quot;Conspicuous&quot;), Fate = c(&quot;Eaten&quot;, &quot;Survived&quot;))) predation_table ## Fate ## Coloration Eaten Survived ## Cryptic 2 8 ## Conspicuous 9 3 Check expected counts and explain why Fisher’s exact test is needed Run Fisher’s exact test Calculate and interpret the odds ratio Write a results statement including the odds ratio 28.6.5 Part 5: Reflection In 2-3 sentences, explain why effect sizes (like Cramér’s V or odds ratios) are important to report alongside p-values in chi-square analyses. What information do they provide that p-values don’t? "],["generalized-linear-models-glms.html", "Chapter 29 Generalized Linear Models (GLMs) 29.1 The problem with “classic” methods 29.2 The GLM framework 29.3 Setup", " Chapter 29 Generalized Linear Models (GLMs) SARA ADD IN PREVIOUS CHAPTER THAT YOU FOUND - NOW LABELED OLD In the previous chapters, you learned three fundamental approaches to statistical analysis: Comparing groups (t-tests, ANOVA) — Is there a difference between categories? Testing relationships (Regression) — How does Y change with X? Analyzing categorical data (Chi-square) — Are categorical variables associated? Here’s the secret: these are all special cases of a single framework—the Generalized Linear Model. Understanding GLMs gives you a unified approach to almost any statistical question, plus the ability to handle data that violate traditional assumptions. 29.1 The problem with “classic” methods Traditional linear models (ANOVA, regression) assume: Normal distribution of residuals Constant variance (homoscedasticity) Linear relationship between predictors and response But ecological data often violate these assumptions: Data type The problem Counts (offspring, species, visits) Can’t be negative; often right-skewed; variance increases with mean Proportions (survival rate, germination %) Bounded between 0 and 1 Binary (present/absent, alive/dead) Only two possible values Time to event (days to flowering) Positive only; often skewed The old solution was data transformation (log, square root, arcsine). But transformations are awkward—you have to back-transform for interpretation, confidence intervals become asymmetric, and sometimes nothing works. GLMs solve this elegantly by letting you specify the appropriate error distribution for your data type. 29.2 The GLM framework A GLM has three components: 29.2.1 1. The linear predictor (η) This is the familiar linear model part: \\[\\eta = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + ... + \\beta_p X_p\\] This is exactly what you learned in regression. The coefficients (β) describe how predictors relate to the response. 29.2.2 2. The error distribution (family) Instead of assuming normal errors, you specify a distribution that matches your data: Data type Distribution R family Continuous, unbounded Normal/Gaussian gaussian() Counts Poisson poisson() Binary (0/1) Binomial binomial() Proportions Binomial (with weights) binomial() Positive continuous, skewed Gamma Gamma() 29.2.3 3. The link function (g) The link function connects the linear predictor to the expected value of the response: \\[g(\\mu) = \\eta\\] or equivalently: \\[\\mu = g^{-1}(\\eta)\\] where μ is the expected value (mean) of the response variable. Why do we need link functions? The linear predictor η can take any value from -∞ to +∞. But: - Counts can’t be negative - Probabilities must be between 0 and 1 - Positive measurements can’t go below zero Link functions transform the expected value to ensure predictions make sense for your data type. 29.3 Setup library(tidyverse) library(car) # For Anova() with Type II tests library(performance) # For model diagnostics library(DHARMa) # For GLM residual diagnostics library(emmeans) # For estimated marginal means library(AER) # For dispersion tests set.seed(42) "],["part-1-everything-you-know-is-a-glm.html", "Chapter 30 Part 1: Everything you know is a GLM 30.1 ANOVA as a GLM 30.2 Regression as a GLM", " Chapter 30 Part 1: Everything you know is a GLM Let’s prove that ANOVA and regression are special cases of GLMs. 30.1 ANOVA as a GLM # Create example data: Specific Leaf Area in 3 populations sla_data &lt;- data.frame( Population = factor(rep(c(&quot;Flagstaff&quot;, &quot;Sedona&quot;, &quot;Camp Verde&quot;), each = 10)), SLA = c(rnorm(10, 15, 2), # Flagstaff: mean = 15 rnorm(10, 22, 2), # Sedona: mean = 22 rnorm(10, 18, 2)) # Camp Verde: mean = 18 ) # Classic ANOVA anova_classic &lt;- aov(SLA ~ Population, data = sla_data) summary(anova_classic) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 2 165.8 82.92 13.25 9.78e-05 *** ## Residuals 27 169.0 6.26 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Same analysis as GLM glm_anova &lt;- glm(SLA ~ Population, family = gaussian(), data = sla_data) Anova(glm_anova) ## Analysis of Deviance Table (Type II tests) ## ## Response: SLA ## LR Chisq Df Pr(&gt;Chisq) ## Population 26.503 2 1.757e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Same analysis as lm() lm_anova &lt;- lm(SLA ~ Population, data = sla_data) summary(lm_anova) ## ## Call: ## lm(formula = SLA ~ Population, data = sla_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9860 -1.2679 -0.2361 1.6352 4.9002 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 17.6438 0.7911 22.304 &lt; 2e-16 *** ## PopulationFlagstaff -1.5492 1.1187 -1.385 0.17744 ## PopulationSedona 4.0292 1.1187 3.602 0.00126 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.502 on 27 degrees of freedom ## Multiple R-squared: 0.4954, Adjusted R-squared: 0.458 ## F-statistic: 13.25 on 2 and 27 DF, p-value: 9.779e-05 The results are identical! All three approaches give the same F-statistic and p-value. 30.2 Regression as a GLM # Example: Seedling height vs. light availability seedling_data &lt;- data.frame( light = runif(30, 10, 90), height = NA ) seedling_data$height &lt;- 5 + 0.3 * seedling_data$light + rnorm(30, 0, 4) # Classic regression lm_model &lt;- lm(height ~ light, data = seedling_data) summary(lm_model) ## ## Call: ## lm(formula = height ~ light, data = seedling_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.769 -2.542 1.064 2.306 4.716 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.32021 1.50065 2.879 0.00756 ** ## light 0.33054 0.03065 10.783 1.78e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.781 on 28 degrees of freedom ## Multiple R-squared: 0.8059, Adjusted R-squared: 0.799 ## F-statistic: 116.3 on 1 and 28 DF, p-value: 1.775e-11 # Same as GLM with Gaussian family glm_model &lt;- glm(height ~ light, family = gaussian(), data = seedling_data) summary(glm_model) ## ## Call: ## glm(formula = height ~ light, family = gaussian(), data = seedling_data) ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 4.32021 1.50065 2.879 0.00756 ** ## light 0.33054 0.03065 10.783 1.78e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for gaussian family taken to be 14.29795) ## ## Null deviance: 2062.89 on 29 degrees of freedom ## Residual deviance: 400.34 on 28 degrees of freedom ## AIC: 168.87 ## ## Number of Fisher Scoring iterations: 2 # Compare coefficients - they&#39;re identical cbind(lm = coef(lm_model), glm = coef(glm_model)) ## lm glm ## (Intercept) 4.3202075 4.3202075 ## light 0.3305415 0.3305415 Key insight: When you use family = gaussian() (normal distribution) with the identity link, a GLM is exactly equivalent to lm(). The power of GLMs comes when we change the family and link function. "],["part-2-understanding-link-functions.html", "Chapter 31 Part 2: Understanding Link Functions 31.1 The identity link (Gaussian GLM) 31.2 The log link (Poisson GLM for counts) 31.3 The logit link (Binomial GLM for proportions/binary)", " Chapter 31 Part 2: Understanding Link Functions Link functions are the heart of GLMs. Let’s understand them deeply. 31.1 The identity link (Gaussian GLM) For normal data, the link function is the identity—it does nothing: \\[g(\\mu) = \\mu\\] The expected value equals the linear predictor directly: \\[\\mu = \\beta_0 + \\beta_1 X\\] This is just ordinary regression. 31.2 The log link (Poisson GLM for counts) For count data, we use the log link: \\[g(\\mu) = \\ln(\\mu)\\] Which means: \\[\\ln(\\mu) = \\beta_0 + \\beta_1 X\\] To get predictions on the original scale, we back-transform using the inverse link (exponential): \\[\\mu = e^{\\beta_0 + \\beta_1 X}\\] Why log link for counts? Counts must be ≥ 0, and exp() always gives positive values Effects are multiplicative: a one-unit increase in X multiplies the count by \\(e^{\\beta_1}\\) Variance naturally increases with the mean (Poisson assumption) 31.3 The logit link (Binomial GLM for proportions/binary) For binary or proportion data, we use the logit link: \\[g(\\pi) = \\ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\text{logit}(\\pi)\\] Where π is the probability of success. The quantity π/(1-π) is called the odds. The linear predictor relates to the log-odds: \\[\\ln\\left(\\frac{\\pi}{1-\\pi}\\right) = \\beta_0 + \\beta_1 X\\] To get predictions (probabilities), we use the inverse logit: \\[\\pi = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\\] This is the famous S-shaped (sigmoid) logistic curve! 31.3.1 Demonstrating the logit transformation Let’s see how the link function works step by step—this is how you might have plotted logistic regression in Excel: # Create a sequence of probabilities prob &lt;- seq(0.01, 0.99, by = 0.01) # Calculate odds and log-odds odds &lt;- prob / (1 - prob) log_odds &lt;- log(odds) # Plot the transformation par(mfrow = c(1, 3)) # Panel 1: Probability scale (S-curve will map to this) plot(log_odds, prob, type = &quot;l&quot;, lwd = 2, col = &quot;steelblue&quot;, xlab = &quot;Log-odds (linear predictor η)&quot;, ylab = &quot;Probability&quot;, main = &quot;Inverse Logit\\n(what we plot)&quot;) # Panel 2: Odds scale plot(prob, odds, type = &quot;l&quot;, lwd = 2, col = &quot;coral&quot;, xlab = &quot;Probability&quot;, ylab = &quot;Odds&quot;, main = &quot;Probability to Odds&quot;) # Panel 3: Log-odds scale (where we fit the linear model) plot(prob, log_odds, type = &quot;l&quot;, lwd = 2, col = &quot;forestgreen&quot;, xlab = &quot;Probability&quot;, ylab = &quot;Log-odds&quot;, main = &quot;Logit Transform\\n(makes it linear)&quot;) Figure 31.1: The logit link function transforms probabilities (bounded 0-1) to the log-odds scale (unbounded), where we can fit a linear model. The inverse logit transforms predictions back to probabilities. par(mfrow = c(1, 1)) 31.3.2 Creating a logistic curve from coefficients Here’s the Excel-style approach: once you have coefficients, you can calculate the predicted probability at any X value: # Suppose our logistic regression gives us: beta_0 &lt;- -3 # Intercept beta_1 &lt;- 0.1 # Slope # For any X, calculate the linear predictor (log-odds) x_values &lt;- seq(0, 100, by = 1) linear_predictor &lt;- beta_0 + beta_1 * x_values # Apply inverse logit to get probability # Method 1: The formula probability &lt;- exp(linear_predictor) / (1 + exp(linear_predictor)) # Method 2: Equivalent formula (more numerically stable) probability2 &lt;- 1 / (1 + exp(-linear_predictor)) # Method 3: Use R&#39;s built-in function probability3 &lt;- plogis(linear_predictor) # All three methods give the same result head(cbind(probability, probability2, probability3)) ## probability probability2 probability3 ## [1,] 0.04742587 0.04742587 0.04742587 ## [2,] 0.05215356 0.05215356 0.05215356 ## [3,] 0.05732418 0.05732418 0.05732418 ## [4,] 0.06297336 0.06297336 0.06297336 ## [5,] 0.06913842 0.06913842 0.06913842 ## [6,] 0.07585818 0.07585818 0.07585818 # Plot the curve plot_data &lt;- data.frame(x = x_values, probability = probability) ggplot(plot_data, aes(x = x, y = probability)) + geom_line(color = &quot;steelblue&quot;, linewidth = 1.5) + geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + geom_vline(xintercept = -beta_0/beta_1, linetype = &quot;dashed&quot;, color = &quot;coral&quot;) + annotate(&quot;text&quot;, x = -beta_0/beta_1 + 5, y = 0.55, label = paste(&quot;X at P=0.5:&quot;, -beta_0/beta_1), hjust = 0) + labs(x = &quot;X variable&quot;, y = &quot;Probability&quot;, title = &quot;Logistic Curve from Coefficients&quot;, subtitle = paste(&quot;π = 1/(1 + exp(-(&quot;, beta_0, &quot;+&quot;, beta_1, &quot;× X)))&quot;)) + scale_y_continuous(limits = c(0, 1)) + theme_minimal() Figure 31.2: Building a logistic curve from coefficients. Once you know β₀ and β₁, you can calculate the predicted probability at any X value using the inverse logit function. Key insight: The X value where probability = 0.5 is where the log-odds = 0, which occurs at X = -β₀/β₁. In this example, that’s X = 30. "],["part-3-logistic-regression-binomial-glm.html", "Chapter 32 Part 3: Logistic Regression (Binomial GLM) 32.1 When to use it 32.2 Ecological example: Plant survival along a moisture gradient", " Chapter 32 Part 3: Logistic Regression (Binomial GLM) 32.1 When to use it Use logistic regression when your response is: - Binary: Success/failure, alive/dead, present/absent (coded as 1/0) - Proportion: Number of successes out of total trials 32.2 Ecological example: Plant survival along a moisture gradient You transplanted seedlings across a soil moisture gradient and recorded survival after one year. # Simulated survival data moisture &lt;- c(5, 8, 12, 15, 18, 22, 25, 28, 32, 35, 38, 42, 45, 48, 52, 55, 58, 62, 65, 68, 72, 75, 78, 82, 85, 88, 92, 95, 98, 100) # True relationship: survival increases with moisture (logistic curve) set.seed(123) log_odds_true &lt;- -4 + 0.08 * moisture prob_true &lt;- plogis(log_odds_true) survival &lt;- rbinom(30, 1, prob_true) plant_data &lt;- data.frame(moisture, survival) # Look at the data table(plant_data$survival) ## ## 0 1 ## 13 17 32.2.1 Visualize the raw data ggplot(plant_data, aes(x = moisture, y = survival)) + geom_jitter(height = 0.05, width = 0, size = 3, alpha = 0.6) + labs(x = &quot;Soil Moisture (%)&quot;, y = &quot;Survival (1 = alive, 0 = dead)&quot;, title = &quot;Seedling Survival Across Moisture Gradient&quot;) + theme_minimal() Figure 32.1: Survival (1) and mortality (0) across a soil moisture gradient. Points are jittered vertically to show overlapping observations. 32.2.2 Fit the model # Logistic regression survival_model &lt;- glm(survival ~ moisture, family = binomial(link = &quot;logit&quot;), data = plant_data) summary(survival_model) ## ## Call: ## glm(formula = survival ~ moisture, family = binomial(link = &quot;logit&quot;), ## data = plant_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.97103 0.95196 -2.071 0.0384 * ## moisture 0.04387 0.01728 2.539 0.0111 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 41.054 on 29 degrees of freedom ## Residual deviance: 32.378 on 28 degrees of freedom ## AIC: 36.378 ## ## Number of Fisher Scoring iterations: 4 32.2.3 Interpret the output 32.2.3.1 Coefficients on the log-odds scale coef(survival_model) ## (Intercept) moisture ## -1.97103451 0.04386725 Intercept (-3.29): The log-odds of survival when moisture = 0. Convert to probability: plogis(-3.29) = 0.036 (3.6% survival at 0% moisture) Slope (0.067): For each 1% increase in moisture, the log-odds of survival increases by 0.067. This isn’t intuitive. Let’s convert to odds ratio. 32.2.3.2 Converting to odds ratios # Odds ratios exp(coef(survival_model)) ## (Intercept) moisture ## 0.1393127 1.0448436 # With confidence intervals exp(confint(survival_model)) ## 2.5 % 97.5 % ## (Intercept) 0.01666365 0.7715562 ## moisture 1.01362217 1.0866933 Interpretation: For each 1% increase in soil moisture, the odds of survival are multiplied by 1.07 (increase by 7%). For a 10% increase in moisture: exp(coef(survival_model)[&quot;moisture&quot;] * 10) ## moisture ## 1.550647 The odds of survival nearly double (1.95×) for every 10% increase in moisture. 32.2.4 Test significance # Likelihood ratio test (preferred) Anova(survival_model, type = &quot;II&quot;) ## Analysis of Deviance Table (Type II tests) ## ## Response: survival ## LR Chisq Df Pr(&gt;Chisq) ## moisture 8.6762 1 0.003224 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Or use drop1 drop1(survival_model, test = &quot;Chisq&quot;) ## Single term deletions ## ## Model: ## survival ~ moisture ## Df Deviance AIC LRT Pr(&gt;Chi) ## &lt;none&gt; 32.378 36.378 ## moisture 1 41.054 43.054 8.6762 0.003224 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 32.2.5 Generate predictions # Create prediction data pred_data &lt;- data.frame(moisture = seq(0, 100, by = 1)) # Predict on the link scale (log-odds) pred_data$log_odds &lt;- predict(survival_model, newdata = pred_data, type = &quot;link&quot;) # Predict on the response scale (probability) pred_data$probability &lt;- predict(survival_model, newdata = pred_data, type = &quot;response&quot;) # Add confidence intervals pred_link &lt;- predict(survival_model, newdata = pred_data, type = &quot;link&quot;, se.fit = TRUE) pred_data$prob_lower &lt;- plogis(pred_link$fit - 1.96 * pred_link$se.fit) pred_data$prob_upper &lt;- plogis(pred_link$fit + 1.96 * pred_link$se.fit) head(pred_data) ## moisture log_odds probability prob_lower prob_upper ## 1 0 -1.971035 0.1222778 0.02110580 0.4737259 ## 2 1 -1.927167 0.1270645 0.02269050 0.4771466 ## 3 2 -1.883300 0.1320103 0.02438808 0.4806024 ## 4 3 -1.839433 0.1371184 0.02620572 0.4840946 ## 5 4 -1.795566 0.1423917 0.02815094 0.4876247 ## 6 5 -1.751698 0.1478331 0.03023157 0.4911944 32.2.6 Visualize with fitted curve ggplot() + # Confidence ribbon geom_ribbon(data = pred_data, aes(x = moisture, ymin = prob_lower, ymax = prob_upper), fill = &quot;steelblue&quot;, alpha = 0.3) + # Fitted curve geom_line(data = pred_data, aes(x = moisture, y = probability), color = &quot;steelblue&quot;, linewidth = 1.2) + # Observed data geom_jitter(data = plant_data, aes(x = moisture, y = survival), height = 0.03, width = 0, size = 2.5, alpha = 0.6) + # Reference lines geom_hline(yintercept = 0.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + labs(x = &quot;Soil Moisture (%)&quot;, y = &quot;Survival Probability&quot;, title = &quot;Seedling Survival vs. Soil Moisture&quot;) + scale_y_continuous(limits = c(0, 1)) + theme_minimal() Figure 32.2: Figure 1. Seedling survival probability increases significantly with soil moisture (logistic regression: χ² = 9.8, p = 0.002). Points show observed outcomes (jittered); curve shows fitted probability with 95% CI. 32.2.7 Calculate the LD50 / ED50 The moisture level at which survival probability = 50%: # At P = 0.5, log-odds = 0 # So: 0 = β₀ + β₁X # X = -β₀/β₁ ld50 &lt;- -coef(survival_model)[1] / coef(survival_model)[2] names(ld50) &lt;- &quot;LD50&quot; ld50 ## LD50 ## 44.9318 # Verify predict(survival_model, newdata = data.frame(moisture = ld50), type = &quot;response&quot;) ## LD50 ## 0.5 At 49% soil moisture, seedlings have a 50% chance of survival. 32.2.8 Sample methods and results Methods: We modeled seedling survival as a function of soil moisture using logistic regression (GLM with binomial family and logit link). Seedlings (n = 30) were transplanted across a moisture gradient (5–100%) and survival was recorded after one year. We tested the significance of moisture using a likelihood ratio test and calculated odds ratios with 95% confidence intervals. The moisture level corresponding to 50% survival probability was calculated as −β₀/β₁. All analyses were performed in R version 4.3.1. Results: Survival probability increased significantly with soil moisture (logistic regression: χ² = 9.8, df = 1, p = 0.002; Fig. 1). For every 10% increase in soil moisture, the odds of survival nearly doubled (OR = 1.95, 95% CI: 1.24–3.47). At low moisture (10%), predicted survival was only 11% (95% CI: 2–41%), while at high moisture (90%), survival reached 97% (95% CI: 78–100%). The moisture level corresponding to 50% survival probability was 49%. "],["part-4-poisson-regression-count-data.html", "Chapter 33 Part 4: Poisson Regression (Count Data) 33.1 When to use it 33.2 Ecological example: Pollinator visits and flower display", " Chapter 33 Part 4: Poisson Regression (Count Data) 33.1 When to use it Use Poisson regression when your response is: - Counts: Non-negative integers (0, 1, 2, 3, …) - Examples: Number of offspring, species counts, flower visits, disease cases The Poisson distribution assumes: - Mean = Variance (we’ll check this!) - Counts are independent 33.2 Ecological example: Pollinator visits and flower display You observed pollinator visitation at 40 plants with varying flower display sizes. # Simulated count data set.seed(456) flowers &lt;- rpois(40, lambda = 15) # Number of flowers per plant flowers[flowers &lt; 3] &lt;- 3 # Minimum 3 flowers # Visits increase with flower number (log-linear relationship) log_visits &lt;- 0.5 + 0.08 * flowers + rnorm(40, 0, 0.3) visits &lt;- rpois(40, lambda = exp(log_visits)) pollinator_data &lt;- data.frame(flowers, visits) # Summary summary(pollinator_data) ## flowers visits ## Min. : 9.00 Min. : 1.00 ## 1st Qu.:13.75 1st Qu.: 3.75 ## Median :15.00 Median : 5.50 ## Mean :15.78 Mean : 6.55 ## 3rd Qu.:18.00 3rd Qu.: 9.00 ## Max. :23.00 Max. :16.00 33.2.1 Visualize ggplot(pollinator_data, aes(x = flowers, y = visits)) + geom_point(size = 3, alpha = 0.7, color = &quot;purple&quot;) + labs(x = &quot;Number of Flowers&quot;, y = &quot;Pollinator Visits&quot;, title = &quot;Pollinator Visitation vs. Floral Display&quot;) + theme_minimal() Figure 33.1: Pollinator visits increase with flower display size. The relationship appears curved because counts are on a multiplicative scale. 33.2.2 Fit the model # Poisson regression visit_model &lt;- glm(visits ~ flowers, family = poisson(link = &quot;log&quot;), data = pollinator_data) summary(visit_model) ## ## Call: ## glm(formula = visits ~ flowers, family = poisson(link = &quot;log&quot;), ## data = pollinator_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.23363 0.31423 0.743 0.457 ## flowers 0.10085 0.01825 5.526 3.27e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 82.021 on 39 degrees of freedom ## Residual deviance: 52.291 on 38 degrees of freedom ## AIC: 199.51 ## ## Number of Fisher Scoring iterations: 4 33.2.3 Interpret coefficients With the log link, coefficients represent log rate ratios. To interpret: # Raw coefficients (log scale) coef(visit_model) ## (Intercept) flowers ## 0.2336268 0.1008533 # Exponentiated coefficients (rate ratios) exp(coef(visit_model)) ## (Intercept) flowers ## 1.263173 1.106114 # Confidence intervals exp(confint(visit_model)) ## 2.5 % 97.5 % ## (Intercept) 0.6786631 2.327287 ## flowers 1.0670930 1.146271 Interpretation: - For each additional flower, visits are multiplied by 1.075 (7.5% increase) - For 10 additional flowers: exp(0.072 * 10) = 2.05 (visits double!) 33.2.4 Check for overdispersion The Poisson distribution assumes mean = variance. If variance &gt; mean, you have overdispersion, which inflates Type I error rates. # Quick check: residual deviance should ≈ residual df # If deviance &gt;&gt; df, you have overdispersion summary(visit_model)$deviance ## [1] 52.29125 summary(visit_model)$df.residual ## [1] 38 # Dispersion ratio dispersion_ratio &lt;- summary(visit_model)$deviance / summary(visit_model)$df.residual dispersion_ratio ## [1] 1.376086 # Formal test dispersiontest(visit_model) ## ## Overdispersion test ## ## data: visit_model ## z = 1.4116, p-value = 0.07903 ## alternative hypothesis: true dispersion is greater than 1 ## sample estimates: ## dispersion ## 1.343146 Dispersion ratio ≈ 1 suggests no overdispersion. If it were &gt; 1.5 or so, we’d need to address it. 33.2.5 Handling overdispersion If overdispersion is detected: # Option 1: Quasi-Poisson (adjusts standard errors) quasi_model &lt;- glm(visits ~ flowers, family = quasipoisson(link = &quot;log&quot;), data = pollinator_data) # Option 2: Negative binomial (MASS package) library(MASS) nb_model &lt;- glm.nb(visits ~ flowers, data = pollinator_data) 33.2.6 Generate predictions # Create prediction data pred_flowers &lt;- data.frame(flowers = seq(3, 25, by = 0.5)) # Predict on link scale (log counts) pred_link &lt;- predict(visit_model, newdata = pred_flowers, type = &quot;link&quot;, se.fit = TRUE) # Transform to response scale pred_flowers$visits &lt;- exp(pred_link$fit) pred_flowers$lower &lt;- exp(pred_link$fit - 1.96 * pred_link$se.fit) pred_flowers$upper &lt;- exp(pred_link$fit + 1.96 * pred_link$se.fit) 33.2.7 Visualize ggplot() + geom_ribbon(data = pred_flowers, aes(x = flowers, ymin = lower, ymax = upper), fill = &quot;purple&quot;, alpha = 0.3) + geom_line(data = pred_flowers, aes(x = flowers, y = visits), color = &quot;purple&quot;, linewidth = 1.2) + geom_point(data = pollinator_data, aes(x = flowers, y = visits), size = 3, alpha = 0.7) + labs(x = &quot;Number of Flowers&quot;, y = &quot;Pollinator Visits&quot;, title = &quot;Pollinator Visitation vs. Floral Display&quot;) + theme_minimal() Figure 33.2: Figure 2. Pollinator visits increase exponentially with flower display size (Poisson regression: χ² = 81.2, p &lt; 0.001). The curve shows predicted visits with 95% CI. 33.2.8 Sample results Pollinator visitation increased significantly with floral display size (Poisson regression: χ² = 81.2, df = 1, p &lt; 0.001; Fig. 2). Each additional flower increased visitation by 7.5% (rate ratio = 1.075, 95% CI: 1.058–1.092). Plants with 20 flowers received an average of 5.8 visits (95% CI: 4.9–6.9), while plants with 10 flowers received only 2.8 visits (95% CI: 2.5–3.2). "],["part-5-glm-diagnostics.html", "Chapter 34 Part 5: GLM Diagnostics 34.1 Using DHARMa for GLM diagnostics 34.2 Traditional GLM diagnostics", " Chapter 34 Part 5: GLM Diagnostics GLM diagnostics differ from linear model diagnostics because we don’t expect normally distributed residuals. 34.1 Using DHARMa for GLM diagnostics The DHARMa package creates “simulation-based residuals” that ARE expected to be uniform, making diagnostics easier: # Simulate residuals sim_resid &lt;- simulateResiduals(visit_model, n = 1000) # Plot diagnostics plot(sim_resid) Figure 34.1: DHARMa diagnostic plots for the Poisson model. Left: QQ plot of simulated residuals (should follow the line). Right: Residuals vs. predicted (should show no pattern). What to look for: - Left panel (QQ plot): Points should follow the diagonal line - Right panel (Residuals vs predicted): No obvious patterns; dispersion test result shown # Formal tests testDispersion(sim_resid) # Overdispersion test ## ## DHARMa nonparametric dispersion test via sd of residuals fitted vs. simulated ## ## data: simulationOutput ## dispersion = 1.305, p-value = 0.238 ## alternative hypothesis: two.sided testZeroInflation(sim_resid) # Zero-inflation test (for count data) ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = fitted ## model ## ## data: simulationOutput ## ratioObsSim = 0, p-value = 1 ## alternative hypothesis: two.sided 34.2 Traditional GLM diagnostics par(mfrow = c(2, 2)) plot(visit_model) Figure 34.2: Traditional residual plots for GLMs. For non-Gaussian models, interpret these with caution—they won’t show normal residuals even when the model is correct. par(mfrow = c(1, 1)) For GLMs, focus on: 1. Residuals vs Fitted: Look for patterns suggesting wrong link function or missing predictors 2. Cook’s distance: Identify influential observations "],["part-6-model-comparison.html", "Chapter 35 Part 6: Model Comparison 35.1 Comparing models with AIC 35.2 Likelihood ratio tests", " Chapter 35 Part 6: Model Comparison 35.1 Comparing models with AIC AIC (Akaike Information Criterion) balances fit against complexity: \\[AIC = 2k - 2\\ln(L)\\] where k = number of parameters and L = likelihood. Lower AIC is better. A difference of 2+ suggests meaningful difference; 10+ is strong evidence. # Compare models for pollinator data model_null &lt;- glm(visits ~ 1, family = poisson(), data = pollinator_data) model_flowers &lt;- glm(visits ~ flowers, family = poisson(), data = pollinator_data) AIC(model_null, model_flowers) ## df AIC ## model_null 1 227.2434 ## model_flowers 2 199.5141 # Delta AIC AIC(model_null) - AIC(model_flowers) ## [1] 27.72929 The flower model is much better (ΔAIC = 79). 35.2 Likelihood ratio tests To formally test whether a term improves the model: # Compare nested models anova(model_null, model_flowers, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: visits ~ 1 ## Model 2: visits ~ flowers ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 39 82.021 ## 2 38 52.291 1 29.729 4.968e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],["part-7-summary-tables.html", "Chapter 36 Part 7: Summary Tables 36.1 Choosing the right GLM 36.2 Quick reference for interpretation 36.3 Assumptions to check 36.4 Key takeaways 36.5 Assignment", " Chapter 36 Part 7: Summary Tables 36.1 Choosing the right GLM Response type Distribution Link R syntax Continuous (normal) Gaussian Identity family = gaussian() Binary (0/1) Binomial Logit family = binomial() Proportion (with n) Binomial Logit family = binomial(), use weights Counts Poisson Log family = poisson() Overdispersed counts Negative binomial Log MASS::glm.nb() Positive continuous Gamma Log or inverse family = Gamma(link = \"log\") 36.2 Quick reference for interpretation Family Coefficient interpretation Exponentiated coefficient Gaussian Change in Y per unit X — Binomial (logit) Change in log-odds Odds ratio Poisson (log) Change in log-count Rate ratio / incidence rate ratio Gamma (log) Change in log-Y Multiplicative effect 36.3 Assumptions to check Assumption How to check If violated Correct distribution DHARMa QQ plot Try different family Correct link function Residuals vs fitted Try different link Independence Study design Mixed models No overdispersion (Poisson) Dispersion test Use quasi-Poisson or negative binomial No zero-inflation DHARMa zero-inflation test Use zero-inflated models 36.4 Key takeaways GLMs unify statistical methods — t-tests, ANOVA, regression, and logistic regression are all special cases Choose the distribution to match your data — Gaussian for continuous, binomial for binary/proportions, Poisson for counts Link functions make it work — They transform predictions to ensure they make sense for your data type Interpret coefficients carefully — Often need to exponentiate for meaningful interpretation (odds ratios, rate ratios) Check for overdispersion in count models — It’s common and can inflate Type I error Use DHARMa for diagnostics — It creates residuals that are easy to interpret 36.5 Assignment 36.5.1 Part 1: Conceptual questions Explain in your own words what a link function does and why it’s necessary. You fit a Poisson regression and get a coefficient of 0.15 for your predictor. What does this mean? Express your interpretation in terms of a rate ratio. Your Poisson model has residual deviance of 89 with 40 df. Is there evidence of overdispersion? What would you do? 36.5.2 Part 2: Logistic regression Use the following data on frog presence across a pH gradient: # Frog presence/absence data frog_data &lt;- data.frame( pH = c(4.2, 4.5, 4.8, 5.0, 5.3, 5.5, 5.8, 6.0, 6.2, 6.5, 6.8, 7.0, 7.2, 7.5, 7.8, 8.0, 8.2, 8.5, 8.8, 9.0), present = c(0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0) ) Complete: 1. Fit a logistic regression 2. Interpret the coefficient as an odds ratio 3. Calculate the pH at which presence probability = 50% 4. Create a figure with the fitted probability curve 5. Write methods and results sections 36.5.3 Part 3: Poisson regression Use the following data on beetle counts across an elevation gradient: # Beetle count data beetle_data &lt;- data.frame( elevation = seq(1000, 2900, by = 100), beetles = c(45, 52, 48, 55, 62, 58, 53, 47, 42, 38, 35, 28, 25, 22, 18, 15, 12, 10, 8, 5) ) Complete: 1. Fit a Poisson regression 2. Check for overdispersion 3. Interpret the coefficient as a rate ratio 4. Create a figure with the fitted curve 5. Write methods and results sections 36.5.4 Part 4: Reflection In 2-3 sentences, explain how understanding GLMs as a unified framework (rather than separate tests) changes how you think about statistical analysis. "],["zero-inflated-and-hurdle-models.html", "Chapter 37 Zero-Inflated and Hurdle Models 37.1 Why so many zeros? 37.2 Setup", " Chapter 37 Zero-Inflated and Hurdle Models You count pollinators visiting flowers. Most flowers get zero visits. You survey streams for fish. Many reaches have no fish. You measure seed production—half your plants set zero seeds. Welcome to zero-inflation, one of the most common features of ecological count data. Standard count models (Poisson, negative binomial) expect some zeros, but ecological data often have far more zeros than these distributions predict. When you fit a Poisson model to zero-inflated data, you get: Poor model fit Biased parameter estimates Misleading p-values Residual plots that scream “something is wrong” This chapter teaches you to recognize zero-inflation, understand its ecological meaning, and model it appropriately using zero-inflated and hurdle models. 37.1 Why so many zeros? Zeros in ecological data often arise from two different processes: Structural zeros: The species/event cannot occur (unsuitable habitat, wrong season, outside range) Sampling zeros: The species/event could occur but wasn’t detected (low abundance, imperfect detection, bad luck) A standard Poisson model assumes all zeros come from one process—low expected counts. But if some zeros are “true zeros” (the organism simply cannot be there), you need a model that accounts for both processes. 37.2 Setup library(tidyverse) library(glmmTMB) # Flexible models including ZI and hurdle library(DHARMa) # Diagnostics for complex models library(pscl) # ZIP and ZINB models (alternative) library(performance) # Model comparison library(emmeans) # Marginal means set.seed(42) "],["part-1-recognizing-zero-inflation.html", "Chapter 38 Part 1: Recognizing Zero-Inflation 38.1 What does zero-inflation look like? 38.2 Testing for zero-inflation 38.3 Formal test with DHARMa", " Chapter 38 Part 1: Recognizing Zero-Inflation 38.1 What does zero-inflation look like? Let’s simulate data with and without zero-inflation: # Regular Poisson data set.seed(123) regular_poisson &lt;- rpois(500, lambda = 2) # Zero-inflated data: 40% structural zeros + Poisson zi_data &lt;- ifelse(runif(500) &lt; 0.4, 0, rpois(500, lambda = 3)) # Compare distributions par(mfrow = c(1, 2)) hist(regular_poisson, breaks = seq(-0.5, max(regular_poisson) + 0.5, 1), main = &quot;Regular Poisson (λ = 2)&quot;, xlab = &quot;Count&quot;, col = &quot;steelblue&quot;) hist(zi_data, breaks = seq(-0.5, max(zi_data) + 0.5, 1), main = &quot;Zero-Inflated Data&quot;, xlab = &quot;Count&quot;, col = &quot;coral&quot;) Figure 38.1: Comparing regular Poisson data (left) with zero-inflated data (right). Zero-inflated data have a spike at zero beyond what the Poisson predicts. par(mfrow = c(1, 1)) The telltale sign: a spike at zero that’s taller than the model predicts. 38.2 Testing for zero-inflation Compare observed zeros to expected zeros under your model: # Fit Poisson to the ZI data pois_fit &lt;- glm(zi_data ~ 1, family = poisson) # Expected zeros under Poisson lambda_hat &lt;- exp(coef(pois_fit)) expected_zeros &lt;- length(zi_data) * dpois(0, lambda_hat) observed_zeros &lt;- sum(zi_data == 0) cat(&quot;Expected zeros (Poisson):&quot;, round(expected_zeros), &quot;\\n&quot;) ## Expected zeros (Poisson): 82 cat(&quot;Observed zeros:&quot;, observed_zeros, &quot;\\n&quot;) ## Observed zeros: 210 cat(&quot;Ratio (observed/expected):&quot;, round(observed_zeros/expected_zeros, 2), &quot;\\n&quot;) ## Ratio (observed/expected): 2.56 If observed zeros &gt;&gt; expected zeros, you likely have zero-inflation. 38.3 Formal test with DHARMa # Simulate residuals sim_res &lt;- simulateResiduals(pois_fit) # Test for zero-inflation testZeroInflation(sim_res) Figure 38.2: DHARMa diagnostic for zero-inflation. The test compares observed zeros to simulated expectations. ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = fitted ## model ## ## data: simulationOutput ## ratioObsSim = 2.5557, p-value &lt; 2.2e-16 ## alternative hypothesis: two.sided A significant test (p &lt; 0.05) indicates more zeros than expected. "],["part-2-the-conceptual-framework.html", "Chapter 39 Part 2: The Conceptual Framework 39.1 Two processes, one dataset", " Chapter 39 Part 2: The Conceptual Framework 39.1 Two processes, one dataset Zero-inflated models assume your data arise from a mixture of two processes: Binomial process: Determines whether the count is a “structural zero” (probability π) or comes from a count distribution (probability 1-π) Count process: For non-structural zeros, generates the count (Poisson or negative binomial) Figure 39.1: Conceptual diagram of a zero-inflated model. Each observation first passes through a binomial filter (zero vs. potential count), then non-zeros are generated from a count distribution. 39.1.1 Ecological interpretation Think about pollinator visits: Structural zeros: Some flowers are in deep shade, have no nectar, or bloom at the wrong time—pollinators cannot visit them Sampling zeros: Other flowers are perfectly suitable but happened to receive zero visits during your observation window—pollinators could have visited A zero-inflated model lets you estimate: - What predicts whether a flower is “available” to pollinators (binomial part) - What predicts how many visits available flowers receive (count part) "],["part-3-zero-inflated-models.html", "Chapter 40 Part 3: Zero-Inflated Models 40.1 Example: Pollinator visits to flowers 40.2 Fitting a zero-inflated Poisson (ZIP) 40.3 Zero-inflated negative binomial (ZINB) 40.4 Comparing models", " Chapter 40 Part 3: Zero-Inflated Models 40.1 Example: Pollinator visits to flowers # Simulate pollinator visit data n &lt;- 200 # Predictors flower_size &lt;- runif(n, 5, 25) # mm shade &lt;- runif(n, 0, 1) # 0 = full sun, 1 = deep shade nectar &lt;- runif(n, 0, 10) # nectar volume (μL) # Zero-inflation probability (structural zeros) # Flowers in deep shade are more likely to be &quot;unavailable&quot; logit_zi &lt;- -1 + 3 * shade # More shade → more likely to be structural zero prob_zi &lt;- plogis(logit_zi) # Count process (for available flowers) log_lambda &lt;- 0.5 + 0.05 * flower_size + 0.1 * nectar lambda &lt;- exp(log_lambda) # Generate data is_structural_zero &lt;- rbinom(n, 1, prob_zi) visits &lt;- ifelse(is_structural_zero == 1, 0, rpois(n, lambda)) # Combine into data frame pollinator_data &lt;- data.frame( visits, flower_size, shade, nectar ) # Check distribution table(pollinator_data$visits) ## ## 0 1 2 3 4 5 6 7 8 9 10 11 13 14 ## 126 4 6 8 8 10 9 9 5 6 2 5 1 1 cat(&quot;Proportion zeros:&quot;, mean(visits == 0), &quot;\\n&quot;) ## Proportion zeros: 0.63 40.2 Fitting a zero-inflated Poisson (ZIP) # Zero-inflated Poisson using glmmTMB # Formula structure: response ~ count_predictors, zi = ~ zi_predictors zip_model &lt;- glmmTMB(visits ~ flower_size + nectar, ziformula = ~ shade, family = poisson, data = pollinator_data) summary(zip_model) ## Family: poisson ( log ) ## Formula: visits ~ flower_size + nectar ## Zero inflation: ~shade ## Data: pollinator_data ## ## AIC BIC logLik -2*log(L) df.resid ## 577.2 593.7 -283.6 567.2 195 ## ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.513404 0.186492 2.753 0.00591 ** ## flower_size 0.053203 0.008835 6.022 1.73e-09 *** ## nectar 0.089937 0.017737 5.071 3.97e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.4847 0.3072 -1.578 0.114615 ## shade 1.9404 0.5351 3.626 0.000288 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 40.2.1 Interpreting ZIP output The summary has two parts: Conditional model (count process): - Coefficients on log scale (like regular Poisson) - Interpretation: For each 1-unit increase in predictor, log(count) changes by β Zero-inflation model: - Coefficients on logit scale - Interpretation: For each 1-unit increase in predictor, log-odds of being a structural zero changes by β - Positive coefficient = MORE likely to be a structural zero # Count model: effect of flower size # exp(coefficient) = multiplicative effect on count exp(fixef(zip_model)$cond[&quot;flower_size&quot;]) ## flower_size ## 1.054644 # Each 1mm increase in flower size multiplies visits by 1.05 (5% increase) # Zero-inflation model: effect of shade # exp(coefficient) = odds ratio for being a structural zero exp(fixef(zip_model)$zi[&quot;shade&quot;]) ## shade ## 6.961605 # Moving from full sun to full shade multiplies odds of structural zero by ~17 40.3 Zero-inflated negative binomial (ZINB) If your count data are also overdispersed, use negative binomial instead of Poisson: # Zero-inflated negative binomial zinb_model &lt;- glmmTMB(visits ~ flower_size + nectar, ziformula = ~ shade, family = nbinom2, # NB with variance = μ + μ²/θ data = pollinator_data) summary(zinb_model) ## Family: nbinom2 ( log ) ## Formula: visits ~ flower_size + nectar ## Zero inflation: ~shade ## Data: pollinator_data ## ## AIC BIC logLik -2*log(L) df.resid ## 579.2 599.0 -283.6 567.2 194 ## ## ## Dispersion parameter for nbinom2 family (): 1.22e+08 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.513404 0.186492 2.753 0.00591 ** ## flower_size 0.053203 0.008835 6.022 1.73e-09 *** ## nectar 0.089937 0.017737 5.071 3.97e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.4847 0.3072 -1.578 0.114615 ## shade 1.9404 0.5351 3.626 0.000288 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 40.4 Comparing models # Compare Poisson, ZIP, NB, ZINB pois_model &lt;- glmmTMB(visits ~ flower_size + nectar, family = poisson, data = pollinator_data) nb_model &lt;- glmmTMB(visits ~ flower_size + nectar, family = nbinom2, data = pollinator_data) # AIC comparison AIC(pois_model, nb_model, zip_model, zinb_model) ## df AIC ## pois_model 3 1211.3990 ## nb_model 4 698.1159 ## zip_model 5 577.1660 ## zinb_model 6 579.1660 Lower AIC = better fit. The zero-inflated model should fit substantially better if zero-inflation is present. "],["part-4-hurdle-models.html", "Chapter 41 Part 4: Hurdle Models 41.1 How hurdle models differ 41.2 Fitting a hurdle model", " Chapter 41 Part 4: Hurdle Models 41.1 How hurdle models differ Hurdle models also handle excess zeros, but with a different structure: Model Zero source Count source Zero-inflated Structural zeros + count zeros Poisson/NB (including its zeros) Hurdle ALL zeros from one process Truncated Poisson/NB (no zeros) In a hurdle model: 1. Hurdle part: Binary model predicts zero vs. non-zero (did you cross the “hurdle”?) 2. Count part: Truncated count model predicts the positive counts 41.1.1 When to use each Use Zero-Inflated when… Use Hurdle when… Zeros come from two distinct processes All zeros represent the same process Some zeros are “true absences,” others are “failed detection” Being present/absent is a meaningful first step You want to model probability of structural zeros You want to model probability of presence Ecological examples for hurdle: - Species occupancy: First, is the species present? Then, how abundant? - Reproduction: Did the plant reproduce? If yes, how many seeds? - Foraging: Did the animal forage? If yes, how many items consumed? 41.2 Fitting a hurdle model # Hurdle model in glmmTMB # Use truncated Poisson (or truncated NB) for count part hurdle_model &lt;- glmmTMB(visits ~ flower_size + nectar, ziformula = ~ shade, family = truncated_poisson, data = pollinator_data) summary(hurdle_model) ## Family: truncated_poisson ( log ) ## Formula: visits ~ flower_size + nectar ## Zero inflation: ~shade ## Data: pollinator_data ## ## AIC BIC logLik -2*log(L) df.resid ## 576.9 593.4 -283.5 566.9 195 ## ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.505498 0.187986 2.689 0.00717 ** ## flower_size 0.053443 0.008877 6.020 1.74e-09 *** ## nectar 0.090537 0.017802 5.086 3.66e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.4592 0.3038 -1.511 0.130672 ## shade 1.9267 0.5312 3.627 0.000287 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 41.2.1 Interpreting hurdle output Conditional model: Effects on count given that count &gt; 0 (truncated distribution) Zero-inflation model: Despite the name, this is actually modeling the probability of zero (the hurdle). Interpretation is reversed: - Positive coefficient = MORE zeros = LESS likely to cross hurdle - To interpret as “probability of presence,” flip the sign mentally # Probability of crossing hurdle (having any visits) # The zi formula models P(zero), so we want 1 - P(zero) # At shade = 0 (full sun): logit_zero_fullsun &lt;- fixef(hurdle_model)$zi[&quot;(Intercept)&quot;] prob_zero_fullsun &lt;- plogis(logit_zero_fullsun) prob_present_fullsun &lt;- 1 - prob_zero_fullsun cat(&quot;P(any visits) in full sun:&quot;, round(prob_present_fullsun, 2), &quot;\\n&quot;) ## P(any visits) in full sun: 0.61 # At shade = 1 (full shade): logit_zero_shade &lt;- fixef(hurdle_model)$zi[&quot;(Intercept)&quot;] + fixef(hurdle_model)$zi[&quot;shade&quot;] prob_zero_shade &lt;- plogis(logit_zero_shade) prob_present_shade &lt;- 1 - prob_zero_shade cat(&quot;P(any visits) in full shade:&quot;, round(prob_present_shade, 2), &quot;\\n&quot;) ## P(any visits) in full shade: 0.19 "],["part-5-choosing-between-approaches.html", "Chapter 42 Part 5: Choosing Between Approaches 42.1 Decision flowchart 42.2 Practical comparison", " Chapter 42 Part 5: Choosing Between Approaches 42.1 Decision flowchart ## ## COUNT DATA WITH MANY ZEROS ## │ ## ▼ ## ┌──────────────────────────────┐ ## │ Test for overdispersion │ ## │ (variance &gt; mean?) │ ## └──────────────────────────────┘ ## │ ## ┌─────┴─────┐ ## No Yes ## │ │ ## ▼ ▼ ## Poisson Neg. Binomial ## │ │ ## └─────┬─────┘ ## │ ## ▼ ## ┌──────────────────────────────┐ ## │ Test for zero-inflation │ ## │ (more zeros than expected?) │ ## └──────────────────────────────┘ ## │ ## ┌─────┴─────┐ ## No Yes ## │ │ ## ▼ ▼ ## Done! Zero-inflated or ## Hurdle model ## │ ## ▼ ## ┌──────────────────────────────┐ ## │ Are zeros from two processes │ ## │ or one process? │ ## └──────────────────────────────┘ ## │ ## ┌─────┴─────┐ ## Two One ## │ │ ## ▼ ▼ ## ZIP/ Hurdle ## ZINB model 42.2 Practical comparison (#tab:16 model-comparison-table)Comparison of zero-inflated and hurdle models Feature Zero-Inflated Hurdle Zero process P(structural zero) P(zero vs non-zero) Count process Poisson or NB Truncated Poisson/NB Zeros in count part? Yes (can be zero) No (always &gt; 0) Best when… Two sources of zeros One source of zeros Interpretation P(unavailable) and count P(presence) and count|present "],["part-6-diagnostics.html", "Chapter 43 Part 6: Diagnostics 43.1 DHARMa for zero-inflated models 43.2 Check zero-inflation 43.3 Check dispersion 43.4 Compare observed vs predicted", " Chapter 43 Part 6: Diagnostics 43.1 DHARMa for zero-inflated models DHARMa works beautifully for diagnosing ZI and hurdle models: # Simulate residuals for ZI model sim_res_zi &lt;- simulateResiduals(zip_model) # Standard diagnostic plots plot(sim_res_zi) Figure 43.1: DHARMa residual diagnostics for the zero-inflated model. Residuals should be uniform; deviations indicate model misspecification. 43.2 Check zero-inflation # After fitting ZI model, zeros should no longer be inflated testZeroInflation(sim_res_zi) ## ## DHARMa zero-inflation test via comparison to expected zeros with simulation under H0 = fitted ## model ## ## data: simulationOutput ## ratioObsSim = 0.9969, p-value = 1 ## alternative hypothesis: two.sided A non-significant test after fitting a ZI model suggests you’ve adequately accounted for zero-inflation. 43.3 Check dispersion # Check for over/underdispersion testDispersion(sim_res_zi) ## ## DHARMa nonparametric dispersion test via sd of residuals fitted vs. simulated ## ## data: simulationOutput ## dispersion = 0.98991, p-value = 0.96 ## alternative hypothesis: two.sided 43.4 Compare observed vs predicted # Rootogram: visual comparison of observed vs expected frequencies # Using base functions since rootogram isn&#39;t always available observed &lt;- table(factor(pollinator_data$visits, levels = 0:max(pollinator_data$visits))) predicted &lt;- predict(zip_model, type = &quot;response&quot;) # Simple comparison barplot(observed, main = &quot;Observed Count Distribution&quot;, xlab = &quot;Visits&quot;, ylab = &quot;Frequency&quot;, col = &quot;steelblue&quot;) Figure 43.2: Rootogram comparing observed frequencies (bars) to predicted frequencies (red line). Bars hanging below the line indicate underprediction; above the line indicates overprediction. "],["part-7-complete-example-with-mixed-effects.html", "Chapter 44 Part 7: Complete Example with Mixed Effects 44.1 Extract variance components 44.2 Estimated marginal means", " Chapter 44 Part 7: Complete Example with Mixed Effects Real ecological data often have hierarchical structure. glmmTMB handles zero-inflation with random effects: # Simulate hierarchical data n_sites &lt;- 15 n_per_site &lt;- 20 n_total &lt;- n_sites * n_per_site # Site random effects site &lt;- rep(1:n_sites, each = n_per_site) site_effect_count &lt;- rep(rnorm(n_sites, 0, 0.5), each = n_per_site) site_effect_zi &lt;- rep(rnorm(n_sites, 0, 0.3), each = n_per_site) # Fixed effects treatment &lt;- rep(c(&quot;Control&quot;, &quot;Fertilized&quot;), length.out = n_total) treatment_effect &lt;- ifelse(treatment == &quot;Fertilized&quot;, 0.5, 0) # Generate data log_lambda &lt;- 1 + treatment_effect + site_effect_count prob_zi &lt;- plogis(-0.5 + site_effect_zi - 0.8 * (treatment == &quot;Fertilized&quot;)) is_zero &lt;- rbinom(n_total, 1, prob_zi) counts &lt;- ifelse(is_zero, 0, rpois(n_total, exp(log_lambda))) hierarchical_data &lt;- data.frame( counts, treatment = factor(treatment), site = factor(site) ) # Fit zero-inflated mixed model zi_mixed &lt;- glmmTMB(counts ~ treatment + (1|site), ziformula = ~ treatment + (1|site), family = poisson, data = hierarchical_data) summary(zi_mixed) ## Family: poisson ( log ) ## Formula: counts ~ treatment + (1 | site) ## Zero inflation: ~treatment + (1 | site) ## Data: hierarchical_data ## ## AIC BIC logLik -2*log(L) df.resid ## 1243.6 1265.8 -615.8 1231.6 294 ## ## Random effects: ## ## Conditional model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.2494 0.4994 ## Number of obs: 300, groups: site, 15 ## ## Zero-inflation model: ## Groups Name Variance Std.Dev. ## site (Intercept) 0.09452 0.3074 ## Number of obs: 300, groups: site, 15 ## ## Conditional model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.14888 0.14336 8.014 1.11e-15 *** ## treatmentFertilized 0.55033 0.07123 7.726 1.11e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Zero-inflation model: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.4669 0.2027 -2.303 0.0213 * ## treatmentFertilized -0.8668 0.2791 -3.105 0.0019 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 44.1 Extract variance components # Random effects variance VarCorr(zi_mixed) ## ## Conditional model: ## Groups Name Std.Dev. ## site (Intercept) 0.49938 ## ## Zero-inflation model: ## Groups Name Std.Dev. ## site (Intercept) 0.30744 44.2 Estimated marginal means # Marginal means from the count model emmeans(zi_mixed, ~ treatment, type = &quot;response&quot;) ## treatment rate SE df asymp.LCL asymp.UCL ## Control 3.15 0.452 Inf 2.38 4.18 ## Fertilized 5.47 0.743 Inf 4.19 7.14 ## ## Confidence level used: 0.95 ## Intervals are back-transformed from the log scale "],["part-8-reporting.html", "Chapter 45 Part 8: Reporting 45.1 What to report 45.2 Sample methods and results 45.3 Common mistakes to avoid 45.4 Key takeaways 45.5 Assignment", " Chapter 45 Part 8: Reporting 45.1 What to report Why you used ZI/hurdle: Proportion of zeros, test for zero-inflation Model structure: Distribution family, predictors in each part Both model components: Results for count AND zero-inflation parts Model fit: AIC comparison, residual diagnostics Effect sizes: On appropriate scale (rate ratios for count, odds ratios for ZI) 45.2 Sample methods and results 45.2.1 Methods We modeled pollinator visit counts using a zero-inflated Poisson regression to account for excess zeros in our data (observed zeros: 45%, expected under Poisson: 18%). The conditional (count) model included flower size and nectar volume as predictors. The zero-inflation model included shade as a predictor, reflecting our hypothesis that shaded flowers are less likely to be available to pollinators. Models were fit using glmmTMB (Brooks et al. 2017) with model selection based on AIC. Residual diagnostics were assessed using DHARMa (Hartig 2022). For comparison, we also fit standard Poisson, negative binomial, and hurdle models. 45.2.2 Results The zero-inflated Poisson model fit substantially better than alternatives (ΔAIC &gt; 15 vs. Poisson and negative binomial). In the count component, both flower size (β = 0.051, SE = 0.008, p &lt; 0.001) and nectar volume (β = 0.098, SE = 0.021, p &lt; 0.001) were positively associated with visit frequency. Each 1-mm increase in flower size was associated with a 5.2% increase in visits (rate ratio = 1.052, 95% CI: 1.036–1.068). In the zero-inflation component, shade significantly increased the probability of a flower being unavailable to pollinators (β = 2.86, SE = 0.45, p &lt; 0.001). Flowers in full shade had 17.4 times the odds (95% CI: 7.2–42.1) of being in the “structural zero” state compared to flowers in full sun (Fig. X). DHARMa diagnostics indicated adequate model fit with no residual zero-inflation (p = 0.42). 45.3 Common mistakes to avoid Adding 1 and log-transforming: This biases estimates and doesn’t address zero-inflation Using Poisson when overdispersed AND zero-inflated: May need ZINB, not just ZIP Same predictors in both parts by default: Think about what predicts presence vs. abundance Ignoring the zero-inflation coefficients: The ZI part often has important biological meaning Not checking if ZI was resolved: Use DHARMa to confirm the model adequately handles zeros 45.4 Key takeaways Zero-inflation is biological — It often reflects two processes: availability and abundance Test for it — Compare observed zeros to expected; use DHARMa’s testZeroInflation ZIP vs ZINB — Choose based on overdispersion in the count part ZI vs Hurdle — ZI for two zero-generating processes; hurdle for one Interpret both parts — The zero-inflation model has biological meaning glmmTMB is your friend — Handles ZI, hurdle, mixed effects, and more Validate with DHARMa — Check that your model resolved the zero-inflation 45.5 Assignment 45.5.1 Part 1: Conceptual questions What’s the difference between a “structural zero” and a “sampling zero”? Give an ecological example of each. You fit a ZIP model and the zero-inflation coefficient for temperature is positive. What does this mean biologically? When would you choose a hurdle model over a zero-inflated model? Describe a specific ecological scenario. 45.5.2 Part 2: Recognizing zero-inflation Using this simulated dataset: set.seed(456) mystery_data &lt;- data.frame( counts = c(rpois(150, 3), rep(0, 100)), # Mixed data predictor = rnorm(250) ) Calculate the proportion of zeros Fit a Poisson model and compare expected vs. observed zeros Use DHARMa to test for zero-inflation What do you conclude? 45.5.3 Part 3: Model fitting Using this dataset of seed counts per plant: set.seed(789) n &lt;- 180 herbivory &lt;- runif(n, 0, 10) plant_size &lt;- rnorm(n, 50, 10) # True model: herbivory increases zeros, plant size increases counts prob_zero &lt;- plogis(-2 + 0.4 * herbivory) lambda &lt;- exp(1 + 0.02 * plant_size) is_zero &lt;- rbinom(n, 1, prob_zero) seeds &lt;- ifelse(is_zero, 0, rpois(n, lambda)) seed_data &lt;- data.frame(seeds, herbivory, plant_size) Fit a standard Poisson model Fit a zero-inflated Poisson with herbivory predicting ZI and plant_size predicting counts Compare models using AIC Interpret the coefficients from both parts of the ZI model Check diagnostics with DHARMa 45.5.4 Part 4: Model comparison Fit both a ZIP and a hurdle model to the seed data: Compare AIC Compare coefficient estimates Which model would you choose, and why? Write a results paragraph for your chosen model 45.5.5 Part 5: Reflection In 2-3 sentences, explain why simply removing zeros or transforming the data (log + 1) is problematic compared to using a zero-inflated model. "],["mixed-effects-models.html", "Chapter 46 Mixed Effects Models 46.1 Why mixed models matter 46.2 Setup", " Chapter 46 Mixed Effects Models In the real world, ecological data are rarely independent. Plants within a plot share soil conditions. Repeated measurements on the same individual are correlated. Sites within a region have similar climate. These dependencies violate the independence assumption of GLMs—and ignoring them can lead to inflated Type I error rates and misleading conclusions. Mixed effects models (also called hierarchical models or multilevel models) solve this by explicitly modeling the non-independence structure of your data. They’re called “mixed” because they contain both: Fixed effects: The variables you’re actually interested in (treatments, predictors) Random effects: Grouping factors that create dependency structure (blocks, individuals, sites) This chapter connects common ecological study designs to their corresponding mixed model structures. By the end, you’ll be able to recognize which random effects structure matches your study design and implement it correctly. 46.1 Why mixed models matter Consider this scenario: You measure plant height in 5 plots, with 20 plants per plot, under two treatments (control vs. fertilized). You have 100 observations—but do you really have 100 independent data points? No! Plants within the same plot share soil, microclimate, and potentially genetics. Your effective sample size is closer to 5 plots (or 10, if treatment is applied at the plot level), not 100 plants. If you run a standard t-test or ANOVA treating all 100 plants as independent, you’ll get: - Artificially small standard errors - Inflated test statistics - False positives (Type I errors) Mixed models account for within-group correlation, giving you honest p-values and appropriate uncertainty estimates. 46.2 Setup library(tidyverse) library(lme4) # Core mixed models package library(lmerTest) # Adds p-values to lmer output library(emmeans) # Estimated marginal means and post-hoc tests library(performance) # Model diagnostics library(MuMIn) # R-squared for mixed models library(nlme) # Alternative mixed models (correlation structures) library(lattice) # dotplot() for random effects visualization set.seed(42) "],["part-1-fixed-vs.-random-effects.html", "Chapter 47 Part 1: Fixed vs. Random Effects 47.1 Fixed effects 47.2 Random effects 47.3 The gray area", " Chapter 47 Part 1: Fixed vs. Random Effects 47.1 Fixed effects Fixed effects are the variables whose specific levels you’re interested in and that you would replicate in future studies. Examples: - Treatment (control vs. fertilized) — you’d use the same treatments again - Species (oak vs. pine vs. maple) — you chose these species deliberately - Temperature (ambient vs. +2°C vs. +4°C) — specific levels of interest Key question: Would you use these exact same levels in a future study? 47.2 Random effects Random effects are grouping factors that introduce correlation structure but whose specific levels are: - Not of primary interest - Sampled from a larger population of possible levels - Not replicable (you wouldn’t use the exact same blocks/sites/individuals) Examples: - Block — you wouldn’t replicate the exact same physical blocks - Site — your 10 sites are a sample of all possible sites - Individual — repeated measures on randomly selected individuals - Year — your 3 study years are a sample of possible years Key question: Are these specific levels a sample from a larger population of possible levels? 47.3 The gray area Some factors could be treated either way: Factor Fixed if… Random if… Site You chose sites for specific characteristics Sites are a sample of possible sites Year Specific years are of interest (El Niño years) Years are a sample of possible years Species You’re comparing these specific species Species are a sample from a community Rule of thumb: If you have few levels (2-5) of a factor and want to make inferences about those specific levels, treat it as fixed. If you have many levels (5+) sampled from a population and want to generalize, treat it as random. "],["part-2-design-model-structure.html", "Chapter 48 Part 2: Design → Model Structure", " Chapter 48 Part 2: Design → Model Structure The random effects structure in your model should match your study design. Here’s a guide: Design Dependency structure Random effect syntax Randomized complete block Obs within blocks (1|block) Nested/hierarchical Plots within sites (1|site/plot) or (1|site) + (1|site:plot) Crossed All sites measured in all years (1|site) + (1|year) Repeated measures Multiple measures per subject (1|subject) Longitudinal Repeated measures over time (1|subject) + possibly (time|subject) Random slopes Effect of X varies by group (1 + x|group) Let’s work through each design with examples. "],["part-3-randomized-complete-block-design.html", "Chapter 49 Part 3: Randomized Complete Block Design 49.1 The design 49.2 The model", " Chapter 49 Part 3: Randomized Complete Block Design 49.1 The design You have an environmental gradient or spatial heterogeneity, so you group experimental units into blocks of similar conditions. Each treatment appears once per block. Example: Testing 4 grass varieties across 6 field blocks (areas with similar soil/drainage). # Simulated oat variety trial data # 8 varieties × 5 blocks = 40 observations variety_data &lt;- expand.grid( variety = paste0(&quot;V&quot;, 1:8), block = paste0(&quot;Block&quot;, 1:5) ) # True variety effects (V1 is reference) variety_effects &lt;- c(V1 = 300, V2 = 320, V3 = 280, V4 = 350, V5 = 310, V6 = 290, V7 = 340, V8 = 305) # Block effects (random, representing field heterogeneity) block_effects &lt;- c(Block1 = -20, Block2 = 15, Block3 = 5, Block4 = -10, Block5 = 10) # Generate yields variety_data$yield &lt;- variety_effects[variety_data$variety] + block_effects[variety_data$block] + rnorm(40, 0, 15) # Look at the data head(variety_data) ## variety block yield ## 1 V1 Block1 300.5644 ## 2 V2 Block1 291.5295 ## 3 V3 Block1 265.4469 ## 4 V4 Block1 339.4929 ## 5 V5 Block1 296.0640 ## 6 V6 Block1 268.4081 # Visualize ggplot(variety_data, aes(x = variety, y = yield, color = block, group = block)) + geom_point(size = 3) + geom_line(alpha = 0.5) + labs(x = &quot;Variety&quot;, y = &quot;Yield (g)&quot;, title = &quot;Oat Variety Trial&quot;) + theme_minimal() Notice how the lines (connecting blocks) are roughly parallel—blocks shift yields up or down, but variety differences are consistent across blocks. 49.2 The model Fixed effect: Variety (we want to compare these specific 8 varieties) Random effect: Block (we don’t care about these specific blocks; they represent field heterogeneity) # Mixed model with random intercept for block rcbd_model &lt;- lmer(yield ~ variety + (1|block), data = variety_data) summary(rcbd_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: yield ~ variety + (1 | block) ## Data: variety_data ## ## REML criterion at convergence: 289.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.9468 -0.3384 0.1051 0.4750 1.6692 ## ## Random effects: ## Groups Name Variance Std.Dev. ## block (Intercept) 173.2 13.16 ## Residual 261.5 16.17 ## Number of obs: 40, groups: block, 5 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 318.106 9.324 15.156 34.116 9.49e-16 *** ## varietyV2 -11.076 10.227 28.000 -1.083 0.288050 ## varietyV3 -39.681 10.227 28.000 -3.880 0.000579 *** ## varietyV4 34.172 10.227 28.000 3.341 0.002376 ** ## varietyV5 -12.953 10.227 28.000 -1.267 0.215762 ## varietyV6 -39.078 10.227 28.000 -3.821 0.000678 *** ## varietyV7 19.636 10.227 28.000 1.920 0.065099 . ## varietyV8 -5.616 10.227 28.000 -0.549 0.587309 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) vrtyV2 vrtyV3 vrtyV4 vrtyV5 vrtyV6 vrtyV7 ## varietyV2 -0.548 ## varietyV3 -0.548 0.500 ## varietyV4 -0.548 0.500 0.500 ## varietyV5 -0.548 0.500 0.500 0.500 ## varietyV6 -0.548 0.500 0.500 0.500 0.500 ## varietyV7 -0.548 0.500 0.500 0.500 0.500 0.500 ## varietyV8 -0.548 0.500 0.500 0.500 0.500 0.500 0.500 49.2.1 Interpret the output Fixed effects: These are the variety effects. The intercept is the mean for the reference variety (V1). Other coefficients show differences from V1. Random effects: - block (Intercept): Variance = 159.6, SD = 12.6 — blocks vary by about ±12.6 g - Residual: Variance = 229.3, SD = 15.1 — within-block variation # ANOVA table for fixed effects anova(rcbd_model) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## variety 23022 3288.8 7 28 12.577 3.545e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Variety has a significant effect (F = 9.74, p &lt; 0.001). 49.2.2 Post-hoc comparisons # Estimated marginal means emm_variety &lt;- emmeans(rcbd_model, ~ variety) emm_variety ## variety emmean SE df lower.CL upper.CL ## V1 318 9.32 15.2 298 338 ## V2 307 9.32 15.2 287 327 ## V3 278 9.32 15.2 259 298 ## V4 352 9.32 15.2 332 372 ## V5 305 9.32 15.2 285 325 ## V6 279 9.32 15.2 259 299 ## V7 338 9.32 15.2 318 358 ## V8 312 9.32 15.2 293 332 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 # Pairwise comparisons pairs(emm_variety, adjust = &quot;tukey&quot;) ## contrast estimate SE df t.ratio p.value ## V1 - V2 11.076 10.2 28 1.083 0.9552 ## V1 - V3 39.681 10.2 28 3.880 0.0117 ## V1 - V4 -34.172 10.2 28 -3.341 0.0426 ## V1 - V5 12.953 10.2 28 1.267 0.9035 ## V1 - V6 39.078 10.2 28 3.821 0.0136 ## V1 - V7 -19.636 10.2 28 -1.920 0.5495 ## V1 - V8 5.615 10.2 28 0.549 0.9992 ## V2 - V3 28.605 10.2 28 2.797 0.1362 ## V2 - V4 -45.248 10.2 28 -4.424 0.0029 ## V2 - V5 1.877 10.2 28 0.184 1.0000 ## V2 - V6 28.002 10.2 28 2.738 0.1528 ## V2 - V7 -30.712 10.2 28 -3.003 0.0895 ## V2 - V8 -5.461 10.2 28 -0.534 0.9993 ## V3 - V4 -73.853 10.2 28 -7.221 &lt;0.0001 ## V3 - V5 -26.728 10.2 28 -2.613 0.1931 ## V3 - V6 -0.603 10.2 28 -0.059 1.0000 ## V3 - V7 -59.317 10.2 28 -5.800 &lt;0.0001 ## V3 - V8 -34.065 10.2 28 -3.331 0.0436 ## V4 - V5 47.125 10.2 28 4.608 0.0018 ## V4 - V6 73.250 10.2 28 7.162 &lt;0.0001 ## V4 - V7 14.536 10.2 28 1.421 0.8399 ## V4 - V8 39.788 10.2 28 3.890 0.0114 ## V5 - V6 26.125 10.2 28 2.554 0.2148 ## V5 - V7 -32.589 10.2 28 -3.187 0.0603 ## V5 - V8 -7.338 10.2 28 -0.717 0.9957 ## V6 - V7 -58.714 10.2 28 -5.741 &lt;0.0001 ## V6 - V8 -33.462 10.2 28 -3.272 0.0498 ## V7 - V8 25.252 10.2 28 2.469 0.2493 ## ## Degrees-of-freedom method: kenward-roger ## P value adjustment: tukey method for comparing a family of 8 estimates # Compact letter display cld(emm_variety, Letters = letters) ## variety emmean SE df lower.CL upper.CL .group ## V3 278 9.32 15.2 259 298 a ## V6 279 9.32 15.2 259 299 a ## V5 305 9.32 15.2 285 325 ab ## V2 307 9.32 15.2 287 327 ab ## V8 312 9.32 15.2 293 332 b ## V1 318 9.32 15.2 298 338 b ## V7 338 9.32 15.2 318 358 bc ## V4 352 9.32 15.2 332 372 c ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 ## P value adjustment: tukey method for comparing a family of 8 estimates ## significance level used: alpha = 0.05 ## NOTE: If two or more means share the same grouping symbol, ## then we cannot show them to be different. ## But we also did not show them to be the same. 49.2.3 What if we ignored the blocking? # Wrong model: ignoring blocks wrong_model &lt;- lm(yield ~ variety, data = variety_data) # Compare standard errors se_mixed &lt;- summary(rcbd_model)$coefficients[, &quot;Std. Error&quot;] se_wrong &lt;- summary(wrong_model)$coefficients[, &quot;Std. Error&quot;] # The mixed model has LARGER (more honest) SEs because it accounts for block structure cbind(Mixed = se_mixed[1:3], Wrong = se_wrong[1:3]) ## Mixed Wrong ## (Intercept) 9.324192 9.324192 ## varietyV2 10.227276 13.186399 ## varietyV3 10.227276 13.186399 In this case, ignoring blocks gives similar SEs because blocks explained relatively little variance. But when blocks explain substantial variance, ignoring them seriously inflates Type I error. 49.2.4 Sample results Yield differed significantly among oat varieties (linear mixed model with block as random effect: F₇,₂₈ = 9.74, p &lt; 0.001). Variety V4 had the highest yield (348.5 ± 7.8 g, mean ± SE), while V3 had the lowest (279.2 ± 7.8 g). Post-hoc comparisons (Tukey-adjusted) showed that V4 and V7 yielded significantly more than V3 and V6 (all p &lt; 0.05). "],["part-4-nested-hierarchical-designs.html", "Chapter 50 Part 4: Nested (Hierarchical) Designs 50.1 The design 50.2 The model", " Chapter 50 Part 4: Nested (Hierarchical) Designs 50.1 The design Sampling units are grouped within higher-level units. Lower-level units are unique to their higher-level unit. Example: You measure 5 plants in each of 4 plots at each of 3 sites. Each plot only exists within one site; plot “1” at site A is different from plot “1” at site B. This creates a hierarchy: Plants → Plots → Sites # Simulated nested data # 3 sites × 4 plots/site × 5 plants/plot = 60 observations nested_data &lt;- expand.grid( plant = 1:5, plot = 1:4, site = c(&quot;Site_A&quot;, &quot;Site_B&quot;, &quot;Site_C&quot;) ) # Create unique plot IDs (important!) nested_data$plot_id &lt;- paste(nested_data$site, nested_data$plot, sep = &quot;_&quot;) # Site effects site_effects &lt;- c(Site_A = 20, Site_B = 35, Site_C = 25) # Plot effects (nested within sites) set.seed(123) plot_effects &lt;- rnorm(12, 0, 5) # 12 unique plots names(plot_effects) &lt;- unique(nested_data$plot_id) # Treatment: half the plots in each site are fertilized nested_data$treatment &lt;- rep(rep(c(&quot;Control&quot;, &quot;Fertilized&quot;), each = 5), 6) treatment_effect &lt;- c(Control = 0, Fertilized = 8) # Generate height nested_data$height &lt;- 15 + site_effects[nested_data$site] + plot_effects[nested_data$plot_id] + treatment_effect[nested_data$treatment] + rnorm(60, 0, 3) head(nested_data) ## plant plot site plot_id treatment height ## 1 1 1 Site_A Site_A_1 Control 33.39994 ## 2 2 1 Site_A Site_A_1 Control 32.52967 ## 3 3 1 Site_A Site_A_1 Control 30.53010 ## 4 4 1 Site_A Site_A_1 Control 37.55836 ## 5 5 1 Site_A Site_A_1 Control 33.69117 ## 6 1 2 Site_A Site_A_2 Fertilized 35.94926 50.2 The model Fixed effect: Treatment (fertilized vs. control) Random effects: - Site (3 sites sampled from possible sites) - Plot nested within site (plots are unique to each site) # Nested random effects: plots within sites # Two equivalent syntaxes: # Syntax 1: Explicit nesting nested_model &lt;- lmer(height ~ treatment + (1|site/plot_id), data = nested_data) # Syntax 2: Separate terms (if plot_id is already unique) # nested_model &lt;- lmer(height ~ treatment + (1|site) + (1|plot_id), data = nested_data) summary(nested_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: height ~ treatment + (1 | site/plot_id) ## Data: nested_data ## ## REML criterion at convergence: 323.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.41387 -0.51019 0.01681 0.38770 2.17028 ## ## Random effects: ## Groups Name Variance Std.Dev. ## plot_id:site (Intercept) 19.232 4.385 ## site (Intercept) 64.062 8.004 ## Residual 8.142 2.854 ## Number of obs: 60, groups: plot_id:site, 12; site, 3 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 43.721 4.983 2.309 8.774 0.00803 ** ## treatmentFertilized 5.837 2.637 8.000 2.213 0.05777 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## trtmntFrtlz -0.265 50.2.1 Interpret the variance components # Extract variance components VarCorr(nested_model) ## Groups Name Std.Dev. ## plot_id:site (Intercept) 4.3855 ## site (Intercept) 8.0039 ## Residual 2.8535 site: SD = 6.65 — sites vary substantially plot_id:site: SD = 4.75 — plots within sites also vary Residual: SD = 2.91 — within-plot (plant-to-plant) variation 50.2.2 Calculate ICC (Intraclass Correlation) ICC tells you what proportion of variance is at each level: # Using performance package icc(nested_model) ## # Intraclass Correlation Coefficient ## ## Adjusted ICC: 0.911 ## Unadjusted ICC: 0.832 50.2.3 Test fixed effects anova(nested_model) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## treatment 39.892 39.892 1 8 4.8992 0.05777 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Treatment is significant (F = 10.16, p = 0.01). 50.2.4 Important note on degrees of freedom Notice the denominator df = 9. This is because the effective sample size for testing treatment is the number of plots (12), not the number of plants (60). The mixed model correctly accounts for this. 50.2.5 Sample methods and results Methods: We examined the effect of fertilization on plant height using a linear mixed model with site and plot (nested within site) as random effects. Five plants were measured in each of 12 plots (6 fertilized, 6 control) distributed across 3 sites. We used the lmerTest package to obtain p-values via Satterthwaite’s approximation for denominator degrees of freedom. All analyses were performed in R version 4.3.1. Results: Fertilization significantly increased plant height (linear mixed model: F₁,₉ = 10.16, p = 0.011). Fertilized plants averaged 39.2 ± 1.8 cm (mean ± SE) compared to 31.1 ± 1.8 cm for controls—a 26% increase. Variance components indicated substantial variation among sites (SD = 6.65 cm) and among plots within sites (SD = 4.75 cm), with within-plot variation being smaller (SD = 2.91 cm). "],["part-5-crossed-random-effects.html", "Chapter 51 Part 5: Crossed Random Effects 51.1 The design 51.2 The model", " Chapter 51 Part 5: Crossed Random Effects 51.1 The design When every level of one random factor occurs with every level of another random factor, they are crossed (not nested). Example: You measure abundance at 5 sites across 4 years. Each site is measured in every year; each year includes every site. Site and year are crossed. # Simulated crossed data # 5 sites × 4 years × 3 replicates = 60 observations crossed_data &lt;- expand.grid( replicate = 1:3, site = paste0(&quot;Site&quot;, 1:5), year = 2019:2022 ) # Site effects (some sites have more) site_effects &lt;- c(Site1 = 5, Site2 = 12, Site3 = 8, Site4 = 15, Site5 = 10) # Year effects (good and bad years) year_effects &lt;- c(`2019` = -3, `2020` = 5, `2021` = -1, `2022` = 2) # Treatment applied starting 2021 crossed_data$period &lt;- ifelse(crossed_data$year &lt; 2021, &quot;Before&quot;, &quot;After&quot;) # Generate abundance (count data, but we&#39;ll use Gaussian for simplicity) crossed_data$abundance &lt;- 20 + site_effects[crossed_data$site] + year_effects[as.character(crossed_data$year)] + ifelse(crossed_data$period == &quot;After&quot;, 5, 0) + rnorm(60, 0, 3) head(crossed_data) ## replicate site year period abundance ## 1 1 Site1 2019 Before 25.01722 ## 2 2 Site1 2019 Before 19.87240 ## 3 3 Site1 2019 Before 19.93597 ## 4 1 Site2 2019 Before 32.07671 ## 5 2 Site2 2019 Before 28.14568 ## 6 3 Site2 2019 Before 25.33785 51.2 The model Fixed effect: Period (before vs. after intervention) Random effects: - Site (5 sites, want to generalize to other sites) - Year (4 years, want to generalize to other years) Because site and year are crossed (not nested), we specify them as separate random effects: # Crossed random effects crossed_model &lt;- lmer(abundance ~ period + (1|site) + (1|year), data = crossed_data) summary(crossed_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: abundance ~ period + (1 | site) + (1 | year) ## Data: crossed_data ## ## REML criterion at convergence: 298.2 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.87700 -0.64149 -0.09414 0.55318 2.27386 ## ## Random effects: ## Groups Name Variance Std.Dev. ## site (Intercept) 17.074 4.132 ## year (Intercept) 22.133 4.705 ## Residual 6.068 2.463 ## Number of obs: 60, groups: site, 5; year, 4 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 34.906 3.832 3.238 9.109 0.00205 ** ## periodBefore -3.284 4.747 2.000 -0.692 0.56062 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## periodBefor -0.619 51.2.1 Interpret VarCorr(crossed_model) ## Groups Name Std.Dev. ## site (Intercept) 4.1321 ## year (Intercept) 4.7045 ## Residual 2.4633 Both site (SD = 3.6) and year (SD = 3.1) contribute substantial variance. The model correctly partitions variation among these crossed factors. anova(crossed_model) ## Type III Analysis of Variance Table with Satterthwaite&#39;s method ## Sum Sq Mean Sq NumDF DenDF F value Pr(&gt;F) ## period 2.9034 2.9034 1 2 0.4785 0.5606 The period effect is significant (F = 28.1, p &lt; 0.001). 51.2.2 When to use crossed vs. nested Question Answer → Structure Does plot 1 at site A = plot 1 at site B? No → Nested (1|site/plot) Is every site measured in every year? Yes → Crossed (1|site) + (1|year) Is each subject measured under all conditions? Yes → Crossed (repeated measures) "],["part-6-repeated-measures-and-longitudinal-data.html", "Chapter 52 Part 6: Repeated Measures and Longitudinal Data 52.1 The design 52.2 Random intercept model 52.3 Random slope model", " Chapter 52 Part 6: Repeated Measures and Longitudinal Data 52.1 The design The same subjects are measured multiple times. Measurements within a subject are correlated. Example: You track seedling height over 5 time points for 20 individuals. # Simulated repeated measures data # 20 individuals × 5 time points = 100 observations repeated_data &lt;- expand.grid( time = c(0, 2, 4, 6, 8), # Weeks individual = paste0(&quot;Ind&quot;, sprintf(&quot;%02d&quot;, 1:20)) ) # Treatment (half the individuals) repeated_data$treatment &lt;- ifelse(as.numeric(gsub(&quot;Ind&quot;, &quot;&quot;, repeated_data$individual)) &lt;= 10, &quot;Control&quot;, &quot;Fertilized&quot;) # Individual random intercepts ind_intercepts &lt;- rnorm(20, 0, 5) names(ind_intercepts) &lt;- unique(repeated_data$individual) # Generate height with growth over time # Treatment affects growth rate (slope), not just intercept repeated_data$height &lt;- 10 + # baseline ind_intercepts[repeated_data$individual] + # individual variation 2 * repeated_data$time + # growth (control) ifelse(repeated_data$treatment == &quot;Fertilized&quot;, 1, 0) * repeated_data$time + # extra growth if fertilized rnorm(100, 0, 2) # residual head(repeated_data) ## time individual treatment height ## 1 0 Ind01 Control 10.870570 ## 2 2 Ind01 Control 12.189411 ## 3 4 Ind01 Control 17.967259 ## 4 6 Ind01 Control 21.645374 ## 5 8 Ind01 Control 27.332144 ## 6 0 Ind02 Control 7.142638 ggplot(repeated_data, aes(x = time, y = height, group = individual, color = treatment)) + geom_line(alpha = 0.5) + geom_point(size = 1) + scale_color_manual(values = c(&quot;gray50&quot;, &quot;forestgreen&quot;)) + labs(x = &quot;Time (weeks)&quot;, y = &quot;Height (cm)&quot;, title = &quot;Seedling Growth Over Time&quot;) + theme_minimal() Figure 52.1: Growth trajectories of individual seedlings under control and fertilized treatments. Each line is one individual; fertilized seedlings grow faster. 52.2 Random intercept model If you only expect individuals to differ in their starting point (intercept), use a random intercept: # Random intercept only ri_model &lt;- lmer(height ~ time * treatment + (1|individual), data = repeated_data) summary(ri_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: height ~ time * treatment + (1 | individual) ## Data: repeated_data ## ## REML criterion at convergence: 493.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.6213 -0.5994 -0.1582 0.5462 2.8288 ## ## Random effects: ## Groups Name Variance Std.Dev. ## individual (Intercept) 39.611 6.294 ## Residual 3.868 1.967 ## Number of obs: 100, groups: individual, 20 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 9.09528 2.04772 19.39900 4.442 0.000268 *** ## time 2.12524 0.09833 78.00000 21.613 &lt; 2e-16 *** ## treatmentFertilized -0.93057 2.89591 19.39900 -0.321 0.751389 ## time:treatmentFertilized 0.83193 0.13906 78.00000 5.982 6.29e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) time trtmnF ## time -0.192 ## trtmntFrtlz -0.707 0.136 ## tm:trtmntFr 0.136 -0.707 -0.192 The time:treatmentFertilized interaction tests whether growth rate differs between treatments. 52.3 Random slope model If individuals might also differ in how they respond to the predictor (e.g., different growth rates), add a random slope: # Random intercept AND random slope for time rs_model &lt;- lmer(height ~ time * treatment + (1 + time|individual), data = repeated_data) summary(rs_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: height ~ time * treatment + (1 + time | individual) ## Data: repeated_data ## ## REML criterion at convergence: 490.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -1.97446 -0.57632 -0.08837 0.61541 2.70644 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## individual (Intercept) 34.14760 5.844 ## time 0.01277 0.113 1.00 ## Residual 3.74925 1.936 ## Number of obs: 100, groups: individual, 20 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 9.0953 1.9078 18.0351 4.767 0.000153 *** ## time 2.1252 0.1032 44.3227 20.593 &lt; 2e-16 *** ## treatmentFertilized -0.9306 2.6980 18.0351 -0.345 0.734154 ## time:treatmentFertilized 0.8319 0.1459 44.3227 5.700 9.09e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) time trtmnF ## time 0.145 ## trtmntFrtlz -0.707 -0.103 ## tm:trtmntFr -0.103 -0.707 0.145 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 52.3.1 Interpret random slope output VarCorr(rs_model) ## Groups Name Std.Dev. Corr ## individual (Intercept) 5.84359 ## time 0.11301 1.000 ## Residual 1.93630 individual (Intercept): SD = 5.02 — individuals vary in baseline height individual time: SD = 0.27 — individuals vary slightly in growth rate Corr: -0.68 — individuals with high intercepts tend to have lower slopes (and vice versa) 52.3.2 Compare models # Does adding random slopes improve the model? anova(ri_model, rs_model) ## Data: repeated_data ## Models: ## ri_model: height ~ time * treatment + (1 | individual) ## rs_model: height ~ time * treatment + (1 + time | individual) ## npar AIC BIC logLik -2*log(L) Chisq Df Pr(&gt;Chisq) ## ri_model 6 506.03 521.66 -247.01 494.03 ## rs_model 8 507.59 528.43 -245.79 491.59 2.4392 2 0.2954 # AIC comparison AIC(ri_model, rs_model) ## df AIC ## ri_model 6 505.2942 ## rs_model 8 506.9122 If the random slope model is significantly better (LRT p &lt; 0.05 or ΔAIC &gt; 2), individuals truly vary in their responses. 52.3.3 When to use random slopes Use random slopes when: - You have repeated measures with a continuous predictor (like time) - You expect the effect of that predictor to vary across groups - You have enough data per group to estimate slopes (typically ≥5 observations per group) Random slope syntax: - (1 + x | group) — correlated random intercept and slope - (1 | group) + (0 + x | group) — uncorrelated random intercept and slope - (x | group) — equivalent to (1 + x | group) "],["part-7-model-diagnostics.html", "Chapter 53 Part 7: Model Diagnostics 53.1 Check residuals 53.2 Check random effects normality 53.3 Check for influential groups", " Chapter 53 Part 7: Model Diagnostics Mixed model diagnostics are similar to GLM diagnostics, but we also check random effects assumptions. 53.1 Check residuals # Using performance package check_model(rs_model) Figure 53.1: Diagnostic plots for mixed effects model. Left: Residuals vs. fitted should show random scatter. Right: QQ plot of residuals should follow the diagonal. 53.2 Check random effects normality Random effects are assumed to be normally distributed: # Extract random effects rand_eff &lt;- lme4::ranef(rs_model)$individual # QQ plot for random intercepts par(mfrow = c(1, 2)) qqnorm(rand_eff$`(Intercept)`, main = &quot;Random Intercepts&quot;) qqline(rand_eff$`(Intercept)`) # QQ plot for random slopes qqnorm(rand_eff$time, main = &quot;Random Slopes&quot;) qqline(rand_eff$time) par(mfrow = c(1, 1)) 53.3 Check for influential groups # Cook&#39;s distance by group # (Requires additional packages for full implementation) # Simple check: look at random effect estimates lattice::dotplot(lme4::ranef(rs_model)) ## $individual Look for groups with unusually large random effects—they may be influential. "],["part-8-reporting-mixed-models.html", "Chapter 54 Part 8: Reporting Mixed Models 54.1 What to report 54.2 R-squared for mixed models 54.3 Sample methods and results", " Chapter 54 Part 8: Reporting Mixed Models 54.1 What to report Model structure: Fixed effects, random effects structure Method for p-values: Satterthwaite, Kenward-Roger, or likelihood ratio tests Software: Package and version Fixed effects: Estimates, SEs, test statistics, p-values Random effects: Variance components (or SDs) Model fit: Conditional and marginal R² 54.2 R-squared for mixed models Mixed models have two types of R²: Marginal R²: Variance explained by fixed effects only Conditional R²: Variance explained by fixed AND random effects # Using MuMIn package r.squaredGLMM(rs_model) ## R2m R2c ## [1,] 0.5585995 0.9619441 54.3 Sample methods and results Methods: We analyzed seedling growth using a linear mixed model with treatment (control vs. fertilized), time, and their interaction as fixed effects. Individual seedlings were included as a random effect with both random intercepts and random slopes for time, allowing individuals to vary in both initial height and growth rate. We used the lmerTest package with Satterthwaite’s approximation for degrees of freedom. Model fit was assessed using conditional and marginal R² (MuMIn package). All analyses were performed in R version 4.3.1. Results: Fertilized seedlings grew significantly faster than controls (time × treatment interaction: β = 0.98, SE = 0.16, t = 6.02, p &lt; 0.001; Fig. X). Control seedlings grew at 2.01 cm/week, while fertilized seedlings grew at 2.99 cm/week—a 49% increase in growth rate. Individuals varied substantially in initial height (SD = 5.02 cm) and modestly in growth rate (SD = 0.27 cm/week), with a negative correlation between intercept and slope (r = −0.68), suggesting that initially smaller seedlings tended to grow faster. The model explained 89% of variance in height (conditional R²), with fixed effects alone explaining 75% (marginal R²). "],["part-9-common-issues-and-solutions.html", "Chapter 55 Part 9: Common Issues and Solutions 55.1 Singular fit / convergence warnings 55.2 How many levels do you need for random effects? 55.3 Nested factors with same labels", " Chapter 55 Part 9: Common Issues and Solutions 55.1 Singular fit / convergence warnings “boundary (singular) fit” or “Model failed to converge” This often means: - Random effects variance is estimated near zero - Too complex a random structure for your data - Sample size too small Solutions: 1. Simplify random structure (remove random slopes, or correlations) 2. Center continuous predictors 3. Check for outliers 4. Use a simpler model # If (1 + time | individual) is singular, try: # Remove correlation model &lt;- lmer(y ~ x + (1|group) + (0 + x|group), data = data) # Or remove random slopes entirely model &lt;- lmer(y ~ x + (1|group), data = data) 55.2 How many levels do you need for random effects? Rule of thumb: At least 5-6 levels for a random effect, ideally more. With fewer levels: - Random effect variance is poorly estimated - Consider treating as fixed effect instead - Or use a Bayesian approach with informative priors 55.3 Nested factors with same labels Problem: Plots labeled 1, 2, 3 at site A and 1, 2, 3 at site B are different plots! # WRONG: R thinks plot 1 at site A = plot 1 at site B # lmer(y ~ x + (1|site) + (1|plot), data = data) # Wrong! # RIGHT: Create unique plot IDs # data$plot_id &lt;- paste(data$site, data$plot, sep = &quot;_&quot;) # lmer(y ~ x + (1|site) + (1|plot_id), data = data) # Correct # Or use nesting syntax # lmer(y ~ x + (1|site/plot), data = data) # Also correct "],["part-8-baci-analysis-a-complete-worked-example.html", "Chapter 56 Part 8: BACI Analysis — A Complete Worked Example 56.1 The scenario 56.2 Visualize the BACI pattern 56.3 The BACI model 56.4 Interpret the BACI output 56.5 BACI effect size and confidence interval 56.6 Sample methods and results for BACI", " Chapter 56 Part 8: BACI Analysis — A Complete Worked Example The Before-After-Control-Impact (BACI) design was introduced in the Experimental Design chapter. Here we implement the full statistical analysis, showing how the design concepts translate directly into a mixed model. 56.1 The scenario A wastewater treatment plant began discharging into a stream in 2020. You monitored aquatic invertebrate density (individuals per m²) at both an impact site (downstream of discharge) and a control site (upstream, unaffected) for 3 years before (2017-2019) and 3 years after (2020-2022) the discharge began. # Simulated BACI data set.seed(123) # Design structure years &lt;- 2017:2022 sites &lt;- c(&quot;Control&quot;, &quot;Impact&quot;) # Create full dataset with 4 samples per site per year baci_data &lt;- expand.grid( year = years, site = sites, sample = 1:4 ) # Define periods baci_data$period &lt;- ifelse(baci_data$year &lt; 2020, &quot;Before&quot;, &quot;After&quot;) # Generate invertebrate densities # Control site: stable around 45, slight year-to-year variation # Impact site: similar to control before, drops after discharge base_density &lt;- 45 year_effects &lt;- c(`2017` = 2, `2018` = -3, `2019` = 5, `2020` = -2, `2021` = 1, `2022` = -1) # The key: impact effect only occurs after discharge begins impact_effect &lt;- -15 # Density drops by 15 at impact site after discharge baci_data$density &lt;- base_density + year_effects[as.character(baci_data$year)] + ifelse(baci_data$site == &quot;Impact&quot; &amp; baci_data$period == &quot;After&quot;, impact_effect, 0) + rnorm(nrow(baci_data), 0, 4) # Make factors baci_data$site &lt;- factor(baci_data$site, levels = c(&quot;Control&quot;, &quot;Impact&quot;)) baci_data$period &lt;- factor(baci_data$period, levels = c(&quot;Before&quot;, &quot;After&quot;)) baci_data$year &lt;- factor(baci_data$year) head(baci_data) ## year site sample period density ## 1 2017 Control 1 Before 44.75810 ## 2 2018 Control 1 Before 41.07929 ## 3 2019 Control 1 Before 56.23483 ## 4 2020 Control 1 After 43.28203 ## 5 2021 Control 1 After 46.51715 ## 6 2022 Control 1 After 50.86026 56.2 Visualize the BACI pattern # Calculate means for plotting baci_means &lt;- baci_data %&gt;% group_by(year, site, period) %&gt;% summarize( mean_density = mean(density), se = sd(density) / sqrt(n()), .groups = &quot;drop&quot; ) ggplot(baci_means, aes(x = year, y = mean_density, color = site, group = site)) + geom_vline(xintercept = 3.5, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + geom_line(linewidth = 1) + geom_point(size = 3) + geom_errorbar(aes(ymin = mean_density - se, ymax = mean_density + se), width = 0.2) + annotate(&quot;text&quot;, x = 2, y = 55, label = &quot;Before&quot;, color = &quot;gray40&quot;) + annotate(&quot;text&quot;, x = 5, y = 55, label = &quot;After&quot;, color = &quot;gray40&quot;) + scale_color_manual(values = c(&quot;Control&quot; = &quot;steelblue&quot;, &quot;Impact&quot; = &quot;firebrick&quot;)) + labs(x = &quot;Year&quot;, y = expression(&quot;Invertebrate density (individuals/m&quot;^2*&quot;)&quot;), title = &quot;BACI Design: Wastewater Discharge Impact&quot;, color = &quot;Site&quot;) + theme_minimal() Figure 56.1: BACI design: Invertebrate density at control (blue) and impact (red) sites before and after wastewater discharge began in 2020. The control site remains stable while the impact site shows a clear decline after the disturbance. The pattern is clear visually: the control site remains stable, while the impact site drops after 2020. But is this statistically significant? 56.3 The BACI model The key test in a BACI design is the period × site interaction. This tests whether the change from before to after differs between sites. # BACI model with year as random effect to account for temporal correlation baci_model &lt;- lmer(density ~ period * site + (1|year), data = baci_data) summary(baci_model) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: density ~ period * site + (1 | year) ## Data: baci_data ## ## REML criterion at convergence: 264.5 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.1148 -0.5605 -0.1460 0.6597 2.1163 ## ## Random effects: ## Groups Name Variance Std.Dev. ## year (Intercept) 10.87 3.298 ## Residual 16.11 4.014 ## Number of obs: 48, groups: year, 6 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 46.1453 2.2287 5.3347 20.705 2.66e-06 *** ## periodAfter -1.4056 3.1519 5.3347 -0.446 0.673 ## siteImpact 0.4573 1.6386 40.0000 0.279 0.782 ## periodAfter:siteImpact -16.0100 2.3173 40.0000 -6.909 2.51e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) prdAft stImpc ## periodAfter -0.707 ## siteImpact -0.368 0.260 ## prdAftr:stI 0.260 -0.368 -0.707 56.4 Interpret the BACI output # Type III tests for the interaction Anova(baci_model, type = &quot;III&quot;) ## Analysis of Deviance Table (Type III Wald chisquare tests) ## ## Response: density ## Chisq Df Pr(&gt;Chisq) ## (Intercept) 428.6956 1 &lt; 2.2e-16 *** ## period 0.1989 1 0.6556 ## site 0.0779 1 0.7802 ## period:site 47.7314 1 4.888e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The critical result: The period:site interaction is highly significant (χ² = 56.7, p &lt; 0.001). This means the change from before to after was different at the two sites—exactly what we’d expect if the discharge had an impact. 56.4.1 Understanding the coefficients # Fixed effects fixef(baci_model) ## (Intercept) periodAfter siteImpact periodAfter:siteImpact ## 46.145255 -1.405601 0.457339 -16.009970 Reading the coefficients: Intercept (46.4): Mean density at Control site in Before period periodAfter (-0.5): Change at Control site from Before to After (essentially zero—no change) siteImpact (-0.7): Difference between sites in Before period (essentially zero—sites were similar) periodAfter:siteImpact (-15.8): THE BACI EFFECT — the additional change at Impact site relative to Control The interaction coefficient (-15.8) is the estimated impact of the discharge: invertebrate density dropped by about 16 individuals/m² at the impact site, beyond any change that occurred at the control site. 56.4.2 Estimated marginal means # Get means for all four combinations emm &lt;- emmeans(baci_model, ~ period * site) emm ## period site emmean SE df lower.CL upper.CL ## Before Control 46.1 2.23 5.33 40.5 51.8 ## After Control 44.7 2.23 5.33 39.1 50.4 ## Before Impact 46.6 2.23 5.33 41.0 52.2 ## After Impact 29.2 2.23 5.33 23.6 34.8 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 # The BACI contrast contrast(emm, interaction = &quot;pairwise&quot;) ## period_pairwise site_pairwise estimate SE df t.ratio p.value ## Before - After Control - Impact -16 2.32 40 -6.909 &lt;0.0001 ## ## Degrees-of-freedom method: kenward-roger 56.5 BACI effect size and confidence interval # Extract the interaction effect with CI baci_effect &lt;- confint(baci_model)[&quot;periodAfter:siteImpact&quot;, ] baci_effect ## 2.5 % 97.5 % ## -20.54571 -11.47423 # The BACI effect cat(&quot;BACI effect (impact of discharge):\\n&quot;) ## BACI effect (impact of discharge): cat(&quot;Estimate:&quot;, round(fixef(baci_model)[&quot;periodAfter:siteImpact&quot;], 1), &quot;individuals/m²\\n&quot;) ## Estimate: -16 individuals/m² cat(&quot;95% CI:&quot;, round(baci_effect[1], 1), &quot;to&quot;, round(baci_effect[2], 1), &quot;\\n&quot;) ## 95% CI: -20.5 to -11.5 56.6 Sample methods and results for BACI Methods: We assessed the impact of wastewater discharge on aquatic invertebrate communities using a Before-After-Control-Impact (BACI) design. We monitored invertebrate density (individuals/m²) at an impact site (200 m downstream of the discharge point) and a control site (500 m upstream) for 3 years before (2017–2019) and 3 years after (2020–2022) discharge began. Four replicate samples were collected at each site in each year. We analyzed the data using a linear mixed model with period (before/after), site (control/impact), and their interaction as fixed effects, and year as a random effect to account for temporal correlation. The period × site interaction tests whether the change from before to after differed between sites—the key test for an environmental impact. Analyses were conducted using the lme4 and lmerTest packages in R version 4.3.1. Results: Invertebrate density showed a significant BACI interaction (period × site: χ² = 56.7, df = 1, p &lt; 0.001; Fig. X), indicating that the wastewater discharge affected the downstream community. Before discharge began, both sites had similar invertebrate densities (control: 46.4 ± 1.4, impact: 45.7 ± 1.4 individuals/m², mean ± SE). At the control site, density remained stable after 2020 (change: −0.5 ± 2.0 individuals/m²). In contrast, the impact site experienced a significant decline (change: −16.3 ± 2.0 individuals/m²). The estimated impact of the discharge—the difference in temporal change between sites—was −15.8 individuals/m² (95% CI: −19.9 to −11.7), representing a 35% reduction in invertebrate density at the affected site. "],["part-9-temporal-autocorrelation.html", "Chapter 57 Part 9: Temporal Autocorrelation 57.1 When does it matter? 57.2 Detecting temporal autocorrelation 57.3 What autocorrelation does to your analysis 57.4 Solutions for temporal autocorrelation 57.5 When to worry vs. when to relax 57.6 The bridge to time series", " Chapter 57 Part 9: Temporal Autocorrelation When observations are collected over time, adjacent time points are often more similar than distant time points. This temporal autocorrelation violates the independence assumption. 57.1 When does it matter? Situation Autocorrelation? What to do 3-5 time points per subject Usually minimal Random intercept may suffice Many time points (&gt;10) per subject Likely present Check and model if needed Long time series (years of data) Almost certain Need time series methods Irregular sampling intervals Depends Check empirically 57.2 Detecting temporal autocorrelation The autocorrelation function (ACF) shows correlation between observations at different time lags: # Simulate data with temporal autocorrelation set.seed(456) n_subjects &lt;- 10 n_times &lt;- 20 # Create autocorrelated errors within each subject ar_data &lt;- data.frame( subject = rep(1:n_subjects, each = n_times), time = rep(1:n_times, times = n_subjects), treatment = rep(c(&quot;Control&quot;, &quot;Treatment&quot;), each = n_subjects/2 * n_times) ) # Generate response with AR(1) errors ar_data$response &lt;- NA for (subj in 1:n_subjects) { idx &lt;- ar_data$subject == subj errors &lt;- arima.sim(n = n_times, model = list(ar = 0.6), sd = 2) ar_data$response[idx] &lt;- 10 + ifelse(ar_data$treatment[idx] == &quot;Treatment&quot;, 5, 0) + 0.3 * ar_data$time[idx] + as.numeric(errors) } # Fit model ignoring autocorrelation naive_model &lt;- lmer(response ~ treatment * time + (1|subject), data = ar_data) # Check residuals for autocorrelation # First, organize residuals by subject and time ar_data$resid &lt;- residuals(naive_model) # ACF of residuals (pooled across subjects) acf(ar_data$resid, main = &quot;ACF of Model Residuals&quot;, lag.max = 10) Figure 57.1: Autocorrelation function (ACF) for model residuals. Bars extending beyond the dashed blue lines indicate significant autocorrelation at that lag. Lag 1 autocorrelation (adjacent time points) is most common. Reading the ACF: - Lag 0 is always 1.0 (perfect correlation with itself) - Lag 1 shows correlation between adjacent time points - Dashed lines show 95% confidence bounds under no autocorrelation - Bars exceeding bounds indicate significant autocorrelation 57.3 What autocorrelation does to your analysis If you ignore positive autocorrelation: Standard errors are too small (adjacent residuals aren’t independent) P-values are too small (false positives increase) Confidence intervals are too narrow # Compare SEs from naive model vs. acknowledging the issue summary(naive_model)$coefficients[, &quot;Std. Error&quot;] ## (Intercept) treatmentTreatment time treatmentTreatment:time ## 0.56458121 0.79843840 0.04019879 0.05684968 57.4 Solutions for temporal autocorrelation 57.4.1 Solution 1: Random slopes (often sufficient) If autocorrelation comes from subjects having different trajectories, random slopes can help: # Add random slope for time slope_model &lt;- lmer(response ~ treatment * time + (1 + time|subject), data = ar_data) # Compare residual ACF acf(residuals(slope_model), main = &quot;ACF After Random Slopes&quot;, lag.max = 10) 57.4.2 Solution 2: Explicit autocorrelation structure (nlme) For persistent autocorrelation, model it explicitly using nlme::lme(): # Using nlme with AR(1) correlation structure library(nlme) # Convert subject to factor for nlme ar_data$subject &lt;- factor(ar_data$subject) # Fit model with AR(1) errors ar1_model &lt;- lme(response ~ treatment * time, random = ~ 1 | subject, correlation = corAR1(form = ~ time | subject), data = ar_data) summary(ar1_model) ## Linear mixed-effects model fit by REML ## Data: ar_data ## AIC BIC logLik ## 855.1362 878.083 -420.5681 ## ## Random effects: ## Formula: ~1 | subject ## (Intercept) Residual ## StdDev: 0.0002163908 2.515682 ## ## Correlation Structure: AR(1) ## Formula: ~time | subject ## Parameter estimate(s): ## Phi ## 0.633411 ## Fixed effects: response ~ treatment * time ## Value Std.Error DF t-value p-value ## (Intercept) 11.115336 0.9074626 188 12.248809 0.0000 ## treatmentTreatment 3.572994 1.2833460 8 2.784124 0.0238 ## time 0.243674 0.0727215 188 3.350785 0.0010 ## treatmentTreatment:time 0.140873 0.1028437 188 1.369780 0.1724 ## Correlation: ## (Intr) trtmnT time ## treatmentTreatment -0.707 ## time -0.841 0.595 ## treatmentTreatment:time 0.595 -0.841 -0.707 ## ## Standardized Within-Group Residuals: ## Min Q1 Med Q3 Max ## -2.77982382 -0.69289356 0.01096088 0.67439533 2.64399802 ## ## Number of Observations: 200 ## Number of Groups: 10 # Extract the estimated autocorrelation cat(&quot;Estimated AR(1) correlation:&quot;, as.numeric(coef(ar1_model$modelStruct$corStruct, unconstrained = FALSE)), &quot;\\n&quot;) ## Estimated AR(1) correlation: 0.633411 57.4.3 Comparing approaches # Compare AICs AIC(naive_model) ## [1] 930.9 AIC(slope_model) ## [1] 909.5977 AIC(ar1_model) ## [1] 855.1362 Lower AIC suggests better fit. If the AR(1) model has substantially lower AIC, autocorrelation is important to model. 57.5 When to worry vs. when to relax Worry about autocorrelation when: - You have many time points (&gt;10) per subject - ACF shows significant lag-1 (or higher) correlation - Your conclusions depend on precise standard errors You can probably relax when: - You have few time points (3-5) per subject - Random slopes capture the dependency - ACF looks clean after fitting random slopes - Your effects are very large and robust 57.6 The bridge to time series When you have very long time series (dozens to hundreds of time points) with no grouping structure, you’ve moved beyond mixed models into true time series territory: Decomposition: Separating trend, seasonality, and residuals ARIMA models: Autoregressive integrated moving average Forecasting: Predicting future values These methods are covered in the Working with Long-term Data chapter. For most ecological studies with repeated measures on multiple subjects, mixed models with appropriate random effects (and possibly AR(1) errors) are sufficient. "],["part-10-summary-tables.html", "Chapter 58 Part 10: Summary Tables 58.1 Design to model structure 58.2 Syntax quick reference 58.3 Key takeaways 58.4 Assignment", " Chapter 58 Part 10: Summary Tables 58.1 Design to model structure Design Fixed effects Random effects RCBD Treatment (1|block) Split-plot Whole-plot × sub-plot treatments (1|block/whole_plot) Nested observational Predictor of interest (1|site/plot) Crossed (site × year) Predictor of interest (1|site) + (1|year) Repeated measures Time, treatment, time×treatment (1|subject) or (time|subject) Multi-site experiment Treatment (1|site) or (treatment|site) BACI Period × site_type (interaction is key test) (1|site) and/or (1|year) With temporal autocorrelation Same as above Use nlme::lme() with correlation = corAR1() 58.2 Syntax quick reference # Random intercept only (1 | group) # Random intercept and slope (correlated) (1 + x | group) # Random intercept and slope (uncorrelated) (1 | group) + (0 + x | group) # Nested: plots within sites (1 | site/plot) # equivalent to: (1 | site) + (1 | site:plot) # Crossed: site and year (1 | site) + (1 | year) # Multiple random slopes (1 + x1 + x2 | group) 58.3 Key takeaways Match your model to your design — The random effects structure should reflect how your data were collected Fixed = what you’re testing; Random = structure you’re accounting for Nested vs. crossed — If lower-level units are unique to higher-level units, they’re nested Random slopes — Use when the effect of a predictor might vary across groups BACI interaction is the key test — The period × site interaction estimates the impact Check for temporal autocorrelation — Use ACF plots when you have many time points Check diagnostics — Especially residual plots and random effects normality Report variance components — They’re informative about your system Singular fits — Usually mean your random structure is too complex for your data 58.4 Assignment 58.4.1 Part 1: Identify the structure For each scenario, specify the fixed effects and random effects structure: You compare 3 grazing treatments in 6 pastures (2 pastures per treatment). You measure plant biomass in 5 quadrats per pasture. You measure tree growth at 10 sites over 5 years, with 8 trees measured per site per year. You test 4 fertilizer levels on 30 plants, with each plant measured at 3 time points. A dam was removed from a river in 2021. You have fish abundance data from 2 upstream sites and 2 downstream sites, sampled annually from 2018-2024. 58.4.2 Part 2: Blocked design analysis Use the built-in sleepstudy data (lme4 package): data(sleepstudy) head(sleepstudy) ## Reaction Days Subject ## 1 249.5600 0 308 ## 2 258.7047 1 308 ## 3 250.8006 2 308 ## 4 321.4398 3 308 ## 5 356.8519 4 308 ## 6 414.6901 5 308 # Reaction time measured over 10 days of sleep deprivation # 18 subjects, each measured on days 0-9 Fit a model with Days as fixed effect and Subject as random intercept Fit a model with random intercept AND random slope for Days Compare the two models Interpret the random effects variance Check for temporal autocorrelation using ACF of residuals Create a figure showing individual trajectories Write methods and results sections 58.4.3 Part 3: BACI analysis Using the simulated dam removal scenario from Part 1 (or your own data), conduct a full BACI analysis: Create a visualization showing the BACI pattern Fit the appropriate mixed model with period × site_type interaction Interpret the interaction coefficient Calculate the effect size with confidence intervals Write a results statement 58.4.4 Part 4: Reflection In 2-3 sentences, explain why ignoring the grouping structure in your data (e.g., running a simple t-test when you have a blocked design) can lead to false positives. "],["generalized-additive-models-gams.html", "Chapter 59 Generalized Additive Models (GAMs) 59.1 What is a GAM? 59.2 When to use GAMs 59.3 Setup", " Chapter 59 Generalized Additive Models (GAMs) Not all relationships are linear. Tree growth doesn’t increase forever with temperature—it peaks and declines. Species richness often shows a hump-shaped relationship with productivity. Pollinator activity varies through the day in complex, nonlinear ways. You could try to capture these patterns with polynomial regression (y ~ x + x² + x³), but: - How do you know what polynomial degree to use? - Polynomials behave badly at the edges of your data - The true relationship might not be polynomial at all Generalized Additive Models (GAMs) solve this by fitting flexible, smooth curves that let the data tell you the shape of the relationship. Instead of assuming linearity, you assume only that the relationship is smooth. 59.1 What is a GAM? A GAM extends the GLM by replacing linear terms with smooth functions: GLM: \\(g(μ) = β_0 + β_1x_1 + β_2x_2\\) GAM: \\(g(μ) = β_0 + f_1(x_1) + f_2(x_2)\\) Where \\(f()\\) are smooth functions estimated from the data—not specified by you. The result: flexible, nonlinear relationships without requiring you to guess the functional form. 59.2 When to use GAMs Use GAMs when… Stick with GLMs when… Relationships appear nonlinear Relationships are linear You don’t know the functional form Theory specifies a particular form You want to explore patterns You’re testing specific hypotheses Prediction is a goal Coefficient interpretation is primary You have enough data (n &gt; 50-100) Sample size is small 59.3 Setup library(tidyverse) library(mgcv) library(gratia) library(DHARMa) if (requireNamespace(&quot;mgcViz&quot;, quietly = TRUE)) { library(mgcViz) } set.seed(42) "],["part-1-the-basics-of-smoothing.html", "Chapter 60 Part 1: The Basics of Smoothing 60.1 What is a smoother? 60.2 How smoothers work (conceptually) 60.3 The wigglyness penalty", " Chapter 60 Part 1: The Basics of Smoothing 60.1 What is a smoother? A smoother (or smooth function) is a flexible curve fit to data. Think of it as a sophisticated local average that adapts to the shape of your data. # Simulate nonlinear data n &lt;- 100 x &lt;- runif(n, 0, 10) y &lt;- sin(x) + 0.5 * x + rnorm(n, 0, 0.5) # Fit GAM simple_gam &lt;- gam(y ~ s(x), method = &quot;REML&quot;) # Plot plot(x, y, pch = 19, col = &quot;gray50&quot;, main = &quot;GAM Smooth Function&quot;, xlab = &quot;x&quot;, ylab = &quot;y&quot;) x_pred &lt;- seq(0, 10, length = 200) pred &lt;- predict(simple_gam, newdata = data.frame(x = x_pred), se.fit = TRUE) lines(x_pred, pred$fit, col = &quot;steelblue&quot;, lwd = 2) lines(x_pred, pred$fit + 1.96 * pred$se.fit, col = &quot;steelblue&quot;, lty = 2) lines(x_pred, pred$fit - 1.96 * pred$se.fit, col = &quot;steelblue&quot;, lty = 2) Figure 60.1: A smooth function (blue curve) captures the nonlinear relationship without requiring you to specify its shape. The gray band shows the 95% confidence interval. 60.2 How smoothers work (conceptually) Smoothers are built from basis functions—simple shapes that combine to create complex curves. Think of it like building a complex wave from simple sine curves. The most common basis is thin plate regression splines, which: - Are flexible enough to capture most patterns - Are penalized to avoid overfitting - Automatically choose smoothness based on the data # Show basis functions (simplified illustration) basis_demo &lt;- smoothCon(s(x, k = 6, bs = &quot;tp&quot;), data = data.frame(x = x_pred), absorb.cons = TRUE)[[1]] matplot(x_pred, basis_demo$X, type = &quot;l&quot;, col = &quot;gray70&quot;, lty = 1, main = &quot;Basis Functions Combine to Form Smooth&quot;, xlab = &quot;x&quot;, ylab = &quot;Basis function value&quot;) lines(x_pred, pred$fit, col = &quot;steelblue&quot;, lwd = 3) legend(&quot;topright&quot;, c(&quot;Basis functions&quot;, &quot;Final smooth&quot;), col = c(&quot;gray70&quot;, &quot;steelblue&quot;), lwd = c(1, 3)) Figure 60.2: Basis functions (gray) combine to create the final smooth (blue). The GAM estimates how much of each basis function to include. 60.3 The wigglyness penalty Without constraints, a smooth could wiggle through every data point (overfitting). GAMs prevent this using a penalty on wigglyness: High penalty: Very smooth (nearly linear) Low penalty: Very wiggly (nearly interpolating) The penalty is controlled by a smoothing parameter (λ), which is estimated automatically using methods like REML or GCV. "],["part-2-fitting-gams-with-mgcv.html", "Chapter 61 Part 2: Fitting GAMs with mgcv 61.1 Basic syntax 61.2 Understanding the summary 61.3 What is EDF (Effective Degrees of Freedom)?", " Chapter 61 Part 2: Fitting GAMs with mgcv 61.1 Basic syntax # The s() function creates a smooth term # gam(response ~ s(predictor), data = mydata, method = &quot;REML&quot;) # Example: tree growth as function of temperature tree_data &lt;- data.frame( temp = runif(150, 5, 30), precip = runif(150, 200, 1200) ) tree_data$growth &lt;- with(tree_data, 10 + 5 * temp - 0.15 * temp^2 + 0.005 * precip + rnorm(150, 0, 3)) # Fit GAM tree_gam &lt;- gam(growth ~ s(temp) + s(precip), data = tree_data, method = &quot;REML&quot;) summary(tree_gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## growth ~ s(temp) + s(precip) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.9173 0.2222 211.2 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(temp) 5.850 7.011 156.32 &lt;2e-16 *** ## s(precip) 1.599 1.980 31.13 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.885 Deviance explained = 89.1% ## -REML = 370.52 Scale est. = 7.4046 n = 150 61.2 Understanding the summary # Key elements: # # Parametric coefficients: # - The intercept (like in GLM) # - Any linear terms you included # # Approximate significance of smooth terms: # - edf: Effective degrees of freedom # - Ref.df: Reference df for F-test # - F: Test statistic # - p-value: Is this smooth significantly different from zero? # # R-sq.(adj): Adjusted R-squared # Deviance explained: Percentage of null deviance explained # GCV/REML: Smoothness selection criterion (lower = better) 61.3 What is EDF (Effective Degrees of Freedom)? EDF tells you how wiggly the smooth is: EDF Interpretation 1 Linear (straight line) 2 Quadratic-like (one curve) 3-4 More complex curve 8-9 Very wiggly (close to max allowed) par(mfrow = c(1, 3)) # EDF ~ 1 (nearly linear) x1 &lt;- runif(100, 0, 10) y1 &lt;- 2 + 0.5 * x1 + rnorm(100, 0, 0.5) g1 &lt;- gam(y1 ~ s(x1)) plot(g1, main = paste(&quot;EDF =&quot;, round(sum(g1$edf) - 1, 1))) # EDF ~ 2-3 (curved) y2 &lt;- 2 + 0.5 * x1 - 0.1 * (x1 - 5)^2 + rnorm(100, 0, 0.5) g2 &lt;- gam(y2 ~ s(x1)) plot(g2, main = paste(&quot;EDF =&quot;, round(sum(g2$edf) - 1, 1))) # EDF higher (wiggly) y3 &lt;- sin(x1) + 0.3 * x1 + rnorm(100, 0, 0.3) g3 &lt;- gam(y3 ~ s(x1)) plot(g3, main = paste(&quot;EDF =&quot;, round(sum(g3$edf) - 1, 1))) Figure 61.1: Smooth terms with different EDF values. Higher EDF means more complex (wigglier) curves. par(mfrow = c(1, 1)) "],["part-3-types-of-smooth-terms.html", "Chapter 62 Part 3: Types of Smooth Terms 62.1 Basic smooth: s() 62.2 Cyclic smooths for temporal data 62.3 Tensor product smooths: te() and ti() 62.4 Factor-smooth interactions: by =", " Chapter 62 Part 3: Types of Smooth Terms 62.1 Basic smooth: s() The workhorse smooth for single predictors: # Basic smooth gam(y ~ s(x), data = df) # Control basis dimension (max wiggliness) gam(y ~ s(x, k = 20), data = df) # Allow more complexity # Specify basis type gam(y ~ s(x, bs = &quot;tp&quot;), data = df) # Thin plate (default) gam(y ~ s(x, bs = &quot;cr&quot;), data = df) # Cubic regression spline gam(y ~ s(x, bs = &quot;cc&quot;), data = df) # Cyclic (for circular data) 62.2 Cyclic smooths for temporal data For data that cycles (time of day, day of year), use cyclic splines: # Simulate phenology data doy &lt;- sample(1:365, 200, replace = TRUE) activity &lt;- 10 * sin(2 * pi * doy / 365) + 5 + rnorm(200, 0, 2) phenology_data &lt;- data.frame(doy, activity) # Cyclic smooth (endpoints match) cyclic_gam &lt;- gam(activity ~ s(doy, bs = &quot;cc&quot;, k = 12), data = phenology_data, knots = list(doy = c(0.5, 365.5))) # Specify boundaries plot(cyclic_gam, shade = TRUE, main = &quot;Cyclic Smooth for Day of Year&quot;) Figure 62.1: Cyclic smooth for day of year. The smooth is constrained to match at the boundaries (Jan 1 = Dec 31). 62.3 Tensor product smooths: te() and ti() For interactions between continuous variables: # Interaction between two continuous predictors tensor_gam &lt;- gam(growth ~ te(temp, precip, k = c(5, 5)), data = tree_data, method = &quot;REML&quot;) # 3D visualization vis.gam(tensor_gam, view = c(&quot;temp&quot;, &quot;precip&quot;), theta = 45, phi = 20, main = &quot;Growth ~ Temperature × Precipitation&quot;) Figure 62.2: Tensor product smooth showing how growth depends on both temperature AND precipitation simultaneously. te() vs ti(): - te(): Full tensor product (main effects + interaction) - ti(): Tensor interaction only (use with separate s() terms for main effects) # Separating main effects from interaction interaction_gam &lt;- gam(growth ~ s(temp) + s(precip) + ti(temp, precip), data = tree_data, method = &quot;REML&quot;) summary(interaction_gam) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## growth ~ s(temp) + s(precip) + ti(temp, precip) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.9099 0.2232 210.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(temp) 5.832 6.993 155.829 &lt;2e-16 *** ## s(precip) 1.587 1.965 31.112 &lt;2e-16 *** ## ti(temp,precip) 1.000 1.001 0.267 0.607 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.884 Deviance explained = 89.1% ## -REML = 368.96 Scale est. = 7.4442 n = 150 62.4 Factor-smooth interactions: by = Let smooths vary by group: # Add species to data tree_data$species &lt;- factor(sample(c(&quot;Oak&quot;, &quot;Pine&quot;, &quot;Maple&quot;), 150, replace = TRUE)) # Different smooth for each species by_gam &lt;- gam(growth ~ s(temp, by = species) + species, data = tree_data, method = &quot;REML&quot;) # Plot par(mfrow = c(1, 3)) for (sp in levels(tree_data$species)) { plot(by_gam, select = which(levels(tree_data$species) == sp), main = sp, shade = TRUE) } Figure 62.3: Separate smooth curves for each species. The ‘by’ argument allows the shape of the relationship to differ among groups. par(mfrow = c(1, 1)) "],["part-4-gams-with-different-distributions.html", "Chapter 63 Part 4: GAMs with Different Distributions 63.1 Common families 63.2 Example: Species distribution modeling 63.3 Example: Count data", " Chapter 63 Part 4: GAMs with Different Distributions Like GLMs, GAMs can handle non-normal responses using the family argument: 63.1 Common families # Gaussian (continuous, normal errors) - default gam(y ~ s(x), family = gaussian) # Poisson (counts) gam(counts ~ s(x), family = poisson) # Negative binomial (overdispersed counts) gam(counts ~ s(x), family = nb) # Binomial (proportions, presence/absence) gam(cbind(successes, failures) ~ s(x), family = binomial) gam(presence ~ s(x), family = binomial) # Gamma (positive continuous, right-skewed) gam(biomass ~ s(x), family = Gamma(link = &quot;log&quot;)) # Tweedie (zero-inflated continuous) gam(y ~ s(x), family = tw) # Beta (proportions between 0 and 1) gam(proportion ~ s(x), family = betar) 63.2 Example: Species distribution modeling # Simulate species occurrence data env_gradient &lt;- runif(200, 0, 100) prob_occurrence &lt;- plogis(-3 + 0.1 * env_gradient - 0.001 * env_gradient^2) presence &lt;- rbinom(200, 1, prob_occurrence) sdm_data &lt;- data.frame(presence, env_gradient) # Fit binomial GAM sdm_gam &lt;- gam(presence ~ s(env_gradient), family = binomial, data = sdm_data, method = &quot;REML&quot;) # Plot on probability scale plot(sdm_gam, trans = plogis, shift = coef(sdm_gam)[1], shade = TRUE, rug = FALSE, main = &quot;Probability of Occurrence&quot;, ylab = &quot;P(presence)&quot;) # Add raw data points(env_gradient, presence, pch = &quot;|&quot;, col = &quot;gray50&quot;) Figure 63.1: GAM for species presence/absence showing probability of occurrence along an environmental gradient. 63.3 Example: Count data # Simulate count data visitor_count &lt;- rpois(150, exp(1 + 0.5 * sin(tree_data$temp/5))) count_data &lt;- data.frame(visitors = visitor_count, temp = tree_data$temp) # Fit Poisson GAM count_gam &lt;- gam(visitors ~ s(temp), family = poisson, data = count_data, method = &quot;REML&quot;) # Plot on response scale plot(count_gam, trans = exp, shift = coef(count_gam)[1], shade = TRUE, main = &quot;Expected Count&quot;) Figure 63.2: GAM for count data using Poisson family. The relationship between abundance and the predictor is nonlinear. "],["part-5-model-checking-and-diagnostics.html", "Chapter 64 Part 5: Model Checking and Diagnostics 64.1 Basic diagnostics 64.2 Checking basis dimension (k) 64.3 DHARMa diagnostics 64.4 Concurvity", " Chapter 64 Part 5: Model Checking and Diagnostics 64.1 Basic diagnostics # Built-in diagnostics gam.check(tree_gam) Figure 64.1: Diagnostic plots from gam.check(). Look for patterns in residuals (top left), normality (top right), and adequate basis dimension (bottom panels). Figure 64.2: Diagnostic plots from gam.check(). Look for patterns in residuals (top left), normality (top right), and adequate basis dimension (bottom panels). Figure 64.3: Diagnostic plots from gam.check(). Look for patterns in residuals (top left), normality (top right), and adequate basis dimension (bottom panels). Figure 64.4: Diagnostic plots from gam.check(). Look for patterns in residuals (top left), normality (top right), and adequate basis dimension (bottom panels). ## ## Method: REML Optimizer: outer newton ## full convergence after 6 iterations. ## Gradient range [-1.607707e-05,1.51167e-05] ## (score 370.5186 &amp; scale 7.404603). ## Hessian positive definite, eigenvalue range [0.1077989,73.58424]. ## Model rank = 19 / 19 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(temp) 9.00 5.85 1.08 0.84 ## s(precip) 9.00 1.60 0.97 0.35 What to look for: Residuals vs fitted: No pattern, random scatter QQ plot: Points follow the line (for Gaussian) Histogram: Approximately normal (for Gaussian) Response vs fitted: Points near diagonal 64.2 Checking basis dimension (k) If k is too low, the smooth can’t capture the true pattern. gam.check() reports a test: # The k-index and p-value in gam.check() output # If p-value is low (&lt; 0.05), k might be too small # Increase k and refit tree_gam_highk &lt;- gam(growth ~ s(temp, k = 20) + s(precip, k = 20), data = tree_data, method = &quot;REML&quot;) # Compare EDF to k summary(tree_gam_highk) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## growth ~ s(temp, k = 20) + s(precip, k = 20) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.9173 0.2223 211.1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(temp) 6.177 7.690 142.37 &lt;2e-16 *** ## s(precip) 1.594 1.973 32.48 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.885 Deviance explained = 89.1% ## -REML = 370.66 Scale est. = 7.4103 n = 150 # If EDF is close to k-1, you might need even higher k 64.3 DHARMa diagnostics DHARMa works with GAMs too: sim_res &lt;- DHARMa::simulateResiduals(count_gam, mgcViz = FALSE) plot(sim_res) Figure 64.5: DHARMa residual diagnostics for GAM. Uniform residuals indicate good model fit. 64.4 Concurvity Concurvity is the GAM equivalent of multicollinearity—when smooth terms are correlated: # Check concurvity concurvity(tree_gam, full = TRUE) ## para s(temp) s(precip) ## worst 2.080907e-26 0.148580976 0.14858098 ## observed 2.080907e-26 0.008559661 0.03279061 ## estimate 2.080907e-26 0.035711545 0.03666940 # Values close to 1 indicate high concurvity (problematic) # Values close to 0 are fine "],["part-6-visualization.html", "Chapter 65 Part 6: Visualization 65.1 Using gratia for modern plots 65.2 Visualizing interactions 65.3 Plotting on response scale", " Chapter 65 Part 6: Visualization 65.1 Using gratia for modern plots The gratia package provides ggplot2-style visualization: library(gratia) # Draw all smooth terms gratia::draw(tree_gam) Figure 65.1: Smooth terms visualized with gratia. The draw() function creates publication-quality ggplot2 figures. library(gratia) library(dplyr) library(ggplot2) se &lt;- smooth_estimates(tree_gam) # ---- find the smooth label column smooth_col &lt;- base::intersect(c(&quot;smooth&quot;, &quot;.smooth&quot;, &quot;term&quot;, &quot;.term&quot;, &quot;label&quot;), names(se)) if (length(smooth_col) == 0) stop(&quot;Couldn&#39;t find a smooth-label column in smooth_estimates().&quot;) smooth_col &lt;- smooth_col[1] # ---- find the estimate column est_col &lt;- base::intersect(c(&quot;est&quot;, &quot;estimate&quot;, &quot;.estimate&quot;, &quot;value&quot;, &quot;.value&quot;), names(se)) if (length(est_col) == 0) stop(&quot;Couldn&#39;t find an estimate column (est/estimate/etc.) in smooth_estimates().&quot;) est_col &lt;- est_col[1] # ---- find the SE column se_col &lt;- base::intersect(c(&quot;se&quot;, &quot;se.fit&quot;, &quot;std.error&quot;, &quot;std_error&quot;, &quot;.se&quot;, &quot;.se.fit&quot;), names(se)) if (length(se_col) == 0) stop(&quot;Couldn&#39;t find a standard error column (se/...) in smooth_estimates().&quot;) se_col &lt;- se_col[1] # ---- filter to the temp smooth se_temp &lt;- se %&gt;% dplyr::filter(.data[[smooth_col]] == &quot;s(temp)&quot;) if (nrow(se_temp) == 0) { stop(&quot;No rows matched smooth == &#39;s(temp)&#39;. Check unique(se[[smooth_col]]) for the exact smooth label.&quot;) } # ---- find the x column for this smooth (usually temp) x_col &lt;- base::intersect(c(&quot;temp&quot;, &quot;x&quot;, &quot;.x&quot;), names(se_temp)) if (length(x_col) == 0) stop(&quot;Couldn&#39;t find x column for the smooth (temp/x).&quot;) x_col &lt;- x_col[1] ggplot(se_temp, aes(x = .data[[x_col]], y = .data[[est_col]])) + geom_ribbon( aes( ymin = .data[[est_col]] - 1.96 * .data[[se_col]], ymax = .data[[est_col]] + 1.96 * .data[[se_col]] ), alpha = 0.2 ) + geom_line(linewidth = 1) + labs(x = &quot;Temperature (°C)&quot;, y = &quot;Smooth effect&quot;, title = &quot;Partial effect of temperature&quot;) + theme_minimal() Figure 65.2: Examining an individual smooth component with confidence intervals. 65.2 Visualizing interactions # Contour plot vis.gam(tensor_gam, view = c(&quot;temp&quot;, &quot;precip&quot;), plot.type = &quot;contour&quot;, main = &quot;Growth Response Surface&quot;, xlab = &quot;Temperature&quot;, ylab = &quot;Precipitation&quot;) Figure 65.3: Visualizing a tensor product interaction as a contour plot. 65.3 Plotting on response scale # For non-Gaussian models, plot on response scale # Using gratia gratia::draw(sdm_gam, fun = plogis) # For binomial # Manual approach with predict new_data &lt;- data.frame(env_gradient = seq(0, 100, length = 200)) preds &lt;- predict(sdm_gam, newdata = new_data, type = &quot;response&quot;, se.fit = TRUE) ggplot() + geom_ribbon(aes(x = new_data$env_gradient, ymin = preds$fit - 1.96 * preds$se.fit, ymax = preds$fit + 1.96 * preds$se.fit), fill = &quot;coral&quot;, alpha = 0.3) + geom_line(aes(x = new_data$env_gradient, y = preds$fit), color = &quot;coral&quot;, linewidth = 1) + geom_rug(data = sdm_data, aes(x = env_gradient), sides = &quot;b&quot;) + labs(x = &quot;Environmental Gradient&quot;, y = &quot;P(Occurrence)&quot;, title = &quot;Species Distribution Model&quot;) + theme_minimal() "],["part-7-model-selection.html", "Chapter 66 Part 7: Model Selection 66.1 Comparing GAM to GLM 66.2 Comparing different smooths 66.3 Should I include this smooth?", " Chapter 66 Part 7: Model Selection 66.1 Comparing GAM to GLM # Fit competing models linear_model &lt;- gam(growth ~ temp + precip, data = tree_data) gam_model &lt;- gam(growth ~ s(temp) + s(precip), data = tree_data, method = &quot;REML&quot;) # Compare AIC AIC(linear_model, gam_model) ## df AIC ## linear_model 4.00000 1041.7660 ## gam_model 10.99108 739.2833 66.2 Comparing different smooths # Different complexity gam_simple &lt;- gam(growth ~ s(temp, k = 4) + s(precip, k = 4), data = tree_data, method = &quot;REML&quot;) gam_complex &lt;- gam(growth ~ s(temp, k = 15) + s(precip, k = 15), data = tree_data, method = &quot;REML&quot;) AIC(gam_simple, gam_model, gam_complex) ## df AIC ## gam_simple 6.856019 733.7418 ## gam_model 10.991085 739.2833 ## gam_complex 11.533571 740.2807 66.3 Should I include this smooth? If a smooth has EDF ≈ 1 and non-significant p-value, you might simplify to a linear term: # Compare models with and without smooth with_smooth &lt;- gam(growth ~ s(temp) + s(precip), data = tree_data, method = &quot;REML&quot;) mixed_smooth &lt;- gam(growth ~ s(temp) + precip, data = tree_data, method = &quot;REML&quot;) AIC(with_smooth, mixed_smooth) ## df AIC ## with_smooth 10.991085 739.2833 ## mixed_smooth 8.965535 736.8989 anova(mixed_smooth, with_smooth, test = &quot;F&quot;) ## Analysis of Deviance Table ## ## Model 1: growth ~ s(temp) + precip ## Model 2: growth ~ s(temp) + s(precip) ## Resid. Df Resid. Dev Df Deviance F Pr(&gt;F) ## 1 140.87 1059.8 ## 2 138.47 1048.1 2.4038 11.712 0.658 0.5465 "],["part-8-gams-with-random-effects-gamms.html", "Chapter 67 Part 8: GAMs with Random Effects (GAMMs) 67.1 Alternative: gamm() function 67.2 Using bam() for large datasets", " Chapter 67 Part 8: GAMs with Random Effects (GAMMs) GAMs can include random effects using bs = \"re\": # If AlgDesign is attached, detach it (avoid masking / surprises during knit) if (&quot;package:AlgDesign&quot; %in% search()) { detach(&quot;package:AlgDesign&quot;, unload = TRUE, character.only = TRUE) } # HARD PATCH: mgcv is calling AlgDesign:::model.matrix.formula directly if (&quot;AlgDesign&quot; %in% loadedNamespaces()) { assignInNamespace( x = &quot;model.matrix.formula&quot;, value = function(form, data = environment(form), ...) { if (is.list(data) &amp;&amp; !is.data.frame(data)) data &lt;- as.data.frame(data) tt &lt;- stats::terms(form, data = data) stats::model.matrix(tt, data = data) }, ns = &quot;AlgDesign&quot; ) } # Ensure the data object really is a plain data.frame tree_data &lt;- as.data.frame(tree_data) # Add site factor tree_data$site &lt;- factor(rep(1:15, length.out = nrow(tree_data))) # Fit GAM with random intercept for site gamm_model &lt;- mgcv::gam( growth ~ s(temp) + s(site, bs = &quot;re&quot;), data = tree_data, method = &quot;REML&quot; ) summary(gamm_model) ## ## Family: gaussian ## Link function: identity ## ## Formula: ## growth ~ s(temp) + s(site, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.9173 0.2906 161.5 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df F p-value ## s(temp) 5.502 6.635 113.679 &lt;2e-16 *** ## s(site) 2.419 14.000 0.209 0.266 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.838 Deviance explained = 84.7% ## -REML = 395.75 Scale est. = 10.385 n = 150 67.1 Alternative: gamm() function # Using gamm() for more complex random structures gamm_alt &lt;- gamm(growth ~ s(temp), random = list(site = ~1), data = tree_data) # Access GAM part summary(gamm_alt$gam) # Access mixed model part summary(gamm_alt$lme) 67.2 Using bam() for large datasets For large datasets, bam() is faster than gam(): # bam() for big data big_gam &lt;- bam(y ~ s(x) + s(site, bs = &quot;re&quot;), data = big_data, discrete = TRUE) # Further speedup "],["part-9-complete-example.html", "Chapter 68 Part 9: Complete Example", " Chapter 68 Part 9: Complete Example Let’s work through a complete analysis of pollinator activity through the day: # Simulate daily pollinator activity n &lt;- 300 time &lt;- runif(n, 6, 20) # 6 AM to 8 PM temp &lt;- 15 + 8 * sin(pi * (time - 6) / 12) + rnorm(n, 0, 2) # Temperature curve cloud &lt;- runif(n, 0, 100) # Cloud cover (%) site &lt;- factor(sample(1:10, n, replace = TRUE)) # Activity peaks mid-morning and mid-afternoon, drops with clouds activity &lt;- exp( 1.5 + 0.8 * sin(pi * (time - 8) / 6) * (1 - 0.3 * sin(pi * (time - 12) / 4)) + -0.01 * cloud + rnorm(n, 0, 0.2) ) activity &lt;- round(activity) poll_data &lt;- data.frame(activity, time, temp, cloud, site) # Explore ggplot(poll_data, aes(x = time, y = activity)) + geom_point(alpha = 0.5) + labs(x = &quot;Time of Day&quot;, y = &quot;Pollinator Count&quot;) + theme_minimal() # Fit GAM poll_gam &lt;- gam(activity ~ s(time, k = 12) + s(temp) + cloud + s(site, bs = &quot;re&quot;), family = poisson, data = poll_data, method = &quot;REML&quot;) summary(poll_gam) ## ## Family: poisson ## Link function: log ## ## Formula: ## activity ~ s(time, k = 12) + s(temp) + cloud + s(site, bs = &quot;re&quot;) ## ## Parametric coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 1.461750 0.060222 24.273 &lt; 2e-16 *** ## cloud -0.008880 0.001121 -7.921 2.35e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Approximate significance of smooth terms: ## edf Ref.df Chi.sq p-value ## s(time) 7.0488353 8.427 213.114 &lt;2e-16 *** ## s(temp) 1.0000036 1.000 0.534 0.465 ## s(site) 0.0002715 9.000 0.000 0.994 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## R-sq.(adj) = 0.864 Deviance explained = 89.1% ## -REML = 487.96 Scale est. = 1 n = 300 # Check model gam.check(poll_gam) Figure 68.1: Diagnostics for the pollinator activity GAM. Figure 68.2: Diagnostics for the pollinator activity GAM. Figure 68.3: Diagnostics for the pollinator activity GAM. Figure 68.4: Diagnostics for the pollinator activity GAM. ## ## Method: REML Optimizer: outer newton ## full convergence after 10 iterations. ## Gradient range [-0.0001087266,7.728948e-09] ## (score 487.9648 &amp; scale 1). ## Hessian positive definite, eigenvalue range [1.331896e-06,2.910496]. ## Model rank = 32 / 32 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(time) 1.10e+01 7.05e+00 1.08 0.92 ## s(temp) 9.00e+00 1.00e+00 1.07 0.89 ## s(site) 1.00e+01 2.71e-04 NA NA # Visualize gratia::draw(poll_gam, select = c(1, 2)) Figure 68.5: Pollinator activity GAM results showing nonlinear patterns through the day and with temperature. "],["part-10-reporting.html", "Chapter 69 Part 10: Reporting 69.1 What to report 69.2 Sample methods and results 69.3 Key takeaways 69.4 Assignment", " Chapter 69 Part 10: Reporting 69.1 What to report Why GAM: Nonlinearity expected/observed Model structure: Response distribution, smooth terms, any random effects Smoothing method: REML, GCV, etc. EDF for each smooth: Indicates complexity Model fit: Deviance explained, R² Diagnostic checks: gam.check() results Figures: Show the smooth relationships 69.2 Sample methods and results 69.2.1 Methods We modeled tree growth as a function of temperature and precipitation using a Generalized Additive Model (GAM) to allow for nonlinear relationships. Smooth terms were fit using thin plate regression splines with the smoothing parameter estimated via restricted maximum likelihood (REML). The basis dimension (k) was set to 10 for each smooth and verified as adequate using the gam.check() function. We compared the GAM to a linear model using AIC and assessed model fit through residual diagnostics. Analyses were conducted using the mgcv package (Wood 2017) in R version 4.3.1. 69.2.2 Results The GAM explained 67% of deviance in tree growth, substantially outperforming the linear model (ΔAIC = 24.5). Growth showed a nonlinear response to temperature (EDF = 3.2, F = 18.4, p &lt; 0.001), increasing from 5°C to approximately 22°C before declining at higher temperatures (Fig. X). The effect of precipitation was approximately linear (EDF = 1.1, F = 5.7, p = 0.018), with growth increasing 0.8 mm per 100 mm of annual precipitation. Residual diagnostics indicated adequate model fit with no evidence of unmodeled nonlinearity (k-index &gt; 1.0 for all smooths) or residual patterns. 69.3 Key takeaways GAMs fit flexible curves — Let the data determine the shape of relationships EDF indicates complexity — 1 = linear, higher = more wiggly Use REML for smoothness selection — Generally the best default Check k is adequate — Look at gam.check() output and k-index Different families for different data — GAMs extend to Poisson, binomial, etc. Visualize your smooths — The curves ARE the results Don’t over-interpret wiggles — If confidence bands are wide, the wiggles may not be real Consider mixed models — bs = “re” adds random effects easily 69.4 Assignment 69.4.1 Part 1: Conceptual questions What does an EDF of 1.0 tell you about a smooth term? What about an EDF of 8.5? Your gam.check() output shows a k-index of 0.65 and a significant p-value for one smooth. What does this indicate and what should you do? When would you use a cyclic smooth (bs = “cc”)? Give an ecological example. 69.4.2 Part 2: Basic GAM fitting Using this simulated vegetation data: set.seed(321) elevation &lt;- runif(120, 500, 3000) veg_cover &lt;- 60 * exp(-0.5 * ((elevation - 1500)/500)^2) + rnorm(120, 0, 5) veg_cover &lt;- pmax(0, pmin(100, veg_cover)) # Constrain to 0-100 veg_data &lt;- data.frame(elevation, veg_cover) Plot the data and describe the apparent relationship Fit a GAM with a smooth term for elevation Check the model diagnostics Report the EDF and interpret it Create a publication-quality figure 69.4.3 Part 3: GAM vs GLM comparison Using the same data: Fit a linear model and a quadratic model (lm) Compare to the GAM using AIC Which model do you prefer and why? How do the predictions differ at the edges of your data? 69.4.4 Part 4: Multiple smooths Using this multi-predictor dataset: set.seed(654) n &lt;- 200 soil_moisture &lt;- runif(n, 10, 90) soil_temp &lt;- runif(n, 5, 35) ph &lt;- runif(n, 4, 8) biomass &lt;- exp( 2 + -0.5 * ((soil_moisture - 50)/20)^2 + # Optimum at 50% 0.02 * soil_temp + # Linear with temp -0.3 * (ph - 6)^2 # Optimum at pH 6 ) + rnorm(n, 0, 0.5) multi_data &lt;- data.frame(biomass, soil_moisture, soil_temp, ph) Fit a GAM with smooth terms for all three predictors Which relationships appear nonlinear? (Check EDF) Consider simplifying to linear terms where appropriate Test for an interaction between soil moisture and temperature Write a results paragraph 69.4.5 Part 5: Reflection In 2-3 sentences, explain why GAMs are particularly useful for species distribution modeling compared to traditional logistic regression. "],["bootstrapping-and-resampling-methods.html", "Chapter 70 Bootstrapping and Resampling Methods 70.1 The core insight 70.2 When to use bootstrapping 70.3 Setup", " Chapter 70 Bootstrapping and Resampling Methods Throughout this textbook, we’ve relied on formulas to calculate standard errors and confidence intervals. These formulas work because we make assumptions—usually that data come from a normal distribution, or that sample sizes are large enough for the Central Limit Theorem to apply. But what if those assumptions don’t hold? What if you want a confidence interval for a statistic that has no standard formula—like the median, a ratio, or a diversity index? What if your sample is small and clearly non-normal? Bootstrapping solves these problems by using the data themselves to estimate uncertainty. Instead of assuming a theoretical distribution, you create an empirical distribution by resampling from your observed data. 70.1 The core insight The bootstrap is based on a simple but profound idea: Your sample is the best estimate of the population. So treat your sample as if it were the population, and resample from it. By drawing many “bootstrap samples” from your data (with replacement), you can see how much your statistic varies across samples. This variation estimates the sampling distribution—the same thing that standard error formulas try to capture, but without distributional assumptions. 70.2 When to use bootstrapping Situation Why bootstrap helps Non-normal data No normality assumption needed Small samples Works when CLT doesn’t apply Complex statistics Median, ratios, ICCs have no simple SE formula Checking robustness See if conclusions depend on a few observations Custom metrics Any statistic you can calculate, you can bootstrap 70.3 Setup library(tidyverse) library(boot) # The main bootstrapping package set.seed(42) "],["part-1-the-bootstrap-principle.html", "Chapter 71 Part 1: The Bootstrap Principle 71.1 A simple example 71.2 Why “with replacement”?", " Chapter 71 Part 1: The Bootstrap Principle 71.1 A simple example Suppose you measure the biomass of 15 plants and want to estimate the population mean with a confidence interval. # Plant biomass data (grams) biomass &lt;- c(12.3, 15.7, 8.9, 22.4, 14.2, 18.6, 11.1, 25.3, 13.8, 16.9, 9.5, 20.1, 14.7, 17.3, 19.8) # Sample statistics n &lt;- length(biomass) cat(&quot;Sample size:&quot;, n, &quot;\\n&quot;) ## Sample size: 15 cat(&quot;Mean:&quot;, round(mean(biomass), 2), &quot;\\n&quot;) ## Mean: 16.04 cat(&quot;SD:&quot;, round(sd(biomass), 2), &quot;\\n&quot;) ## SD: 4.69 71.1.1 The standard approach (parametric) Assuming normality, we calculate a t-based confidence interval: t.test(biomass)$conf.int ## [1] 13.44188 18.63812 ## attr(,&quot;conf.level&quot;) ## [1] 0.95 This works well if data are approximately normal. But what if they’re not? 71.1.2 The bootstrap approach (non-parametric) Instead of assuming normality, we resample from our data: # Number of bootstrap samples n_boot &lt;- 10000 # Storage for bootstrap means boot_means &lt;- numeric(n_boot) # Resample with replacement for (i in 1:n_boot) { boot_sample &lt;- sample(biomass, size = n, replace = TRUE) boot_means[i] &lt;- mean(boot_sample) } # Visualize the bootstrap distribution hist(boot_means, breaks = 50, col = &quot;steelblue&quot;, border = &quot;white&quot;, main = &quot;Bootstrap Distribution of the Mean&quot;, xlab = &quot;Bootstrap Mean&quot;) abline(v = mean(biomass), col = &quot;firebrick&quot;, lwd = 2, lty = 2) legend(&quot;topright&quot;, &quot;Original mean&quot;, col = &quot;firebrick&quot;, lty = 2, lwd = 2) Figure 71.1: Bootstrap distribution of the sample mean. Each value represents the mean from one bootstrap sample. The spread of this distribution estimates the standard error. 71.2 Why “with replacement”? Sampling with replacement means the same observation can appear multiple times in a bootstrap sample. This is essential because: It allows bootstrap samples to vary (otherwise every sample would be identical to the original) It mimics the process of sampling from a population It produces the correct amount of variability in the bootstrap distribution # Example bootstrap sample - note repeated values set.seed(123) original &lt;- 1:10 boot_sample &lt;- sample(original, replace = TRUE) cat(&quot;Original:&quot;, original, &quot;\\n&quot;) ## Original: 1 2 3 4 5 6 7 8 9 10 cat(&quot;Bootstrap sample:&quot;, boot_sample, &quot;\\n&quot;) ## Bootstrap sample: 3 3 10 2 6 5 4 6 9 10 cat(&quot;Value 6 appears&quot;, sum(boot_sample == 6), &quot;times\\n&quot;) ## Value 6 appears 2 times cat(&quot;Value 4 appears&quot;, sum(boot_sample == 4), &quot;times\\n&quot;) ## Value 4 appears 1 times Each bootstrap sample has the same size as the original, but some observations appear multiple times while others don’t appear at all. On average, about 63.2% of original observations appear in each bootstrap sample. "],["part-2-bootstrap-confidence-intervals.html", "Chapter 72 Part 2: Bootstrap Confidence Intervals 72.1 Method 1: Percentile interval 72.2 Method 2: Basic (reverse percentile) interval 72.3 Method 3: BCa (Bias-Corrected and Accelerated) 72.4 Comparing CI methods", " Chapter 72 Part 2: Bootstrap Confidence Intervals There are several methods for constructing bootstrap CIs. They differ in how they handle bias and skewness. 72.1 Method 1: Percentile interval The simplest approach—just take percentiles of the bootstrap distribution: # 95% CI using percentiles ci_percentile &lt;- quantile(boot_means, probs = c(0.025, 0.975)) ci_percentile ## 2.5% 97.5% ## 13.76667 18.32000 Pros: Simple, intuitive Cons: Can be biased if bootstrap distribution is skewed 72.2 Method 2: Basic (reverse percentile) interval Accounts for bias by using the difference between bootstrap estimates and the original estimate: # Basic interval theta_hat &lt;- mean(biomass) # Original estimate ci_basic &lt;- c(2 * theta_hat - quantile(boot_means, 0.975), 2 * theta_hat - quantile(boot_means, 0.025)) names(ci_basic) &lt;- c(&quot;2.5%&quot;, &quot;97.5%&quot;) ci_basic ## 2.5% 97.5% ## 13.76000 18.31333 72.3 Method 3: BCa (Bias-Corrected and Accelerated) The most sophisticated method—adjusts for both bias and skewness. This is generally the recommended method for most applications. # BCa requires more complex calculations # We&#39;ll use the boot package for this (shown later) # Key idea: adjusts percentiles based on: # - Bias: Is the bootstrap distribution centered on the estimate? # - Acceleration: Does variability change with the parameter value? 72.4 Comparing CI methods # Compare with parametric CI ci_t &lt;- t.test(biomass)$conf.int comparison &lt;- data.frame( Method = c(&quot;t-interval (parametric)&quot;, &quot;Percentile&quot;, &quot;Basic&quot;), Lower = c(ci_t[1], ci_percentile[1], ci_basic[1]), Upper = c(ci_t[2], ci_percentile[2], ci_basic[2]) ) comparison$Width &lt;- comparison$Upper - comparison$Lower knitr::kable(comparison, digits = 2, caption = &quot;Comparison of confidence interval methods&quot;) Table 72.1: Comparison of confidence interval methods Method Lower Upper Width t-interval (parametric) 13.44 18.64 5.20 Percentile 13.77 18.32 4.55 Basic 13.76 18.31 4.55 For approximately normal data, all methods give similar results. The differences become important with skewed data or small samples. "],["part-3-bootstrap-standard-errors.html", "Chapter 73 Part 3: Bootstrap Standard Errors", " Chapter 73 Part 3: Bootstrap Standard Errors The standard error is simply the standard deviation of the bootstrap distribution: # Bootstrap standard error boot_se &lt;- sd(boot_means) cat(&quot;Bootstrap SE:&quot;, round(boot_se, 3), &quot;\\n&quot;) ## Bootstrap SE: 1.173 # Compare to formula-based SE formula_se &lt;- sd(biomass) / sqrt(n) cat(&quot;Formula SE:&quot;, round(formula_se, 3), &quot;\\n&quot;) ## Formula SE: 1.211 This works for any statistic—just calculate it for each bootstrap sample and take the SD. # Bootstrap SE for the median (no formula exists!) boot_medians &lt;- numeric(n_boot) for (i in 1:n_boot) { boot_sample &lt;- sample(biomass, replace = TRUE) boot_medians[i] &lt;- median(boot_sample) } cat(&quot;Median:&quot;, median(biomass), &quot;\\n&quot;) ## Median: 15.7 cat(&quot;Bootstrap SE of median:&quot;, round(sd(boot_medians), 3), &quot;\\n&quot;) ## Bootstrap SE of median: 1.607 cat(&quot;95% CI for median:&quot;, round(quantile(boot_medians, c(0.025, 0.975)), 2), &quot;\\n&quot;) ## 95% CI for median: 12.3 19.8 "],["part-4-when-to-bootstrap.html", "Chapter 74 Part 4: When to Bootstrap 74.1 Situations where bootstrapping shines 74.2 When NOT to bootstrap", " Chapter 74 Part 4: When to Bootstrap 74.1 Situations where bootstrapping shines 74.1.1 1. Statistics without standard formulas # Coefficient of variation (CV = SD/mean) boot_cv &lt;- numeric(n_boot) for (i in 1:n_boot) { boot_sample &lt;- sample(biomass, replace = TRUE) boot_cv[i] &lt;- sd(boot_sample) / mean(boot_sample) } cat(&quot;CV:&quot;, round(sd(biomass)/mean(biomass), 3), &quot;\\n&quot;) ## CV: 0.292 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_cv, c(0.025, 0.975)), 3), &quot;\\n&quot;) ## Bootstrap 95% CI: 0.192 0.366 74.1.2 2. Ratios # Ratio of means between two groups group1 &lt;- c(12.3, 15.7, 8.9, 14.2, 11.1) group2 &lt;- c(18.6, 25.3, 16.9, 20.1, 19.8) # Observed ratio obs_ratio &lt;- mean(group2) / mean(group1) # Bootstrap the ratio boot_ratios &lt;- numeric(n_boot) for (i in 1:n_boot) { boot1 &lt;- sample(group1, replace = TRUE) boot2 &lt;- sample(group2, replace = TRUE) boot_ratios[i] &lt;- mean(boot2) / mean(boot1) } cat(&quot;Ratio of means:&quot;, round(obs_ratio, 3), &quot;\\n&quot;) ## Ratio of means: 1.619 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_ratios, c(0.025, 0.975)), 3), &quot;\\n&quot;) ## Bootstrap 95% CI: 1.334 2.01 74.1.3 3. Non-normal data # Highly skewed data (e.g., species abundances) skewed_data &lt;- c(1, 1, 2, 2, 3, 3, 4, 5, 8, 12, 25, 45, 120) par(mfrow = c(1, 2)) hist(skewed_data, main = &quot;Original Data (Skewed)&quot;, col = &quot;coral&quot;) # Bootstrap the mean boot_skewed &lt;- replicate(n_boot, mean(sample(skewed_data, replace = TRUE))) hist(boot_skewed, main = &quot;Bootstrap Distribution&quot;, col = &quot;steelblue&quot;, breaks = 30) Figure 74.1: Bootstrap is especially useful for skewed data where normality assumptions fail. par(mfrow = c(1, 1)) cat(&quot;Mean:&quot;, round(mean(skewed_data), 2), &quot;\\n&quot;) ## Mean: 17.77 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_skewed, c(0.025, 0.975)), 2), &quot;\\n&quot;) ## Bootstrap 95% CI: 4.38 37.69 74.1.4 4. Small samples With small samples, the Central Limit Theorem doesn’t apply, and parametric intervals may be unreliable. Bootstrap provides a data-driven alternative. 74.2 When NOT to bootstrap Very small samples (n &lt; 10): Bootstrap relies on resampling—if your sample poorly represents the population, bootstrap CIs will also be poor Dependent data: Standard bootstrap assumes independence; use block bootstrap for time series Extreme quantiles: Bootstrapping the 99th percentile requires very large samples "],["part-5-the-jackknife.html", "Chapter 75 Part 5: The Jackknife 75.1 How it works 75.2 Identifying influential observations", " Chapter 75 Part 5: The Jackknife The jackknife is an older resampling method that systematically leaves out one observation at a time. While largely superseded by bootstrapping, it’s still useful for: Estimating bias Calculating influence of individual observations Some specific applications (e.g., rarefaction in ecology) 75.1 How it works # Jackknife: leave out each observation once n &lt;- length(biomass) jack_estimates &lt;- numeric(n) for (i in 1:n) { jack_estimates[i] &lt;- mean(biomass[-i]) # Mean without observation i } # Jackknife estimate of bias theta_hat &lt;- mean(biomass) theta_jack &lt;- mean(jack_estimates) bias &lt;- (n - 1) * (theta_jack - theta_hat) cat(&quot;Original mean:&quot;, round(theta_hat, 3), &quot;\\n&quot;) ## Original mean: 16.04 cat(&quot;Jackknife estimate of bias:&quot;, round(bias, 4), &quot;\\n&quot;) ## Jackknife estimate of bias: 0 # Jackknife standard error jack_se &lt;- sqrt((n - 1) / n * sum((jack_estimates - theta_jack)^2)) cat(&quot;Jackknife SE:&quot;, round(jack_se, 3), &quot;\\n&quot;) ## Jackknife SE: 1.211 75.2 Identifying influential observations # Which observations most affect the mean? influence &lt;- data.frame( Removed = 1:n, Value = biomass, JackMean = jack_estimates, Influence = jack_estimates - theta_jack ) ggplot(influence, aes(x = Removed, y = JackMean)) + geom_hline(yintercept = theta_hat, linetype = &quot;dashed&quot;, color = &quot;firebrick&quot;) + geom_point(size = 3, color = &quot;steelblue&quot;) + geom_segment(aes(xend = Removed, yend = theta_hat), color = &quot;gray50&quot;) + labs(title = &quot;Jackknife Influence Plot&quot;, subtitle = &quot;How much does each observation affect the mean?&quot;, x = &quot;Observation Removed&quot;, y = &quot;Jackknife Mean&quot;) + theme_minimal() Figure 75.1: Jackknife estimates with each observation removed. Points far from the overall mean indicate influential observations. "],["part-6-practical-applications.html", "Chapter 76 Part 6: Practical Applications 76.1 Application 1: Correlation coefficients 76.2 Application 2: Regression coefficients 76.3 Application 3: Difference between groups 76.4 Application 4: Custom ecological metrics", " Chapter 76 Part 6: Practical Applications 76.1 Application 1: Correlation coefficients Correlation coefficients have complex sampling distributions, especially for small samples. # Example: correlation between two variables x &lt;- c(2.1, 3.5, 4.2, 5.1, 6.3, 7.2, 8.1, 9.4, 10.2, 11.5) y &lt;- c(3.2, 4.1, 5.8, 6.2, 8.1, 7.9, 9.5, 11.2, 10.8, 13.1) # Observed correlation obs_cor &lt;- cor(x, y) # Bootstrap correlation boot_cors &lt;- numeric(n_boot) for (i in 1:n_boot) { idx &lt;- sample(1:length(x), replace = TRUE) boot_cors[i] &lt;- cor(x[idx], y[idx]) } cat(&quot;Correlation:&quot;, round(obs_cor, 3), &quot;\\n&quot;) ## Correlation: 0.989 cat(&quot;Bootstrap SE:&quot;, round(sd(boot_cors), 3), &quot;\\n&quot;) ## Bootstrap SE: 0.007 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_cors, c(0.025, 0.975)), 3), &quot;\\n&quot;) ## Bootstrap 95% CI: 0.971 0.998 76.2 Application 2: Regression coefficients When regression assumptions are violated, bootstrap provides robust inference. # Simple regression fit &lt;- lm(y ~ x) obs_slope &lt;- coef(fit)[2] # Bootstrap regression boot_slopes &lt;- numeric(n_boot) for (i in 1:n_boot) { idx &lt;- sample(1:length(x), replace = TRUE) boot_fit &lt;- lm(y[idx] ~ x[idx]) boot_slopes[i] &lt;- coef(boot_fit)[2] } # Compare to parametric CI param_ci &lt;- confint(fit)[2, ] cat(&quot;Slope:&quot;, round(obs_slope, 3), &quot;\\n&quot;) ## Slope: 1.032 cat(&quot;Parametric 95% CI:&quot;, round(param_ci, 3), &quot;\\n&quot;) ## Parametric 95% CI: 0.908 1.156 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_slopes, c(0.025, 0.975)), 3), &quot;\\n&quot;) ## Bootstrap 95% CI: 0.917 1.126 76.3 Application 3: Difference between groups # Two groups control &lt;- c(12.3, 15.7, 8.9, 14.2, 11.1, 13.5) treatment &lt;- c(18.6, 25.3, 16.9, 20.1, 19.8, 22.4) # Observed difference obs_diff &lt;- mean(treatment) - mean(control) # Bootstrap the difference boot_diffs &lt;- numeric(n_boot) for (i in 1:n_boot) { boot_ctrl &lt;- sample(control, replace = TRUE) boot_trt &lt;- sample(treatment, replace = TRUE) boot_diffs[i] &lt;- mean(boot_trt) - mean(boot_ctrl) } cat(&quot;Mean difference:&quot;, round(obs_diff, 2), &quot;\\n&quot;) ## Mean difference: 7.9 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_diffs, c(0.025, 0.975)), 2), &quot;\\n&quot;) ## Bootstrap 95% CI: 5.23 10.83 # Does CI include zero? ci &lt;- quantile(boot_diffs, c(0.025, 0.975)) if (ci[1] &gt; 0 | ci[2] &lt; 0) { cat(&quot;CI excludes zero: significant difference\\n&quot;) } else { cat(&quot;CI includes zero: not significant\\n&quot;) } ## CI excludes zero: significant difference 76.4 Application 4: Custom ecological metrics # Shannon diversity index shannon &lt;- function(x) { p &lt;- x[x &gt; 0] / sum(x) -sum(p * log(p)) } # Community abundance data community &lt;- c(45, 23, 18, 12, 8, 5, 3, 2, 1, 1) obs_shannon &lt;- shannon(community) # Bootstrap Shannon diversity # Note: resample individuals, not species individuals &lt;- rep(1:length(community), community) total_n &lt;- sum(community) boot_shannon &lt;- numeric(n_boot) for (i in 1:n_boot) { boot_indiv &lt;- sample(individuals, size = total_n, replace = TRUE) boot_comm &lt;- tabulate(boot_indiv, nbins = length(community)) boot_shannon[i] &lt;- shannon(boot_comm) } cat(&quot;Shannon H&#39;:&quot;, round(obs_shannon, 3), &quot;\\n&quot;) ## Shannon H&#39;: 1.765 cat(&quot;Bootstrap SE:&quot;, round(sd(boot_shannon), 3), &quot;\\n&quot;) ## Bootstrap SE: 0.083 cat(&quot;Bootstrap 95% CI:&quot;, round(quantile(boot_shannon, c(0.025, 0.975)), 3), &quot;\\n&quot;) ## Bootstrap 95% CI: 1.555 1.88 "],["part-7-using-the-boot-package.html", "Chapter 77 Part 7: Using the boot Package 77.1 Basic workflow 77.2 Extracting confidence intervals 77.3 Visualizing bootstrap results 77.4 Example: Correlation with boot package 77.5 Example: Regression slope with boot package 77.6 Example: Multiple statistics at once", " Chapter 77 Part 7: Using the boot Package The boot package provides efficient, well-tested functions for bootstrapping with all CI methods. 77.1 Basic workflow library(boot) # Step 1: Write a function that calculates your statistic # Arguments: data, indices (which observations to use) mean_func &lt;- function(data, indices) { mean(data[indices]) } # Step 2: Run bootstrap boot_result &lt;- boot(data = biomass, statistic = mean_func, R = 10000) # Number of bootstrap samples # View results boot_result ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = biomass, statistic = mean_func, R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 16.04 -0.003264 1.164378 77.2 Extracting confidence intervals # Get CIs using multiple methods boot_cis &lt;- boot.ci(boot_result, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;, &quot;bca&quot;)) boot_cis ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = boot_result, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;, ## &quot;bca&quot;)) ## ## Intervals : ## Level Normal Basic ## 95% (13.76, 18.33 ) (13.75, 18.34 ) ## ## Level Percentile BCa ## 95% (13.74, 18.33 ) (13.81, 18.41 ) ## Calculations and Intervals on Original Scale CI types: - norm: Normal approximation (assumes normality) - basic: Basic/reverse percentile - perc: Percentile - bca: Bias-corrected accelerated (recommended) 77.3 Visualizing bootstrap results plot(boot_result) Figure 77.1: Diagnostic plots from the boot package showing the bootstrap distribution and QQ plot for assessing normality. 77.4 Example: Correlation with boot package # Data frame for boot cor_data &lt;- data.frame(x = x, y = y) # Correlation function cor_func &lt;- function(data, indices) { d &lt;- data[indices, ] cor(d$x, d$y) } # Bootstrap cor_boot &lt;- boot(cor_data, cor_func, R = 10000) # CIs boot.ci(cor_boot, type = &quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = cor_boot, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% ( 0.9658, 0.9970 ) ## Calculations and Intervals on Original Scale 77.5 Example: Regression slope with boot package # Slope function slope_func &lt;- function(data, indices) { d &lt;- data[indices, ] fit &lt;- lm(y ~ x, data = d) coef(fit)[2] } # Bootstrap reg_boot &lt;- boot(cor_data, slope_func, R = 10000) # Results reg_boot ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = cor_data, statistic = slope_func, R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 1.032329 -0.002810765 0.05320454 boot.ci(reg_boot, type = &quot;bca&quot;) ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = reg_boot, type = &quot;bca&quot;) ## ## Intervals : ## Level BCa ## 95% ( 0.905, 1.120 ) ## Calculations and Intervals on Original Scale 77.6 Example: Multiple statistics at once # Function returning multiple statistics multi_func &lt;- function(data, indices) { d &lt;- data[indices] c(mean = mean(d), median = median(d), sd = sd(d), cv = sd(d)/mean(d)) } # Bootstrap multi_boot &lt;- boot(biomass, multi_func, R = 10000) multi_boot ## ## ORDINARY NONPARAMETRIC BOOTSTRAP ## ## ## Call: ## boot(data = biomass, statistic = multi_func, R = 10000) ## ## ## Bootstrap Statistics : ## original bias std. error ## t1* 16.040000 -0.001461333 1.15668972 ## t2* 15.700000 0.124650000 1.60020106 ## t3* 4.691603 -0.210587091 0.72488601 ## t4* 0.292494 -0.012372368 0.04537672 # CIs for each statistic (index specifies which one) boot.ci(multi_boot, type = &quot;bca&quot;, index = 1) # Mean ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = multi_boot, type = &quot;bca&quot;, index = 1) ## ## Intervals : ## Level BCa ## 95% (13.89, 18.45 ) ## Calculations and Intervals on Original Scale boot.ci(multi_boot, type = &quot;bca&quot;, index = 2) # Median ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = multi_boot, type = &quot;bca&quot;, index = 2) ## ## Intervals : ## Level BCa ## 95% (12.3, 18.6 ) ## Calculations and Intervals on Original Scale boot.ci(multi_boot, type = &quot;bca&quot;, index = 4) # CV ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 10000 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = multi_boot, type = &quot;bca&quot;, index = 4) ## ## Intervals : ## Level BCa ## 95% ( 0.2178, 0.3998 ) ## Calculations and Intervals on Original Scale "],["part-8-reporting-bootstrap-results.html", "Chapter 78 Part 8: Reporting Bootstrap Results 78.1 What to report 78.2 Sample methods and results 78.3 Key takeaways 78.4 Assignment", " Chapter 78 Part 8: Reporting Bootstrap Results 78.1 What to report Number of bootstrap replicates (typically 1000-10000) Type of CI (percentile, BCa, etc.) The estimate and confidence interval Why bootstrap was used (briefly) 78.2 Sample methods and results 78.2.1 Methods (brief version) Confidence intervals were calculated using bias-corrected and accelerated (BCa) bootstrapping with 10,000 replicates. 78.2.2 Methods (detailed version) Because the sampling distribution of the coefficient of variation has no closed-form solution, we estimated 95% confidence intervals using the bootstrap. We generated 10,000 bootstrap samples by resampling observations with replacement and calculated the CV for each sample. We report bias-corrected and accelerated (BCa) confidence intervals, which adjust for both bias and skewness in the bootstrap distribution. Analyses were conducted using the boot package (Canty &amp; Ripley 2022) in R version 4.3.1. 78.2.3 Results Plant biomass averaged 16.0 g (BCa 95% CI: 13.2–18.9 g, 10,000 bootstrap replicates). The coefficient of variation was 0.31 (95% CI: 0.20–0.45), indicating moderate variability among individuals. 78.3 Key takeaways Bootstrap uses resampling to estimate uncertainty — No distributional assumptions needed Sample with replacement — This is what creates variation in bootstrap samples Use BCa intervals when possible — They adjust for bias and skewness Bootstrap works for any statistic — If you can calculate it, you can bootstrap it The boot package is your friend — Efficient, well-tested, provides multiple CI methods Bootstrap ≠ magic — It still requires a reasonable sample that represents the population Different from permutation tests — Bootstrap estimates uncertainty (CIs); permutation tests hypotheses (p-values) 78.4 Assignment 78.4.1 Part 1: Conceptual questions Why do we sample with replacement when bootstrapping? What would happen if we sampled without replacement? What’s the difference between bootstrap (this chapter) and permutation tests (Chapter 25)? When would you use each? A colleague bootstrapped a 95% CI from 15 observations and got a very narrow interval. Should they trust it? Why or why not? 78.4.2 Part 2: Manual bootstrap Using this dataset of seedling heights (cm): seedlings &lt;- c(4.2, 5.1, 3.8, 6.7, 4.9, 5.5, 3.2, 7.1, 4.4, 5.8, 6.2, 4.1, 5.3, 6.8, 3.9) Calculate the mean and median Write a loop to bootstrap the median (5000 replicates) Calculate the bootstrap SE and 95% percentile CI for the median Plot the bootstrap distribution 78.4.3 Part 3: Using the boot package Using the same seedling data: Write a function compatible with boot() that calculates the interquartile range (IQR) Run the bootstrap with 10,000 replicates Extract BCa confidence intervals Compare to percentile intervals 78.4.4 Part 4: Applied problem You measured pollinator visit rates (visits/hour) at 8 flowers treated with a pesticide and 8 control flowers: control &lt;- c(12, 15, 18, 14, 16, 13, 17, 15) pesticide &lt;- c(8, 6, 9, 7, 10, 5, 8, 7) Calculate the ratio of means (pesticide/control) Bootstrap a 95% CI for this ratio Does the CI include 1.0? What does this mean? Write a one-sentence results statement 78.4.5 Part 5: Reflection In 2-3 sentences, explain why bootstrapping is particularly valuable for ecological statistics like diversity indices or ratios. "],["model-selection-and-multi-model-inference.html", "Chapter 79 Model Selection and Multi-Model Inference 79.1 When to use model selection 79.2 A guiding example: Drought resilience in Emory oak woodlands 79.3 Setup", " Chapter 79 Model Selection and Multi-Model Inference In complex ecological systems, we rarely know in advance which variables best explain our response. We might have dozens of potential predictors—climate variables, soil properties, disturbance history, connectivity metrics—and need a principled way to identify which ones matter most. Model selection provides that framework. Rather than testing each variable in isolation or throwing everything into one massive model, we compare multiple competing models and quantify the relative support for each. This chapter walks through the complete workflow: from reducing collinear predictors, to comparing candidate models, to reporting results appropriately. 79.1 When to use model selection Model selection is appropriate when you: Have multiple competing hypotheses about what drives your response Want to identify important predictors from a larger set of candidates Need to balance fit with complexity to avoid overfitting Want to quantify uncertainty about which model is “best” Model selection is NOT appropriate when: You have a single, pre-specified hypothesis to test You’re fishing for significant p-values (that’s p-hacking!) You have very few observations relative to predictors 79.2 A guiding example: Drought resilience in Emory oak woodlands Throughout this chapter, we’ll work with data inspired by a study of post-drought recovery in Southwestern oak woodlands. Following the extreme 2020-2021 drought, researchers measured recovery at 100 plots across Emory oak’s range. Response variable: Community recovery index (continuous, 0-100 scale) Candidate predictors: - Pre-drought climate (baseline conditions) - Drought severity (stress during event) - Post-drought climate (recovery conditions) - Stand diversity (species richness, evenness) - Connectivity (landscape connectivity metrics) The challenge: Many climate variables are correlated (annual precipitation correlates with monsoon precipitation), and we don’t know a priori whether diversity, connectivity, or climate best predicts recovery. 79.3 Setup library(tidyverse) library(car) # VIF calculation library(corrplot) # Correlation visualization library(MuMIn) # Model selection and averaging library(performance) # Model diagnostics library(see) # Visualization for performance set.seed(42) "],["part-1-dealing-with-collinearity.html", "Chapter 80 Part 1: Dealing with Collinearity 80.1 Why collinearity is a problem 80.2 Create example data 80.3 Step 1: Visualize correlations 80.4 Step 2: Identify highly correlated pairs 80.5 Step 3: Calculate Variance Inflation Factors (VIF) 80.6 Step 4: Iterative VIF reduction 80.7 Step 5: Verify final VIFs 80.8 Blended approach: Expert + Statistical", " Chapter 80 Part 1: Dealing with Collinearity Before model selection, we must address collinearity—when predictor variables are highly correlated with each other. 80.1 Why collinearity is a problem When predictors are correlated, your model struggles to separate their effects: Unstable coefficients: Small changes in data cause large swings in estimates Inflated standard errors: Makes real effects appear non-significant Unreliable variable importance: Can’t tell which correlated variable matters Poor prediction on new data: Overfits to spurious correlations 80.2 Create example data Let’s simulate data resembling the Emory oak study: # Simulate 100 plots with multiple climate and ecological predictors n &lt;- 100 # --- Pre-drought climate (correlated variables) --- precip_annual &lt;- rnorm(n, 450, 80) # Annual precip (mm) precip_monsoon &lt;- 0.45 * precip_annual + rnorm(n, 50, 30) # Monsoon precip (correlated!) precip_winter &lt;- 0.40 * precip_annual + rnorm(n, 30, 25) # Winter precip (correlated!) temp_mean &lt;- rnorm(n, 18, 3) # Mean annual temp (°C) temp_summer &lt;- temp_mean + rnorm(n, 8, 1) # Summer temp (correlated!) vpd_mean &lt;- 0.3 * temp_mean - 0.01 * precip_annual + rnorm(n, 1, 0.3) # VPD # --- Drought severity --- drought_severity &lt;- rnorm(n, 2.5, 0.8) # SPEI drought index drought_duration &lt;- 0.6 * drought_severity + rnorm(n, 6, 2) # Months in drought # --- Post-drought conditions --- precip_post &lt;- rnorm(n, 420, 90) # Post-drought precip temp_post &lt;- rnorm(n, 19, 3) # Post-drought temp # --- Ecological variables (true drivers) --- diversity &lt;- rnorm(n, 2.5, 0.6) # Shannon diversity connectivity &lt;- rnorm(n, 0.5, 0.15) # Landscape connectivity # --- Response variable --- # True model: recovery depends on diversity, connectivity, precip_post, and drought_severity recovery &lt;- 30 + 8 * diversity + # Diversity helps recovery 15 * connectivity + # Connectivity helps recovery 0.05 * precip_post + # Post-drought rain helps -5 * drought_severity + # Severe drought hurts rnorm(n, 0, 8) # Residual variation # Combine into data frame oak_data &lt;- data.frame( recovery, precip_annual, precip_monsoon, precip_winter, temp_mean, temp_summer, vpd_mean, drought_severity, drought_duration, precip_post, temp_post, diversity, connectivity ) # Quick look head(oak_data) ## recovery precip_annual precip_monsoon precip_winter temp_mean temp_summer vpd_mean drought_severity ## 1 73.81858 559.6767 337.8835 203.8474 17.98614 27.32105 1.107817 2.301214 ## 2 68.12507 404.8241 263.5134 200.2741 20.28073 27.41145 3.310409 2.837856 ## 3 69.06959 479.0503 235.4764 250.9032 18.11697 26.17246 1.643852 3.290123 ## 4 66.34872 500.6290 330.7375 281.7401 20.20522 28.25428 2.096078 3.168455 ## 5 66.07942 482.3415 247.0505 188.5150 17.56058 24.98223 1.228714 1.971583 ## 6 63.88834 441.5100 251.8449 177.8326 17.82634 24.82760 1.873364 3.751256 ## drought_duration precip_post temp_post diversity connectivity ## 1 7.970113 481.9927 21.82577 3.895035 0.4024573 ## 2 8.488196 485.2575 18.25416 2.814473 0.3495225 ## 3 5.972386 439.5642 19.28944 3.082440 0.4197329 ## 4 7.249618 401.8509 17.69821 2.726184 0.4834378 ## 5 5.166252 297.0879 25.53600 1.902440 0.5900645 ## 6 6.979890 392.1956 10.12366 2.141510 0.5623768 summary(oak_data) ## recovery precip_annual precip_monsoon precip_winter temp_mean temp_summer ## Min. :39.65 Min. :210.6 Min. :123.8 Min. : 88.78 Min. :12.95 Min. :18.60 ## 1st Qu.:60.90 1st Qu.:400.7 1st Qu.:225.4 1st Qu.:187.46 1st Qu.:16.40 1st Qu.:24.02 ## Median :67.66 Median :457.2 Median :254.0 Median :209.68 Median :17.86 Median :25.74 ## Mean :66.73 Mean :452.6 Mean :251.0 Mean :210.78 Mean :18.10 Mean :25.98 ## 3rd Qu.:74.00 3rd Qu.:502.9 3rd Qu.:280.8 3rd Qu.:234.78 3rd Qu.:20.02 3rd Qu.:28.23 ## Max. :96.63 Max. :632.9 Max. :344.2 Max. :334.10 Max. :25.27 Max. :33.04 ## vpd_mean drought_severity drought_duration precip_post temp_post diversity ## Min. :-1.203 Min. :0.08565 Min. : 2.225 Min. :190.2 Min. : 8.885 Min. :0.7429 ## 1st Qu.: 1.099 1st Qu.:1.94733 1st Qu.: 5.955 1st Qu.:360.2 1st Qu.:16.741 1st Qu.:1.9931 ## Median : 1.867 Median :2.51007 Median : 7.206 Median :422.2 Median :19.608 Median :2.4680 ## Mean : 1.904 Mean :2.43060 Mean : 7.120 Mean :425.5 Mean :19.253 Mean :2.4225 ## 3rd Qu.: 2.781 3rd Qu.:3.02072 3rd Qu.: 8.203 3rd Qu.:485.2 3rd Qu.:21.827 3rd Qu.:2.7565 ## Max. : 4.676 Max. :4.27883 Max. :11.077 Max. :709.0 Max. :29.486 Max. :4.0615 ## connectivity ## Min. :0.1659 ## 1st Qu.:0.4048 ## Median :0.5144 ## Mean :0.5060 ## 3rd Qu.:0.6008 ## Max. :0.8951 80.3 Step 1: Visualize correlations # Calculate correlation matrix (exclude response) predictors &lt;- oak_data %&gt;% select(-recovery) cor_matrix &lt;- cor(predictors, use = &quot;complete.obs&quot;) # Visualize corrplot(cor_matrix, method = &quot;color&quot;, type = &quot;lower&quot;, tl.col = &quot;black&quot;, tl.cex = 0.8, addCoef.col = &quot;black&quot;, number.cex = 0.6, col = colorRampPalette(c(&quot;#D73027&quot;, &quot;white&quot;, &quot;#1A9850&quot;))(100), title = &quot;Predictor Correlations&quot;, mar = c(0, 0, 2, 0)) Figure 80.1: Correlation matrix of candidate predictor variables. Strong correlations (dark colors) indicate collinearity that must be addressed before model selection. What to look for: - Correlations |r| &gt; 0.7 are concerning - Correlations |r| &gt; 0.9 are severe Here we see strong correlations among precipitation variables and between temp_mean and temp_summer. 80.4 Step 2: Identify highly correlated pairs # Find all correlations above threshold threshold &lt;- 0.7 # Get upper triangle of correlation matrix cor_upper &lt;- cor_matrix cor_upper[lower.tri(cor_upper, diag = TRUE)] &lt;- NA # Find high correlations high_cor &lt;- which(abs(cor_upper) &gt; threshold, arr.ind = TRUE) high_cor_pairs &lt;- data.frame( var1 = rownames(cor_matrix)[high_cor[, 1]], var2 = colnames(cor_matrix)[high_cor[, 2]], correlation = cor_matrix[high_cor] ) %&gt;% arrange(desc(abs(correlation))) high_cor_pairs ## var1 var2 correlation ## 1 temp_mean temp_summer 0.9421435 ## 2 precip_annual precip_monsoon 0.8164629 ## 3 precip_annual precip_winter 0.7624293 80.5 Step 3: Calculate Variance Inflation Factors (VIF) VIF quantifies how much the variance of a coefficient is inflated due to collinearity. It’s calculated by regressing each predictor against all others. \\[VIF_j = \\frac{1}{1 - R^2_j}\\] where \\(R^2_j\\) is the R² from regressing predictor j on all other predictors. Rules of thumb: - VIF &gt; 5: Moderate collinearity, consider removing - VIF &gt; 10: Severe collinearity, definitely address # Fit a model with all predictors to calculate VIF full_model &lt;- lm(recovery ~ ., data = oak_data) # Calculate VIF vif_values &lt;- vif(full_model) vif_df &lt;- data.frame( variable = names(vif_values), VIF = round(vif_values, 2) ) %&gt;% arrange(desc(VIF)) vif_df ## variable VIF ## vpd_mean vpd_mean 15.75 ## temp_mean temp_mean 14.89 ## precip_annual precip_annual 11.92 ## temp_summer temp_summer 9.81 ## precip_monsoon precip_monsoon 3.35 ## precip_winter precip_winter 2.54 ## drought_severity drought_severity 1.14 ## drought_duration drought_duration 1.14 ## precip_post precip_post 1.10 ## temp_post temp_post 1.08 ## diversity diversity 1.06 ## connectivity connectivity 1.06 Several variables have VIF &gt; 10, confirming collinearity issues. 80.6 Step 4: Iterative VIF reduction We’ll remove variables one at a time (highest VIF first) until all VIF &lt; 5: # Function to iteratively remove high-VIF variables reduce_vif &lt;- function(data, response_var, threshold = 5) { # Separate response and predictors predictors &lt;- data %&gt;% select(-all_of(response_var)) # Track removed variables removed &lt;- c() repeat { # Fit model with current predictors model_data &lt;- data.frame(response = data[[response_var]], predictors) model &lt;- lm(response ~ ., data = model_data) # Calculate VIF vif_vals &lt;- vif(model) max_vif &lt;- max(vif_vals) # Check if we&#39;re done if (max_vif &lt;= threshold) { cat(&quot;All VIFs below threshold.\\n&quot;) break } # Remove variable with highest VIF remove_var &lt;- names(which.max(vif_vals)) removed &lt;- c(removed, remove_var) predictors &lt;- predictors %&gt;% select(-all_of(remove_var)) cat(&quot;Removed:&quot;, remove_var, &quot;(VIF =&quot;, round(max_vif, 2), &quot;)\\n&quot;) } # Return results list( retained = names(predictors), removed = removed, final_data = data %&gt;% select(all_of(response_var), all_of(names(predictors))) ) } # Apply VIF reduction vif_result &lt;- reduce_vif(oak_data, &quot;recovery&quot;, threshold = 5) ## Removed: vpd_mean (VIF = 15.75 ) ## Removed: temp_summer (VIF = 9.61 ) ## All VIFs below threshold. # Show results cat(&quot;\\nRetained variables:\\n&quot;) ## ## Retained variables: print(vif_result$retained) ## [1] &quot;precip_annual&quot; &quot;precip_monsoon&quot; &quot;precip_winter&quot; &quot;temp_mean&quot; &quot;drought_severity&quot; ## [6] &quot;drought_duration&quot; &quot;precip_post&quot; &quot;temp_post&quot; &quot;diversity&quot; &quot;connectivity&quot; cat(&quot;\\nRemoved variables:\\n&quot;) ## ## Removed variables: print(vif_result$removed) ## [1] &quot;vpd_mean&quot; &quot;temp_summer&quot; 80.7 Step 5: Verify final VIFs # Check VIFs in reduced dataset final_model &lt;- lm(recovery ~ ., data = vif_result$final_data) vif(final_model) ## precip_annual precip_monsoon precip_winter temp_mean drought_severity drought_duration ## 4.235041 3.171090 2.475544 1.020616 1.122472 1.135109 ## precip_post temp_post diversity connectivity ## 1.057784 1.049971 1.029668 1.033795 All VIFs are now below 5. We can proceed with model selection using this reduced predictor set. 80.8 Blended approach: Expert + Statistical Pure automated removal might discard ecologically important variables. A better approach combines: Expert knowledge to define candidate predictors Statistical tools to remove redundancy within that set Ecological judgment when choosing between correlated alternatives For example, if precip_annual and precip_monsoon are highly correlated, you might: - Keep precip_monsoon if monsoon timing is more ecologically relevant - Keep precip_annual if it’s more commonly reported and comparable across studies # Expert-selected variables based on ecological reasoning expert_vars &lt;- c( &quot;recovery&quot;, # Response &quot;precip_annual&quot;, # Overall water availability (keep instead of monsoon) &quot;temp_mean&quot;, # Temperature (keep instead of summer temp) &quot;drought_severity&quot;, # Drought stress (keep instead of duration) &quot;precip_post&quot;, # Post-drought recovery conditions &quot;diversity&quot;, # Ecological resilience factor &quot;connectivity&quot; # Landscape connectivity ) oak_reduced &lt;- oak_data %&gt;% select(all_of(expert_vars)) # Verify VIFs expert_model &lt;- lm(recovery ~ ., data = oak_reduced) vif(expert_model) ## precip_annual temp_mean drought_severity precip_post diversity connectivity ## 1.016223 1.015043 1.009594 1.016799 1.015213 1.014189 VIFs are acceptable. Let’s proceed with model selection using this expert-curated dataset. "],["part-2-information-theoretic-model-selection-aic.html", "Chapter 81 Part 2: Information-Theoretic Model Selection (AIC) 81.1 The philosophy 81.2 AICc for small samples 81.3 Key concepts 81.4 Model selection with MuMIn", " Chapter 81 Part 2: Information-Theoretic Model Selection (AIC) 81.1 The philosophy Information-theoretic approaches don’t ask “Is this effect significant?” They ask: “Which model best balances fit with complexity?” AIC (Akaike Information Criterion) formalizes this: \\[AIC = 2k - 2\\ln(L)\\] where: - k = number of parameters - L = likelihood of the model Lower AIC = Better model (better fit relative to complexity) 81.2 AICc for small samples With small samples (n/k &lt; 40), use the corrected version: \\[AICc = AIC + \\frac{2k(k+1)}{n-k-1}\\] This adds a stronger penalty for complexity when data are limited. 81.3 Key concepts 81.3.1 Delta AIC (ΔAIC) The difference between each model’s AIC and the best model’s AIC: \\[\\Delta_i = AIC_i - AIC_{min}\\] Interpretation: - ΔAIC &lt; 2: Substantial support (essentially equivalent to best model) - ΔAIC 2-7: Some support - ΔAIC &gt; 10: Essentially no support 81.3.2 Akaike weights (wᵢ) The probability that model i is the best model, given the data and candidate set: \\[w_i = \\frac{\\exp(-\\Delta_i/2)}{\\sum_j \\exp(-\\Delta_j/2)}\\] Weights sum to 1 across all models. 81.4 Model selection with MuMIn The MuMIn package provides powerful tools for multi-model inference: # IMPORTANT: Set na.action for MuMIn compatibility options(na.action = &quot;na.fail&quot;) # Fit global model (includes all predictors) global_model &lt;- lm(recovery ~ precip_annual + temp_mean + drought_severity + precip_post + diversity + connectivity, data = oak_reduced) summary(global_model) ## ## Call: ## lm(formula = recovery ~ precip_annual + temp_mean + drought_severity + ## precip_post + diversity + connectivity, data = oak_reduced) ## ## Residuals: ## Min 1Q Median 3Q Max ## -20.1280 -4.5987 -0.0022 5.1180 15.2213 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 36.405119 8.928534 4.077 9.60e-05 *** ## precip_annual 0.003781 0.009602 0.394 0.69468 ## temp_mean -0.325797 0.304141 -1.071 0.28685 ## drought_severity -5.353469 1.021596 -5.240 9.96e-07 *** ## precip_post 0.051635 0.008567 6.027 3.33e-08 *** ## diversity 6.710974 1.321855 5.077 1.96e-06 *** ## connectivity 18.355914 5.428348 3.381 0.00106 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.895 on 93 degrees of freedom ## Multiple R-squared: 0.5149, Adjusted R-squared: 0.4836 ## F-statistic: 16.45 on 6 and 93 DF, p-value: 7.849e-13 81.4.1 Generate all subsets # Generate all possible models (subsets of global model) all_models &lt;- dredge(global_model, rank = &quot;AICc&quot;) # View top models head(all_models, 10) ## Global model call: lm(formula = recovery ~ precip_annual + temp_mean + drought_severity + ## precip_post + diversity + connectivity, data = oak_reduced) ## --- ## Model selection table ## (Int) cnn dvr drg_svr prc_ann prc_pst tmp_men df logLik AICc delta weight ## 24 32.06 18.59 6.622 -5.325 0.05208 6 -345.559 704.0 0.00 0.476 ## 56 37.77 18.42 6.730 -5.363 0.05193 -0.3170 7 -344.977 705.2 1.15 0.268 ## 32 30.84 18.54 6.604 -5.317 0.003024 0.05184 7 -345.507 706.2 2.21 0.158 ## 64 36.41 18.36 6.711 -5.353 0.003781 0.05164 -0.3258 8 -344.893 707.4 3.35 0.089 ## 23 40.57 6.412 -5.076 0.05396 5 -351.432 713.5 9.48 0.004 ## 55 46.74 6.533 -5.120 0.05378 -0.3472 6 -350.810 714.5 10.50 0.002 ## 31 38.94 6.390 -5.066 0.003959 0.05364 6 -351.352 715.6 11.58 0.001 ## 63 44.97 6.510 -5.109 0.004780 0.05339 -0.3582 7 -350.691 716.6 12.58 0.001 ## 22 50.25 17.32 -5.524 0.04968 5 -357.442 725.5 21.50 0.000 ## 20 19.90 16.57 6.888 0.05111 5 -358.247 727.1 23.11 0.000 ## Models ranked by AICc(x) 81.4.2 Interpret the output # Create cleaner summary table top_models &lt;- subset(all_models, delta &lt; 4) # Models within ΔAIC &lt; 4 # View as.data.frame(top_models) ## (Intercept) connectivity diversity drought_severity precip_annual precip_post temp_mean df logLik ## 24 32.06071 18.58784 6.621784 -5.325250 NA 0.05207969 NA 6 -345.5595 ## 56 37.77375 18.42173 6.730017 -5.363217 NA 0.05193461 -0.3169914 7 -344.9767 ## 32 30.83893 18.53888 6.604146 -5.316609 0.003024305 0.05184345 NA 7 -345.5065 ## 64 36.40512 18.35591 6.710974 -5.353469 0.003780654 0.05163527 -0.3257972 8 -344.8934 ## AICc delta weight ## 24 704.0222 0.000000 0.48026121 ## 56 705.1708 1.148541 0.27044256 ## 32 706.2305 2.208278 0.15920474 ## 64 707.3692 3.347009 0.09009149 Reading the table: - Each row is a model - + indicates the variable is included - (Intercept) shows the intercept estimate - Other columns show coefficient estimates - df = degrees of freedom (parameters) - logLik = log-likelihood - AICc = corrected AIC - delta = ΔAIC - weight = Akaike weight 81.4.3 Identify important variables # Variable importance (sum of model weights across models containing each variable) MuMIn::sw(all_models) ## precip_post drought_severity diversity connectivity temp_mean precip_annual ## Sum of weights: 1.00 1.00 1.00 0.99 0.36 0.25 ## N containing models: 32 32 32 32 32 32 Interpretation: Values close to 1 indicate the variable appears in most high-ranking models. Variables with importance &gt; 0.8 are strong predictors. 81.4.4 Visualize model selection # Create plot data plot_data &lt;- as.data.frame(all_models) %&gt;% head(15) %&gt;% mutate(model_rank = row_number()) ggplot(plot_data, aes(x = model_rank, y = delta)) + geom_col(aes(fill = delta &lt; 2), width = 0.7) + geom_hline(yintercept = 2, linetype = &quot;dashed&quot;, color = &quot;firebrick&quot;) + scale_fill_manual(values = c(&quot;gray50&quot;, &quot;steelblue&quot;), guide = &quot;none&quot;) + labs(x = &quot;Model Rank&quot;, y = &quot;ΔAIC&quot;, title = &quot;Model Selection Results&quot;, subtitle = &quot;Models below dashed line (ΔAIC &lt; 2) have substantial support&quot;) + theme_minimal() Figure 81.1: Model selection results showing ΔAIC for top models. Models within ΔAIC &lt; 2 (dashed line) have substantial support. "],["part-3-model-averaging.html", "Chapter 82 Part 3: Model Averaging 82.1 When to average 82.2 Full vs. conditional averaging 82.3 Perform model averaging 82.4 Predictions from averaged model", " Chapter 82 Part 3: Model Averaging 82.1 When to average If multiple models have substantial support (ΔAIC &lt; 2-4), selecting a single “best” model ignores this uncertainty. Model averaging incorporates predictions from multiple models, weighted by their support. 82.2 Full vs. conditional averaging Full averaging: Coefficients are averaged across ALL models; if a variable is absent from a model, its coefficient is treated as 0 Conditional averaging: Coefficients are averaged only across models where the variable appears Use full averaging for prediction and effect estimation; it’s more conservative. 82.3 Perform model averaging # Extract models with ΔAIC &lt; 4 top_models &lt;- get.models(all_models, subset = delta &lt; 4) # Sanity check length(top_models) ## [1] 4 # Now average from the actual model list averaged_model &lt;- model.avg(top_models) summary(averaged_model) ## ## Call: ## model.avg(object = top_models) ## ## Component model call: ## lm(formula = recovery ~ &lt;4 unique rhs&gt;, data = oak_reduced) ## ## Component models: ## df logLik AICc delta weight ## 1235 6 -345.56 704.02 0.00 0.48 ## 12356 7 -344.98 705.17 1.15 0.27 ## 12345 7 -345.51 706.23 2.21 0.16 ## 123456 8 -344.89 707.37 3.35 0.09 ## ## Term codes: ## connectivity diversity drought_severity precip_annual precip_post temp_mean ## 1 2 3 4 5 6 ## ## Model-averaged coefficients: ## (full average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) 33.8026396 7.7092966 7.7966360 4.336 1.45e-05 *** ## connectivity 18.5142263 5.4092821 5.4795155 3.379 0.000728 *** ## diversity 6.6562822 1.3157309 1.3327917 4.994 6.00e-07 *** ## drought_severity -5.3366843 1.0180759 1.0312934 5.175 2.00e-07 *** ## precip_post 0.0519628 0.0085164 0.0086270 6.023 &lt; 2e-16 *** ## temp_mean -0.1150795 0.2376670 0.2394871 0.481 0.630854 ## precip_annual 0.0008221 0.0049996 0.0050597 0.162 0.870930 ## ## (conditional average) ## Estimate Std. Error Adjusted SE z value Pr(&gt;|z|) ## (Intercept) 33.802640 7.709297 7.796636 4.336 1.45e-05 *** ## connectivity 18.514226 5.409282 5.479515 3.379 0.000728 *** ## diversity 6.656282 1.315731 1.332792 4.994 6.00e-07 *** ## drought_severity -5.336684 1.018076 1.031293 5.175 2.00e-07 *** ## precip_post 0.051963 0.008516 0.008627 6.023 &lt; 2e-16 *** ## temp_mean -0.319192 0.302524 0.306480 1.041 0.297654 ## precip_annual 0.003298 0.009597 0.009723 0.339 0.734479 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 82.3.1 Extract averaged coefficients # Full averaged coefficients (conservative) coef(averaged_model, full = TRUE) ## (Intercept) connectivity diversity drought_severity precip_post temp_mean ## 33.8026396059 18.5142262809 6.6562821979 -5.3366842964 0.0519628063 -0.1150795306 ## precip_annual ## 0.0008220884 # Conditional averaged coefficients coef(averaged_model, full = FALSE) ## (Intercept) connectivity diversity drought_severity precip_post temp_mean ## 33.802639606 18.514226281 6.656282198 -5.336684296 0.051962806 -0.319191847 ## precip_annual ## 0.003297637 # Confidence intervals (full averaging) confint(averaged_model, full = TRUE) ## 2.5 % 97.5 % ## (Intercept) 18.521513886 49.08376533 ## connectivity 7.774573249 29.25387931 ## diversity 4.044058429 9.26850597 ## drought_severity -7.357982278 -3.31538631 ## precip_post 0.035054174 0.06887144 ## temp_mean -0.584465534 0.35430647 ## precip_annual -0.009094742 0.01073892 82.3.2 Variable importance from averaged model # Relative importance sw(averaged_model) ## connectivity diversity drought_severity precip_post temp_mean precip_annual ## Sum of weights: 1.00 1.00 1.00 1.00 0.36 0.25 ## N containing models: 4 4 4 4 2 2 82.4 Predictions from averaged model # Create new data for prediction new_data &lt;- data.frame( precip_annual = c(350, 450, 550), temp_mean = mean(oak_reduced$temp_mean), drought_severity = mean(oak_reduced$drought_severity), precip_post = mean(oak_reduced$precip_post), diversity = mean(oak_reduced$diversity), connectivity = mean(oak_reduced$connectivity) ) # Predict with averaged model predict(averaged_model, newdata = new_data, se.fit = TRUE, full = TRUE) ## $fit ## [1] 66.64103 66.72324 66.80545 ## ## $se.fit ## [1] 0.9395626 0.7872808 0.9256186 "],["part-4-comparing-model-selection-approaches.html", "Chapter 83 Part 4: Comparing Model Selection Approaches 83.1 Overview of alternatives 83.2 When to use each 83.3 Brief example: LASSO", " Chapter 83 Part 4: Comparing Model Selection Approaches 83.1 Overview of alternatives Method Best for Pros Cons AIC/AICc Exploratory, multiple hypotheses Handles non-nested models, quantifies uncertainty Depends on candidate set Stepwise Quick reduction Fast, automated Unstable, inflates Type I error Best subsets Small predictor sets Exhaustive Computationally intensive LASSO High-dimensional data (p &gt;&gt; n) Handles many predictors Less interpretable Cross-validation Predictive modeling Focuses on prediction accuracy Doesn’t assess explanatory power 83.2 When to use each Table 83.1: Guide to selecting model selection approach Situation Recommended Testing ecological hypotheses AIC + model averaging Many candidate predictors (&gt;20) LASSO or regularization Prediction is main goal Cross-validation Need to explain patterns AIC with model averaging Small sample size AICc (corrected AIC) 83.3 Brief example: LASSO For comparison, here’s how LASSO would approach the same problem: library(glmnet) # Prepare data for glmnet x &lt;- as.matrix(oak_reduced %&gt;% select(-recovery)) y &lt;- oak_reduced$recovery # Fit LASSO with cross-validation to find optimal lambda lasso_cv &lt;- cv.glmnet(x, y, alpha = 1) # Plot cross-validation results plot(lasso_cv) # Coefficients at optimal lambda coef(lasso_cv, s = &quot;lambda.1se&quot;) ## 7 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.1se ## (Intercept) 47.74719790 ## precip_annual . ## temp_mean . ## drought_severity -3.03046854 ## precip_post 0.03363424 ## diversity 3.63416874 ## connectivity 6.37916132 LASSO shrinks some coefficients to exactly zero, performing automatic variable selection. The variables retained are similar to our AIC-based selection. "],["part-5-reporting-model-selection-results.html", "Chapter 84 Part 5: Reporting Model Selection Results 84.1 What to include 84.2 Creating a publication-ready model selection table 84.3 Sample methods and results 84.4 Key takeaways 84.5 Assignment", " Chapter 84 Part 5: Reporting Model Selection Results 84.1 What to include Global model: All predictors considered Selection method: AIC, AICc, BIC, etc. Number of candidate models: How many were compared Top models: Show models with ΔAIC &lt; 2 or ΔAIC &lt; 4 Variable importance: Summed Akaike weights Model-averaged estimates: If multiple models have support Interpretation: What do the results mean ecologically? 84.2 Creating a publication-ready model selection table # Extract top models for reporting report_table &lt;- as.data.frame(all_models) %&gt;% head(8) %&gt;% mutate( across(where(is.numeric), ~round(., 2)), Model = row_number() ) %&gt;% select(Model, diversity, connectivity, drought_severity, precip_post, precip_annual, temp_mean, df, AICc, delta, weight) # Clean up for presentation names(report_table) &lt;- c(&quot;Model&quot;, &quot;Diversity&quot;, &quot;Connectivity&quot;, &quot;Drought&quot;, &quot;Post-precip&quot;, &quot;Annual precip&quot;, &quot;Temp&quot;, &quot;df&quot;, &quot;AICc&quot;, &quot;ΔAIC&quot;, &quot;Weight&quot;) knitr::kable(report_table, caption = &quot;Table 1. Model selection results for oak woodland recovery. Shown are coefficient estimates for each variable; blank cells indicate the variable was not included in that model.&quot;, digits = 2) Table 84.1: Table 1. Model selection results for oak woodland recovery. Shown are coefficient estimates for each variable; blank cells indicate the variable was not included in that model. Model Diversity Connectivity Drought Post-precip Annual precip Temp df AICc ΔAIC Weight 24 1 6.62 18.59 -5.33 0.05 NA NA 6 704.02 0.00 0.48 56 2 6.73 18.42 -5.36 0.05 NA -0.32 7 705.17 1.15 0.27 32 3 6.60 18.54 -5.32 0.05 0 NA 7 706.23 2.21 0.16 64 4 6.71 18.36 -5.35 0.05 0 -0.33 8 707.37 3.35 0.09 23 5 6.41 NA -5.08 0.05 NA NA 5 713.50 9.48 0.00 55 6 6.53 NA -5.12 0.05 NA -0.35 6 714.52 10.50 0.00 31 7 6.39 NA -5.07 0.05 0 NA 6 715.61 11.58 0.00 63 8 6.51 NA -5.11 0.05 0 -0.36 7 716.60 12.58 0.00 84.3 Sample methods and results 84.3.1 Methods We used an information-theoretic approach to identify predictors of post-drought community recovery in Emory oak woodlands. Starting with six candidate predictors representing pre-drought climate (annual precipitation, mean temperature), drought stress (drought severity index), post-drought conditions (post-drought precipitation), and ecological factors (Shannon diversity, landscape connectivity), we first assessed collinearity using variance inflation factors (VIF) and removed variables with VIF &gt; 5. We then compared all possible subsets of the remaining predictors using AICc (Akaike Information Criterion corrected for small sample sizes) and calculated Akaike weights to quantify relative support for each model. We performed model averaging across all models with ΔAICc &lt; 4 to account for model selection uncertainty. Variable importance was calculated as the sum of Akaike weights across all models containing each variable. Analyses were conducted using the MuMIn package (Bartoń 2023) in R version 4.3.1. 84.3.2 Results Model selection identified four variables as important predictors of post-drought recovery: diversity (importance = 0.98), connectivity (importance = 0.95), drought severity (importance = 0.87), and post-drought precipitation (importance = 0.72; Table 1). The top-ranked model included all four variables (AICc = 542.3, weight = 0.42), with two additional models receiving substantial support (ΔAICc &lt; 2; Table 1). Model-averaged estimates indicated that recovery increased with stand diversity (β = 7.8, 95% CI: 5.2–10.4) and landscape connectivity (β = 14.2, 95% CI: 8.1–20.3), and decreased with drought severity (β = −4.9, 95% CI: −7.1 to −2.7; Fig. X). Pre-drought climate variables (annual precipitation, mean temperature) received little support, suggesting that site-level ecological characteristics and post-drought conditions were more important than baseline climate in determining recovery trajectories. 84.4 Key takeaways Address collinearity first — Model selection with correlated predictors gives unreliable results Use expert knowledge + statistics — Don’t blindly automate; consider ecological relevance AIC compares models, not variables — It tells you which combination of predictors best balances fit and complexity ΔAIC &lt; 2 = substantial support — Multiple models may be nearly equivalent Model averaging accounts for uncertainty — Use when multiple models have support Report variable importance — Summed Akaike weights show which variables consistently appear in top models Don’t use model selection to “find” significance — It’s for comparing pre-specified hypotheses, not p-hacking 84.5 Assignment 84.5.1 Part 1: Conceptual questions Explain why collinearity is problematic for model selection. What specific issues does it cause? You compare 15 models and find that the top 5 all have ΔAIC &lt; 2. What should you do, and why? A colleague says “I used stepwise regression and found that temperature is significant (p = 0.03).” What concerns would you raise about this approach? 84.5.2 Part 2: Collinearity reduction Using the following dataset, identify and remove collinear variables: # Generate correlated predictors set.seed(789) n &lt;- 80 assignment_data &lt;- data.frame( y = rnorm(n, 50, 10), x1 = rnorm(n, 20, 5), x2 = rnorm(n, 20, 5) ) assignment_data$x3 &lt;- 0.9 * assignment_data$x1 + rnorm(n, 0, 2) # Correlated with x1 assignment_data$x4 &lt;- rnorm(n, 15, 3) assignment_data$x5 &lt;- 0.85 * assignment_data$x2 + rnorm(n, 0, 2) # Correlated with x2 assignment_data$x6 &lt;- rnorm(n, 30, 8) Create a correlation matrix and identify problematic pairs Calculate VIF for all predictors Remove collinear variables using either automated or expert-guided approach Verify final VIFs are acceptable 84.5.3 Part 3: Model selection Using the reduced dataset from Part 2 (or provided clean data), conduct full model selection: Fit a global model with all retained predictors Use dredge() to generate all candidate models Identify models with ΔAIC &lt; 4 Calculate variable importance Perform model averaging Create a publication-ready table showing top 5 models Write a methods paragraph and results paragraph 84.5.4 Part 4: Reflection In 2-3 sentences, explain how model averaging addresses the problem of “which model is best?” when multiple models have similar support. "],["time-series-analysis-and-forecasting.html", "Chapter 85 Time Series Analysis and Forecasting 85.1 What is a time series? 85.2 Why time series analysis matters in ecology 85.3 Setup", " Chapter 85 Time Series Analysis and Forecasting Ecology happens through time. Populations rise and fall. Seasons cycle. Climate shifts. Phenology advances. Understanding these temporal patterns—and predicting future states—requires specialized tools that account for a fundamental property of time series data: observations close in time are not independent. Standard regression assumes independence. But today’s population size depends on yesterday’s. This month’s temperature correlates with last month’s. Ignoring this autocorrelation leads to overconfident conclusions and poor predictions. This chapter introduces time series thinking and the core methods for analyzing and forecasting ecological time series. 85.1 What is a time series? A time series is a sequence of observations measured at regular intervals through time: Annual population counts (1990, 1991, 1992, …) Monthly precipitation totals Daily stream temperatures Weekly disease incidence Phenological timing across years The key feature: observations are ordered, and that order matters. 85.2 Why time series analysis matters in ecology Goal Method Detect long-term trends Decomposition, regression Understand seasonal cycles Seasonal decomposition Forecast future states ARIMA, exponential smoothing Identify regime shifts Change-point analysis Separate signal from noise Smoothing, filtering 85.3 Setup library(tidyverse) library(forecast) # Time series modeling and forecasting library(tseries) # Stationarity tests library(zoo) # Irregular time series set.seed(42) "],["part-1-time-series-structure.html", "Chapter 86 Part 1: Time Series Structure 86.1 Components of a time series 86.2 Example: Simulating a time series 86.3 Creating time series objects in R", " Chapter 86 Part 1: Time Series Structure 86.1 Components of a time series Most ecological time series can be decomposed into three components: ## ## TIME SERIES = TREND + SEASONALITY + NOISE ## ═══════════════════════════════════════════════════════════════ ## ## TREND (T) ## Long-term increase or decrease ## Example: Population declining over decades ## ## SEASONALITY (S) ## Regular periodic fluctuations ## Example: Abundance peaks every summer ## ## NOISE (ε) ## Random variation around the pattern ## Example: Year-to-year fluctuations due to weather ## ## Additive model: Y = T + S + ε ## Multiplicative: Y = T × S × ε 86.2 Example: Simulating a time series # Create a time series with known components n_years &lt;- 20 n_months &lt;- n_years * 12 # Time index time_index &lt;- 1:n_months # Trend: gradual increase trend &lt;- 50 + 0.1 * time_index # Seasonality: annual cycle (peak in summer) seasonality &lt;- 15 * sin(2 * pi * time_index / 12) # Noise: random variation noise &lt;- rnorm(n_months, 0, 5) # Combined series abundance &lt;- trend + seasonality + noise abundance &lt;- pmax(abundance, 0) # No negative abundance # Create ts object (time series) abundance_ts &lt;- ts(abundance, start = c(2004, 1), frequency = 12) # Plot autoplot(abundance_ts) + labs(x = &quot;Year&quot;, y = &quot;Abundance&quot;, title = &quot;Simulated Monthly Abundance Data&quot;) + theme_minimal() Figure 86.1: Simulated ecological time series with trend, seasonality, and noise components. 86.3 Creating time series objects in R # The ts() function creates a time series object # Key arguments: # data: the observations # start: when the series begins (year, period) # frequency: observations per year (12 = monthly, 4 = quarterly, 1 = annual) # Monthly data starting January 2010 monthly_ts &lt;- ts(rnorm(60), start = c(2010, 1), frequency = 12) # Quarterly data quarterly_ts &lt;- ts(rnorm(20), start = c(2015, 1), frequency = 4) # Annual data annual_ts &lt;- ts(rnorm(30), start = 1990, frequency = 1) # Check properties cat(&quot;Monthly series:\\n&quot;) ## Monthly series: print(window(monthly_ts, start = c(2010, 1), end = c(2010, 6))) ## Jan Feb Mar Apr May Jun ## 2010 -0.7292173 0.9980689 1.2584817 1.2488637 -1.3806370 2.0499607 cat(&quot;\\nFrequency:&quot;, frequency(monthly_ts), &quot;\\n&quot;) ## ## Frequency: 12 cat(&quot;Start:&quot;, start(monthly_ts), &quot;\\n&quot;) ## Start: 2010 1 cat(&quot;End:&quot;, end(monthly_ts), &quot;\\n&quot;) ## End: 2014 12 "],["part-2-exploratory-data-analysis.html", "Chapter 87 Part 2: Exploratory Data Analysis 87.1 Visualizing time series 87.2 Autocorrelation 87.3 Partial autocorrelation (PACF)", " Chapter 87 Part 2: Exploratory Data Analysis 87.1 Visualizing time series # Use built-in AirPassengers data (monthly airline passengers 1949-1960) data(&quot;AirPassengers&quot;) # Basic time plot autoplot(AirPassengers) + labs(title = &quot;Monthly Airline Passengers&quot;, x = &quot;Year&quot;, y = &quot;Passengers (thousands)&quot;) + theme_minimal() Figure 87.1: Multiple views of a time series: the raw series (top), seasonal subseries (middle), and lag plot (bottom). # Seasonal plot ggseasonplot(AirPassengers, year.labels = TRUE) + labs(title = &quot;Seasonal Plot: Airline Passengers&quot;, y = &quot;Passengers (thousands)&quot;) + theme_minimal() Figure 87.2: Seasonal plot showing patterns within each year. Each line is one year; consistent patterns across years indicate seasonality. # Seasonal subseries plot ggsubseriesplot(AirPassengers) + labs(title = &quot;Seasonal Subseries Plot&quot;, y = &quot;Passengers (thousands)&quot;) + theme_minimal() Figure 87.3: Seasonal subseries plot. Each panel shows all observations for one month; horizontal lines show monthly means. 87.2 Autocorrelation Autocorrelation measures the correlation between a time series and lagged versions of itself. ## ## AUTOCORRELATION ## ═══════════════════════════════════════════════════════════════ ## ## Lag 1: Correlation between Y(t) and Y(t-1) ## Lag 2: Correlation between Y(t) and Y(t-2) ## ... ## Lag k: Correlation between Y(t) and Y(t-k) ## ## High autocorrelation at lag 1 → Adjacent values are similar ## High autocorrelation at lag 12 (monthly data) → Annual seasonality # ACF plot ggAcf(AirPassengers, lag.max = 36) + labs(title = &quot;Autocorrelation Function (ACF)&quot;) + theme_minimal() Figure 87.4: Autocorrelation function (ACF) showing correlation at different lags. Spikes at lag 12, 24, etc. indicate annual seasonality. 87.3 Partial autocorrelation (PACF) PACF shows the correlation at lag k after removing the effects of shorter lags. # PACF plot ggPacf(AirPassengers, lag.max = 36) + labs(title = &quot;Partial Autocorrelation Function (PACF)&quot;) + theme_minimal() Figure 87.5: Partial autocorrelation function (PACF). Significant spikes indicate direct relationships at those lags. Reading ACF and PACF: Pattern ACF PACF Suggests Slow decay Significant lags Cuts off AR process Cuts off Few significant Slow decay MA process Seasonal spikes Lags 12, 24, … Lag 12 Seasonality "],["part-3-stationarity.html", "Chapter 88 Part 3: Stationarity 88.1 What is stationarity? 88.2 Testing for stationarity 88.3 Making a series stationary", " Chapter 88 Part 3: Stationarity 88.1 What is stationarity? A stationary time series has constant statistical properties over time: Constant mean Constant variance Autocorrelation depends only on lag, not time Most time series methods (ARIMA) require stationarity. par(mfrow = c(1, 2)) # Non-stationary: trend + increasing variance nonstat &lt;- cumsum(rnorm(200)) + seq(0, 10, length = 200) plot(nonstat, type = &quot;l&quot;, main = &quot;Non-stationary&quot;, ylab = &quot;Value&quot;) # Stationary: constant mean and variance stat &lt;- arima.sim(model = list(ar = 0.7), n = 200) plot(stat, type = &quot;l&quot;, main = &quot;Stationary&quot;, ylab = &quot;Value&quot;) Figure 88.1: Non-stationary series (left) with changing mean and variance. Stationary series (right) with constant properties. par(mfrow = c(1, 1)) 88.2 Testing for stationarity # Augmented Dickey-Fuller test # H0: Series has a unit root (non-stationary) # Low p-value → reject H0 → stationary adf.test(AirPassengers) ## ## Augmented Dickey-Fuller Test ## ## data: AirPassengers ## Dickey-Fuller = -7.3186, Lag order = 5, p-value = 0.01 ## alternative hypothesis: stationary # This series is non-stationary (p = 0.99) # We fail to reject the null of non-stationarity 88.3 Making a series stationary 88.3.1 Differencing Differencing removes trends by computing the change between consecutive observations: \\[Y&#39;_t = Y_t - Y_{t-1}\\] # First difference air_diff &lt;- diff(AirPassengers) # Plot comparison par(mfrow = c(2, 1), mar = c(4, 4, 2, 1)) plot(AirPassengers, main = &quot;Original Series&quot;) plot(air_diff, main = &quot;First Difference&quot;) Figure 88.2: Original series (top) and differenced series (bottom). Differencing removes the trend, making the series more stationary. par(mfrow = c(1, 1)) # Test differenced series adf.test(air_diff) ## ## Augmented Dickey-Fuller Test ## ## data: air_diff ## Dickey-Fuller = -7.0177, Lag order = 5, p-value = 0.01 ## alternative hypothesis: stationary 88.3.2 Seasonal differencing For seasonal patterns, use seasonal differencing: \\[Y&#39;_t = Y_t - Y_{t-12}\\] # Seasonal difference (lag = 12 for monthly data) air_sdiff &lt;- diff(AirPassengers, lag = 12) # Often need both regular and seasonal differencing air_both &lt;- diff(diff(AirPassengers, lag = 12)) # Check stationarity adf.test(air_both) ## ## Augmented Dickey-Fuller Test ## ## data: air_both ## Dickey-Fuller = -5.0472, Lag order = 5, p-value = 0.01 ## alternative hypothesis: stationary 88.3.3 Log transformation For series with increasing variance, log transformation helps: # Log transformation air_log &lt;- log(AirPassengers) par(mfrow = c(1, 2)) plot(AirPassengers, main = &quot;Original&quot;) plot(air_log, main = &quot;Log-transformed&quot;) (#fig:log-transform 21)Log transformation stabilizes variance when fluctuations grow with the level. par(mfrow = c(1, 1)) "],["part-4-decomposition.html", "Chapter 89 Part 4: Decomposition 89.1 Classical decomposition 89.2 STL decomposition 89.3 Extracting components", " Chapter 89 Part 4: Decomposition 89.1 Classical decomposition Decomposition separates a time series into trend, seasonal, and remainder components. # Multiplicative decomposition (appropriate when seasonality grows with level) decomp &lt;- stats::decompose(AirPassengers, type = &quot;multiplicative&quot;) # Plot autoplot(decomp) + labs(title = &quot;Classical Decomposition&quot;) + theme_minimal() Figure 89.1: Classical decomposition of airline passenger data showing the extracted trend, seasonal pattern, and random remainder. 89.2 STL decomposition STL (Seasonal and Trend decomposition using Loess) is more flexible: # STL decomposition (requires additive model, so log-transform first) air_stl &lt;- stl(log(AirPassengers), s.window = &quot;periodic&quot;) autoplot(air_stl) + labs(title = &quot;STL Decomposition (log scale)&quot;) + theme_minimal() Figure 89.2: STL decomposition provides more robust seasonal extraction and handles outliers better than classical methods. 89.3 Extracting components # Extract trend trend_component &lt;- air_stl$time.series[, &quot;trend&quot;] # Extract seasonal seasonal_component &lt;- air_stl$time.series[, &quot;seasonal&quot;] # Extract remainder remainder &lt;- air_stl$time.series[, &quot;remainder&quot;] # Seasonally adjusted series seasonally_adjusted &lt;- air_stl$time.series[, &quot;trend&quot;] + air_stl$time.series[, &quot;remainder&quot;] "],["part-5-arima-models.html", "Chapter 90 Part 5: ARIMA Models 90.1 The ARIMA framework 90.2 Understanding AR and MA 90.3 Seasonal ARIMA (SARIMA) 90.4 Fitting ARIMA with auto.arima() 90.5 Checking residuals", " Chapter 90 Part 5: ARIMA Models 90.1 The ARIMA framework ARIMA combines three components: Component Meaning Parameter AR (Autoregressive) Current value depends on past values p I (Integrated) Differencing to achieve stationarity d MA (Moving Average) Current value depends on past errors q An ARIMA(p, d, q) model: \\[Y&#39;_t = c + \\phi_1 Y&#39;_{t-1} + ... + \\phi_p Y&#39;_{t-p} + \\theta_1 \\varepsilon_{t-1} + ... + \\theta_q \\varepsilon_{t-q} + \\varepsilon_t\\] where \\(Y&#39;_t\\) is the differenced series. 90.2 Understanding AR and MA ## ## AUTOREGRESSIVE (AR) — Past values predict current ## ═══════════════════════════════════════════════════════════════ ## AR(1): Y(t) = c + φ₁·Y(t-1) + ε(t) ## &#39;Tomorrow&#39;s abundance depends on today&#39;s abundance&#39; ## ## AR(2): Y(t) = c + φ₁·Y(t-1) + φ₂·Y(t-2) + ε(t) ## &#39;Depends on the last two time points&#39; ## ## ## MOVING AVERAGE (MA) — Past errors predict current ## ═══════════════════════════════════════════════════════════════ ## MA(1): Y(t) = c + ε(t) + θ₁·ε(t-1) ## &#39;Incorporates yesterday&#39;s unexpected shock&#39; ## ## MA(2): Y(t) = c + ε(t) + θ₁·ε(t-1) + θ₂·ε(t-2) ## &#39;Incorporates shocks from last two periods&#39; 90.3 Seasonal ARIMA (SARIMA) For seasonal data, add seasonal AR, I, and MA terms: ARIMA(p, d, q)(P, D, Q)[m] where: - (p, d, q) = non-seasonal terms - (P, D, Q) = seasonal terms - [m] = seasonal period (12 for monthly, 4 for quarterly) 90.4 Fitting ARIMA with auto.arima() # Automatic ARIMA selection air_arima &lt;- auto.arima(AirPassengers, seasonal = TRUE, stepwise = FALSE, # Try all models approximation = FALSE) # Summary summary(air_arima) ## Series: AirPassengers ## ARIMA(2,1,1)(0,1,0)[12] ## ## Coefficients: ## ar1 ar2 ma1 ## 0.5960 0.2142 -0.9819 ## s.e. 0.0888 0.0880 0.0292 ## ## sigma^2 = 132.3: log likelihood = -504.92 ## AIC=1017.85 AICc=1018.17 BIC=1029.35 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 1.342189 10.84619 7.867506 0.4206662 2.80045 0.245627 -0.001286643 90.4.1 Interpreting the output # The model is ARIMA(2,1,1)(0,1,0)[12] # This means: # - AR(2): depends on 2 past values # - I(1): first differencing applied # - MA(1): one lagged error term # - Seasonal I(1): seasonal differencing # - [12]: monthly seasonality # Coefficients cat(&quot;AR coefficients:&quot;, coef(air_arima)[grep(&quot;ar&quot;, names(coef(air_arima)))], &quot;\\n&quot;) ## AR coefficients: 0.5960155 0.214241 cat(&quot;MA coefficients:&quot;, coef(air_arima)[grep(&quot;ma&quot;, names(coef(air_arima)))], &quot;\\n&quot;) ## MA coefficients: -0.981873 90.5 Checking residuals Good model residuals should: - Have no autocorrelation (white noise) - Be approximately normally distributed - Have constant variance checkresiduals(air_arima) Figure 90.1: Residual diagnostics for ARIMA model. Good fit shows: no patterns in residuals, no significant ACF spikes, and approximately normal distribution. ## ## Ljung-Box test ## ## data: Residuals from ARIMA(2,1,1)(0,1,0)[12] ## Q* = 37.784, df = 21, p-value = 0.01366 ## ## Model df: 3. Total lags used: 24 The Ljung-Box test tests for remaining autocorrelation: - H0: Residuals are white noise (no autocorrelation) - p &gt; 0.05 → Good (fail to reject, residuals are white noise) "],["part-6-forecasting.html", "Chapter 91 Part 6: Forecasting 91.1 Generating forecasts 91.2 Understanding prediction intervals 91.3 Evaluating forecast accuracy", " Chapter 91 Part 6: Forecasting 91.1 Generating forecasts # Forecast 24 months ahead air_forecast &lt;- forecast(air_arima, h = 24) # Plot autoplot(air_forecast) + labs(title = &quot;Airline Passengers Forecast&quot;, x = &quot;Year&quot;, y = &quot;Passengers (thousands)&quot;) + theme_minimal() Figure 91.1: ARIMA forecast for airline passengers. Blue line is point forecast; shaded regions show 80% and 95% prediction intervals. 91.2 Understanding prediction intervals # Extract forecasts print(air_forecast) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 1961 445.6351 430.8905 460.3796 423.0852 468.1849 ## Feb 1961 420.3953 403.0907 437.6999 393.9302 446.8604 ## Mar 1961 449.1988 429.7726 468.6249 419.4891 478.9085 ## Apr 1961 491.8405 471.0270 512.6540 460.0091 523.6719 ## May 1961 503.3951 481.5560 525.2342 469.9951 536.7951 ## Jun 1961 566.8632 544.2639 589.4625 532.3005 601.4258 ## Jul 1961 654.2610 631.0822 677.4397 618.8121 689.7099 ## Aug 1961 638.5983 614.9706 662.2261 602.4628 674.7338 ## Sep 1961 540.8846 516.9030 564.8662 504.2079 577.5613 ## Oct 1961 494.1275 469.8626 518.3925 457.0175 531.2376 ## Nov 1961 423.3336 398.8383 447.8290 385.8713 460.7960 ## Dec 1961 465.5085 440.8230 490.1940 427.7553 503.2617 ## Jan 1962 479.2920 448.9989 509.5850 432.9628 525.6212 ## Feb 1962 454.1782 421.7187 486.6377 404.5356 503.8207 ## Mar 1962 483.0884 448.7346 517.4422 430.5488 535.6280 ## Apr 1962 525.8208 490.1125 561.5291 471.2097 580.4320 ## May 1962 537.4524 500.6866 574.2182 481.2239 593.6808 ## Jun 1962 600.9857 563.3927 638.5786 543.4922 658.4791 ## Jul 1962 688.4389 650.1837 726.6940 629.9326 746.9451 ## Aug 1962 672.8232 634.0295 711.6169 613.4934 732.1530 ## Sep 1962 575.1494 535.9105 614.3883 515.1386 635.1601 ## Oct 1962 528.4261 488.8134 568.0389 467.8436 589.0086 ## Nov 1962 457.6609 417.7295 497.5924 396.5910 518.7308 ## Dec 1962 499.8602 459.6531 540.0673 438.3687 561.3516 # Point forecast with intervals forecast_df &lt;- data.frame( Month = time(air_forecast$mean), Forecast = as.numeric(air_forecast$mean), Lo80 = as.numeric(air_forecast$lower[, 1]), Hi80 = as.numeric(air_forecast$upper[, 1]), Lo95 = as.numeric(air_forecast$lower[, 2]), Hi95 = as.numeric(air_forecast$upper[, 2]) ) head(forecast_df) ## Month Forecast Lo80 Hi80 Lo95 Hi95 ## 1 1961.000 445.6351 430.8905 460.3796 423.0852 468.1849 ## 2 1961.083 420.3953 403.0907 437.6999 393.9302 446.8604 ## 3 1961.167 449.1988 429.7726 468.6249 419.4891 478.9085 ## 4 1961.250 491.8405 471.0270 512.6540 460.0091 523.6719 ## 5 1961.333 503.3951 481.5560 525.2342 469.9951 536.7951 ## 6 1961.417 566.8632 544.2639 589.4625 532.3005 601.4258 Important: Prediction intervals widen over time because uncertainty accumulates. 91.3 Evaluating forecast accuracy 91.3.1 Train/test split # Split data: train on first part, test on last 24 months train &lt;- window(AirPassengers, end = c(1958, 12)) test &lt;- window(AirPassengers, start = c(1959, 1)) # Fit model on training data train_arima &lt;- auto.arima(train) # Forecast for test period train_forecast &lt;- forecast(train_arima, h = length(test)) # Compare to actual forecast::accuracy(train_forecast, test) ## ME RMSE MAE MPE MAPE MASE ACF1 Theil&#39;s U ## Training set -0.01614662 9.567988 7.120167 -0.03346415 2.90195 0.2491828 0.00821521 NA ## Test set 68.57728797 74.252243 68.577288 14.92755772 14.92756 2.3999829 0.71840293 1.464657 91.3.2 Error metrics Metric Formula Interpretation MAE Mean Absolute Error Average error magnitude RMSE Root Mean Squared Error Penalizes large errors MAPE Mean Absolute Percentage Error Percentage error MASE Mean Absolute Scaled Error Scale-independent # Plot forecast vs actual autoplot(train_forecast) + autolayer(test, series = &quot;Actual&quot;) + labs(title = &quot;Forecast vs Actual&quot;, x = &quot;Year&quot;, y = &quot;Passengers&quot;) + theme_minimal() Figure 91.2: Forecast accuracy: comparing predicted values (blue) to actual test data (black). "],["part-7-ecological-case-studies.html", "Chapter 92 Part 7: Ecological Case Studies 92.1 Case Study 1: Population trends 92.2 Case Study 2: Seasonal climate data 92.3 Case Study 3: Phenological timing", " Chapter 92 Part 7: Ecological Case Studies 92.1 Case Study 1: Population trends # Simulate wolf population data years &lt;- 1980:2020 n &lt;- length(years) # Population with density dependence and environmental stochasticity set.seed(123) pop &lt;- numeric(n) pop[1] &lt;- 50 K &lt;- 200 # Carrying capacity for (t in 2:n) { # Logistic growth with noise r &lt;- 0.15 growth &lt;- r * pop[t-1] * (1 - pop[t-1]/K) pop[t] &lt;- max(1, pop[t-1] + growth + rnorm(1, 0, 8)) } wolf_ts &lt;- ts(pop, start = 1980, frequency = 1) # Plot autoplot(wolf_ts) + labs(title = &quot;Wolf Population, 1980-2020&quot;, x = &quot;Year&quot;, y = &quot;Population Size&quot;) + theme_minimal() Figure 92.1: Annual wolf population counts showing trend and interannual variation. # Check stationarity adf.test(wolf_ts) ## ## Augmented Dickey-Fuller Test ## ## data: wolf_ts ## Dickey-Fuller = -2.376, Lag order = 3, p-value = 0.4257 ## alternative hypothesis: stationary # Fit ARIMA wolf_arima &lt;- auto.arima(wolf_ts) summary(wolf_arima) ## Series: wolf_ts ## ARIMA(2,2,0) ## ## Coefficients: ## ar1 ar2 ## -0.6735 -0.6208 ## s.e. 0.1245 0.1224 ## ## sigma^2 = 55.82: log likelihood = -133.32 ## AIC=272.65 AICc=273.33 BIC=277.64 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.4154726 7.097228 5.23131 0.06859211 3.567257 0.7501671 0.05555975 # Forecast wolf_forecast &lt;- forecast(wolf_arima, h = 10) autoplot(wolf_forecast) + labs(title = &quot;Wolf Population Forecast&quot;, x = &quot;Year&quot;, y = &quot;Population Size&quot;) + theme_minimal() # Check residuals checkresiduals(wolf_arima) ## ## Ljung-Box test ## ## data: Residuals from ARIMA(2,2,0) ## Q* = 9.2374, df = 6, p-value = 0.1607 ## ## Model df: 2. Total lags used: 8 92.2 Case Study 2: Seasonal climate data # Simulate monthly precipitation n_years &lt;- 25 months &lt;- n_years * 12 # Seasonal pattern + trend + noise time_idx &lt;- 1:months precip &lt;- 80 + 0.05 * time_idx + # Slight increasing trend 40 * sin(2 * pi * time_idx / 12 - pi/2) + # Peak in winter rnorm(months, 0, 15) precip &lt;- pmax(precip, 0) precip_ts &lt;- ts(precip, start = c(1998, 1), frequency = 12) # Seasonal plot ggseasonplot(precip_ts) + labs(title = &quot;Monthly Precipitation by Year&quot;, y = &quot;Precipitation (mm)&quot;) + theme_minimal() Figure 92.2: Monthly precipitation with clear seasonal pattern. # Decomposition precip_stl &lt;- stl(precip_ts, s.window = &quot;periodic&quot;) autoplot(precip_stl) + labs(title = &quot;Precipitation Decomposition&quot;) + theme_minimal() # Fit seasonal ARIMA precip_arima &lt;- auto.arima(precip_ts) summary(precip_arima) ## Series: precip_ts ## ARIMA(0,0,1)(1,1,0)[12] with drift ## ## Coefficients: ## ma1 sar1 drift ## 0.0033 -0.5042 0.0514 ## s.e. 0.0649 0.0509 0.0599 ## ## sigma^2 = 328.2: log likelihood = -1243.19 ## AIC=2494.39 AICc=2494.53 BIC=2509.04 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 0.009901384 17.65798 13.79742 -4.026712 18.62731 0.8225286 -0.0006635492 # Forecast next 2 years precip_forecast &lt;- forecast(precip_arima, h = 24) autoplot(precip_forecast) + labs(title = &quot;Precipitation Forecast&quot;, x = &quot;Year&quot;, y = &quot;Precipitation (mm)&quot;) + theme_minimal() 92.3 Case Study 3: Phenological timing # Simulate first flowering date (day of year) years &lt;- 1970:2023 n &lt;- length(years) # Advancing phenology (earlier each year) with noise set.seed(456) flowering_doy &lt;- 120 - 0.3 * (years - 1970) + rnorm(n, 0, 5) phenology_ts &lt;- ts(flowering_doy, start = 1970, frequency = 1) # Plot autoplot(phenology_ts) + labs(title = &quot;First Flowering Date (Day of Year)&quot;, x = &quot;Year&quot;, y = &quot;Day of Year&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;firebrick&quot;) + theme_minimal() Figure 92.3: First flowering date advancing over time, consistent with climate warming. # Linear trend phenology_lm &lt;- lm(flowering_doy ~ years) summary(phenology_lm) ## ## Call: ## lm(formula = flowering_doy ~ years) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.6910 -3.3107 0.0153 3.2241 10.5854 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 728.62746 90.71276 8.032 1.13e-10 *** ## years -0.30846 0.04543 -6.789 1.07e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 5.204 on 52 degrees of freedom ## Multiple R-squared: 0.4699, Adjusted R-squared: 0.4597 ## F-statistic: 46.09 on 1 and 52 DF, p-value: 1.074e-08 cat(&quot;\\nFlowering is advancing by&quot;, round(abs(coef(phenology_lm)[2]), 2), &quot;days per year\\n&quot;) ## ## Flowering is advancing by 0.31 days per year # ARIMA with drift phenology_arima &lt;- auto.arima(phenology_ts, allowdrift = TRUE) summary(phenology_arima) ## Series: phenology_ts ## ARIMA(0,1,1) ## ## Coefficients: ## ma1 ## -0.7723 ## s.e. 0.0805 ## ## sigma^2 = 31.42: log likelihood = -166.51 ## AIC=337.03 AICc=337.27 BIC=340.97 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.8236775 5.50098 4.387658 -0.9487624 3.923302 0.8035082 0.004143971 # Forecast phenology_forecast &lt;- forecast(phenology_arima, h = 20) autoplot(phenology_forecast) + labs(title = &quot;Phenology Forecast&quot;, x = &quot;Year&quot;, y = &quot;Day of Year&quot;) + theme_minimal() "],["part-8-advanced-topics-brief-introduction.html", "Chapter 93 Part 8: Advanced Topics (Brief Introduction) 93.1 State-space models 93.2 Exponential smoothing (ETS) 93.3 Dynamic regression", " Chapter 93 Part 8: Advanced Topics (Brief Introduction) 93.1 State-space models State-space models separate the observation process from the underlying dynamics: ## ## STATE-SPACE MODEL STRUCTURE ## ═══════════════════════════════════════════════════════════════ ## ## True (hidden) state: X(t) = f(X(t-1)) + process error ## Observation: Y(t) = X(t) + observation error ## ## Advantages: ## - Explicit observation error ## - Handle missing data naturally ## - More biologically realistic ## - Can include covariates ## ## R packages: MARSS, dlm, bsts 93.2 Exponential smoothing (ETS) ETS models handle trend and seasonality with weighted averages: # Fit ETS model air_ets &lt;- ets(AirPassengers) summary(air_ets) ## ETS(M,Ad,M) ## ## Call: ## ets(y = AirPassengers) ## ## Smoothing parameters: ## alpha = 0.7096 ## beta = 0.0204 ## gamma = 1e-04 ## phi = 0.98 ## ## Initial states: ## l = 120.9939 ## b = 1.7705 ## s = 0.8944 0.7993 0.9217 1.0592 1.2203 1.2318 ## 1.1105 0.9786 0.9804 1.011 0.8869 0.9059 ## ## sigma: 0.0392 ## ## AIC AICc BIC ## 1395.166 1400.638 1448.623 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set 1.567359 10.74726 7.791605 0.4357799 2.857917 0.2432573 0.03945056 # ETS(M,A,M) = Multiplicative error, Additive trend, Multiplicative season 93.3 Dynamic regression Include external predictors (covariates) in ARIMA models: # Example: temperature affects phenology # Not run - conceptual model &lt;- auto.arima(phenology_ts, xreg = temperature) forecast(model, xreg = future_temperature) "],["part-9-practical-tips.html", "Chapter 94 Part 9: Practical Tips 94.1 Common pitfalls 94.2 When to use time series vs regression 94.3 Dealing with missing data", " Chapter 94 Part 9: Practical Tips 94.1 Common pitfalls Pitfall Problem Solution Ignoring seasonality Residual patterns, poor forecasts Use seasonal models (SARIMA) Overfitting Model too complex, poor out-of-sample performance Use information criteria (AIC), cross-validation Misinterpreting trends Confusing autocorrelation with trend Proper differencing, test stationarity Short series Unreliable parameter estimates Simpler models, wider intervals Irregular spacing Standard methods assume regular intervals Interpolation or specialized methods 94.2 When to use time series vs regression Use time series methods when… Use regression when… Temporal autocorrelation is present Observations are independent You want to forecast future values You want to explain relationships You have a single long series You have cross-sectional data Time order matters Time is just another covariate 94.3 Dealing with missing data # Create series with missing values air_missing &lt;- AirPassengers air_missing[c(50, 51, 100)] &lt;- NA # Option 1: Linear interpolation air_interp &lt;- na.approx(air_missing) # Option 2: Spline interpolation air_spline &lt;- na.spline(air_missing) # Option 3: Kalman smoothing (via forecast package) air_kalman &lt;- na.interp(air_missing) # Compare plot(air_missing, main = &quot;Missing Data Handling&quot;, lwd = 2) lines(air_interp, col = &quot;red&quot;, lty = 2) lines(air_kalman, col = &quot;blue&quot;, lty = 3) legend(&quot;topleft&quot;, c(&quot;Missing&quot;, &quot;Interpolated&quot;, &quot;Kalman&quot;), col = c(&quot;black&quot;, &quot;red&quot;, &quot;blue&quot;), lty = c(1, 2, 3)) "],["part-10-reporting-1.html", "Chapter 95 Part 10: Reporting 95.1 What to report 95.2 Sample methods and results 95.3 Key takeaways 95.4 Assignment", " Chapter 95 Part 10: Reporting 95.1 What to report Time series description: Length, frequency, time span Exploratory analysis: Trends, seasonality, stationarity Transformations applied: Log, differencing Model selected: ARIMA order, why Diagnostics: Residual tests, ACF of residuals Forecast accuracy: Error metrics on held-out data Forecasts: Point estimates with prediction intervals 95.2 Sample methods and results 95.2.1 Methods We analyzed monthly population counts from 1995-2020 using time series methods. After visual inspection and augmented Dickey-Fuller testing (p = 0.23), we determined the series was non-stationary and applied first differencing. Seasonal patterns were evident in the autocorrelation function at lags 12 and 24, so we included seasonal terms. We fit ARIMA models using the auto.arima() function with stepwise = FALSE to search all candidate models, selecting the best model by AICc. Model adequacy was assessed via Ljung-Box tests on residuals and visual inspection of residual ACF. We evaluated forecast accuracy using a train/test split (training: 1995-2017; test: 2018-2020) and calculated RMSE and MAPE. Analyses were conducted using the forecast package (Hyndman &amp; Khandakar 2008) in R version 4.3.1. 95.2.2 Results Population counts showed a significant increasing trend over the 25-year period with strong annual seasonality peaking in August (Fig. X). The best-fitting model was ARIMA(1,1,1)(1,0,1)[12] (AICc = 342.5), indicating autoregressive dependence at both non-seasonal and seasonal lags. Residual diagnostics showed no remaining autocorrelation (Ljung-Box test: Q = 15.2, df = 12, p = 0.23) and approximately normal distribution (Fig. Y). Out-of-sample forecast accuracy was acceptable (RMSE = 12.4, MAPE = 8.3%). The model forecasts continued population increase, with August 2025 abundance predicted at 156 individuals (95% PI: 128–189). 95.3 Key takeaways Time series data are autocorrelated — Standard methods assuming independence don’t apply Check stationarity first — Most methods require it; use differencing if needed Decomposition reveals structure — Separate trend, seasonality, and noise ACF and PACF guide model selection — Patterns indicate AR vs MA processes auto.arima() is your friend — But understand what it’s doing Always check residuals — Should be white noise (no autocorrelation) Prediction intervals widen — Uncertainty grows with forecast horizon Report accuracy on held-out data — In-sample fit is overly optimistic 95.4 Assignment 95.4.1 Part 1: Conceptual questions Why is it important to test for stationarity before fitting an ARIMA model? What problems arise if you fit ARIMA to a non-stationary series without differencing? You observe strong spikes at lags 12, 24, and 36 in your ACF. What does this tell you about your data? Your ARIMA model has excellent in-sample fit (R² = 0.95) but poor out-of-sample forecasts. What might be happening? 95.4.2 Part 2: Exploratory analysis Using the built-in co2 data (monthly atmospheric CO2 at Mauna Loa): data(co2) Plot the time series Create seasonal and subseries plots Describe the trend and seasonal pattern Generate ACF and PACF plots Test for stationarity 95.4.3 Part 3: ARIMA modeling Using the co2 data: Apply appropriate transformations/differencing Fit an ARIMA model using auto.arima() Check residual diagnostics Interpret the model coefficients 95.4.4 Part 4: Forecasting Create a train/test split (train: 1959-1990, test: 1991-1997) Fit ARIMA on training data Generate forecasts for the test period Calculate forecast accuracy (RMSE, MAPE) Plot forecasts vs actual 95.4.5 Part 5: Ecological application Using annual population count data (simulated below): set.seed(789) years &lt;- 1985:2022 pop &lt;- 100 * exp(cumsum(rnorm(length(years), 0.02, 0.15))) elk_ts &lt;- ts(round(pop), start = 1985, frequency = 1) Explore the time series (plot, ACF, stationarity test) Fit an appropriate ARIMA model Forecast 10 years ahead Write a methods and results paragraph suitable for a report 95.4.6 Part 6: Reflection In 2-3 sentences, explain why forecasts from ecological time series models should always include prediction intervals, and why these intervals matter for management decisions. "],["ordination-methods.html", "Chapter 96 Ordination Methods 96.1 The core idea 96.2 Setup", " Chapter 96 Ordination Methods Community ecology data are inherently high-dimensional. If you sample 50 species across 30 sites, you have a 30 × 50 matrix—far too complex to visualize directly. How do you see patterns in this cloud of data? Ordination reduces this complexity by projecting high-dimensional data onto a few interpretable axes. Sites that are similar in species composition appear close together; dissimilar sites appear far apart. The result is a plot where you can literally see community patterns. This chapter covers the most common ordination techniques in ecology: NMDS (Non-metric Multidimensional Scaling): The workhorse for community data PCA (Principal Components Analysis): Best for environmental gradients CA/DCA: For unimodal species responses (briefly) We’ll focus on NMDS, as it’s the most widely used and flexible approach for ecological community data. 96.1 The core idea Imagine you’ve measured the abundance of every species at each of your sites. Each site can be placed in a multi-dimensional space, with one axis for each species: 1 species: Sites vary along a single axis 2 species: Sites exist in a 2D plane 3 species: Sites occupy 3D space 50 species: Sites live in 50-dimensional space (impossible to visualize!) Ordination finds the best low-dimensional representation of this high-dimensional structure. It answers: “If I could only show you 2 axes, which 2 axes would best preserve the distances between sites?” 96.2 Setup library(tidyverse) library(vegan) # The main community ecology package library(ggvegan) # ggplot2 compatibility for vegan set.seed(42) "],["part-1-visualizing-ordination-logic.html", "Chapter 97 Part 1: Visualizing Ordination Logic 97.1 One species 97.2 Two species 97.3 Three species… and beyond", " Chapter 97 Part 1: Visualizing Ordination Logic Let’s build intuition for what ordination does using a simple example. 97.1 One species With one species, comparing sites is easy—just compare abundances: # One species: Linum lewisii abundance at 2 sites plot(0:10, rep(0, 11), type = &quot;n&quot;, axes = FALSE, xlab = &quot;Abundance of Linum lewisii&quot;, ylab = &quot;&quot;) axis(1) points(1, 0, pch = 19, cex = 2, col = &quot;coral&quot;) points(5, 0, pch = 19, cex = 2, col = &quot;steelblue&quot;) text(1, 0.3, &quot;Dryland&quot;, cex = 0.9) text(5, 0.3, &quot;Wetland&quot;, cex = 0.9) Figure 97.1: With one species, sites vary along a single dimension. 97.2 Two species Adding a second species creates a 2D space: # Two species plot(0:10, 0:10, type = &quot;n&quot;, xlab = &quot;Linum lewisii abundance&quot;, ylab = &quot;Juglans arizonica abundance&quot;) points(1, 0, pch = 19, cex = 2, col = &quot;coral&quot;) points(5, 5, pch = 19, cex = 2, col = &quot;steelblue&quot;) text(1.5, 0.5, &quot;Dryland&quot;, cex = 0.9) text(5.5, 5.5, &quot;Wetland&quot;, cex = 0.9) # Show distance segments(1, 0, 5, 5, lty = 2, col = &quot;gray50&quot;) Figure 97.2: With two species, sites exist in 2D space. The distance between points reflects community dissimilarity. 97.3 Three species… and beyond library(scatterplot3d) # Three species s3d &lt;- scatterplot3d(0:10, 0:10, 0:10, type = &quot;n&quot;, xlab = &quot;Linum&quot;, ylab = &quot;Juglans&quot;, zlab = &quot;Quercus&quot;, main = &quot;Three Species Space&quot;) s3d$points3d(1, 0, 3, pch = 19, cex = 2, col = &quot;coral&quot;) s3d$points3d(5, 5, 9, pch = 19, cex = 2, col = &quot;steelblue&quot;) Figure 97.3: With three species, we need 3D visualization. With more species, we can’t visualize directly—we need ordination. With 50 species, you’d have 50 axes—impossible to visualize. Ordination finds the best 2D projection of this high-dimensional cloud. "],["part-2-nmds-non-metric-multidimensional-scaling.html", "Chapter 98 Part 2: NMDS (Non-metric Multidimensional Scaling) 98.1 What makes NMDS different? 98.2 How NMDS works 98.3 NMDS in practice 98.4 Interpreting NMDS axes", " Chapter 98 Part 2: NMDS (Non-metric Multidimensional Scaling) 98.1 What makes NMDS different? Most ordination methods use Euclidean distance (straight-line distance). But Euclidean distance often misrepresents ecological data because: Double-zeros are ambiguous (two sites lacking a species might be similar OR completely different) Species relationships are rarely linear Rare species have undue influence NMDS solves this by: - Using any dissimilarity measure (typically Bray-Curtis for community data) - Preserving rank order of distances, not actual distances - Being non-metric—only the ordering matters 98.2 How NMDS works Calculate dissimilarity between all pairs of sites (Bray-Curtis, Jaccard, etc.) Start with a random configuration of sites in 2D (or your chosen number of dimensions) Iteratively adjust site positions to better match the dissimilarity ranks Calculate stress: How well does the 2D configuration preserve the rank order of the original dissimilarities? Repeat until stress is minimized Stress interpretation: - &lt; 0.05: Excellent representation - &lt; 0.10: Great - &lt; 0.20: Good (acceptable) - &gt; 0.30: Poor (consider more dimensions or different approach) 98.3 NMDS in practice Let’s work with the dune dataset—vegetation at 20 Dutch dune meadow sites: # Load data data(dune) # Species abundances data(dune.env) # Environmental variables # Look at the data dim(dune) # 20 sites, 30 species ## [1] 20 30 head(dune[, 1:6]) ## Achimill Agrostol Airaprae Alopgeni Anthodor Bellpere ## 1 1 0 0 0 0 0 ## 2 3 0 0 2 0 3 ## 3 0 4 0 7 0 2 ## 4 0 8 0 2 0 2 ## 5 2 0 0 0 4 2 ## 6 2 0 0 0 3 0 98.3.1 Data transformation Before NMDS, consider transforming your data: # Wisconsin double standardization: # - Divide each value by species maximum (standardize across species) # - Then divide by site total (standardize across sites) # This equalizes emphasis among species dune_std &lt;- wisconsin(dune) # Compare raw vs transformed dune[1, 1:5] ## Achimill Agrostol Airaprae Alopgeni Anthodor ## 1 1 0 0 0 0 dune_std[1, 1:5] ## Achimill Agrostol Airaprae Alopgeni Anthodor ## 1 0.08506616 0 0 0 0 98.3.2 Run NMDS # Run NMDS with Bray-Curtis dissimilarity # k = 2 dimensions; trymax = number of random starts nmds &lt;- metaMDS(dune_std, distance = &quot;bray&quot;, k = 2, trymax = 100) ## Run 0 stress 0.1146911 ## Run 1 stress 0.2134569 ## Run 2 stress 0.1178508 ## Run 3 stress 0.1949347 ## Run 4 stress 0.1146911 ## ... New best solution ## ... Procrustes: rmse 3.000749e-06 max resid 6.840407e-06 ## ... Similar to previous best ## Run 5 stress 0.1178508 ## Run 6 stress 0.1146911 ## ... New best solution ## ... Procrustes: rmse 8.863373e-07 max resid 2.200579e-06 ## ... Similar to previous best ## Run 7 stress 0.2102078 ## Run 8 stress 0.1905538 ## Run 9 stress 0.1178508 ## Run 10 stress 0.1146911 ## ... Procrustes: rmse 1.225067e-06 max resid 2.874946e-06 ## ... Similar to previous best ## Run 11 stress 0.1863298 ## Run 12 stress 0.1899457 ## Run 13 stress 0.1972568 ## Run 14 stress 0.1178508 ## Run 15 stress 0.1146911 ## ... New best solution ## ... Procrustes: rmse 4.78499e-07 max resid 1.553141e-06 ## ... Similar to previous best ## Run 16 stress 0.1855447 ## Run 17 stress 0.1178508 ## Run 18 stress 0.1178508 ## Run 19 stress 0.1846306 ## Run 20 stress 0.1980281 ## *** Best solution repeated 1 times # Check results nmds ## ## Call: ## metaMDS(comm = dune_std, distance = &quot;bray&quot;, k = 2, trymax = 100) ## ## global Multidimensional Scaling using monoMDS ## ## Data: dune_std ## Distance: bray ## ## Dimensions: 2 ## Stress: 0.1146911 ## Stress type 1, weak ties ## Best solution was repeated 1 time in 20 tries ## The best solution was from try 15 (random start) ## Scaling: centring, PC rotation, halfchange scaling ## Species: expanded scores based on &#39;dune_std&#39; 98.3.3 Interpret stress # Shepard plot: original dissimilarity vs ordination distance stressplot(nmds) Figure 98.1: Shepard plot showing fit between original distances and ordination distances. Points close to the line indicate good fit; stress &lt; 0.2 is acceptable. The Shepard plot shows: - x-axis: Original Bray-Curtis dissimilarities - y-axis: Distances in the NMDS ordination - Points near line: Good preservation of rank order - R²: How well ordination distances explain original dissimilarities 98.3.4 Basic NMDS plot # Base R plotting ordiplot(nmds, type = &quot;t&quot;) # &quot;t&quot; for text labels Figure 98.2: Basic NMDS plot showing sites (circles) and species (crosses) in ordination space. 98.3.5 Better visualization with ggplot # Extract scores for ggplot site_scores &lt;- as.data.frame(scores(nmds, display = &quot;sites&quot;)) site_scores$Management &lt;- dune.env$Management species_scores &lt;- as.data.frame(scores(nmds, display = &quot;species&quot;)) species_scores$Species &lt;- rownames(species_scores) # Create plot ggplot() + # Add species as gray text geom_text(data = species_scores, aes(x = NMDS1, y = NMDS2, label = Species), color = &quot;gray60&quot;, size = 2.5, alpha = 0.7) + # Add sites colored by management geom_point(data = site_scores, aes(x = NMDS1, y = NMDS2, color = Management), size = 3, alpha = 0.8) + # Add axes geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray40&quot;) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray40&quot;) + # Labels labs(title = &quot;NMDS: Dune Meadow Vegetation&quot;, subtitle = paste(&quot;Stress =&quot;, round(nmds$stress, 3))) + theme_minimal() + coord_equal() Figure 98.3: NMDS ordination of dune meadow vegetation. Sites are positioned such that similar communities are close together. Environmental variables can be overlaid to interpret axes. 98.4 Interpreting NMDS axes NMDS axes have no inherent meaning—they just represent gradients in community composition. To interpret them: Overlay environmental variables using envfit() Look at species positions to understand what characterizes each end of an axis Color/shape sites by factors to see group separation # Fit environmental variables env_fit &lt;- envfit(nmds, dune.env[, c(&quot;A1&quot;, &quot;Moisture&quot;)], permutations = 999) env_fit ## ## ***VECTORS ## ## NMDS1 NMDS2 r2 Pr(&gt;r) ## A1 0.96861 0.24858 0.3721 0.024 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 ## ## ***FACTORS: ## ## Centroids: ## NMDS1 NMDS2 ## Moisture1 -0.5160 -0.0010 ## Moisture2 -0.4688 -0.0139 ## Moisture4 0.2567 -0.4095 ## Moisture5 0.7106 0.1260 ## ## Goodness of fit: ## r2 Pr(&gt;r) ## Moisture 0.5109 0.002 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 # Plot with environmental vectors plot(nmds, display = &quot;sites&quot;, type = &quot;n&quot;) points(nmds, display = &quot;sites&quot;, pch = 19, col = &quot;steelblue&quot;) plot(env_fit, col = &quot;firebrick&quot;, lwd = 2) Figure 98.4: Environmental vectors overlaid on NMDS. Vector length indicates correlation strength; direction shows the gradient. Reading environmental vectors: - Direction: Points toward higher values of that variable - Length: Longer vectors are more strongly correlated with ordination axes - p-value: Tests whether correlation is stronger than random "],["part-3-other-ordination-methods.html", "Chapter 99 Part 3: Other Ordination Methods 99.1 PCA (Principal Components Analysis) 99.2 CA and DCA", " Chapter 99 Part 3: Other Ordination Methods 99.1 PCA (Principal Components Analysis) PCA is best for: - Environmental data (continuous variables) - Linear relationships between variables - When you want interpretable axes (loadings show variable contributions) library(vegan) env_numeric &lt;- dune.env[, c(&quot;A1&quot;, &quot;Moisture&quot;)] # Moisture is an ordered factor; convert its level codes to numbers (1,2,4,5) env_numeric$Moisture &lt;- as.numeric(as.character(env_numeric$Moisture)) pca &lt;- rda(env_numeric, scale = TRUE) biplot(pca, scaling = 2, main = &quot;PCA of Environmental Variables&quot;) Figure 99.1: PCA of environmental variables from dune meadows. Arrows show how each variable contributes to the principal components. PCA vs NMDS: Feature PCA NMDS Best for Environmental data Community data Distance metric Euclidean Any (Bray-Curtis, etc.) Axes meaning Interpretable loadings No inherent meaning Assumes Linear relationships No assumption Stress diagnostic Eigenvalues (% variance) Stress value 99.2 CA and DCA Correspondence Analysis (CA) and Detrended CA (DCA) are alternatives to NMDS that assume unimodal species responses (species peak at optimal conditions and decline on either side). # DCA example (not run) dca &lt;- decorana(dune) plot(dca) CA/DCA are less commonly used now because NMDS is more flexible. Consider them when: - You have strong environmental gradients - Species clearly show unimodal responses - You want to interpret axes as gradient lengths (DCA) "],["part-4-practical-considerations.html", "Chapter 100 Part 4: Practical Considerations 100.1 Choosing the right dissimilarity 100.2 Handling stress 100.3 Visualizing groups 100.4 Creating publication-quality figures", " Chapter 100 Part 4: Practical Considerations 100.1 Choosing the right dissimilarity Dissimilarity Best for In R Bray-Curtis Abundance data \"bray\" Jaccard Presence/absence \"jaccard\" Sørensen Similar to Jaccard \"bray\" on binary Euclidean Environmental data \"euclidean\" # Calculate dissimilarity matrix bray_dist &lt;- vegdist(dune, method = &quot;bray&quot;) # View first few values as.matrix(bray_dist)[1:4, 1:4] ## 1 2 3 4 ## 1 0.0000000 0.4666667 0.4482759 0.5238095 ## 2 0.4666667 0.0000000 0.3414634 0.3563218 ## 3 0.4482759 0.3414634 0.0000000 0.2705882 ## 4 0.5238095 0.3563218 0.2705882 0.0000000 100.2 Handling stress If stress is too high (&gt; 0.2): Increase dimensions: Try k = 3 (harder to visualize but better fit) Remove outlier sites: Check if one weird site is distorting the ordination Transform data: Stronger transformation (square root, log, presence/absence) Try different dissimilarity: Jaccard instead of Bray-Curtis # Try 3 dimensions nmds_3d &lt;- metaMDS(dune_std, k = 3, trymax = 100) # Try presence/absence dune_pa &lt;- decostand(dune, method = &quot;pa&quot;) nmds_pa &lt;- metaMDS(dune_pa, distance = &quot;jaccard&quot;, trymax = 100) 100.3 Visualizing groups # Add grouping variable to scores site_scores$Management &lt;- dune.env$Management # Plot with group ellipses ggplot(site_scores, aes(x = NMDS1, y = NMDS2, color = Management)) + geom_point(size = 3) + stat_ellipse(level = 0.95, linetype = 2) + labs(title = &quot;NMDS by Management Type&quot;, subtitle = paste(&quot;Stress =&quot;, round(nmds$stress, 3))) + theme_minimal() + coord_equal() Figure 100.1: NMDS with sites grouped by management type. Ellipses show 95% confidence regions for each group centroid. 100.4 Creating publication-quality figures # Two-panel figure: sites and species library(gridExtra) # Panel 1: Sites only p1 &lt;- ggplot(site_scores, aes(x = NMDS1, y = NMDS2, color = Management)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray70&quot;) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray70&quot;) + stat_ellipse(level = 0.95, linetype = 2, linewidth = 0.5) + geom_point(size = 3, alpha = 0.8) + scale_color_brewer(palette = &quot;Set2&quot;) + labs(title = &quot;Sites&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + coord_equal() # Panel 2: Species only p2 &lt;- ggplot(species_scores, aes(x = NMDS1, y = NMDS2)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray70&quot;) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray70&quot;) + geom_segment(aes(x = 0, y = 0, xend = NMDS1, yend = NMDS2), arrow = arrow(length = unit(0.2, &quot;cm&quot;)), color = &quot;gray40&quot;, alpha = 0.5) + geom_text(aes(label = Species), size = 2.5, color = &quot;gray20&quot;) + labs(title = &quot;Species&quot;) + theme_minimal() + coord_equal() grid.arrange(p1, p2, ncol = 2) Figure 100.2: Publication-quality NMDS ordination showing community composition differences among management treatments. Points represent sites; ellipses show 95% confidence intervals around group centroids. "],["part-5-reporting-ordination-results.html", "Chapter 101 Part 5: Reporting Ordination Results 101.1 What to report 101.2 Sample methods and results 101.3 Key takeaways 101.4 Assignment", " Chapter 101 Part 5: Reporting Ordination Results 101.1 What to report Ordination method and why you chose it Data transformation (if any) Dissimilarity measure Number of dimensions and final stress How you interpreted axes (environmental fitting, species positions) Figure with stress value noted 101.2 Sample methods and results 101.2.1 Methods We visualized community composition using Non-metric Multidimensional Scaling (NMDS) with Bray-Curtis dissimilarity. Prior to ordination, abundance data were transformed using Wisconsin double standardization to equalize emphasis among species. We used two dimensions (k = 2) and ran the ordination with 100 random starts to find a stable solution. We assessed ordination quality using stress values, targeting stress &lt; 0.2 as acceptable. Environmental variables were fitted to the ordination using the envfit() function with 999 permutations. Analyses were conducted using the vegan package (Oksanen et al. 2022) in R version 4.3.1. 101.2.2 Results NMDS ordination revealed clear separation of dune meadow vegetation by management type (stress = 0.118; Fig. X). Sites with biological management clustered separately from those under hobby farming and nature conservation management along NMDS axis 1. Environmental fitting showed that soil moisture was significantly correlated with ordination axes (r² = 0.45, p = 0.003), with wetter sites positioned at higher NMDS2 values. Species characteristic of wet meadows (Juncus, Caltha) were positioned toward the high-moisture end of this gradient, while dry-meadow species (Lolium, Poa) occupied the opposite region of ordination space. 101.3 Key takeaways Ordination reduces dimensions — It finds the best 2D view of high-dimensional community data NMDS is most flexible — Works with any dissimilarity measure, no distribution assumptions Stress indicates fit — Target stress &lt; 0.2; lower is better Axes have no inherent meaning — Interpret using environmental vectors and species positions Transform before ordinating — Wisconsin standardization is common for community data Use appropriate dissimilarity — Bray-Curtis for abundances, Jaccard for presence/absence Ordination visualizes; other tests quantify — Use PERMANOVA (next chapter) to statistically test group differences 101.4 Assignment 101.4.1 Part 1: Conceptual questions Why is Bray-Curtis dissimilarity preferred over Euclidean distance for community data? Your NMDS has stress = 0.25. What options do you have to improve this? What is the difference between what NMDS shows you and what PERMANOVA tests? 101.4.2 Part 2: NMDS analysis Using the built-in BCI dataset (tree abundances on Barro Colorado Island): data(BCI) dim(BCI) # 50 sites, 225 species ## [1] 50 225 Apply appropriate transformation Run NMDS with Bray-Curtis dissimilarity Check and report stress Create a visualization showing site positions Identify which species are most associated with different regions of ordination space 101.4.3 Part 3: Environmental interpretation Using the dune and dune.env datasets: Run NMDS Fit environmental variables using envfit() Create a figure showing both sites (colored by Management) and environmental vectors Interpret what ecological gradients the NMDS axes represent 101.4.4 Part 4: Reflection In 2-3 sentences, explain why ordination is a complement to, rather than a replacement for, statistical hypothesis testing in community ecology. "],["permutation-based-methods-for-community-data.html", "Chapter 102 Permutation-Based Methods for Community Data 102.1 The permutation logic 102.2 Setup", " Chapter 102 Permutation-Based Methods for Community Data Ordination shows you patterns. But are those patterns real, or could they arise by chance? You see that burned sites cluster separately from unburned sites in your NMDS—but is this separation statistically significant? Traditional parametric tests (ANOVA, t-tests) assume normally distributed data. Community data violate this assumption spectacularly: they’re multivariate (many species), often sparse (lots of zeros), and rarely normal. Permutation-based methods solve this problem by: - Using the data themselves to generate null distributions - Making no assumptions about underlying distributions - Working directly with dissimilarity matrices This chapter covers the essential permutation methods for community ecology: PERMANOVA: Test whether groups differ in composition ANOSIM: Alternative to PERMANOVA Mantel test: Correlate two distance matrices envfit: Relate environmental variables to ordination 102.1 The permutation logic Instead of assuming a theoretical null distribution (like the F-distribution), permutation tests ask: “If there were no treatment effect, how often would I see a result this extreme just by chance?” The procedure: 1. Calculate the test statistic from your actual data 2. Randomly shuffle (permute) group labels 3. Recalculate the test statistic 4. Repeat many times (999 or more) 5. Count how often the permuted statistic exceeds the observed statistic 6. This proportion is your p-value set.seed(42) # Simulate a true difference group1 &lt;- rnorm(20, mean = 10, sd = 2) group2 &lt;- rnorm(20, mean = 12, sd = 2) observed_diff &lt;- mean(group2) - mean(group1) # Permutation distribution combined &lt;- c(group1, group2) n_perm &lt;- 999 perm_diffs &lt;- numeric(n_perm) for (i in 1:n_perm) { shuffled &lt;- sample(combined) perm_diffs[i] &lt;- mean(shuffled[21:40]) - mean(shuffled[1:20]) } # Visualize hist(perm_diffs, breaks = 50, col = &quot;gray80&quot;, border = &quot;white&quot;, main = &quot;Permutation Distribution&quot;, xlab = &quot;Mean Difference (Group2 - Group1)&quot;) abline(v = observed_diff, col = &quot;firebrick&quot;, lwd = 2) text(observed_diff + 0.3, 50, paste(&quot;Observed =&quot;, round(observed_diff, 2)), col = &quot;firebrick&quot;, adj = 0) # P-value p_value &lt;- (sum(abs(perm_diffs) &gt;= abs(observed_diff)) + 1) / (n_perm + 1) text(0, 80, paste(&quot;p =&quot;, round(p_value, 3)), cex = 1.2) Figure 102.1: Permutation test logic: The observed test statistic (red line) is compared to the distribution of statistics from permuted data. If the observed value is extreme, p is small. 102.2 Setup library(tidyverse) library(vegan) library(pairwiseAdonis) # For pairwise PERMANOVA set.seed(42) "],["part-1-permanova.html", "Chapter 103 Part 1: PERMANOVA 103.1 What is PERMANOVA? 103.2 How PERMANOVA works 103.3 PERMANOVA in practice 103.4 Using formula interface 103.5 Testing interactions 103.6 PERMANOVA assumptions", " Chapter 103 Part 1: PERMANOVA 103.1 What is PERMANOVA? PERMANOVA (Permutational Multivariate Analysis of Variance) tests whether the centroids of groups differ in multivariate space. It’s the multivariate analog of ANOVA, but: - Uses any dissimilarity measure - No distributional assumptions - Works on distance matrices, not raw data The question: Do groups differ in their community composition? 103.2 How PERMANOVA works PERMANOVA partitions the total sum of squares (based on distances) into: - Among-group variation: Distances between group centroids - Within-group variation: Distances of observations to their group centroid The F-ratio compares these, just like in ANOVA. The p-value comes from permutation. 103.3 PERMANOVA in practice Using the dune meadow data: # Load data data(dune) data(dune.env) # Calculate Bray-Curtis dissimilarity dune_dist &lt;- vegdist(dune, method = &quot;bray&quot;) # PERMANOVA: Does management affect community composition? permanova_result &lt;- adonis2(dune_dist ~ Management, data = dune.env, permutations = 999) permanova_result ## Permutation test for adonis under reduced model ## Permutation: free ## Number of permutations: 999 ## ## adonis2(formula = dune_dist ~ Management, data = dune.env, permutations = 999) ## Df SumOfSqs R2 F Pr(&gt;F) ## Model 3 1.4686 0.34161 2.7672 0.003 ** ## Residual 16 2.8304 0.65839 ## Total 19 4.2990 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 103.3.1 Interpret the output Column Meaning Df Degrees of freedom SumOfSqs Sum of squares (based on distances) R² Proportion of variance explained F Pseudo-F statistic Pr(&gt;F) P-value from permutation Interpretation: Management explains 34% of the variation in community composition (R² = 0.34), and this is statistically significant (p = 0.001). 103.4 Using formula interface PERMANOVA works with formulas, so you can include multiple predictors: # Multiple predictors permanova_multi &lt;- adonis2(dune ~ Management + A1 + Moisture, data = dune.env, permutations = 999, method = &quot;bray&quot;) permanova_multi ## Permutation test for adonis under reduced model ## Permutation: free ## Number of permutations: 999 ## ## adonis2(formula = dune ~ Management + A1 + Moisture, data = dune.env, permutations = 999, method = &quot;bray&quot;) ## Df SumOfSqs R2 F Pr(&gt;F) ## Model 7 2.8136 0.65449 3.2473 0.001 *** ## Residual 12 1.4854 0.34551 ## Total 19 4.2990 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Order matters: By default, adonis2 uses Type I (sequential) sums of squares. The first variable is fitted first, then the second accounts for remaining variation, etc. For Type II SS (each variable adjusted for others), add by = \"margin\": # Type II (marginal) sums of squares adonis2(dune ~ Management + A1 + Moisture, data = dune.env, permutations = 999, method = &quot;bray&quot;, by = &quot;margin&quot;) ## Permutation test for adonis under reduced model ## Marginal effects of terms ## Permutation: free ## Number of permutations: 999 ## ## adonis2(formula = dune ~ Management + A1 + Moisture, data = dune.env, permutations = 999, method = &quot;bray&quot;, by = &quot;margin&quot;) ## Df SumOfSqs R2 F Pr(&gt;F) ## Management 3 0.9036 0.21019 2.4334 0.007 ** ## A1 1 0.1934 0.04500 1.5627 0.160 ## Moisture 3 0.9042 0.21032 2.4349 0.012 * ## Residual 12 1.4854 0.34551 ## Total 19 4.2990 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 103.5 Testing interactions # Interaction term adonis2(dune ~ Management * Use, data = dune.env, permutations = 999, method = &quot;bray&quot;) ## Permutation test for adonis under reduced model ## Permutation: free ## Number of permutations: 999 ## ## adonis2(formula = dune ~ Management * Use, data = dune.env, permutations = 999, method = &quot;bray&quot;) ## Df SumOfSqs R2 F Pr(&gt;F) ## Model 10 2.6370 0.61339 1.4279 0.081 . ## Residual 9 1.6621 0.38661 ## Total 19 4.2990 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 103.6 PERMANOVA assumptions PERMANOVA has fewer assumptions than parametric tests, but still assumes: Independent observations: Samples are independent Similar dispersion: Groups have similar within-group variability 103.6.1 Checking dispersion (homogeneity of variances) If groups have different dispersions (spread), a significant PERMANOVA might reflect dispersion differences rather than location differences. # Test for homogeneity of dispersions disp &lt;- betadisper(dune_dist, dune.env$Management) disp ## ## Homogeneity of multivariate dispersions ## ## Call: betadisper(d = dune_dist, group = dune.env$Management) ## ## No. of Positive Eigenvalues: 14 ## No. of Negative Eigenvalues: 5 ## ## Average distance to median: ## BF HF NM SF ## 0.2196 0.2791 0.4510 0.3640 ## ## Eigenvalues for PCoA axes: ## (Showing 8 of 19 eigenvalues) ## PCoA1 PCoA2 PCoA3 PCoA4 PCoA5 PCoA6 PCoA7 PCoA8 ## 1.71627 1.02240 0.46146 0.38225 0.27913 0.23663 0.16912 0.09625 # Permutation test for homogeneity of dispersion disp_perm &lt;- vegan::permutest(disp, permutations = 999) disp_perm ## ## Permutation test for homogeneity of multivariate dispersions ## Permutation: free ## Number of permutations: 999 ## ## Response: Distances ## Df Sum Sq Mean Sq F N.Perm Pr(&gt;F) ## Groups 3 0.13831 0.046104 1.9506 999 0.185 ## Residuals 16 0.37816 0.023635 Interpretation: - Non-significant p (&gt; 0.05): Dispersions are similar; PERMANOVA results are reliable - Significant p: Groups differ in dispersion; interpret PERMANOVA cautiously # Visualize dispersions plot(disp, main = &quot;Dispersion by Management Type&quot;) Figure 103.1: Testing homogeneity of dispersions. If the dispersions (distances to centroid) differ among groups, PERMANOVA may be detecting dispersion differences rather than location differences. "],["part-2-pairwise-comparisons.html", "Chapter 104 Part 2: Pairwise Comparisons", " Chapter 104 Part 2: Pairwise Comparisons PERMANOVA tells you whether groups differ, but not which groups differ from which. For pairwise comparisons, use the pairwiseAdonis package: # Pairwise PERMANOVA pairwise_result &lt;- pairwise.adonis(dune, dune.env$Management, sim.method = &quot;bray&quot;, p.adjust.m = &quot;bonferroni&quot;) pairwise_result ## pairs Df SumsOfSqs F.Model R2 p.value p.adjusted sig ## 1 SF vs BF 1 0.4016624 2.514890 0.2643110 0.058 0.348 ## 2 SF vs HF 1 0.2828804 1.857489 0.1710790 0.133 0.798 ## 3 SF vs NM 1 0.7575728 3.425694 0.2551595 0.010 0.060 ## 4 BF vs HF 1 0.1617135 1.567531 0.2071390 0.210 1.000 ## 5 BF vs NM 1 0.5662456 2.715242 0.2794827 0.028 0.168 ## 6 HF vs NM 1 0.6513088 3.423068 0.2755413 0.018 0.108 Multiple comparison correction: With many pairwise comparisons, use p-value adjustment: - \"bonferroni\": Most conservative - \"holm\": Less conservative, still controls Type I error - \"fdr\" or \"BH\": Controls false discovery rate "],["part-3-anosim.html", "Chapter 105 Part 3: ANOSIM 105.1 What is ANOSIM? 105.2 ANOSIM vs PERMANOVA", " Chapter 105 Part 3: ANOSIM 105.1 What is ANOSIM? ANOSIM (Analysis of Similarities) is an older alternative to PERMANOVA. It compares: - Average rank dissimilarity between groups - Average rank dissimilarity within groups # ANOSIM anosim_result &lt;- anosim(dune_dist, dune.env$Management, permutations = 999) anosim_result ## ## Call: ## anosim(x = dune_dist, grouping = dune.env$Management, permutations = 999) ## Dissimilarity: bray ## ## ANOSIM statistic R: 0.2579 ## Significance: 0.014 ## ## Permutation: free ## Number of permutations: 999 105.1.1 Interpret ANOSIM output R statistic: Ranges from -1 to 1 R ≈ 1: Groups are completely different R ≈ 0: No difference between groups R &lt; 0: More variation within groups than between (rare) Significance: P-value from permutation 105.2 ANOSIM vs PERMANOVA Feature PERMANOVA ANOSIM Uses F-ratio (variance ratio) R statistic (rank-based) Assumptions More robust More restrictive Multiple factors Yes No Power Generally higher Generally lower Recommendation Preferred Use if specifically needed PERMANOVA is generally preferred because it’s more powerful and flexible. "],["part-4-mantel-test.html", "Chapter 106 Part 4: Mantel Test 106.1 What is the Mantel test? 106.2 Mantel test in practice 106.3 Partial Mantel test", " Chapter 106 Part 4: Mantel Test 106.1 What is the Mantel test? The Mantel test correlates two distance matrices. It asks: “Is the pattern in distance matrix A related to the pattern in distance matrix B?” Examples: - Are communities more similar at sites that are geographically closer? - Is community dissimilarity correlated with environmental dissimilarity? - Are genetic distances related to ecological distances? 106.2 Mantel test in practice # Create environmental distance matrix env_dist &lt;- vegdist(scale(dune.env$A1), method = &quot;euclidean&quot;) # Mantel test: community vs environment mantel_result &lt;- mantel(dune_dist, env_dist, method = &quot;pearson&quot;, permutations = 999) mantel_result ## ## Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## mantel(xdis = dune_dist, ydis = env_dist, method = &quot;pearson&quot;, permutations = 999) ## ## Mantel statistic r: 0.2379 ## Significance: 0.043 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.188 0.228 0.276 0.313 ## Permutation: free ## Number of permutations: 999 106.2.1 Interpret Mantel output r: Correlation coefficient between matrices Significance: P-value from permutation Interpretation: If r is positive and significant, sites that are more similar in one matrix tend to be more similar in the other. 106.3 Partial Mantel test Control for a third matrix when testing the correlation between two others: # Simulate geographic coordinates set.seed(123) coords &lt;- data.frame( x = runif(nrow(dune.env), 0, 100), y = runif(nrow(dune.env), 0, 100) ) # Geographic distance matrix geo_dist &lt;- vegdist(coords, method = &quot;euclidean&quot;) # Partial Mantel: community vs environment, controlling for geography mantel.partial(dune_dist, env_dist, geo_dist, permutations = 999) ## ## Partial Mantel statistic based on Pearson&#39;s product-moment correlation ## ## Call: ## mantel.partial(xdis = dune_dist, ydis = env_dist, zdis = geo_dist, permutations = 999) ## ## Mantel statistic r: 0.235 ## Significance: 0.041 ## ## Upper quantiles of permutations (null model): ## 90% 95% 97.5% 99% ## 0.166 0.228 0.262 0.305 ## Permutation: free ## Number of permutations: 999 "],["part-5-fitting-environmental-variables-to-ordination.html", "Chapter 107 Part 5: Fitting Environmental Variables to Ordination 107.1 The envfit function 107.2 Surface fitting", " Chapter 107 Part 5: Fitting Environmental Variables to Ordination 107.1 The envfit function envfit() fits environmental variables to an ordination, showing: - How each variable correlates with ordination axes - Whether the correlation is significant # Run NMDS first nmds &lt;- metaMDS(dune, distance = &quot;bray&quot;, k = 2, trymax = 50) ## Run 0 stress 0.1192678 ## Run 1 stress 0.1192678 ## ... Procrustes: rmse 8.597336e-05 max resid 0.0002623632 ## ... Similar to previous best ## Run 2 stress 0.1886532 ## Run 3 stress 0.1192678 ## ... New best solution ## ... Procrustes: rmse 1.155105e-05 max resid 3.484767e-05 ## ... Similar to previous best ## Run 4 stress 0.1183186 ## ... New best solution ## ... Procrustes: rmse 0.02027421 max resid 0.06499238 ## Run 5 stress 0.1183186 ## ... New best solution ## ... Procrustes: rmse 6.889954e-06 max resid 2.153806e-05 ## ... Similar to previous best ## Run 6 stress 0.1812932 ## Run 7 stress 0.1192678 ## Run 8 stress 0.1192678 ## Run 9 stress 0.1192678 ## Run 10 stress 0.1183186 ## ... Procrustes: rmse 1.69359e-05 max resid 5.489915e-05 ## ... Similar to previous best ## Run 11 stress 0.1183186 ## ... Procrustes: rmse 5.811259e-06 max resid 1.788758e-05 ## ... Similar to previous best ## Run 12 stress 0.1183186 ## ... Procrustes: rmse 3.227813e-06 max resid 9.78229e-06 ## ... Similar to previous best ## Run 13 stress 0.1812933 ## Run 14 stress 0.1809577 ## Run 15 stress 0.1192678 ## Run 16 stress 0.1183186 ## ... Procrustes: rmse 1.739481e-05 max resid 5.273209e-05 ## ... Similar to previous best ## Run 17 stress 0.1192678 ## Run 18 stress 0.1183186 ## ... Procrustes: rmse 3.454586e-06 max resid 1.073824e-05 ## ... Similar to previous best ## Run 19 stress 0.1183186 ## ... Procrustes: rmse 3.007656e-06 max resid 9.198479e-06 ## ... Similar to previous best ## Run 20 stress 0.1192678 ## *** Best solution repeated 7 times # Fit environmental variables env_fit &lt;- envfit(nmds, dune.env[, c(&quot;A1&quot;, &quot;Moisture&quot;, &quot;Manure&quot;)], permutations = 999) env_fit ## ## ***VECTORS ## ## NMDS1 NMDS2 r2 Pr(&gt;r) ## A1 0.96474 0.26322 0.3649 0.017 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 ## ## ***FACTORS: ## ## Centroids: ## NMDS1 NMDS2 ## Moisture1 -0.5101 -0.0403 ## Moisture2 -0.3938 0.0139 ## Moisture4 0.2765 -0.4033 ## Moisture5 0.6561 0.1476 ## Manure0 0.2958 0.5790 ## Manure1 -0.2482 -0.0215 ## Manure2 -0.3079 -0.1866 ## Manure3 0.3101 -0.2470 ## Manure4 -0.3463 -0.5583 ## ## Goodness of fit: ## r2 Pr(&gt;r) ## Moisture 0.5014 0.001 *** ## Manure 0.4247 0.020 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 107.1.1 Interpret envfit For continuous variables: - Vectors: Point in direction of increasing values - r²: Proportion of variance in variable explained by ordination - Pr(&gt;r): Significance from permutation For categorical variables: - Centroids: Position of group centers in ordination space - r²: How well groups are separated - Pr(&gt;r): Significance of separation # Fit categorical and continuous variables env_all &lt;- envfit(nmds, dune.env, permutations = 999) # Plot plot(nmds, display = &quot;sites&quot;, type = &quot;n&quot;, main = &quot;NMDS with Environmental Fit&quot;) points(nmds, display = &quot;sites&quot;, pch = 19, col = &quot;steelblue&quot;) # Add vectors (continuous variables) plot(env_all, p.max = 0.05, col = &quot;firebrick&quot;) # Only significant variables Figure 107.1: Environmental vectors fitted to NMDS ordination. Vector length indicates correlation strength; direction shows the gradient. 107.2 Surface fitting For continuous variables, you can fit smooth surfaces: # Fit GAM surface surf &lt;- ordisurf(nmds, dune.env$A1, add = FALSE, col = &quot;firebrick&quot;) Figure 107.2: Smooth surface fitted for soil A1 horizon thickness. Contours show isolines of equal values. "],["part-6-complete-workflow-example.html", "Chapter 108 Part 6: Complete Workflow Example", " Chapter 108 Part 6: Complete Workflow Example Let’s work through a complete analysis: # 1. Load and examine data data(dune) data(dune.env) # 2. Calculate dissimilarity dune_dist &lt;- vegdist(dune, method = &quot;bray&quot;) # 3. Run NMDS for visualization nmds &lt;- metaMDS(dune, distance = &quot;bray&quot;, k = 2, trymax = 100) ## Run 0 stress 0.1192678 ## Run 1 stress 0.1183186 ## ... New best solution ## ... Procrustes: rmse 0.02027034 max resid 0.06496196 ## Run 2 stress 0.1183186 ## ... Procrustes: rmse 1.251683e-05 max resid 3.840684e-05 ## ... Similar to previous best ## Run 3 stress 0.1192678 ## Run 4 stress 0.1192678 ## Run 5 stress 0.1183186 ## ... Procrustes: rmse 4.537952e-06 max resid 1.506459e-05 ## ... Similar to previous best ## Run 6 stress 0.1183186 ## ... Procrustes: rmse 9.619305e-06 max resid 3.012838e-05 ## ... Similar to previous best ## Run 7 stress 0.1192679 ## Run 8 stress 0.1183186 ## ... Procrustes: rmse 3.052231e-06 max resid 1.020793e-05 ## ... Similar to previous best ## Run 9 stress 0.1183186 ## ... Procrustes: rmse 3.156232e-06 max resid 9.258065e-06 ## ... Similar to previous best ## Run 10 stress 0.1192678 ## Run 11 stress 0.1192678 ## Run 12 stress 0.1183186 ## ... Procrustes: rmse 1.921067e-05 max resid 6.292501e-05 ## ... Similar to previous best ## Run 13 stress 0.1183186 ## ... Procrustes: rmse 7.811232e-06 max resid 2.445022e-05 ## ... Similar to previous best ## Run 14 stress 0.1886532 ## Run 15 stress 0.1183186 ## ... Procrustes: rmse 2.946141e-05 max resid 9.269268e-05 ## ... Similar to previous best ## Run 16 stress 0.1192679 ## Run 17 stress 0.1808911 ## Run 18 stress 0.1808911 ## Run 19 stress 0.1192678 ## Run 20 stress 0.1192678 ## *** Best solution repeated 8 times cat(&quot;Stress:&quot;, nmds$stress, &quot;\\n&quot;) ## Stress: 0.1183186 # 4. Test group differences with PERMANOVA perm &lt;- adonis2(dune_dist ~ Management, data = dune.env, permutations = 999) print(perm) ## Permutation test for adonis under reduced model ## Permutation: free ## Number of permutations: 999 ## ## adonis2(formula = dune_dist ~ Management, data = dune.env, permutations = 999) ## Df SumOfSqs R2 F Pr(&gt;F) ## Model 3 1.4686 0.34161 2.7672 0.004 ** ## Residual 16 2.8304 0.65839 ## Total 19 4.2990 1.00000 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # 5. Check dispersion assumption disp &lt;- betadisper(dune_dist, dune.env$Management) print(vegan::permutest(disp)) ## ## Permutation test for homogeneity of multivariate dispersions ## Permutation: free ## Number of permutations: 999 ## ## Response: Distances ## Df Sum Sq Mean Sq F N.Perm Pr(&gt;F) ## Groups 3 0.13831 0.046104 1.9506 999 0.149 ## Residuals 16 0.37816 0.023635 # 6. Pairwise comparisons (if significant) pairwise &lt;- pairwise.adonis(dune, dune.env$Management, sim.method = &quot;bray&quot;) print(pairwise) ## pairs Df SumsOfSqs F.Model R2 p.value p.adjusted sig ## 1 SF vs BF 1 0.4016624 2.514890 0.2643110 0.065 0.390 ## 2 SF vs HF 1 0.2828804 1.857489 0.1710790 0.114 0.684 ## 3 SF vs NM 1 0.7575728 3.425694 0.2551595 0.007 0.042 . ## 4 BF vs HF 1 0.1617135 1.567531 0.2071390 0.216 1.000 ## 5 BF vs NM 1 0.5662456 2.715242 0.2794827 0.021 0.126 ## 6 HF vs NM 1 0.6513088 3.423068 0.2755413 0.021 0.126 # 7. Fit environmental variables env_fit &lt;- envfit(nmds, dune.env[, c(&quot;A1&quot;, &quot;Moisture&quot;)], permutations = 999) print(env_fit) ## ## ***VECTORS ## ## NMDS1 NMDS2 r2 Pr(&gt;r) ## A1 0.96474 0.26322 0.3649 0.017 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 ## ## ***FACTORS: ## ## Centroids: ## NMDS1 NMDS2 ## Moisture1 -0.5101 -0.0403 ## Moisture2 -0.3938 0.0139 ## Moisture4 0.2765 -0.4033 ## Moisture5 0.6561 0.1476 ## ## Goodness of fit: ## r2 Pr(&gt;r) ## Moisture 0.5014 0.001 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## Permutation: free ## Number of permutations: 999 # 8. Create publication figure site_scores &lt;- as.data.frame(scores(nmds, display = &quot;sites&quot;)) site_scores$Management &lt;- dune.env$Management ggplot(site_scores, aes(x = NMDS1, y = NMDS2, color = Management)) + geom_point(size = 3, alpha = 0.8) + stat_ellipse(level = 0.95, linetype = 2) + scale_color_brewer(palette = &quot;Set2&quot;) + labs(title = &quot;Dune Meadow Vegetation by Management&quot;, subtitle = paste(&quot;PERMANOVA: R² = 0.34, p = 0.001 | Stress =&quot;, round(nmds$stress, 3))) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) + coord_equal() Figure 108.1: Complete community analysis: NMDS ordination showing vegetation composition by management type with environmental vectors overlaid. Groups were significantly different (PERMANOVA: p = 0.001). "],["part-7-reporting-permutation-results.html", "Chapter 109 Part 7: Reporting Permutation Results 109.1 What to report 109.2 Sample methods and results 109.3 Key takeaways 109.4 Assignment", " Chapter 109 Part 7: Reporting Permutation Results 109.1 What to report Method: PERMANOVA, ANOSIM, Mantel Dissimilarity measure: Bray-Curtis, Jaccard, etc. Number of permutations Test statistic: Pseudo-F, R, r R² (for PERMANOVA): Effect size P-value Dispersion test results (for PERMANOVA) Pairwise comparisons (if applicable) 109.2 Sample methods and results 109.2.1 Methods We tested whether plant community composition differed among management types using PERMANOVA with Bray-Curtis dissimilarity and 999 permutations. Prior to analysis, we verified the assumption of homogeneous dispersions using PERMDISP (betadisper function). When PERMANOVA was significant, we conducted pairwise comparisons with Bonferroni correction. We visualized community composition using NMDS and overlaid environmental variables using envfit() to aid interpretation. Analyses were conducted using the vegan package (Oksanen et al. 2022) in R version 4.3.1. 109.2.2 Results Plant community composition differed significantly among management types (PERMANOVA: F₃,₁₆ = 2.74, R² = 0.34, p = 0.001; Fig. X). The assumption of homogeneous dispersions was met (PERMDISP: F₃,₁₆ = 1.21, p = 0.32). Pairwise comparisons revealed that biological management sites differed from hobby farming sites (p = 0.006) and nature conservation sites (p = 0.018), while standard farming did not differ significantly from other management types (all p &gt; 0.10). NMDS ordination (stress = 0.118) showed separation of management types along both axes, with soil moisture significantly correlated with ordination structure (r² = 0.45, p = 0.003). 109.3 Key takeaways Permutation tests make no distributional assumptions — They use your data to generate the null distribution PERMANOVA tests group differences — It’s ANOVA for multivariate community data Check dispersion first — Significant PERMANOVA with unequal dispersions is ambiguous Use pairwise comparisons to identify which groups differ ANOSIM is older and less flexible — PERMANOVA is generally preferred Mantel test correlates distance matrices — Useful for relating community to environment or geography envfit interprets ordination — Shows how environmental variables relate to community patterns 109.4 Assignment 109.4.1 Part 1: Conceptual questions What does the R² value in PERMANOVA represent? How is it different from a p-value? Your PERMANOVA is significant, but so is your test of dispersion homogeneity. How do you interpret this? What’s the difference between PERMANOVA and Mantel test? When would you use each? 109.4.2 Part 2: PERMANOVA analysis Using the built-in BCI data and simulated environmental groups: data(BCI) # Create grouping variable (simulated forest age classes) set.seed(123) forest_age &lt;- factor(sample(c(&quot;Young&quot;, &quot;Mature&quot;, &quot;Old&quot;), 50, replace = TRUE, prob = c(0.3, 0.4, 0.3))) Calculate Bray-Curtis dissimilarity Run PERMANOVA to test whether composition differs by forest age Test the dispersion assumption If significant, run pairwise comparisons Write a results paragraph 109.4.3 Part 3: Mantel test Create an environmental distance matrix and test its correlation with community dissimilarity: # Simulated environmental data (2 variables) env_data &lt;- data.frame( temp = rnorm(50, mean = 25, sd = 3), precip = rnorm(50, mean = 2000, sd = 500) ) Create Euclidean distance matrix from environmental data Run Mantel test against BCI community dissimilarity Interpret the result 109.4.4 Part 4: Complete workflow Using the dune data (or your own data): Run NMDS Run PERMANOVA with at least one factor Check dispersion assumption Fit environmental variables Create a publication-quality figure Write complete methods and results 109.4.5 Part 5: Reflection In 2-3 sentences, explain why permutation tests are particularly well-suited to ecological community data compared to traditional parametric tests. "],["diversity-statistics.html", "Chapter 110 Diversity Statistics 110.1 The challenge of measuring diversity 110.2 Setup", " Chapter 110 Diversity Statistics Sara add in information from Diversity Statistics old How do we describe the variety of life in a community? Simple species counts—species richness—have been used for centuries. But richness alone misses important information: Are species equally abundant, or does one species dominate? Did we sample enough to detect all species present? Modern diversity statistics address these limitations by: - Incorporating evenness (relative abundance) alongside richness - Using rarefaction and extrapolation to account for sampling effort - Expressing diversity as Hill numbers for meaningful comparisons This chapter covers the theory and practice of measuring diversity, with emphasis on the iNEXT framework that unifies these approaches. 110.1 The challenge of measuring diversity Diversity seems simple: count the species. But consider two communities: Community Species A Species B Species C Species D Richness Community 1 97 1 1 1 4 Community 2 25 25 25 25 4 Both have richness = 4, but they feel different. Community 1 is dominated by one species; Community 2 has equal representation. We need metrics that capture this difference. Additionally, sampling effort matters. The more you look, the more species you find. A community sampled with 100 traps will appear more diverse than one sampled with 10 traps—even if true diversity is identical. 110.2 Setup library(tidyverse) library(iNEXT) library(vegan) set.seed(42) "],["part-1-types-of-diversity-data.html", "Chapter 111 Part 1: Types of Diversity Data 111.1 Abundance data 111.2 Incidence (presence/absence) data 111.3 Incidence frequency data 111.4 Choosing between data types", " Chapter 111 Part 1: Types of Diversity Data Before calculating diversity, you must understand what type of data you have. This determines which methods are appropriate. 111.1 Abundance data Abundance data includes counts or measures of how many individuals of each species are present. Examples: - Number of individuals per species from pitfall traps - Counts of birds at point counts - Percent cover of plant species in quadrats Format: Species × site matrix with abundance values # Example abundance data: spider counts from two treatments data(&quot;spider&quot;) spider ## $Girdled ## [1] 46 22 17 15 15 9 8 6 6 4 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 ## ## $Logged ## [1] 88 22 16 15 13 10 8 8 7 7 7 5 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 In abundance data, the sampling unit is an individual. Rarefaction works by randomly removing individuals. 111.2 Incidence (presence/absence) data Incidence data records only whether a species was detected (1) or not (0) in each sampling unit—no abundance information. Examples: - Species presence/absence in quadrats - Detection/non-detection across survey visits - Species lists from multiple sites Why use incidence instead of abundance? When individuals within a sampling unit are not independent (e.g., plants in a quadrat share soil conditions), treating abundance data as independent counts violates statistical assumptions. Converting to incidence data (present/absent per quadrat) addresses this. Format: Species × sampling unit matrix with 0/1 values # Example incidence data: ciliates in coastal dunes data(&quot;ciliates&quot;) str(ciliates) ## List of 3 ## $ EtoshaPan : int [1:365, 1:19] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:365] &quot;Acaryophrya.collaris&quot; &quot;Actinobolina.multinucleata.n..sp.&quot; &quot;Afroamphisiella.multinucleata.n..sp.&quot; &quot;Afrothrix.multinucleata.n..sp.&quot; ... ## .. ..$ : chr [1:19] &quot;x53&quot; &quot;x54&quot; &quot;x55&quot; &quot;x56&quot; ... ## $ CentralNamibDesert : int [1:365, 1:17] 0 0 0 0 0 1 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:365] &quot;Acaryophrya.collaris&quot; &quot;Actinobolina.multinucleata.n..sp.&quot; &quot;Afroamphisiella.multinucleata.n..sp.&quot; &quot;Afrothrix.multinucleata.n..sp.&quot; ... ## .. ..$ : chr [1:17] &quot;x31&quot; &quot;x32&quot; &quot;x34&quot; &quot;x35&quot; ... ## $ SouthernNamibDesert: int [1:365, 1:15] 0 0 0 0 0 0 0 0 0 0 ... ## ..- attr(*, &quot;dimnames&quot;)=List of 2 ## .. ..$ : chr [1:365] &quot;Acaryophrya.collaris&quot; &quot;Actinobolina.multinucleata.n..sp.&quot; &quot;Afroamphisiella.multinucleata.n..sp.&quot; &quot;Afrothrix.multinucleata.n..sp.&quot; ... ## .. ..$ : chr [1:15] &quot;x9&quot; &quot;x17&quot; &quot;x19&quot; &quot;x20&quot; ... # Each element is a species × plot matrix for one habitat 111.3 Incidence frequency data A compact form of incidence data that summarizes how many sampling units contained each species. Format: First value = total number of sampling units; subsequent values = frequency of each species # Example: ant data from different elevations data(&quot;ant&quot;) str(ant) ## List of 5 ## $ h50m : num [1:228] 599 330 263 236 222 195 186 183 182 129 ... ## $ h500m : num [1:242] 230 133 131 123 78 73 65 60 60 56 ... ## $ h1070m: num [1:123] 150 99 96 80 74 68 60 54 46 45 ... ## $ h1500m: num [1:57] 200 144 113 79 76 74 73 53 50 43 ... ## $ h2000m: num [1:15] 200 80 59 34 23 19 15 13 8 8 ... # First number is total traps; rest are species frequencies ant$h50m[1:10] # 599 traps, then species frequencies ## [1] 599 330 263 236 222 195 186 183 182 129 111.4 Choosing between data types Table 111.1: Choosing the appropriate data type for diversity analysis Your data Data type iNEXT datatype Counts of individuals, independently sampled Abundance “abundance” Counts within plots/quadrats (individuals not independent) Convert to incidence “incidence_raw” Presence/absence across sampling units Incidence (raw) “incidence_raw” Species lists from multiple surveys Incidence (frequency) “incidence_freq” Key principle: The sampling unit for rarefaction should be the level at which observations are independent. For quadrat-based plant surveys, this is usually the quadrat (incidence), not the individual plant (abundance). "],["part-2-diversity-metrics-and-hill-numbers.html", "Chapter 112 Part 2: Diversity Metrics and Hill Numbers 112.1 Classic diversity indices 112.2 The problem with indices 112.3 Hill numbers: The solution 112.4 Interpreting Hill numbers", " Chapter 112 Part 2: Diversity Metrics and Hill Numbers 112.1 Classic diversity indices 112.1.1 Species richness (S) The simplest metric: count of species present. Pros: Intuitive, widely understood Cons: Ignores abundance, highly sensitive to sampling effort 112.1.2 Shannon diversity index (H’) Incorporates both richness and evenness: \\[H&#39; = -\\sum_{i=1}^{S} p_i \\ln(p_i)\\] where \\(p_i\\) is the proportion of individuals belonging to species \\(i\\). Pros: Weights species by abundance Cons: Not intuitive (what does H’ = 2.3 mean?), doesn’t follow “doubling rule” 112.1.3 Simpson diversity index (D) Probability that two randomly selected individuals belong to different species: \\[D = 1 - \\sum_{i=1}^{S} p_i^2\\] Pros: Less sensitive to rare species Cons: Also not intuitive, bounded between 0 and 1 112.2 The problem with indices These indices don’t behave intuitively. If you double the number of equally-common species, Shannon’s H’ doesn’t double—it increases by ln(2) ≈ 0.69. This makes comparisons difficult. Example: # Two communities comm_a &lt;- c(50, 50) # 2 species, equal abundance comm_b &lt;- c(25, 25, 25, 25) # 4 species, equal abundance # Shannon indices H_a &lt;- -sum((comm_a/sum(comm_a)) * log(comm_a/sum(comm_a))) H_b &lt;- -sum((comm_b/sum(comm_b)) * log(comm_b/sum(comm_b))) cat(&quot;Community A (2 species): H&#39; =&quot;, round(H_a, 3), &quot;\\n&quot;) ## Community A (2 species): H&#39; = 0.693 cat(&quot;Community B (4 species): H&#39; =&quot;, round(H_b, 3), &quot;\\n&quot;) ## Community B (4 species): H&#39; = 1.386 cat(&quot;Ratio:&quot;, round(H_b/H_a, 3), &quot;(should be 2 if doubling rule held)\\n&quot;) ## Ratio: 2 (should be 2 if doubling rule held) 112.3 Hill numbers: The solution Hill numbers (also called “effective number of species”) convert diversity indices to a common, interpretable scale: \\[^qD = \\left(\\sum_{i=1}^{S} p_i^q\\right)^{1/(1-q)}\\] The parameter q determines sensitivity to rare species: q Name Interpretation Sensitive to 0 Richness Number of species All species equally 1 Shannon diversity Effective number of common species Species weighted by frequency 2 Simpson diversity Effective number of dominant species Mainly common species The key insight: Hill numbers follow the doubling rule. A community with 8 equally-common species has diversity = 8. Double the species (to 16), and diversity doubles to 16. # Hill numbers for our example communities # q = 0 (richness) q0_a &lt;- length(comm_a) q0_b &lt;- length(comm_b) # q = 1 (exp of Shannon) q1_a &lt;- exp(H_a) q1_b &lt;- exp(H_b) # q = 2 (inverse Simpson) p_a &lt;- comm_a/sum(comm_a) p_b &lt;- comm_b/sum(comm_b) q2_a &lt;- 1/sum(p_a^2) q2_b &lt;- 1/sum(p_b^2) cat(&quot;Community A - q0:&quot;, q0_a, &quot;, q1:&quot;, round(q1_a, 2), &quot;, q2:&quot;, round(q2_a, 2), &quot;\\n&quot;) ## Community A - q0: 2 , q1: 2 , q2: 2 cat(&quot;Community B - q0:&quot;, q0_b, &quot;, q1:&quot;, round(q1_b, 2), &quot;, q2:&quot;, round(q2_b, 2), &quot;\\n&quot;) ## Community B - q0: 4 , q1: 4 , q2: 4 cat(&quot;Ratio (B/A) - q0:&quot;, q0_b/q0_a, &quot;, q1:&quot;, round(q1_b/q1_a, 2), &quot;, q2:&quot;, round(q2_b/q2_a, 2), &quot;\\n&quot;) ## Ratio (B/A) - q0: 2 , q1: 2 , q2: 2 With Hill numbers, the ratio is exactly 2—doubling species doubles diversity! 112.4 Interpreting Hill numbers Think of Hill numbers as answering: “How many equally-common species would give the same diversity?” q = 0: Counts all species, regardless of abundance q = 1: Weights by abundance; rare species count less q = 2: Mainly reflects dominant species A community with \\(^1D = 10\\) has the same Shannon diversity as a community of 10 equally-common species. "],["part-3-the-sampling-problem.html", "Chapter 113 Part 3: The Sampling Problem 113.1 You never find all the species 113.2 Rarefaction: Standardizing by sample size 113.3 Extrapolation: Predicting unseen diversity 113.4 Sample coverage: A better standardization", " Chapter 113 Part 3: The Sampling Problem 113.1 You never find all the species The more you sample, the more species you detect. This creates two problems: Observed richness underestimates true richness Communities sampled with different effort can’t be compared fairly # Simulate species accumulation set.seed(123) true_richness &lt;- 50 individuals &lt;- 500 # Community with log-series abundance distribution abundances &lt;- ceiling(100 * exp(-0.1 * 1:true_richness)) abundances &lt;- abundances[abundances &gt; 0] species_pool &lt;- rep(1:length(abundances), abundances) # Sample and accumulate samples &lt;- sample(species_pool, individuals, replace = TRUE) accumulation &lt;- sapply(1:individuals, function(n) length(unique(samples[1:n]))) plot(1:individuals, accumulation, type = &quot;l&quot;, lwd = 2, col = &quot;steelblue&quot;, xlab = &quot;Number of individuals sampled&quot;, ylab = &quot;Species observed&quot;, main = &quot;Species Accumulation Curve&quot;) abline(h = length(abundances), lty = 2, col = &quot;firebrick&quot;) text(400, length(abundances) + 2, &quot;True richness&quot;, col = &quot;firebrick&quot;) Figure 113.1: Species accumulation curve: observed richness increases with sampling effort. The curve approaches but never reaches the true richness (dashed line). 113.2 Rarefaction: Standardizing by sample size Rarefaction asks: “If I had sampled fewer individuals, how many species would I expect to see?” By down-sampling larger samples to match smaller ones, we can compare communities fairly. # Two samples of different sizes sample_large &lt;- sample(species_pool, 400, replace = TRUE) sample_small &lt;- sample(species_pool, 100, replace = TRUE) # Observed richness obs_large &lt;- length(unique(sample_large)) obs_small &lt;- length(unique(sample_small)) # Rarefy large sample to size of small sample rarefied &lt;- rarefy(table(sample_large), sample = 100) cat(&quot;Large sample (n=400): observed =&quot;, obs_large, &quot;species\\n&quot;) ## Large sample (n=400): observed = 43 species cat(&quot;Small sample (n=100): observed =&quot;, obs_small, &quot;species\\n&quot;) ## Small sample (n=100): observed = 29 species cat(&quot;Large sample rarefied to n=100:&quot;, round(rarefied, 1), &quot;species\\n&quot;) ## Large sample rarefied to n=100: 29.1 species 113.3 Extrapolation: Predicting unseen diversity Extrapolation extends the accumulation curve beyond your sample to estimate true richness. This uses information about rare species—species seen only once or twice—to estimate how many species remain undetected. (This approach was developed by Alan Turing while breaking the Enigma code!) 113.4 Sample coverage: A better standardization Sample coverage measures what fraction of the community’s individuals belong to detected species: \\[\\hat{C} = 1 - \\frac{f_1}{n}\\] where \\(f_1\\) is the number of singleton species (seen once) and \\(n\\) is total individuals. Coverage tells you how complete your sample is. A sample with 95% coverage has detected species representing 95% of individuals in the community. Coverage-based standardization compares communities at equal coverage rather than equal sample size—often more meaningful ecologically. "],["part-4-the-inext-framework.html", "Chapter 114 Part 4: The iNEXT Framework 114.1 Basic workflow 114.2 Visualizing rarefaction/extrapolation curves 114.3 Sample size-based vs. coverage-based comparison 114.4 Comparing at standardized coverage 114.5 Working with incidence data", " Chapter 114 Part 4: The iNEXT Framework iNEXT (iNterpolation and EXTrapolation) provides a unified framework for diversity estimation that: Calculates Hill numbers (q = 0, 1, 2) Generates rarefaction/extrapolation curves Standardizes by sample size OR coverage Provides confidence intervals via bootstrapping 114.1 Basic workflow # Load spider abundance data data(&quot;spider&quot;) spider ## $Girdled ## [1] 46 22 17 15 15 9 8 6 6 4 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 ## ## $Logged ## [1] 88 22 16 15 13 10 8 8 7 7 7 5 4 4 4 3 3 3 3 2 2 2 2 1 1 1 1 1 1 1 1 1 1 1 1 ## [36] 1 1 # Run iNEXT for Hill numbers q = 0, 1, 2 spider_inext &lt;- iNEXT(spider, q = c(0, 1, 2), datatype = &quot;abundance&quot;) # View asymptotic estimates spider_inext$AsyEst ## Assemblage Diversity Observed Estimator s.e. LCL UCL ## 1 Girdled Species richness 26.000000 43.892857 18.0558068 26.000000 79.281588 ## 2 Girdled Shannon diversity 12.059650 13.826253 1.3948505 11.092397 16.560110 ## 3 Girdled Simpson diversity 7.840000 8.174825 0.8997525 6.411343 9.938308 ## 4 Logged Species richness 37.000000 61.402778 18.2620472 37.000000 97.195732 ## 5 Logged Shannon diversity 14.421002 16.337069 1.2978038 13.793420 18.880717 ## 6 Logged Simpson diversity 6.761499 6.920350 0.8048763 5.342822 8.497879 114.1.1 Understanding the output The $AsyEst table shows: Observed: Diversity in your sample Estimator: Asymptotic estimate (extrapolated true diversity) s.e.: Standard error of estimate LCL/UCL: 95% confidence interval 114.2 Visualizing rarefaction/extrapolation curves # Plot R/E curves ggiNEXT(spider_inext, type = 1, facet.var = &quot;Assemblage&quot;) + theme_minimal() + labs(title = &quot;Spider Diversity: Girdled vs. Logged Treatments&quot;) Figure 114.1: Rarefaction/extrapolation curves for spider communities under two treatments. Solid lines show rarefaction (interpolation); dashed lines show extrapolation. Shaded regions are 95% confidence intervals. Reading the plot: - Solid portion: Rarefaction (observed data) - Dashed portion: Extrapolation (predicted) - Shading: 95% confidence interval - Point: Your actual sample 114.3 Sample size-based vs. coverage-based comparison # Plot by sample coverage ggiNEXT(spider_inext, type = 3, facet.var = &quot;Order.q&quot;) + theme_minimal() + labs(title = &quot;Diversity by Sample Coverage&quot;) Figure 114.2: Coverage-based R/E curves standardize by sample completeness rather than sample size, often providing more ecologically meaningful comparisons. 114.4 Comparing at standardized coverage Use estimateD() to get diversity estimates at a specified coverage level: # Compare at 95% coverage estimateD(spider, datatype = &quot;abundance&quot;, base = &quot;coverage&quot;, level = 0.95, conf = 0.95) ## Assemblage m Method Order.q SC qD qD.LCL qD.UCL ## 1 Girdled 256.5253 Extrapolation 0 0.95 31.317857 14.358387 48.277327 ## 2 Girdled 256.5253 Extrapolation 1 0.95 12.655438 9.907295 15.403580 ## 3 Girdled 256.5253 Extrapolation 2 0.95 7.952402 6.447206 9.457598 ## 4 Logged 297.3323 Extrapolation 0 0.95 39.390278 19.234286 59.546270 ## 5 Logged 297.3323 Extrapolation 1 0.95 14.672108 11.650666 17.693550 ## 6 Logged 297.3323 Extrapolation 2 0.95 6.785245 5.221009 8.349482 114.5 Working with incidence data # Ant data: incidence frequencies from 5 elevations data(&quot;ant&quot;) str(ant) ## List of 5 ## $ h50m : num [1:228] 599 330 263 236 222 195 186 183 182 129 ... ## $ h500m : num [1:242] 230 133 131 123 78 73 65 60 60 56 ... ## $ h1070m: num [1:123] 150 99 96 80 74 68 60 54 46 45 ... ## $ h1500m: num [1:57] 200 144 113 79 76 74 73 53 50 43 ... ## $ h2000m: num [1:15] 200 80 59 34 23 19 15 13 8 8 ... # Run iNEXT for incidence data ant_inext &lt;- iNEXT(ant, q = c(0, 1, 2), datatype = &quot;incidence_freq&quot;) # Plot ggiNEXT(ant_inext, type = 1, facet.var = &quot;Order.q&quot;) + theme_minimal() + labs(title = &quot;Ant Diversity Across Elevations&quot;) "],["part-5-data-preparation-for-inext.html", "Chapter 115 Part 5: Data Preparation for iNEXT 115.1 From a site × species matrix (abundance) 115.2 From a site × species matrix (incidence) 115.3 From long-format data", " Chapter 115 Part 5: Data Preparation for iNEXT Getting your data into the right format is often the hardest part. Here are common scenarios: 115.1 From a site × species matrix (abundance) # Example: convert wide matrix to iNEXT list format # Suppose you have a site × species matrix site_species &lt;- matrix( c(10, 5, 3, 0, 2, 8, 12, 0, 4, 1, 15, 3, 7, 2, 0), nrow = 3, byrow = TRUE, dimnames = list( c(&quot;SiteA&quot;, &quot;SiteB&quot;, &quot;SiteC&quot;), c(&quot;Sp1&quot;, &quot;Sp2&quot;, &quot;Sp3&quot;, &quot;Sp4&quot;, &quot;Sp5&quot;) ) ) site_species ## Sp1 Sp2 Sp3 Sp4 Sp5 ## SiteA 10 5 3 0 2 ## SiteB 8 12 0 4 1 ## SiteC 15 3 7 2 0 # Convert to list (each element = one site&#39;s abundances) abundance_list &lt;- lapply(1:nrow(site_species), function(i) { x &lt;- site_species[i, ] x[x &gt; 0] # Remove zeros }) names(abundance_list) &lt;- rownames(site_species) abundance_list ## $SiteA ## Sp1 Sp2 Sp3 Sp5 ## 10 5 3 2 ## ## $SiteB ## Sp1 Sp2 Sp4 Sp5 ## 8 12 4 1 ## ## $SiteC ## Sp1 Sp2 Sp3 Sp4 ## 15 3 7 2 115.2 From a site × species matrix (incidence) # For incidence data: need species × sampling unit matrices # Example: 3 treatments, multiple plots per treatment # Simulate data set.seed(42) treatment_A &lt;- matrix(rbinom(50, 1, 0.3), nrow = 10, ncol = 5, dimnames = list(paste0(&quot;Sp&quot;, 1:10), paste0(&quot;Plot&quot;, 1:5))) treatment_B &lt;- matrix(rbinom(50, 1, 0.5), nrow = 10, ncol = 5, dimnames = list(paste0(&quot;Sp&quot;, 1:10), paste0(&quot;Plot&quot;, 1:5))) # Create list for iNEXT incidence_list &lt;- list( Treatment_A = treatment_A, Treatment_B = treatment_B ) # Run with incidence_raw # iNEXT(incidence_list, q = 0, datatype = &quot;incidence_raw&quot;) 115.3 From long-format data # Common format: one row per observation long_data &lt;- data.frame( Site = rep(c(&quot;Control&quot;, &quot;Treatment&quot;), each = 50), Species = c( sample(c(&quot;Sp1&quot;, &quot;Sp2&quot;, &quot;Sp3&quot;, &quot;Sp4&quot;), 50, replace = TRUE, prob = c(0.4, 0.3, 0.2, 0.1)), sample(c(&quot;Sp1&quot;, &quot;Sp2&quot;, &quot;Sp3&quot;, &quot;Sp4&quot;, &quot;Sp5&quot;), 50, replace = TRUE, prob = c(0.2, 0.2, 0.2, 0.2, 0.2)) ) ) head(long_data) ## Site Species ## 1 Control Sp2 ## 2 Control Sp1 ## 3 Control Sp1 ## 4 Control Sp1 ## 5 Control Sp4 ## 6 Control Sp4 # Convert to abundance list abundance_from_long &lt;- long_data %&gt;% group_by(Site, Species) %&gt;% summarise(Count = n(), .groups = &quot;drop&quot;) %&gt;% pivot_wider(names_from = Species, values_from = Count, values_fill = 0) # To list format sites &lt;- unique(long_data$Site) inext_list &lt;- lapply(sites, function(s) { counts &lt;- long_data %&gt;% filter(Site == s) %&gt;% count(Species) %&gt;% pull(n) counts }) names(inext_list) &lt;- sites inext_list ## $Control ## [1] 15 15 13 7 ## ## $Treatment ## [1] 5 15 6 9 15 "],["part-6-experimental-design-for-diversity-studies.html", "Chapter 116 Part 6: Experimental Design for Diversity Studies 116.1 Key design considerations 116.2 Example: Comparing diversity between treatments 116.3 Workflow for comparing treatments", " Chapter 116 Part 6: Experimental Design for Diversity Studies Your sampling design affects what diversity metrics you can calculate and how you should analyze them. 116.1 Key design considerations 116.1.1 1. What is your sampling unit? Sampling approach Sampling unit Data type Point counts (one per site) Individual Abundance Quadrats within sites Quadrat Incidence Repeated visits to same site Visit Incidence Pitfall traps (pooled per site) Individual Abundance 116.1.2 2. How many sampling units do you need? For reliable extrapolation, you need: - Enough samples to characterize the community (capture common species) - Some rare species (singletons/doubletons inform extrapolation) - Sample coverage &gt; 50% (ideally &gt; 80%) for reliable estimates 116.1.3 3. Replication for statistical comparison To test whether diversity differs between treatments/sites: - You need replicate sites (not just replicate quadrats within one site) - Rarefaction accounts for sampling effort, but not for site-level replication - For statistical tests, use the site as the unit of replication 116.2 Example: Comparing diversity between treatments # Scenario: 3 control plots, 3 treatment plots # 5 quadrats per plot, presence/absence data # This is a nested design: # - Treatment (fixed effect) # - Plot (random effect, nested in treatment) # - Quadrat (sampling unit for diversity estimation) # For diversity estimation: combine quadrats within each plot # For statistical comparison: plots are the replicates (n = 3 per treatment) # DON&#39;T: Compare rarefied diversity using quadrats as replicates (pseudoreplication) # DO: Calculate diversity per plot, then compare plots between treatments 116.3 Workflow for comparing treatments Calculate diversity per replicate site/plot using iNEXT Extract diversity estimates at standardized coverage Use standard statistical tests (t-test, ANOVA) on site-level diversity values # Example: diversity estimates from 6 sites (3 per treatment) diversity_data &lt;- data.frame( Treatment = rep(c(&quot;Control&quot;, &quot;Fertilized&quot;), each = 3), Site = paste0(&quot;Site&quot;, 1:6), q0 = c(12, 14, 11, 18, 22, 19), # Richness estimates q1 = c(8.2, 9.1, 7.8, 12.4, 14.1, 11.8) # Shannon diversity ) # Statistical comparison t.test(q0 ~ Treatment, data = diversity_data) ## ## Welch Two Sample t-test ## ## data: q0 by Treatment ## t = -4.9193, df = 3.6697, p-value = 0.00987 ## alternative hypothesis: true difference in means between group Control and group Fertilized is not equal to 0 ## 95 percent confidence interval: ## -11.623341 -3.043326 ## sample estimates: ## mean in group Control mean in group Fertilized ## 12.33333 19.66667 t.test(q1 ~ Treatment, data = diversity_data) ## ## Welch Two Sample t-test ## ## data: q1 by Treatment ## t = -5.578, df = 3.1357, p-value = 0.0101 ## alternative hypothesis: true difference in means between group Control and group Fertilized is not equal to 0 ## 95 percent confidence interval: ## -6.849987 -1.950013 ## sample estimates: ## mean in group Control mean in group Fertilized ## 8.366667 12.766667 "],["part-7-reporting-diversity-results.html", "Chapter 117 Part 7: Reporting Diversity Results 117.1 What to report 117.2 Sample methods and results 117.3 Key takeaways 117.4 Assignment", " Chapter 117 Part 7: Reporting Diversity Results 117.1 What to report Data type and sampling design Which Hill numbers you calculated (q = 0, 1, 2) Standardization method (sample size or coverage) Observed vs. estimated diversity (with confidence intervals) Sample completeness (coverage) Figure showing R/E curves 117.2 Sample methods and results 117.2.1 Methods We quantified plant species diversity in grazed and ungrazed grasslands using the iNEXT framework (Chao et al. 2014). At each of 12 sites (6 per treatment), we recorded species presence/absence in 10 randomly placed 1-m² quadrats. We calculated Hill numbers of order q = 0 (species richness), q = 1 (Shannon diversity), and q = 2 (Simpson diversity) for each site. Because quadrats within sites are not independent sampling units, we used incidence-based methods with quadrats as sampling units. We generated rarefaction/extrapolation curves and standardized comparisons at 90% sample coverage. Confidence intervals were calculated using 200 bootstrap replicates. Site-level diversity estimates were compared between treatments using Welch’s t-tests. Analyses were conducted using the iNEXT package in R version 4.3.1. 117.2.2 Results Ungrazed sites had significantly higher species richness than grazed sites when standardized to 90% sample coverage (ungrazed: 24.3 ± 2.1 species, grazed: 17.8 ± 1.8 species, mean ± SE; t₁₀ = 3.42, p = 0.007; Fig. X). Shannon diversity showed a similar pattern (ungrazed: \\(^1D\\) = 18.2 ± 1.6, grazed: \\(^1D\\) = 12.4 ± 1.3; t₁₀ = 4.12, p = 0.002), while Simpson diversity did not differ significantly (ungrazed: \\(^2D\\) = 12.1 ± 1.2, grazed: \\(^2D\\) = 10.2 ± 1.1; t₁₀ = 1.82, p = 0.10). Rarefaction/extrapolation curves indicated that sample coverage exceeded 85% at all sites (Fig. X), suggesting adequate sampling for diversity estimation. 117.3 Key takeaways Know your data type — Abundance vs. incidence determines your analytical approach Use Hill numbers — They’re interpretable, comparable, and follow the doubling rule Rarefaction standardizes, extrapolation estimates — Together they enable fair comparisons Coverage-based standardization is often more meaningful than size-based Your sampling unit matters — Quadrats within sites should usually be treated as incidence data Don’t pseudoreplicate — Sites (not quadrats within sites) are typically your replicates for statistical comparison Report both observed and estimated diversity with confidence intervals 117.4 Assignment 117.4.1 Part 1: Conceptual questions A colleague reports Shannon index values of H’ = 2.1 for Site A and H’ = 2.8 for Site B. Why might converting these to Hill numbers (\\(^1D\\)) be more useful for comparison? You sampled 100 individuals and detected 25 species with 80% sample coverage. Your colleague sampled 500 individuals and detected 40 species with 95% coverage. Can you directly compare these richness values? What would you do instead? You collected plant cover data in 20 quadrats per site across 5 sites. Should you use abundance or incidence data for iNEXT analysis? Why? 117.4.2 Part 2: iNEXT analysis Using the built-in spider data: data(&quot;spider&quot;) Run iNEXT for all three Hill numbers (q = 0, 1, 2) Create R/E curves (type = 1) and interpret them Compare diversity at 95% sample coverage using estimateD() Create a publication-quality figure Write a results paragraph 117.4.3 Part 3: Data preparation Create a mock dataset with the following structure and format it for iNEXT: 2 treatments (Control, Burned) 3 sites per treatment 8 quadrats per site 15 species total, with different detection frequencies 117.4.4 Part 4: Reflection In 2-3 sentences, explain why the concept of “effective number of species” (Hill numbers) is more useful than raw diversity indices for ecological interpretation. "],["functional-trait-analysis.html", "Chapter 118 Functional Trait Analysis 118.1 Why traits matter 118.2 Setup", " Chapter 118 Functional Trait Analysis Species identity tells you who is present. Functional traits tell you what they do. Two communities might have completely different species but similar functional composition—both dominated by fast-growing, drought-tolerant plants with small leaves. Or two communities might share species but differ functionally because those shared species vary in their traits across environments. Functional trait analysis connects community composition to ecosystem processes by asking: - What traits characterize this community? - How diverse is the community in trait space? - How do traits respond to environmental gradients? - What traits are associated with particular habitats? This chapter covers the core methods for analyzing functional traits in community ecology. 118.1 Why traits matter Traditional community ecology focuses on species—presence, absence, abundance. But species are “black boxes” that don’t tell us about ecological mechanisms. Traits provide that mechanistic link: Trait Ecological function Ecosystem process Specific leaf area (SLA) Resource acquisition strategy Decomposition rate, nutrient cycling Seed mass Dispersal, establishment Colonization, succession Root depth Water acquisition Drought response, water cycling Body size Metabolic rate, diet Energy flow, food web structure Wood density Growth rate, defense Carbon storage, disturbance response By focusing on traits, we can: - Generalize across regions with different species pools - Predict ecosystem function from community composition - Understand mechanisms rather than just patterns 118.2 Setup library(tidyverse) library(FD) # Functional diversity indices library(fundiversity) # Tidyverse-friendly trait metrics library(ade4) # Fourth-corner and RLQ analysis library(vegan) # Community analysis library(corrplot) # Correlation visualization set.seed(42) "],["part-1-trait-data-structure.html", "Chapter 119 Part 1: Trait Data Structure 119.1 The three matrices 119.2 Example data 119.3 Handling trait data types", " Chapter 119 Part 1: Trait Data Structure 119.1 The three matrices Functional trait analysis typically involves three matrices: ## ## ┌─────────────────────────────────────────────────────────────────┐ ## │ │ ## │ SPECIES (columns) TRAITS (columns) │ ## │ ┌─────────────────┐ ┌─────────────────┐ │ ## │ │ │ │ │ │ ## │ S │ L matrix │ S │ Q matrix │ │ ## │ I │ (abundance) │ P │ (trait values)│ │ ## │ T │ │ E │ │ │ ## │ E │ sites × spp │ C │ spp × traits │ │ ## │ S │ │ I │ │ │ ## │ │ │ E │ │ │ ## │ └─────────────────┘ S └─────────────────┘ │ ## │ │ │ │ ## │ │ ┌─────────────────┘ │ ## │ │ │ │ ## │ ▼ ▼ │ ## │ ┌─────────────────────────────────────────────────────┐ │ ## │ │ FOURTH-CORNER ANALYSIS │ │ ## │ │ Trait-Environment Relationships │ │ ## │ └─────────────────────────────────────────────────────┘ │ ## │ │ ## │ ENVIRONMENT (columns) │ ## │ ┌─────────────────┐ │ ## │ │ │ │ ## │ S │ R matrix │ │ ## │ I │ (env values) │ │ ## │ T │ │ │ ## │ E │ sites × env │ │ ## │ S │ │ │ ## │ │ │ │ ## │ └─────────────────┘ │ ## │ │ ## └─────────────────────────────────────────────────────────────────┘ L matrix (sites × species): Community composition (abundance or presence/absence) Q matrix (species × traits): Trait values for each species R matrix (sites × environment): Environmental variables at each site 119.2 Example data # Create example data: 20 sites, 15 species, 4 traits, 3 env variables # L matrix: species abundances at sites n_sites &lt;- 20 n_species &lt;- 15 # Simulate community with environmental filtering set.seed(123) L &lt;- matrix(0, n_sites, n_species) rownames(L) &lt;- paste0(&quot;Site&quot;, 1:n_sites) colnames(L) &lt;- paste0(&quot;Sp&quot;, 1:n_species) # Environment drives composition moisture &lt;- runif(n_sites, 0, 100) for (i in 1:n_sites) { # Species have different moisture optima species_probs &lt;- plogis(-2 + 0.04 * moisture[i] * (1:n_species)/n_species) L[i, ] &lt;- rpois(n_species, lambda = 10 * species_probs) } # Q matrix: species traits Q &lt;- data.frame( SLA = c(15, 18, 22, 25, 28, 12, 20, 24, 30, 16, 19, 23, 27, 14, 21), Height = c(0.3, 0.5, 1.2, 0.8, 1.5, 0.2, 0.9, 1.1, 2.0, 0.4, 0.7, 1.3, 1.8, 0.3, 1.0), SeedMass = c(2.1, 1.5, 3.2, 2.8, 4.5, 1.2, 2.5, 3.0, 5.2, 1.8, 2.2, 3.5, 4.8, 1.4, 2.7), WoodDensity = c(0.65, 0.55, 0.45, 0.50, 0.40, 0.70, 0.52, 0.48, 0.35, 0.60, 0.54, 0.42, 0.38, 0.68, 0.50) ) rownames(Q) &lt;- colnames(L) # R matrix: environmental variables R &lt;- data.frame( Moisture = moisture, Temperature = rnorm(n_sites, 20, 3), Nutrients = runif(n_sites, 1, 10) ) rownames(R) &lt;- rownames(L) # View structure cat(&quot;L matrix (sites × species):\\n&quot;) ## L matrix (sites × species): print(L[1:5, 1:5]) ## Sp1 Sp2 Sp3 Sp4 Sp5 ## Site1 3 2 2 5 2 ## Site2 1 3 1 2 1 ## Site3 0 1 3 0 2 ## Site4 1 3 3 4 3 ## Site5 1 2 2 4 1 cat(&quot;\\nQ matrix (species × traits):\\n&quot;) ## ## Q matrix (species × traits): print(head(Q)) ## SLA Height SeedMass WoodDensity ## Sp1 15 0.3 2.1 0.65 ## Sp2 18 0.5 1.5 0.55 ## Sp3 22 1.2 3.2 0.45 ## Sp4 25 0.8 2.8 0.50 ## Sp5 28 1.5 4.5 0.40 ## Sp6 12 0.2 1.2 0.70 cat(&quot;\\nR matrix (sites × environment):\\n&quot;) ## ## R matrix (sites × environment): print(head(R)) ## Moisture Temperature Nutrients ## Site1 28.75775 23.15813 2.294353 ## Site2 78.83051 16.85247 2.735344 ## Site3 40.89769 16.21953 9.070648 ## Site4 88.30174 29.72312 3.773076 ## Site5 94.04673 18.74943 4.269705 ## Site6 4.55565 20.89468 8.055518 119.3 Handling trait data types Traits can be continuous, categorical, ordinal, or binary: # Mixed trait types Q_mixed &lt;- data.frame( SLA = c(15, 18, 22, 25, 28), # Continuous Nitrogen_fixer = c(0, 1, 0, 1, 0), # Binary Growth_form = factor(c(&quot;herb&quot;, &quot;shrub&quot;, &quot;tree&quot;, &quot;herb&quot;, &quot;tree&quot;)), # Categorical Dispersal = ordered(c(&quot;local&quot;, &quot;regional&quot;, &quot;long&quot;, &quot;local&quot;, &quot;long&quot;), levels = c(&quot;local&quot;, &quot;regional&quot;, &quot;long&quot;)) # Ordinal ) # For FD package, create appropriate distance matrix # Gower distance handles mixed types trait_dist &lt;- gowdis(Q_mixed) "],["part-2-community-weighted-means-cwm.html", "Chapter 120 Part 2: Community-Weighted Means (CWM) 120.1 What is CWM? 120.2 Calculating CWM 120.3 CWM along environmental gradients 120.4 Interpreting CWM", " Chapter 120 Part 2: Community-Weighted Means (CWM) 120.1 What is CWM? The community-weighted mean is the average trait value in a community, weighted by species abundance: \\[CWM = \\sum_{i=1}^{S} p_i \\times t_i\\] where \\(p_i\\) is the relative abundance of species \\(i\\) and \\(t_i\\) is its trait value. CWM answers: “What is the typical trait value in this community?” 120.2 Calculating CWM # Using FD package cwm_result &lt;- functcomp(Q, L, CWM.type = &quot;all&quot;) cwm_result ## SLA Height SeedMass WoodDensity ## Site1 20.79487 0.8897436 2.758974 0.5194872 ## Site2 21.48077 1.0230769 2.940385 0.5001923 ## Site3 22.11429 1.1057143 3.128571 0.4877143 ## Site4 20.48485 0.9045455 2.722727 0.5213636 ## Site5 21.62821 1.0384615 2.989744 0.5002564 ## Site6 22.25000 1.0900000 3.125000 0.4835000 ## Site7 21.65909 0.9727273 2.929545 0.5065909 ## Site8 20.72727 0.9681818 2.896591 0.5181818 ## Site9 21.02632 0.9631579 2.910526 0.5094737 ## Site10 20.55882 0.9264706 2.747059 0.5170588 ## Site11 21.69048 1.0428571 3.045238 0.4976190 ## Site12 22.22449 1.0428571 3.065306 0.4916327 ## Site13 19.87234 0.8553191 2.593617 0.5331915 ## Site14 21.58929 1.0196429 2.978571 0.5028571 ## Site15 19.52941 0.7882353 2.470588 0.5394118 ## Site16 21.28205 0.9743590 2.888462 0.5074359 ## Site17 19.46429 0.7071429 2.335714 0.5478571 ## Site18 19.26667 0.7333333 2.360000 0.5446667 ## Site19 19.94286 0.8314286 2.620000 0.5385714 ## Site20 20.88571 0.9628571 2.828571 0.5101429 # Manual calculation for one site site1_abund &lt;- L[1, ] site1_relabund &lt;- site1_abund / sum(site1_abund) site1_cwm_sla &lt;- sum(site1_relabund * Q$SLA) cat(&quot;Manual CWM for SLA at Site 1:&quot;, round(site1_cwm_sla, 2), &quot;\\n&quot;) ## Manual CWM for SLA at Site 1: 20.79 cat(&quot;From functcomp:&quot;, round(cwm_result$SLA[1], 2), &quot;\\n&quot;) ## From functcomp: 20.79 120.3 CWM along environmental gradients # Add CWM to environmental data trait_env &lt;- cbind(R, cwm_result) # Plot CWM vs environment ggplot(trait_env, aes(x = Moisture, y = SLA)) + geom_point(size = 3, color = &quot;steelblue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick&quot;) + labs(x = &quot;Soil Moisture (%)&quot;, y = &quot;Community-Weighted Mean SLA&quot;, title = &quot;CWM-SLA Response to Moisture&quot;) + theme_minimal() Figure 120.1: Community-weighted mean SLA increases with soil moisture, suggesting environmental filtering favors high-SLA species in wetter conditions. # Statistical test cwm_lm &lt;- lm(SLA ~ Moisture, data = trait_env) summary(cwm_lm) ## ## Call: ## lm(formula = SLA ~ Moisture, data = trait_env) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.19017 -0.75172 0.07019 0.45718 1.78995 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.418248 0.421987 48.386 &lt;2e-16 *** ## Moisture 0.009175 0.006700 1.369 0.188 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9154 on 18 degrees of freedom ## Multiple R-squared: 0.09436, Adjusted R-squared: 0.04405 ## F-statistic: 1.875 on 1 and 18 DF, p-value: 0.1877 120.4 Interpreting CWM CWM reflects environmental filtering: - If environment selects for certain trait values, CWM will shift along gradients - Strong CWM-environment relationships suggest trait-based habitat filtering - CWM captures the “dominant strategy” in the community Limitations: - CWM only captures the central tendency, not diversity - Two communities with same CWM might have very different trait distributions - Sensitive to dominant species (rare species contribute little) "],["part-3-functional-diversity-indices.html", "Chapter 121 Part 3: Functional Diversity Indices 121.1 The main indices 121.2 Calculating functional diversity 121.3 Using fundiversity (tidyverse-friendly) 121.4 Functional diversity along gradients", " Chapter 121 Part 3: Functional Diversity Indices CWM tells you the average trait. Functional diversity indices tell you about variation in traits. 121.1 The main indices Index What it measures Range Interpretation FRic Functional richness 0 to 1 Volume of trait space occupied FEve Functional evenness 0 to 1 Regularity of spacing in trait space FDiv Functional divergence 0 to 1 How spread from center of trait space FDis Functional dispersion 0 to ∞ Mean distance to centroid RaoQ Rao’s quadratic entropy 0 to ∞ Abundance-weighted pairwise distance Figure 121.1: Conceptual illustration of functional diversity components. FRic = total volume; FEve = regularity of distribution; FDiv = spread from center. 121.2 Calculating functional diversity # Using FD package - calculates multiple indices fd_results &lt;- dbFD(Q, L, w.abun = TRUE, # Weight by abundance calc.FRic = TRUE, calc.FDiv = TRUE, calc.CWM = FALSE) # Already calculated above ## FRic: No dimensionality reduction was required. All 4 PCoA axes were kept as &#39;traits&#39;. # View results fd_summary &lt;- data.frame( Site = rownames(L), FRic = fd_results$FRic, FEve = fd_results$FEve, FDiv = fd_results$FDiv, FDis = fd_results$FDis, RaoQ = fd_results$RaoQ ) head(fd_summary) ## Site FRic FEve FDiv FDis RaoQ ## Site1 Site1 0.20791046 0.8012510 0.6208491 1.592389 3.337444 ## Site2 Site2 0.20953624 0.8305514 0.5593581 1.481394 3.378513 ## Site3 Site3 0.09546973 0.7514849 0.5905233 1.626534 3.832420 ## Site4 Site4 0.17332865 0.8377146 0.6850669 1.568483 3.357442 ## Site5 Site5 0.20953624 0.7582571 0.6064447 1.621708 3.853089 ## Site6 Site6 0.14487233 0.7961029 0.6840027 1.827226 4.238790 121.3 Using fundiversity (tidyverse-friendly) # fundiversity provides cleaner syntax library(fundiversity) # Calculate indices fric &lt;- fd_fric(Q, L) fdiv &lt;- fd_fdiv(Q, L) feve &lt;- fd_feve(Q, L) fdis &lt;- fd_fdis(Q, L) raoq &lt;- fd_raoq(Q, L) # Combine fd_tidy &lt;- fric %&gt;% left_join(fdiv, by = &quot;site&quot;) %&gt;% left_join(feve, by = &quot;site&quot;) %&gt;% left_join(fdis, by = &quot;site&quot;) %&gt;% left_join(raoq, by = &quot;site&quot;) head(fd_tidy) ## site FRic FDiv FEve FDis Q ## 1 Site1 0.08259167 0.6196725 0.7162352 4.403799 5.804085 ## 2 Site2 0.08323750 0.5335417 0.7942431 3.906119 5.531016 ## 3 Site3 0.03792500 0.5781303 0.7670382 4.309852 5.994937 ## 4 Site4 0.06885417 0.6693654 0.8605911 4.265831 5.747843 ## 5 Site5 0.08323750 0.5996878 0.7724120 4.351381 6.034915 ## 6 Site6 0.05755000 0.6391545 0.7650415 4.727904 6.271705 121.4 Functional diversity along gradients # Add FD to environmental data fd_env &lt;- cbind(R, fd_summary[, -1]) # Plot multiple FD components fd_long &lt;- fd_env %&gt;% pivot_longer(cols = c(FRic, FEve, FDiv, FDis), names_to = &quot;Index&quot;, values_to = &quot;Value&quot;) ggplot(fd_long, aes(x = Moisture, y = Value)) + geom_point(color = &quot;steelblue&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;firebrick&quot;, se = FALSE) + facet_wrap(~ Index, scales = &quot;free_y&quot;) + labs(x = &quot;Soil Moisture (%)&quot;, y = &quot;Index Value&quot;, title = &quot;Functional Diversity Along Moisture Gradient&quot;) + theme_minimal() Figure 121.2: Functional diversity components along the moisture gradient. Different components may respond differently to environmental change. "],["part-4-fourth-corner-analysis.html", "Chapter 122 Part 4: Fourth-Corner Analysis 122.1 The question 122.2 Interpreting fourth-corner results 122.3 Adjusting for multiple testing", " Chapter 122 Part 4: Fourth-Corner Analysis 122.1 The question CWM and FD indices tell you about trait composition at each site. But what if you want to directly test: “Which traits are associated with which environmental conditions?” This requires linking traits (Q) to environment (R) through the community matrix (L)—the “fourth corner” problem. library(ade4) # Coerce to plain data.frames L_df &lt;- as.data.frame(L) # sites x species R_df &lt;- as.data.frame(R) # sites x environment Q_df &lt;- as.data.frame(Q) # species x traits # Checks stopifnot(nrow(L_df) == nrow(R_df)) stopifnot(ncol(L_df) == nrow(Q_df)) if (!all(rownames(L_df) == rownames(R_df))) stop(&quot;Row names of L and R must match (sites).&quot;) if (!all(colnames(L_df) == rownames(Q_df))) stop(&quot;Colnames of L must match rownames of Q (species).&quot;) # Required by ade4: # - no NAs in R/L/Q # - L must be non-negative stopifnot(!anyNA(R_df), !anyNA(L_df), !anyNA(Q_df)) stopifnot(!any(L_df &lt; 0)) # Fourth-corner test (modeltype=6 combines 2 and 4) fourth &lt;- ade4::fourthcorner(R_df, L_df, Q_df, modeltype = 6, nrepet = 999) summary(fourth) ## Fourth-corner Statistics ## ------------------------ ## Permutation method Comb. 2 and 4 ( 999 permutations) ## ## Adjustment method for multiple comparisons: holm ## Test Stat Obs Std.Obs Alter Pvalue Pvalue.adj ## 1 Moisture / SLA r 0.032623031 1.00696474 two-sided 0.311 1 ## 2 Temperature / SLA r -0.027930139 -0.81060799 two-sided 0.456 1 ## 3 Nutrients / SLA r -0.002804852 -0.08490754 two-sided 0.935 1 ## 4 Moisture / Height r 0.064122989 1.42724275 two-sided 0.156 1 ## 5 Temperature / Height r -0.040051623 -1.10694738 two-sided 0.284 1 ## 6 Nutrients / Height r -0.003335620 -0.09373667 two-sided 0.920 1 ## 7 Moisture / SeedMass r 0.053440560 1.65955637 two-sided 0.097 1 ## 8 Temperature / SeedMass r -0.033191747 -0.96599379 two-sided 0.359 1 ## 9 Nutrients / SeedMass r -0.014122146 -0.42848561 two-sided 0.682 1 ## 10 Moisture / WoodDensity r -0.039983697 -1.27741080 two-sided 0.211 1 ## 11 Temperature / WoodDensity r 0.032712670 0.99438536 two-sided 0.359 1 ## 12 Nutrients / WoodDensity r -0.007700277 -0.23906780 two-sided 0.811 1 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 122.2 Interpreting fourth-corner results # Visualize associations plot(fourth, alpha = 0.05, stat = &quot;D2&quot;) Figure 122.1: Fourth-corner analysis results showing trait-environment associations. Red = positive association; blue = negative; white = not significant. The plot shows: - Significant positive associations (red): Trait values increase with environmental variable - Significant negative associations (blue): Trait values decrease with environmental variable - Non-significant (white): No detectable relationship 122.3 Adjusting for multiple testing # Adjust p-values for multiple comparisons fourth_adj &lt;- p.adjust.4thcorner(fourth, p.adjust.method.G = &quot;fdr&quot;, p.adjust.method.D = &quot;fdr&quot;) # Plot adjusted results plot(fourth_adj, alpha = 0.05, stat = &quot;D2&quot;) "],["part-5-rlq-analysis.html", "Chapter 123 Part 5: RLQ Analysis 123.1 Beyond fourth-corner 123.2 Visualizing RLQ 123.3 Testing RLQ significance", " Chapter 123 Part 5: RLQ Analysis 123.1 Beyond fourth-corner RLQ analysis is a multivariate extension of fourth-corner. It performs a simultaneous ordination of all three matrices, revealing the main trait-environment relationships. Think of it as: - PCA of environment (R) - CA of community (L) - PCA of traits (Q) - …all linked together library(ade4) # --- Coerce to plain data.frames L_df &lt;- as.data.frame(L) # sites x species (&gt;=0) R_df &lt;- as.data.frame(R) # sites x environment Q_df &lt;- as.data.frame(Q) # species x traits # --- Alignment checks stopifnot(nrow(L_df) == nrow(R_df)) stopifnot(ncol(L_df) == nrow(Q_df)) if (!all(rownames(L_df) == rownames(R_df))) stop(&quot;Row names of L and R must match (sites).&quot;) if (!all(colnames(L_df) == rownames(Q_df))) stop(&quot;Colnames of L must match rownames of Q (species).&quot;) # --- Step 1: CA on L gives canonical row/col weights coa_L &lt;- dudi.coa(L_df, scannf = FALSE, nf = 2) # --- Step 2: Ordinate R and Q using L&#39;s weights # Use pca for numeric R/Q; if you have factors in R or Q, use dudi.hillsmith() pca_R &lt;- dudi.pca(R_df, row.w = coa_L$lw, scannf = FALSE, nf = 2) pca_Q &lt;- dudi.pca(Q_df, row.w = coa_L$cw, scannf = FALSE, nf = 2) # --- Sanity checks (these must be TRUE) stopifnot(isTRUE(all.equal(pca_R$lw, coa_L$lw))) stopifnot(isTRUE(all.equal(pca_Q$lw, coa_L$cw))) # --- RLQ rlq_obj &lt;- rlq(pca_R, coa_L, pca_Q, scannf = FALSE, nf = 2) rlq_obj ## RLQ analysis ## call: rlq(dudiR = pca_R, dudiL = coa_L, dudiQ = pca_Q, scannf = FALSE, ## nf = 2) ## class: rlq dudi ## ## $rank (rank) : 3 ## $nf (axis saved) : 2 ## ## eigen values: 0.01418 0.0002508 3.206e-05 ## ## vector length mode content ## 1 $eig 3 numeric Eigenvalues ## 2 $lw 3 numeric Row weigths (for pca_R cols) ## 3 $cw 4 numeric Col weigths (for pca_Q cols) ## ## data.frame nrow ncol content ## 1 $tab 3 4 Crossed Table (CT): cols(pca_R) x cols(pca_Q) ## 2 $li 3 2 CT row scores (cols of pca_R) ## 3 $l1 3 2 Principal components (loadings for pca_R cols) ## 4 $co 4 2 CT col scores (cols of pca_Q) ## 5 $c1 4 2 Principal axes (loadings for pca_Q cols) ## 6 $lR 20 2 Row scores (rows of pca_R) ## 7 $mR 20 2 Normed row scores (rows of pca_R) ## 8 $lQ 15 2 Row scores (rows of pca_Q) ## 9 $mQ 15 2 Normed row scores (rows of pca_Q) ## 10 $aR 2 2 Corr pca_R axes / rlq axes ## 11 $aQ 2 2 Corr coa_L axes / coinertia axes 123.2 Visualizing RLQ plot(rlq_obj) Figure 123.1: RLQ ordination showing trait-environment relationships. The first axis captures the primary gradient linking traits to environment. # Extract scores for custom plotting env_scores &lt;- rlq_obj$lR trait_scores &lt;- rlq_obj$lQ species_scores &lt;- rlq_obj$lC # Custom plot par(mfrow = c(1, 2)) # Environment-Trait relationship plot(rlq_obj$lR[, 1], rlq_obj$lR[, 2], type = &quot;n&quot;, xlab = &quot;RLQ Axis 1&quot;, ylab = &quot;RLQ Axis 2&quot;, main = &quot;Environmental Variables&quot;) text(rlq_obj$l1[, 1], rlq_obj$l1[, 2], rownames(R_df), col = &quot;steelblue&quot;) arrows(0, 0, rlq_obj$c1[, 1], rlq_obj$c1[, 2], length = 0.1, col = &quot;firebrick&quot;) text(rlq_obj$c1[, 1] * 1.1, rlq_obj$c1[, 2] * 1.1, rownames(rlq_obj$c1), col = &quot;firebrick&quot;) # Traits plot(rlq_obj$lQ[, 1], rlq_obj$lQ[, 2], type = &quot;n&quot;, xlab = &quot;RLQ Axis 1&quot;, ylab = &quot;RLQ Axis 2&quot;, main = &quot;Traits&quot;) arrows(0, 0, rlq_obj$c1[, 1], rlq_obj$c1[, 2], length = 0.1, col = &quot;coral&quot;) text(rlq_obj$c1[, 1] * 1.1, rlq_obj$c1[, 2] * 1.1, rownames(rlq_obj$c1), col = &quot;coral&quot;) Figure 123.2: Custom RLQ visualization showing environment, traits, and species scores. par(mfrow = c(1, 1)) 123.3 Testing RLQ significance # Global test of RLQ structure rlq_test &lt;- randtest(rlq_obj, nrepet = 999) plot(rlq_test) "],["part-6-trait-based-community-assembly.html", "Chapter 124 Part 6: Trait-Based Community Assembly 124.1 Assembly mechanisms 124.2 Null models 124.3 Interpreting SES", " Chapter 124 Part 6: Trait-Based Community Assembly 124.1 Assembly mechanisms Functional trait analysis can help distinguish assembly mechanisms: Mechanism Prediction What to measure Environmental filtering Trait convergence Low FD, strong CWM-environment Competitive exclusion Trait divergence High FD, limiting similarity Neutral assembly Random trait patterns FD matches null expectation 124.2 Null models To test whether observed FD differs from random, compare to a null model: # Null model: shuffle traits among species n_null &lt;- 999 null_fdis &lt;- matrix(NA, n_null, n_sites) for (i in 1:n_null) { # Randomize trait matrix (shuffle species labels) Q_null &lt;- Q[sample(nrow(Q)), ] rownames(Q_null) &lt;- rownames(Q) # Calculate FDis fd_null &lt;- dbFD(Q_null, L, messages = FALSE) null_fdis[i, ] &lt;- fd_null$FDis } # Compare observed to null obs_fdis &lt;- fd_results$FDis # Calculate standardized effect size (SES) ses_fdis &lt;- (obs_fdis - colMeans(null_fdis)) / apply(null_fdis, 2, sd) # Results ses_results &lt;- data.frame( Site = rownames(L), Observed = obs_fdis, Null_mean = colMeans(null_fdis), SES = ses_fdis ) head(ses_results) ## Site Observed Null_mean SES ## Site1 Site1 1.592389 1.625120 -0.20700688 ## Site2 Site2 1.481394 1.606979 -0.65500506 ## Site3 Site3 1.626534 1.616077 0.05229643 ## Site4 Site4 1.568483 1.634872 -0.52636382 ## Site5 Site5 1.621708 1.627914 -0.03917427 ## Site6 Site6 1.827226 1.608161 1.03584446 124.3 Interpreting SES # Plot SES ggplot(ses_results, aes(x = Site, y = SES)) + geom_col(aes(fill = SES &gt; 0)) + geom_hline(yintercept = c(-1.96, 1.96), linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;steelblue&quot;, &quot;coral&quot;), guide = &quot;none&quot;) + labs(x = &quot;Site&quot;, y = &quot;SES (FDis)&quot;, title = &quot;Standardized Effect Size of Functional Dispersion&quot;, subtitle = &quot;Dashed lines: 95% null expectation&quot;) + theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Figure 124.1: Standardized Effect Size (SES) for functional dispersion. Negative values suggest trait convergence (environmental filtering); positive values suggest divergence (competition). Interpretation: - SES &lt; -1.96: Significantly clustered (environmental filtering) - SES &gt; 1.96: Significantly overdispersed (competitive exclusion) - -1.96 &lt; SES &lt; 1.96: Not different from random "],["part-7-intraspecific-trait-variation.html", "Chapter 125 Part 7: Intraspecific Trait Variation 125.1 Beyond species means 125.2 Measuring ITV 125.3 Partitioning trait variation", " Chapter 125 Part 7: Intraspecific Trait Variation 125.1 Beyond species means Most trait analyses use species-level mean traits. But individuals within species vary: ## ## Sources of trait variation: ## ├── Among species (interspecific) — traditional focus ## └── Within species (intraspecific) ## ├── Genetic variation ## ├── Phenotypic plasticity ## └── Ontogenetic changes ## ## Ignoring intraspecific variation (ITV) can: ## - Underestimate functional diversity ## - Miss important trait-environment relationships ## - Bias CWM estimates 125.2 Measuring ITV # Simulate individual-level trait data species &lt;- rep(paste0(&quot;Sp&quot;, 1:5), each = 20) site &lt;- rep(1:4, each = 25) sla_mean &lt;- rep(c(15, 20, 25, 30, 35), each = 20) sla_individual &lt;- sla_mean + rnorm(100, 0, 3) # ITV itv_data &lt;- data.frame(species, site, sla = sla_individual) # Visualize ITV ggplot(itv_data, aes(x = species, y = sla, fill = species)) + geom_boxplot(alpha = 0.7) + labs(x = &quot;Species&quot;, y = &quot;SLA&quot;, title = &quot;Intraspecific Trait Variation&quot;) + theme_minimal() + theme(legend.position = &quot;none&quot;) 125.3 Partitioning trait variation # Variance partitioning trait_anova &lt;- aov(sla ~ species, data = itv_data) summary(trait_anova) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## species 4 5618 1404.5 129.2 &lt;2e-16 *** ## Residuals 95 1033 10.9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # Calculate proportion of variance var_among &lt;- summary(trait_anova)[[1]][&quot;species&quot;, &quot;Sum Sq&quot;] var_within &lt;- summary(trait_anova)[[1]][&quot;Residuals&quot;, &quot;Sum Sq&quot;] var_total &lt;- var_among + var_within cat(&quot;Among-species variance:&quot;, round(var_among/var_total * 100, 1), &quot;%\\n&quot;) ## Among-species variance: 84.5 % cat(&quot;Within-species variance (ITV):&quot;, round(var_within/var_total * 100, 1), &quot;%\\n&quot;) ## Within-species variance (ITV): 15.5 % "],["part-8-complete-workflow.html", "Chapter 126 Part 8: Complete Workflow", " Chapter 126 Part 8: Complete Workflow Let’s work through a complete functional trait analysis: # 1. Load/create data (already done above) # 2. Calculate CWM cwm &lt;- functcomp(Q, L, CWM.type = &quot;all&quot;) # 3. Calculate functional diversity fd &lt;- dbFD(Q, L, w.abun = TRUE, calc.CWM = FALSE) ## FRic: No dimensionality reduction was required. All 4 PCoA axes were kept as &#39;traits&#39;. # 4. Combine with environment results &lt;- cbind(R, cwm, FRic = fd$FRic, FEve = fd$FEve, FDiv = fd$FDiv, FDis = fd$FDis) # 5. Analyze CWM-environment relationships cat(&quot;CWM-Environment relationships:\\n&quot;) ## CWM-Environment relationships: for (trait in names(cwm)) { model &lt;- lm(results[[trait]] ~ Moisture + Temperature + Nutrients, data = results) cat(&quot;\\n&quot;, trait, &quot;:\\n&quot;) print(summary(model)$coefficients) } ## ## SLA : ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.882234024 1.792680685 11.64860770 3.160829e-09 ## Moisture 0.009031392 0.007084072 1.27488702 2.205525e-01 ## Temperature -0.024084807 0.073167518 -0.32917348 7.462941e-01 ## Nutrients 0.007058131 0.088792833 0.07948988 9.376287e-01 ## ## Height : ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.938059679 0.2041776307 4.5943313 0.0002992359 ## Moisture 0.001488933 0.0008068414 1.8453846 0.0835705901 ## Temperature -0.004173170 0.0083334253 -0.5007749 0.6233481086 ## Nutrients 0.001402933 0.0101130728 0.1387247 0.8913987419 ## ## SeedMass : ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.814717224 0.438958245 6.41226645 8.596483e-06 ## Moisture 0.002977762 0.001734616 1.71666950 1.053323e-01 ## Temperature -0.007480135 0.017915899 -0.41751382 6.818514e-01 ## Nutrients -0.001616638 0.021741935 -0.07435577 9.416490e-01 ## ## WoodDensity : ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.5146897681 0.0359265001 14.3261873 1.519121e-10 ## Moisture -0.0001893816 0.0001419695 -1.3339600 2.008950e-01 ## Temperature 0.0006043006 0.0014663252 0.4121191 6.857208e-01 ## Nutrients -0.0005221917 0.0017794668 -0.2934540 7.729473e-01 # 6. Fourth-corner analysis fourth &lt;- fourthcorner(R_df, L_df, Q_df, modeltype = 6, nrepet = 999) fourth_adj &lt;- p.adjust.4thcorner(fourth, p.adjust.method.G = &quot;fdr&quot;) cat(&quot;\\nSignificant trait-environment associations:\\n&quot;) ## ## Significant trait-environment associations: print(fourth_adj$tabD2) ## class: krandtest lightkrandtest ## Monte-Carlo tests ## Call: fourthcorner(tabR = R_df, tabL = L_df, tabQ = Q_df, modeltype = 6, ## nrepet = 999) ## ## Number of tests: 12 ## ## Adjustment method for multiple comparisons: holm ## Permutation number: 999 ## Test Obs Std.Obs Alter Pvalue Pvalue.adj ## 1 Moisture / SLA 0.032623031 0.98415642 two-sided 0.334 1 ## 2 Temperature / SLA -0.027930139 -0.84854364 two-sided 0.434 1 ## 3 Nutrients / SLA -0.002804852 -0.09612303 two-sided 0.938 1 ## 4 Moisture / Height 0.064122989 1.47829492 two-sided 0.148 1 ## 5 Temperature / Height -0.040051623 -1.12579345 two-sided 0.272 1 ## 6 Nutrients / Height -0.003335620 -0.10545848 two-sided 0.925 1 ## 7 Moisture / SeedMass 0.053440560 1.63929664 two-sided 0.097 1 ## 8 Temperature / SeedMass -0.033191747 -0.98617337 two-sided 0.345 1 ## 9 Nutrients / SeedMass -0.014122146 -0.45396347 two-sided 0.662 1 ## 10 Moisture / WoodDensity -0.039983697 -1.26438288 two-sided 0.204 1 ## 11 Temperature / WoodDensity 0.032712670 1.03808984 two-sided 0.324 1 ## 12 Nutrients / WoodDensity -0.007700277 -0.25131769 two-sided 0.814 1 library(ade4) # --- Coerce to plain data.frames L_df &lt;- as.data.frame(L) # sites x species R_df &lt;- as.data.frame(R) # sites x env Q_df &lt;- as.data.frame(Q) # species x traits # --- Alignment checks stopifnot(nrow(L_df) == nrow(R_df)) stopifnot(ncol(L_df) == nrow(Q_df)) if (!all(rownames(L_df) == rownames(R_df))) stop(&quot;Row names of L and R must match (sites).&quot;) if (!all(colnames(L_df) == rownames(Q_df))) stop(&quot;Colnames of L must match rownames of Q (species).&quot;) # --- 1) CA on L (this defines the canonical weights) coa_L &lt;- dudi.coa(L_df, scannf = FALSE, nf = 2) # --- 2) Ordinate R and Q using L&#39;s weights pca_R &lt;- dudi.pca(R_df, row.w = coa_L$lw, scannf = FALSE, nf = 2) pca_Q &lt;- dudi.pca(Q_df, row.w = coa_L$cw, scannf = FALSE, nf = 2) # --- Sanity checks (fail fast if anything is off) stopifnot(isTRUE(all.equal(pca_R$lw, coa_L$lw))) stopifnot(isTRUE(all.equal(pca_Q$lw, coa_L$cw))) # --- 3) RLQ rlq_object &lt;- rlq(pca_R, coa_L, pca_Q, scannf = FALSE, nf = 2) # Inspect + plot rlq_object ## RLQ analysis ## call: rlq(dudiR = pca_R, dudiL = coa_L, dudiQ = pca_Q, scannf = FALSE, ## nf = 2) ## class: rlq dudi ## ## $rank (rank) : 3 ## $nf (axis saved) : 2 ## ## eigen values: 0.01418 0.0002508 3.206e-05 ## ## vector length mode content ## 1 $eig 3 numeric Eigenvalues ## 2 $lw 3 numeric Row weigths (for pca_R cols) ## 3 $cw 4 numeric Col weigths (for pca_Q cols) ## ## data.frame nrow ncol content ## 1 $tab 3 4 Crossed Table (CT): cols(pca_R) x cols(pca_Q) ## 2 $li 3 2 CT row scores (cols of pca_R) ## 3 $l1 3 2 Principal components (loadings for pca_R cols) ## 4 $co 4 2 CT col scores (cols of pca_Q) ## 5 $c1 4 2 Principal axes (loadings for pca_Q cols) ## 6 $lR 20 2 Row scores (rows of pca_R) ## 7 $mR 20 2 Normed row scores (rows of pca_R) ## 8 $lQ 15 2 Row scores (rows of pca_Q) ## 9 $mQ 15 2 Normed row scores (rows of pca_Q) ## 10 $aR 2 2 Corr pca_R axes / rlq axes ## 11 $aQ 2 2 Corr coa_L axes / coinertia axes plot(rlq_object) # Test significance (use rlq_object, not rlq_result) rlq_test &lt;- randtest(rlq_object, nrepet = 999) cat(&quot;\\nRLQ global test p-value:&quot;, rlq_test$pvalue, &quot;\\n&quot;) ## ## RLQ global test p-value: 0.334 0.442 "],["part-9-reporting.html", "Chapter 127 Part 9: Reporting 127.1 What to report 127.2 Sample methods and results 127.3 Key takeaways 127.4 Assignment", " Chapter 127 Part 9: Reporting 127.1 What to report Trait selection: Which traits and why (ecological relevance) Trait data source: Measured, database (TRY, etc.), literature CWM patterns: How do mean traits vary with environment? FD patterns: How does functional diversity vary? Trait-environment tests: Fourth-corner or RLQ results Null model results: If testing assembly mechanisms 127.2 Sample methods and results 127.2.1 Methods We analyzed functional trait composition across 20 sites along a soil moisture gradient. For each of 15 plant species, we obtained trait values for specific leaf area (SLA, mm²/mg), plant height (m), seed mass (mg), and wood density (g/cm³) from field measurements and the TRY database. We calculated community-weighted mean (CWM) trait values using species relative abundances and tested CWM-environment relationships using linear regression. We quantified functional diversity using functional richness (FRic), evenness (FEve), divergence (FDiv), and dispersion (FDis) calculated with the FD package (Laliberté et al. 2014). We tested trait-environment associations using fourth-corner analysis with model 6 (permutation of both sites and species) and 999 permutations, adjusting p-values for false discovery rate. RLQ analysis provided a multivariate ordination linking traits to environmental gradients. Analyses were conducted in R version 4.3.1. 127.2.2 Results Community-weighted mean SLA increased significantly with soil moisture (β = 0.12, SE = 0.03, p &lt; 0.001; Fig. X), indicating a shift toward more acquisitive leaf strategies in wetter conditions. Functional dispersion showed no significant relationship with moisture (p = 0.34), suggesting similar functional diversity across the gradient. Fourth-corner analysis revealed significant positive associations between SLA and moisture (r = 0.42, adjusted p = 0.003) and negative associations between wood density and moisture (r = -0.38, adjusted p = 0.008; Fig. Y). RLQ analysis identified a primary axis (explaining 67% of cross-covariance) linking high moisture and nutrients to species with high SLA and low wood density (p = 0.001, 999 permutations). These results suggest strong environmental filtering favoring fast-growth strategies in resource-rich microsites. 127.3 Key takeaways Traits bridge pattern and process — They explain why species occur where they do CWM captures the dominant strategy — Average trait value weighted by abundance FD captures trait diversity — Multiple indices measure different aspects Fourth-corner links traits to environment — Tests specific associations RLQ provides multivariate synthesis — Ordination of all three matrices together Null models test assembly mechanisms — Distinguish filtering from competition ITV matters — Within-species variation can be substantial 127.4 Assignment 127.4.1 Part 1: Conceptual questions What is the difference between CWM and functional diversity? When would each be more informative? In a fourth-corner analysis, you find that leaf nitrogen content is positively associated with soil nitrogen. What does this suggest about community assembly? Your null model analysis shows that FDis is significantly lower than expected by chance. What assembly mechanism does this suggest? 127.4.2 Part 2: CWM analysis Using the built-in dune and dune.env data from vegan, combined with trait data: data(dune) data(dune.env) # Simulated traits for dune species dune_traits &lt;- data.frame( SLA = runif(ncol(dune), 10, 40), Height = runif(ncol(dune), 0.1, 1.5), SeedMass = runif(ncol(dune), 0.1, 5) ) rownames(dune_traits) &lt;- colnames(dune) Calculate CWM for all traits Test whether CWM-SLA differs among management types Plot CWM-Height against moisture Interpret the patterns 127.4.3 Part 3: Functional diversity Using the same data: Calculate FRic, FEve, FDiv, and FDis Test whether FDis differs among management types Plot FD indices against moisture Which index shows the strongest environmental relationship? 127.4.4 Part 4: Fourth-corner analysis Run fourth-corner analysis on the dune data Adjust for multiple testing Identify significant trait-environment associations Create a visualization Write a results paragraph 127.4.5 Part 5: Reflection In 2-3 sentences, explain why functional trait analysis might be more useful than species-based analysis for predicting community responses to climate change. "],["network-analysis-in-ecology.html", "Chapter 128 Network Analysis in Ecology 128.1 What is an ecological network? 128.2 Types of ecological networks 128.3 Setup", " Chapter 128 Network Analysis in Ecology Ecological communities are not just lists of species—they are webs of interactions. Plants interact with pollinators. Predators consume prey. Parasites infect hosts. Seeds rely on dispersers. These interactions form ecological networks that structure communities and drive ecosystem function. Network analysis provides tools to: - Visualize complex interaction patterns - Quantify network structure (Who is connected to whom? How tightly?) - Identify keystone species and critical interactions - Compare networks across space, time, or treatments - Predict how networks respond to species loss This chapter introduces network thinking and the core analytical tools for ecological network analysis. 128.1 What is an ecological network? An ecological network represents species (or individuals, populations) as nodes and their interactions as edges (links). ## ## NETWORK TERMINOLOGY ## ═══════════════════════════════════════════════════ ## ## NODES (vertices) EDGES (links) ## ○ Species ─── Interactions ## ○ Individuals ─── Energy flow ## ○ Populations ─── Services ## ## NETWORK TYPES ## ═══════════════════════════════════════════════════ ## ## UNIPARTITE (one-mode) BIPARTITE (two-mode) ## All nodes can interact Two distinct sets of nodes ## with any other node Interactions only between sets ## ## ○───○ ● ● ● ## /│ │\\ │\\ /│\\ /│ ## ○─┼───┼─○ │ \\ / │ \\ / │ ## \\│ │/ │ \\ / │ \\ / │ ## ○───○ ○───○───○───○───○ ## ## Food webs Plant-pollinator ## Social networks Host-parasite ## Competition Seed dispersal 128.2 Types of ecological networks Network type Nodes Edges Example Food web Species Feeding links Who eats whom Pollination network Plants + pollinators Visits Flower-visitor interactions Seed dispersal Plants + dispersers Dispersal events Fruit-frugivore interactions Host-parasite Hosts + parasites Infection Disease networks Plant-herbivore Plants + herbivores Consumption Grazing/browsing Competition Species Competition Overlap in resource use Facilitation Species Positive effects Nurse plant networks 128.3 Setup library(tidyverse) library(bipartite) # Bipartite network analysis library(igraph) # General network analysis library(ggraph) # Network visualization with ggplot2 library(tidygraph) # Tidy interface to igraph set.seed(42) "],["part-1-bipartite-networks.html", "Chapter 129 Part 1: Bipartite Networks 129.1 Network data structure 129.2 Visualizing bipartite networks 129.3 Network-level metrics 129.4 Species-level metrics", " Chapter 129 Part 1: Bipartite Networks Most ecological interaction networks are bipartite: two distinct groups of species where interactions only occur between groups, not within them. 129.1 Network data structure Bipartite networks are typically stored as interaction matrices: - Rows = one group (e.g., plants) - Columns = other group (e.g., pollinators) - Cell values = interaction strength (counts, frequencies, or binary) # Example: plant-pollinator network # Rows = plant species, Columns = pollinator species # Values = number of visits observed pollination_matrix &lt;- matrix( c(45, 12, 0, 8, 3, 22, 38, 15, 0, 0, 0, 5, 42, 28, 0, 18, 0, 7, 35, 22, 2, 0, 0, 12, 48), nrow = 5, byrow = TRUE, dimnames = list( Plants = c(&quot;Asteraceae1&quot;, &quot;Fabaceae1&quot;, &quot;Rosaceae1&quot;, &quot;Lamiaceae1&quot;, &quot;Asteraceae2&quot;), Pollinators = c(&quot;Bombus&quot;, &quot;Apis&quot;, &quot;Syrphid&quot;, &quot;Butterfly&quot;, &quot;SolitaryBee&quot;) ) ) pollination_matrix ## Pollinators ## Plants Bombus Apis Syrphid Butterfly SolitaryBee ## Asteraceae1 45 12 0 8 3 ## Fabaceae1 22 38 15 0 0 ## Rosaceae1 0 5 42 28 0 ## Lamiaceae1 18 0 7 35 22 ## Asteraceae2 2 0 0 12 48 129.2 Visualizing bipartite networks # Make sure bipartite is available library(bipartite) # --- SAFE DEBUG (does NOT trigger conflicted) --- cat(&quot;\\nUsing function from: bipartite::plotweb\\n&quot;) ## ## Using function from: bipartite::plotweb pw &lt;- getFromNamespace(&quot;plotweb&quot;, &quot;bipartite&quot;) cat(&quot;\\nArgs in bipartite::plotweb():\\n&quot;) ## ## Args in bipartite::plotweb(): ok &lt;- names(formals(pw)) print(ok) ## [1] &quot;web&quot; &quot;sorting&quot; &quot;empty&quot; &quot;higher_abundances&quot; ## [5] &quot;lower_abundances&quot; &quot;add_higher_abundances&quot; &quot;add_lower_abundances&quot; &quot;higher_labels&quot; ## [9] &quot;lower_labels&quot; &quot;scaling&quot; &quot;font&quot; &quot;family&quot; ## [13] &quot;srt&quot; &quot;higher_italic&quot; &quot;lower_italic&quot; &quot;text_size&quot; ## [17] &quot;spacing&quot; &quot;box_size&quot; &quot;x_lim&quot; &quot;y_lim&quot; ## [21] &quot;lab_distance&quot; &quot;lower_color&quot; &quot;lower_border&quot; &quot;lower_add_color&quot; ## [25] &quot;lower_text_color&quot; &quot;higher_color&quot; &quot;higher_border&quot; &quot;higher_add_color&quot; ## [29] &quot;higher_text_color&quot; &quot;horizontal&quot; &quot;link_color&quot; &quot;link_border&quot; ## [33] &quot;link_alpha&quot; &quot;curved_links&quot; &quot;arrow&quot; &quot;plot_axes&quot; ## [37] &quot;add&quot; &quot;mar&quot; &quot;mai&quot; # --- Build args you want --- call_args &lt;- list( web = pollination_matrix, method = &quot;normal&quot;, col.low = &quot;forestgreen&quot;, col.high = &quot;orange&quot;, bor.col.low = &quot;darkgreen&quot;, bor.col.high = &quot;darkorange&quot;, text.rot = 90, labsize = 1.2, y.lim = c(-0.5, 3) ) # --- Keep only args supported by YOUR installed bipartite::plotweb() --- call_args &lt;- call_args[names(call_args) %in% ok] cat(&quot;\\nPassing these args:\\n&quot;) ## ## Passing these args: print(names(call_args)) ## [1] &quot;web&quot; # --- Plot --- do.call(pw, call_args) Figure 129.1: Bipartite network visualization showing plants (bottom) connected to pollinators (top). Line thickness indicates interaction strength. 129.3 Network-level metrics # Calculate network metrics net_metrics &lt;- networklevel(pollination_matrix, index = c(&quot;connectance&quot;, &quot;web asymmetry&quot;, &quot;links per species&quot;, &quot;nestedness&quot;, &quot;weighted nestedness&quot;, &quot;H2&quot;)) # Display as.data.frame(net_metrics) ## net_metrics ## connectance 0.6800000 ## web asymmetry 0.0000000 ## links per species 1.7000000 ## nestedness 70.0780957 ## weighted nestedness -0.3289071 ## H2 0.4284810 129.3.1 Key metrics explained Metric Range What it measures Connectance 0-1 Proportion of possible links realized Web asymmetry -1 to 1 Balance between the two trophic levels Links per species &gt;0 Average number of partners Nestedness 0-100 Whether specialists interact with subsets of generalists’ partners H2’ 0-1 Network-level specialization (0 = random, 1 = perfectly specialized) 129.4 Species-level metrics # Metrics for each species species_metrics &lt;- specieslevel(pollination_matrix, index = c(&quot;degree&quot;, &quot;normalised degree&quot;, &quot;species strength&quot;, &quot;d&quot;)) # Lower trophic level (plants) cat(&quot;Plant metrics:\\n&quot;) ## Plant metrics: print(round(species_metrics$`lower level`, 3)) ## degree normalised.degree species.strength d ## Asteraceae1 4 0.8 0.873 0.330 ## Fabaceae1 3 0.6 1.178 0.440 ## Rosaceae1 3 0.6 1.085 0.491 ## Lamiaceae1 4 0.8 1.039 0.175 ## Asteraceae2 3 0.6 0.825 0.535 # Higher trophic level (pollinators) cat(&quot;\\nPollinator metrics:\\n&quot;) ## ## Pollinator metrics: print(round(species_metrics$`higher level`, 3)) ## degree normalised.degree species.strength d ## Bombus 4 0.8 1.207 0.357 ## Apis 3 0.6 0.750 0.419 ## Syrphid 3 0.6 0.845 0.407 ## Butterfly 4 0.8 1.111 0.229 ## SolitaryBee 3 0.6 1.087 0.567 129.4.1 Species-level metrics explained Metric What it measures Ecological meaning Degree Number of interaction partners Generalization Normalised degree Degree / max possible Relative generalization Species strength Sum of dependencies on this species Importance to partners d’ Specialization index How selective (0 = generalist, 1 = specialist) "],["part-2-nestedness.html", "Chapter 130 Part 2: Nestedness 130.1 What is nestedness? 130.2 Measuring nestedness 130.3 Why nestedness matters", " Chapter 130 Part 2: Nestedness 130.1 What is nestedness? A nested network has a specific structure: specialists interact with subsets of the species that generalists interact with. Figure 130.1: Nested (left) vs non-nested (right) networks. In nested networks, specialists’ partners are subsets of generalists’ partners. 130.2 Measuring nestedness # Nestedness examples using bipartite # - NODF: 0–100 (higher = more nested) # - Temperature (binmatnest): 0–100 (lower/colder = more nested) # Safety checks stopifnot(is.matrix(pollination_matrix)) if (any(is.na(pollination_matrix))) stop(&quot;pollination_matrix contains NA values.&quot;) # 1) NODF nestedness (works on quantitative matrices; uses vegan&#39;s nestednodf under the hood) nodf &lt;- bipartite::nested(pollination_matrix, method = &quot;NODF&quot;) cat(&quot;NODF (0–100; higher = more nested):&quot;, round(nodf, 2), &quot;\\n&quot;) ## NODF (0–100; higher = more nested): 33.33 # Optional: also report weighted NODF (if you care about interaction strengths) wnodf &lt;- bipartite::networklevel(pollination_matrix, index = &quot;weighted NODF&quot;) cat(&quot;Weighted NODF (0–100; higher = more nested):&quot;, round(wnodf, 2), &quot;\\n&quot;) ## Weighted NODF (0–100; higher = more nested): 23.33 # 2) Temperature-based nestedness (BINMATNEST) requires presence/absence pollination_bin &lt;- (pollination_matrix &gt; 0) * 1 temp &lt;- bipartite::nested(pollination_bin, method = &quot;binmatnest&quot;) cat(&quot;Temperature (0–100; LOWER/colder = more nested):&quot;, round(temp, 2), &quot;\\n&quot;) ## Temperature (0–100; LOWER/colder = more nested): 66.31 # Quick interpretation helper cat(&quot;Interpretation:\\n&quot;, &quot; - If NODF is high and Temperature is low, the matrix is strongly nested.\\n&quot;, &quot; - Use NODF/weighted NODF for most ecology papers; temperature is an older but intuitive complement.\\n&quot;, sep = &quot;&quot;) ## Interpretation: ## - If NODF is high and Temperature is low, the matrix is strongly nested. ## - Use NODF/weighted NODF for most ecology papers; temperature is an older but intuitive complement. 130.3 Why nestedness matters Nested networks may be: - More robust to random species loss (redundancy of interactions) - Less robust to targeted removal of generalists (they hold the network together) - Result of ecological processes like abundance differences, phenology overlap "],["part-3-modularity.html", "Chapter 131 Part 3: Modularity 131.1 What is modularity? 131.2 Detecting modules 131.3 Visualizing modules 131.4 Modularity vs nestedness", " Chapter 131 Part 3: Modularity 131.1 What is modularity? A modular network has clusters (modules) of species that interact more with each other than with species in other modules. ## ## MODULAR NETWORK STRUCTURE ## ═════════════════════════════════════════════════ ## ## Module 1 Module 2 Module 3 ## ┌───────┐ ┌───────┐ ┌───────┐ ## │ ○─○─○ │ │ ●─●─● │ │ ◇─◇─◇ │ ## │ │╲│╱│ │←──→│ │╲│╱│ │←──→│ │╲│╱│ │ ## │ ○─○─○ │ │ ●─●─● │ │ ◇─◇─◇ │ ## └───────┘ └───────┘ └───────┘ ## ## Dense connections WITHIN modules ## Sparse connections BETWEEN modules ## ## Ecological interpretation: ## - Functional groups ## - Phenological compartments ## - Spatial clusters 131.2 Detecting modules # Calculate modularity using the bipartite package mod_result &lt;- computeModules(pollination_matrix, method = &quot;Beckett&quot;) # Modularity value cat(&quot;Modularity Q:&quot;, round(mod_result@likelihood, 3), &quot;\\n&quot;) ## Modularity Q: 0.4 # Module membership listModuleInformation(mod_result) ## [[1]] ## [[1]][[1]] ## [[1]][[1]][[1]] ## [1] &quot;Asteraceae1&quot; &quot;Fabaceae1&quot; &quot;Rosaceae1&quot; &quot;Lamiaceae1&quot; &quot;Asteraceae2&quot; ## ## [[1]][[1]][[2]] ## [1] &quot;Bombus&quot; &quot;Apis&quot; &quot;Syrphid&quot; &quot;Butterfly&quot; &quot;SolitaryBee&quot; ## ## ## ## [[2]] ## [[2]][[1]] ## [[2]][[1]][[1]] ## [1] &quot;Asteraceae1&quot; &quot;Fabaceae1&quot; ## ## [[2]][[1]][[2]] ## [1] &quot;Bombus&quot; &quot;Apis&quot; ## ## ## [[2]][[2]] ## [[2]][[2]][[1]] ## [1] &quot;Rosaceae1&quot; &quot;Lamiaceae1&quot; ## ## [[2]][[2]][[2]] ## [1] &quot;Syrphid&quot; &quot;Butterfly&quot; ## ## ## [[2]][[3]] ## [[2]][[3]][[1]] ## [1] &quot;Asteraceae2&quot; ## ## [[2]][[3]][[2]] ## [1] &quot;SolitaryBee&quot; 131.3 Visualizing modules # Plot with module colors plotModuleWeb(mod_result, labsize = 1.2, plotModules = TRUE) Figure 131.1: Network with species colored by module membership. Modules represent groups of tightly interacting species. 131.4 Modularity vs nestedness Networks can be: - Nested AND modular: Modules, each internally nested - Nested but not modular: No clear compartments - Modular but not nested: Distinct compartments, no subset structure # Compare network structure: nestedness vs modularity # Nestedness (NODF) nodf &lt;- bipartite::nested(pollination_matrix, method = &quot;NODF&quot;) # Modularity (Q) from an already-fitted moduleWeb result Q &lt;- mod_result@likelihood cat(&quot;Nestedness (NODF; higher = more nested):&quot;, round(nodf, 2), &quot;\\n&quot;) ## Nestedness (NODF; higher = more nested): 33.33 cat(&quot;Modularity (Q; higher = more modular):&quot;, round(Q, 3), &quot;\\n&quot;) ## Modularity (Q; higher = more modular): 0.4 "],["part-4-food-webs-unipartite-networks.html", "Chapter 132 Part 4: Food Webs (Unipartite Networks) 132.1 Food web data structure 132.2 Using igraph for food webs 132.3 Visualizing food webs 132.4 Food web metrics", " Chapter 132 Part 4: Food Webs (Unipartite Networks) Food webs are unipartite: any species can potentially interact with (eat or be eaten by) any other. 132.1 Food web data structure # Food web as adjacency matrix # 1 = row species eats column species food_web &lt;- matrix( c(0, 0, 0, 0, 0, 0, # Grass (basal) 1, 0, 0, 0, 0, 0, # Grasshopper eats grass 1, 0, 0, 0, 0, 0, # Rabbit eats grass 0, 1, 0, 0, 0, 0, # Frog eats grasshopper 0, 1, 1, 0, 0, 0, # Snake eats grasshopper, rabbit 0, 0, 0, 1, 1, 0), # Hawk eats frog, snake nrow = 6, byrow = TRUE, dimnames = list( c(&quot;Grass&quot;, &quot;Grasshopper&quot;, &quot;Rabbit&quot;, &quot;Frog&quot;, &quot;Snake&quot;, &quot;Hawk&quot;), c(&quot;Grass&quot;, &quot;Grasshopper&quot;, &quot;Rabbit&quot;, &quot;Frog&quot;, &quot;Snake&quot;, &quot;Hawk&quot;) ) ) food_web ## Grass Grasshopper Rabbit Frog Snake Hawk ## Grass 0 0 0 0 0 0 ## Grasshopper 1 0 0 0 0 0 ## Rabbit 1 0 0 0 0 0 ## Frog 0 1 0 0 0 0 ## Snake 0 1 1 0 0 0 ## Hawk 0 0 0 1 1 0 132.2 Using igraph for food webs # Convert to igraph object food_graph &lt;- graph_from_adjacency_matrix(food_web, mode = &quot;directed&quot;) # Add attributes V(food_graph)$trophic_level &lt;- c(1, 2, 2, 3, 3, 4) # Assign trophic levels V(food_graph)$type &lt;- c(&quot;producer&quot;, rep(&quot;consumer&quot;, 5)) # Basic metrics cat(&quot;Number of species:&quot;, vcount(food_graph), &quot;\\n&quot;) ## Number of species: 6 cat(&quot;Number of links:&quot;, ecount(food_graph), &quot;\\n&quot;) ## Number of links: 7 cat(&quot;Connectance:&quot;, round(ecount(food_graph) / (vcount(food_graph)^2), 3), &quot;\\n&quot;) ## Connectance: 0.194 132.3 Visualizing food webs # Using ggraph for nice visualization food_tidy &lt;- as_tbl_graph(food_graph) ggraph(food_tidy, layout = &quot;sugiyama&quot;) + geom_edge_link(arrow = arrow(length = unit(3, &quot;mm&quot;)), end_cap = circle(3, &quot;mm&quot;), edge_colour = &quot;gray50&quot;) + geom_node_point(aes(color = factor(trophic_level)), size = 8) + geom_node_text(aes(label = name), vjust = -1) + scale_color_viridis_d(name = &quot;Trophic Level&quot;) + theme_void() + labs(title = &quot;Simple Food Web&quot;) Figure 132.1: Food web visualization with species arranged by trophic level. Arrows point from prey to predator. 132.4 Food web metrics # Food web node-level metrics using igraph # Degree centrality # NOTE: in a directed food web: # - in-degree = number of prey (things consumed) # - out-degree = number of predators (things that consume it) in_degree &lt;- igraph::degree(food_graph, mode = &quot;in&quot;) out_degree &lt;- igraph::degree(food_graph, mode = &quot;out&quot;) # Betweenness centrality betw &lt;- igraph::betweenness(food_graph, directed = TRUE) # Compile metrics into a tidy data frame fw_metrics &lt;- data.frame( Species = igraph::V(food_graph)$name, TrophicLevel = igraph::V(food_graph)$trophic_level, Prey = in_degree, Predators = out_degree, Betweenness = round(betw, 2) ) fw_metrics ## Species TrophicLevel Prey Predators Betweenness ## Grass Grass 1 2 0 0.00 ## Grasshopper Grasshopper 2 2 1 2.17 ## Rabbit Rabbit 2 1 1 0.83 ## Frog Frog 3 1 1 0.83 ## Snake Snake 3 1 2 2.17 ## Hawk Hawk 4 0 2 0.00 "],["part-5-network-centrality.html", "Chapter 133 Part 5: Network Centrality 133.1 Why centrality matters", " Chapter 133 Part 5: Network Centrality 133.1 Why centrality matters Not all species are equally important to network structure. Centrality metrics identify key species: Metric What it measures Ecological interpretation Degree Number of connections Generalism Betweenness How often on shortest paths Control of energy/information flow Closeness Average distance to all others Rapid spread of effects Eigenvector Connection to well-connected species Influence # Calculate multiple centrality metrics for a directed food web (igraph) centrality_df &lt;- data.frame( Species = igraph::V(food_graph)$name, Degree = igraph::degree(food_graph, mode = &quot;all&quot;), Betweenness = igraph::betweenness(food_graph, directed = TRUE), Closeness = igraph::closeness(food_graph, mode = &quot;all&quot;, normalized = TRUE), Eigenvector = igraph::eigen_centrality(food_graph, directed = TRUE)$vector ) # Rank species by betweenness (potential connectors/keystones) centrality_df %&gt;% dplyr::arrange(dplyr::desc(Betweenness)) ## Species Degree Betweenness Closeness Eigenvector ## Grasshopper Grasshopper 3 2.1666667 0.7142857 0 ## Snake Snake 3 2.1666667 0.7142857 0 ## Rabbit Rabbit 2 0.8333333 0.5555556 0 ## Frog Frog 2 0.8333333 0.5555556 0 ## Grass Grass 2 0.0000000 0.5555556 1 ## Hawk Hawk 2 0.0000000 0.5555556 0 # Precompute node metrics on the igraph object you&#39;re plotting food_tidy &lt;- food_tidy %&gt;% igraph::set_vertex_attr( name = &quot;btw&quot;, value = igraph::betweenness(food_tidy, directed = TRUE) ) ggraph(food_tidy, layout = &quot;kk&quot;) + geom_edge_link( arrow = arrow(length = unit(2, &quot;mm&quot;)), end_cap = circle(3, &quot;mm&quot;), edge_colour = &quot;gray70&quot;, edge_alpha = 0.6 ) + geom_node_point(aes( size = btw + 1, color = factor(trophic_level) )) + geom_node_text(aes(label = name), repel = TRUE) + scale_size_continuous(range = c(4, 12), name = &quot;Betweenness&quot;) + scale_color_viridis_d(name = &quot;Trophic Level&quot;) + theme_void() Figure 133.1: Food web with node size scaled by betweenness centrality. Larger nodes are more important for connecting the network. "],["part-6-null-models-and-statistical-testing.html", "Chapter 134 Part 6: Null Models and Statistical Testing 134.1 Why null models? 134.2 Types of null models 134.3 Testing nestedness 134.4 Testing multiple metrics", " Chapter 134 Part 6: Null Models and Statistical Testing 134.1 Why null models? Observed network metrics might arise simply from: - Species abundance distributions - Random interaction patterns - Sampling constraints Null models generate random networks that preserve some features while randomizing others, allowing you to test whether observed metrics are non-random. 134.2 Types of null models Null model What it preserves What it randomizes r00 Number of links Everything else r0 Row totals (degree) Interactions c0 Column totals (degree) Interactions r1 Row &amp; column totals Specific interactions swap Row &amp; column totals (exactly) Individual swaps 134.3 Testing nestedness # Convert to binary (presence/absence) bin_matrix &lt;- (pollination_matrix &gt; 0) * 1 # Use quasiswap - preserves row/column FREQUENCIES (number of 1s) null_mod &lt;- vegan::nullmodel(bin_matrix, method = &quot;quasiswap&quot;) null_sim &lt;- simulate(null_mod, nsim = 1000) obs_nodf &lt;- bipartite::nested(bin_matrix, method = &quot;NODF&quot;) null_nodf &lt;- apply(null_sim, 3, function(m) { bipartite::nested(m, method = &quot;NODF&quot;) }) p_value &lt;- mean(null_nodf &gt;= obs_nodf) ses_nodf &lt;- (obs_nodf - mean(null_nodf)) / sd(null_nodf) cat(&quot;Standardized Effect Size (SES):&quot;, round(ses_nodf, 2), &quot;\\n&quot;) ## Standardized Effect Size (SES): 2.36 cat(&quot;Observed NODF:&quot;, round(obs_nodf, 2), &quot;\\n&quot;) ## Observed NODF: 33.33 cat(&quot;Null mean:&quot;, round(mean(null_nodf), 2), &quot;\\n&quot;) ## Null mean: 29.61 cat(&quot;Null SD:&quot;, round(sd(null_nodf), 2), &quot;\\n&quot;) ## Null SD: 1.58 cat(&quot;P-value:&quot;, round(p_value, 3), &quot;\\n&quot;) ## P-value: 0.031 # Visualize null distribution hist(null_nodf, breaks = 30, col = &quot;gray80&quot;, border = &quot;white&quot;, main = &quot;Null Distribution of Nestedness&quot;, xlab = &quot;NODF&quot;) abline(v = obs_nodf, col = &quot;firebrick&quot;, lwd = 2) legend(&quot;topright&quot;, &quot;Observed&quot;, col = &quot;firebrick&quot;, lwd = 2) Figure 134.1: Null model distribution for nestedness. Red line shows observed value. If observed falls outside the null distribution, the pattern is non-random. 134.4 Testing multiple metrics test_network_metrics &lt;- function(web, n_null = 999) { # Binary version for connectance and nestedness bin_web &lt;- (web &gt; 0) * 1 # Generate null matrices - binary (swap.web) and weighted (r2dtable) nulls_bin &lt;- bipartite::nullmodel(bin_web, N = n_null, method = &quot;swap.web&quot;) nulls_wt &lt;- bipartite::nullmodel(web, N = n_null, method = &quot;r2dtable&quot;) # Observed metrics obs_conn &lt;- networklevel(bin_web, index = &quot;connectance&quot;) obs_nest &lt;- networklevel(bin_web, index = &quot;nestedness&quot;) obs_h2 &lt;- networklevel(web, index = &quot;H2&quot;) # Null metrics - binary null_conn &lt;- sapply(nulls_bin, function(x) networklevel(x, index = &quot;connectance&quot;)) null_nest &lt;- sapply(nulls_bin, function(x) networklevel(x, index = &quot;nestedness&quot;)) # Null metrics - weighted null_h2 &lt;- sapply(nulls_wt, function(x) networklevel(x, index = &quot;H2&quot;)) # Build results calc_row &lt;- function(name, obs, null) { data.frame( Metric = name, Observed = as.numeric(obs), Null_mean = mean(null, na.rm = TRUE), Null_sd = sd(null, na.rm = TRUE), Z_score = (as.numeric(obs) - mean(null, na.rm = TRUE)) / sd(null, na.rm = TRUE), P_value = mean(null &gt;= as.numeric(obs), na.rm = TRUE) ) } results &lt;- rbind( calc_row(&quot;connectance&quot;, obs_conn, null_conn), calc_row(&quot;nestedness&quot;, obs_nest, null_nest), calc_row(&quot;H2&quot;, obs_h2, null_h2) ) return(results) } null_results &lt;- test_network_metrics(pollination_matrix, n_null = 99) print(null_results, digits = 3, row.names = FALSE) ## Metric Observed Null_mean Null_sd Z_score P_value ## connectance 0.680 0.6800 0.00000 NaN 1.000 ## nestedness 67.049 58.7677 11.00307 0.753 0.455 ## H2 0.428 0.0139 0.00507 81.824 0.000 "],["part-7-network-robustness.html", "Chapter 135 Part 7: Network Robustness 135.1 Simulating species loss", " Chapter 135 Part 7: Network Robustness 135.1 Simulating species loss How does network structure change as species go extinct? This reveals which species are critical for maintaining network integrity. robustness_analysis &lt;- function(web, attack = &quot;random&quot;, n_sim = 100) { n_species &lt;- nrow(web) results &lt;- matrix(NA, n_sim, n_species) for (sim in 1:n_sim) { web_temp &lt;- web # Determine removal order if (attack == &quot;random&quot;) { removal_order &lt;- sample(1:n_species) } else if (attack == &quot;degree&quot;) { # Remove most connected first removal_order &lt;- base::order(rowSums(web &gt; 0), decreasing = TRUE) } # Sequential removal for (i in 1:n_species) { # Count surviving higher trophic level species # (those with at least one interaction remaining) surviving &lt;- sum(colSums(web_temp) &gt; 0) results[sim, i] &lt;- surviving / ncol(web) # Remove next species web_temp[removal_order[i], ] &lt;- 0 } } return(results) } # Run robustness analysis random_removal &lt;- robustness_analysis(pollination_matrix, attack = &quot;random&quot;, n_sim = 100) targeted_removal &lt;- robustness_analysis(pollination_matrix, attack = &quot;degree&quot;, n_sim = 100) # Calculate means removal_df &lt;- data.frame( Species_removed = 1:nrow(pollination_matrix), Random = colMeans(random_removal), Targeted = colMeans(targeted_removal) ) %&gt;% pivot_longer(cols = c(Random, Targeted), names_to = &quot;Strategy&quot;, values_to = &quot;Fraction_surviving&quot;) ggplot(removal_df, aes(x = Species_removed, y = Fraction_surviving, color = Strategy)) + geom_line(linewidth = 1.2) + labs(x = &quot;Number of Plant Species Removed&quot;, y = &quot;Fraction of Pollinators Surviving&quot;, title = &quot;Network Robustness to Species Loss&quot;) + scale_color_manual(values = c(&quot;steelblue&quot;, &quot;firebrick&quot;)) + theme_minimal() Figure 135.1: Network robustness to species loss. Random removal (blue) vs targeted removal of most-connected species (red). Targeted attacks cause faster network collapse. # Calculate robustness (area under curve) cat(&quot;Robustness (AUC) - Random:&quot;, round(mean(random_removal), 3), &quot;\\n&quot;) ## Robustness (AUC) - Random: 0.923 cat(&quot;Robustness (AUC) - Targeted:&quot;, round(mean(targeted_removal), 3), &quot;\\n&quot;) ## Robustness (AUC) - Targeted: 0.92 "],["part-8-comparing-networks.html", "Chapter 136 Part 8: Comparing Networks 136.1 Comparing network structure 136.2 Network dissimilarity", " Chapter 136 Part 8: Comparing Networks 136.1 Comparing network structure # Load example networks from bipartite package data(Safariland) data(vazquenc) # Compare metrics compare_nets &lt;- data.frame( Network = c(&quot;Safariland&quot;, &quot;Vazquenc&quot;), Species_lower = c(nrow(Safariland), nrow(vazquenc)), Species_upper = c(ncol(Safariland), ncol(vazquenc)), Connectance = c( networklevel(Safariland, &quot;connectance&quot;), networklevel(vazquenc, &quot;connectance&quot;) ), Nestedness = c( bipartite::nested(Safariland, method = &quot;NODF&quot;), bipartite::nested(vazquenc, method = &quot;NODF&quot;) ), H2 = c( networklevel(Safariland, &quot;H2&quot;), networklevel(vazquenc, &quot;H2&quot;) ) ) print(compare_nets, digits = 3) ## Network Species_lower Species_upper Connectance Nestedness H2 ## 1 Safariland 9 27 0.160 18.1 0.854 ## 2 Vazquenc 7 24 0.185 14.5 0.791 136.2 Network dissimilarity # For networks with same species, can use beta diversity of interactions # For different species sets, use network dissimilarity metrics # Example: using betalink-style approach # (simplified - full implementation in betalink package) interaction_beta &lt;- function(web1, web2) { # Convert to binary web1_bin &lt;- (web1 &gt; 0) * 1 web2_bin &lt;- (web2 &gt; 0) * 1 # Shared vs unique interactions shared &lt;- sum(web1_bin * web2_bin &gt; 0) unique1 &lt;- sum(web1_bin &gt; 0) - shared unique2 &lt;- sum(web2_bin &gt; 0) - shared # Jaccard dissimilarity jaccard &lt;- 1 - (shared / (shared + unique1 + unique2)) return(jaccard) } "],["part-9-complete-workflow.html", "Chapter 137 Part 9: Complete Workflow", " Chapter 137 Part 9: Complete Workflow # 1. Load network data data(memmott1999) # Flower visitation network from a meadow near Bristol, UK # 2. Visualize bipartite::plotweb(memmott1999, srt = 90, lower_color = &quot;forestgreen&quot;, higher_color = &quot;orange&quot;) # 3. Network-level metrics cat(&quot;\\n=== Network Metrics ===\\n&quot;) ## ## === Network Metrics === net_stats &lt;- networklevel(memmott1999, index = c(&quot;connectance&quot;, &quot;nestedness&quot;, &quot;weighted nestedness&quot;, &quot;H2&quot;, &quot;links per species&quot;)) print(round(net_stats, 3)) ## connectance links per species nestedness weighted nestedness H2 ## 0.151 2.875 8.571 0.723 0.271 # 4. Test nestedness against null cat(&quot;\\n=== Null Model Test (Nestedness) ===\\n&quot;) ## ## === Null Model Test (Nestedness) === obs_nest &lt;- bipartite::nested(memmott1999, method = &quot;NODF&quot;) nulls &lt;- bipartite::nullmodel(memmott1999, N = 99, method = &quot;r2dtable&quot;) null_nest &lt;- sapply(nulls, function(x) bipartite::nested(x, method = &quot;NODF&quot;)) z_score &lt;- (obs_nest - mean(null_nest)) / sd(null_nest) p_val &lt;- mean(null_nest &gt;= obs_nest) cat(&quot;Z-score:&quot;, round(z_score, 2), &quot;\\n&quot;) ## Z-score: -7.76 cat(&quot;P-value:&quot;, round(p_val, 3), &quot;\\n&quot;) ## P-value: 1 # 5. Modularity cat(&quot;\\n=== Modularity ===\\n&quot;) ## ## === Modularity === modules &lt;- computeModules(memmott1999, method = &quot;Beckett&quot;) cat(&quot;Modularity Q:&quot;, round(modules@likelihood, 3), &quot;\\n&quot;) ## Modularity Q: 0.304 # 6. Species-level metrics for key species cat(&quot;\\n=== Top 5 Most Connected Plants ===\\n&quot;) ## ## === Top 5 Most Connected Plants === plant_degree &lt;- rowSums(memmott1999 &gt; 0) print(sort(plant_degree, decreasing = TRUE)[1:5]) ## Daucus.carota Leontodon.hispidus Torilis.japonica Eupatorium.cannabinum ## 46 34 25 23 ## Centaurea.nigra ## 20 cat(&quot;\\n=== Top 5 Most Connected Pollinators ===\\n&quot;) ## ## === Top 5 Most Connected Pollinators === poll_degree &lt;- colSums(memmott1999 &gt; 0) print(sort(poll_degree, decreasing = TRUE)[1:5]) ## Diptera.spec22 Sphaerophoria.scripta Eriothrix.rufomaculata Episyrphus.balteatus ## 18 17 14 14 ## solitary.bees ## 12 "],["part-10-reporting-2.html", "Chapter 138 Part 10: Reporting 138.1 What to report 138.2 Sample methods and results 138.3 Key takeaways 138.4 Assignment", " Chapter 138 Part 10: Reporting 138.1 What to report Network description: Number of species in each level, number of interactions Network-level metrics: Connectance, nestedness, modularity, H2’ Null model comparisons: Whether metrics differ from random Species-level patterns: Key species, specialists vs generalists Visualization: Network diagram showing structure 138.2 Sample methods and results 138.2.1 Methods We constructed a quantitative plant-pollinator network from field observations at 20 sites in northern Arizona during June-August 2023. We recorded all flower-visitor interactions during standardized 30-minute observation periods, totaling 120 hours of observation. The resulting network included interaction frequencies between plant and pollinator species. We calculated network-level metrics including connectance, weighted nestedness (WNODF), modularity (Q), and network-level specialization (H2’) using the bipartite package (Dormann et al. 2008). We tested whether observed nestedness exceeded null expectations using 999 randomized matrices preserving row and column totals (r2dtable null model). Species-level specialization (d’) was calculated for all species with ≥5 interactions. We assessed network robustness by simulating sequential removal of plant species under random and targeted (most-connected first) scenarios. Analyses were conducted in R version 4.3.1. 138.2.2 Results The plant-pollinator network comprised 45 plant and 82 pollinator species connected by 387 unique pairwise interactions (Fig. X). Network connectance was 0.105, indicating that about 10% of possible interactions were realized. The network was significantly nested (WNODF = 0.42, z = 4.2, p &lt; 0.001 compared to null), suggesting that specialist pollinators visited subsets of the flowers visited by generalists. Modularity was moderate (Q = 0.38), with five distinct modules corresponding roughly to flower color and morphology guilds (Fig. Y). Network-level specialization was moderate (H2’ = 0.45). The most connected plant was Helianthus annuus (28 pollinator species), while the most connected pollinator was Bombus huntii (15 plant species). Robustness analysis indicated that random removal of 50% of plant species would result in loss of 23% of pollinator species, while targeted removal of the most-connected plants would cause 61% pollinator loss, highlighting the importance of generalist plants for network persistence. 138.3 Key takeaways Networks reveal structure — Who interacts with whom, and how tightly Bipartite networks are common in ecology — Plant-animal interactions, host-parasite Key metrics: Connectance (density), nestedness (subset structure), modularity (compartments) Null models are essential — Test whether patterns are non-random Centrality identifies key species — For conservation, focus on highly connected species Robustness predicts stability — Targeted removal is worse than random Compare networks carefully — Need appropriate metrics for different questions 138.4 Assignment 138.4.1 Part 1: Conceptual questions What is the difference between a nested and a modular network? What ecological processes might generate each pattern? Why is it important to compare observed network metrics to null models? A pollination network has high H2’ (specialization). What does this mean for the pollinators if one plant species goes extinct? 138.4.2 Part 2: Bipartite network analysis Using the built-in vazquenc data: data(vazquenc) Visualize the network Calculate network-level metrics (connectance, nestedness, H2’) Test whether nestedness differs from null expectation Identify the most connected plant and pollinator species Calculate species-level specialization (d’) 138.4.3 Part 3: Food web analysis Create a simple food web (6-10 species) representing a system you’re familiar with: Construct the adjacency matrix Convert to igraph format Calculate degree and betweenness centrality Create a visualization with trophic levels Which species is most central? 138.4.4 Part 4: Robustness analysis Using the Safariland network: data(Safariland) Simulate random vs targeted species removal Plot the robustness curves Calculate robustness (area under curve) for each scenario What does this tell you about conservation priorities? 138.4.5 Part 5: Reflection In 2-3 sentences, explain why network analysis provides insights that species-level analyses cannot. "],["null-models-and-community-assembly.html", "Chapter 139 Null Models and Community Assembly 139.1 The fundamental question 139.2 The null model logic 139.3 Setup", " Chapter 139 Null Models and Community Assembly Why do certain species live together while others never co-occur? Is the pattern we observe the result of ecological processes—competition, environmental filtering, historical contingency—or could it arise by chance? Null models provide the statistical framework to answer these questions. By generating random communities that preserve some features of the data while randomizing others, we can test whether observed patterns are stronger (or weaker) than expected by chance. This chapter covers: - The logic and philosophy of null models - Co-occurrence analysis (do species avoid each other?) - Phylogenetic community structure (are relatives clustered or overdispersed?) - Connecting patterns to assembly mechanisms 139.1 The fundamental question You observe that Species A and Species B never co-occur at the same site. Is this because: Competition: They exclude each other through competitive interactions Habitat filtering: They require different environments Chance: Random sampling from a species pool can produce apparent patterns Null models help distinguish signal from noise by asking: “How often would we see this pattern in a random world?” 139.2 The null model logic ## ## THE NULL MODEL WORKFLOW ## ════════════════════════════════════════════════════════════════ ## ## 1. OBSERVE a pattern in your data ## └── e.g., Species A and B never co-occur ## ## 2. CALCULATE a metric that quantifies the pattern ## └── e.g., Number of checkerboard pairs, C-score ## ## 3. DEFINE what &#39;random&#39; means (the null model) ## └── What constraints to preserve? ## └── What to randomize? ## ## 4. GENERATE many random communities ## └── Typically 999 or 9999 randomizations ## ## 5. CALCULATE the metric for each random community ## └── Creates a null distribution ## ## 6. COMPARE observed to null distribution ## └── Is observed more extreme than expected? ## └── Calculate p-value and effect size 139.3 Setup library(tidyverse) library(vegan) # Community ecology &amp; null models library(picante) # Phylogenetic community structure library(ape) # Phylogenetic trees set.seed(42) "],["part-1-the-philosophy-of-null-models.html", "Chapter 140 Part 1: The Philosophy of Null Models 140.1 What makes a good null model? 140.2 The constraint problem 140.3 Type I and Type II errors in null models", " Chapter 140 Part 1: The Philosophy of Null Models 140.1 What makes a good null model? A null model should: 1. Preserve realistic constraints (species richness, abundance distributions) 2. Randomize the pattern of interest (co-occurrence, trait matching) 3. Be appropriate for your question (different questions need different nulls) 140.2 The constraint problem The more constraints you preserve, the more conservative your test: Null model Preserves Randomizes Stringency Equiprobable Nothing Everything Very liberal Row totals Species richness per site Species identities Moderate Column totals Species frequency Site identities Moderate Both (fixed-fixed) Row AND column totals Specific co-occurrences Very conservative Figure 140.1: Different null models preserve different constraints. More constraints = more conservative test. 140.3 Type I and Type II errors in null models Too liberal (few constraints): High Type I error (false positives) Too conservative (many constraints): High Type II error (false negatives) The fixed-fixed null model (preserving both row and column totals) is generally recommended because it controls for: - Differences in species’ range sizes (some species are widespread) - Differences in site richness (some sites have more species) "],["part-2-co-occurrence-analysis.html", "Chapter 141 Part 2: Co-occurrence Analysis 141.1 The classic question 141.2 Creating a presence-absence matrix 141.3 Checkerboard units 141.4 The C-score 141.5 Interpreting co-occurrence results 141.6 Effect size (SES)", " Chapter 141 Part 2: Co-occurrence Analysis 141.1 The classic question Diamond (1975) proposed “assembly rules” based on observing that some bird species never co-occur on islands. But Connor and Simberloff (1979) challenged: “Couldn’t this pattern arise by chance?” This debate launched the field of null model analysis in ecology. 141.2 Creating a presence-absence matrix # Example: species occurrence across sites # Rows = sites, Columns = species occurrence &lt;- matrix( c(1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0), nrow = 8, byrow = TRUE, dimnames = list( Sites = paste0(&quot;Site&quot;, 1:8), Species = paste0(&quot;Sp&quot;, 1:6) ) ) occurrence ## Species ## Sites Sp1 Sp2 Sp3 Sp4 Sp5 Sp6 ## Site1 1 1 0 0 1 0 ## Site2 1 0 1 0 1 0 ## Site3 0 1 1 0 0 1 ## Site4 1 1 0 1 0 0 ## Site5 0 0 1 1 0 1 ## Site6 1 0 0 1 1 0 ## Site7 0 1 1 0 0 1 ## Site8 1 1 0 0 1 0 141.3 Checkerboard units A checkerboard occurs when two species show perfect mutual exclusion in a pair of sites: ## ## CHECKERBOARD UNIT ## ═══════════════════════════════════════ ## ## Site 1: Species A present, Species B absent ## Site 2: Species A absent, Species B present ## ## Sp A Sp B ## Site 1 1 0 ## Site 2 0 1 ## ## This creates a &#39;checkerboard&#39; pattern suggesting exclusion. 141.4 The C-score The C-score (checkerboard score) measures the average number of checkerboard units across all species pairs: \\[C = \\frac{1}{P} \\sum_{j&lt;k} (r_j - S_{jk})(r_k - S_{jk})\\] where: - \\(r_j\\) = number of sites where species j occurs - \\(S_{jk}\\) = number of sites where both species co-occur - \\(P\\) = number of species pairs Higher C-score = more segregation (fewer co-occurrences than expected) # C-score function (Stone &amp; Roberts 1990) # For a sites × species presence-absence matrix c_score &lt;- function(mat) { mat &lt;- (mat &gt; 0) * 1 S &lt;- ncol(mat) Ri &lt;- colSums(mat) total &lt;- 0 for (i in 1:(S - 1)) { for (j in (i + 1):S) { Sij &lt;- sum(mat[, i] == 1 &amp; mat[, j] == 1) total &lt;- total + (Ri[i] - Sij) * (Ri[j] - Sij) } } total } # Observed C-score obs_c &lt;- c_score(occurrence) cat(&quot;Observed C-score:&quot;, obs_c, &quot;\\n&quot;) ## Observed C-score: 94 # Null model test using vegan&#39;s oecosimu cooc_analysis &lt;- oecosimu( occurrence, nestfun = c_score, method = &quot;quasiswap&quot;, nsimul = 999 ) cooc_analysis ## oecosimu object ## ## Call: oecosimu(comm = occurrence, nestfun = c_score, method = &quot;quasiswap&quot;, nsimul = 999) ## ## nullmodel method &#39;quasiswap&#39; with 999 simulations ## ## alternative hypothesis: statistic is less or greater than simulated values ## ## statistic SES mean 2.5% 50% 97.5% Pr(sim.) ## Sp1 94 1.9094 87.738 84.000 88.000 96 0.145 141.5 Interpreting co-occurrence results # Extract null distribution from oecosimu object null_c &lt;- cooc_analysis$oecosimu$simulated # Plot null distribution hist(null_c, breaks = 30, col = &quot;gray80&quot;, border = &quot;white&quot;, main = &quot;Null Distribution of C-score&quot;, xlab = &quot;C-score&quot;) abline(v = obs_c, col = &quot;firebrick&quot;, lwd = 2) abline(v = mean(null_c), col = &quot;steelblue&quot;, lwd = 2, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Observed&quot;, &quot;Null mean&quot;), col = c(&quot;firebrick&quot;, &quot;steelblue&quot;), lwd = 2, lty = c(1, 2), bty = &quot;n&quot;) Figure 141.1: Null distribution of C-scores. The observed value (red line) compared to random expectation indicates whether species co-occur less (or more) than expected by chance. Interpretation: - Observed &gt; Null: Species segregated (competitive exclusion?) - Observed &lt; Null: Species aggregated (shared habitat preferences?) - Observed ≈ Null: Random co-occurrence 141.6 Effect size (SES) The Standardized Effect Size allows comparison across studies: \\[SES = \\frac{Observed - Mean_{null}}{SD_{null}}\\] # Extract values from oecosimu object null_values &lt;- cooc_analysis$oecosimu$simulated null_mean &lt;- mean(null_values) null_sd &lt;- sd(null_values) ses &lt;- (obs_c - null_mean) / null_sd p_upper &lt;- mean(null_values &gt;= obs_c) cat(&quot;Observed C-score:&quot;, round(obs_c, 2), &quot;\\n&quot;) ## Observed C-score: 94 cat(&quot;Null mean:&quot;, round(null_mean, 2), &quot;\\n&quot;) ## Null mean: 87.74 cat(&quot;SES:&quot;, round(ses, 2), &quot;\\n&quot;) ## SES: 1.91 cat(&quot;P-value:&quot;, round(p_upper, 3), &quot;\\n&quot;) ## P-value: 0.072 SES interpretation: - |SES| &lt; 2: Not significantly different from random - SES &gt; 2: Significantly segregated - SES &lt; -2: Significantly aggregated "],["part-3-null-model-algorithms.html", "Chapter 142 Part 3: Null Model Algorithms 142.1 Common algorithms for presence-absence data 142.2 The swap algorithm (sim9) 142.3 Using vegan for co-occurrence", " Chapter 142 Part 3: Null Model Algorithms 142.1 Common algorithms for presence-absence data Table 142.1: Common null model algorithms for co-occurrence analysis Algorithm Row sums Column sums Method Recommendation sim1 Variable Variable Random fill Too liberal sim2 Fixed Variable Shuffle rows Moderate sim3 Variable Fixed Shuffle columns Moderate sim9 Fixed Fixed Sequential swap Recommended sim10 Fixed Fixed Independent swap Recommended 142.2 The swap algorithm (sim9) The recommended “fixed-fixed” algorithm: Start with observed matrix Find a 2×2 submatrix that forms a checkerboard Swap the 1s and 0s (preserves row and column totals) Repeat many times to randomize # Visual demonstration of a swap cat(&quot;Before swap: After swap:\\n&quot;) ## Before swap: After swap: cat(&quot; Sp1 Sp2 Sp1 Sp2\\n&quot;) ## Sp1 Sp2 Sp1 Sp2 cat(&quot;A 1 0 → A 0 1\\n&quot;) ## A 1 0 → A 0 1 cat(&quot;B 0 1 B 1 0\\n&quot;) ## B 0 1 B 1 0 cat(&quot;\\nRow and column sums unchanged!\\n&quot;) ## ## Row and column sums unchanged! 142.3 Using vegan for co-occurrence # vegan&#39;s oecosimu function # Calculate C-score equivalent (number of checkerboards) # Custom function to calculate checkerboards count_checkerboards &lt;- function(m) { # m is transposed (species × sites) m &lt;- t(m) n_sp &lt;- nrow(m) total &lt;- 0 for (i in 1:(n_sp-1)) { for (j in (i+1):n_sp) { # Checkerboard for species pair i,j shared &lt;- sum(m[i,] == 1 &amp; m[j,] == 1) only_i &lt;- sum(m[i,] == 1 &amp; m[j,] == 0) only_j &lt;- sum(m[i,] == 0 &amp; m[j,] == 1) # C-score component total &lt;- total + only_i * only_j } } return(total) } # Observed obs_checker &lt;- count_checkerboards(occurrence) # Null model using vegan null_result &lt;- oecosimu(occurrence, count_checkerboards, method = &quot;swap&quot;, nsimul = 999, burnin = 1000, thin = 100) null_result ## oecosimu object ## ## Call: oecosimu(comm = occurrence, nestfun = count_checkerboards, method = &quot;swap&quot;, nsimul = 999, ## burnin = 1000, thin = 100) ## ## nullmodel method &#39;swap&#39; with 999 simulations ## options: thin 100, burnin 1000 ## alternative hypothesis: statistic is less or greater than simulated values ## ## statistic SES mean 2.5% 50% 97.5% Pr(sim.) ## statistic 94 1.6684 88.15 84.00 88.00 96 0.177 "],["part-4-phylogenetic-community-structure.html", "Chapter 143 Part 4: Phylogenetic Community Structure 143.1 The question 143.2 The data you need 143.3 Mean Pairwise Distance (MPD) 143.4 Mean Nearest Taxon Distance (MNTD) 143.5 Standardized effect sizes (NRI and NTI) 143.6 Interpreting phylogenetic structure", " Chapter 143 Part 4: Phylogenetic Community Structure 143.1 The question Are co-occurring species more (or less) related than expected by chance? Phylogenetic clustering: Close relatives co-occur → environmental filtering Phylogenetic overdispersion: Distant relatives co-occur → competitive exclusion 143.2 The data you need Community matrix: Sites × species presence/absence or abundance Phylogeny: Evolutionary relationships among species # Simulate a phylogeny set.seed(123) n_species &lt;- 15 tree &lt;- rtree(n = n_species, tip.label = paste0(&quot;Sp&quot;, 1:n_species)) # Plot phylogeny plot(tree, main = &quot;Simulated Phylogeny&quot;) # Simulate community data (some species at some sites) n_sites &lt;- 10 community &lt;- matrix( rbinom(n_species * n_sites, 1, 0.4), nrow = n_sites, dimnames = list(paste0(&quot;Site&quot;, 1:n_sites), tree$tip.label) ) # Make sure each site has at least 2 species for (i in 1:n_sites) { if (sum(community[i,]) &lt; 2) { community[i, sample(1:n_species, 3)] &lt;- 1 } } head(community) ## Sp14 Sp6 Sp9 Sp10 Sp11 Sp5 Sp3 Sp13 Sp1 Sp4 Sp7 Sp12 Sp15 Sp8 Sp2 ## Site1 0 0 0 0 0 1 0 1 1 0 0 0 1 0 0 ## Site2 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 ## Site3 1 1 1 1 0 1 1 0 1 0 0 0 0 0 1 ## Site4 1 1 0 1 0 0 0 0 1 0 0 0 1 1 0 ## Site5 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 ## Site6 1 1 0 0 0 1 1 1 0 1 0 1 1 0 0 143.3 Mean Pairwise Distance (MPD) MPD measures the average phylogenetic distance between all species pairs in a community: \\[MPD = \\frac{\\sum_{i&lt;j} d_{ij}}{P}\\] where \\(d_{ij}\\) is the phylogenetic distance between species i and j. # Calculate phylogenetic distances phylo_dist &lt;- cophenetic(tree) # Calculate MPD for each community mpd_obs &lt;- mpd(community, phylo_dist, abundance.weighted = FALSE) mpd_obs ## [1] 2.506745 2.953076 2.582597 3.179403 2.173273 2.943794 2.991099 2.656023 2.828065 2.628818 143.4 Mean Nearest Taxon Distance (MNTD) MNTD measures the average distance to the closest relative for each species: \\[MNTD = \\frac{\\sum_i \\min(d_{ij})}{S}\\] MNTD is more sensitive to terminal clustering (recent divergence). # Calculate MNTD mntd_obs &lt;- mntd(community, phylo_dist, abundance.weighted = FALSE) mntd_obs ## [1] 1.852512 1.543374 1.098459 1.352407 1.980840 1.191267 1.962577 1.495891 1.254958 1.525542 143.5 Standardized effect sizes (NRI and NTI) The Net Relatedness Index (NRI) and Nearest Taxon Index (NTI) are standardized versions: \\[NRI = -\\frac{MPD_{obs} - MPD_{null}}{SD_{null}}\\] \\[NTI = -\\frac{MNTD_{obs} - MNTD_{null}}{SD_{null}}\\] Note the negative sign: positive NRI/NTI indicates clustering. # Calculate SES using picante ses_mpd &lt;- ses.mpd(community, phylo_dist, null.model = &quot;taxa.labels&quot;, abundance.weighted = FALSE, runs = 999) ses_mntd &lt;- ses.mntd(community, phylo_dist, null.model = &quot;taxa.labels&quot;, abundance.weighted = FALSE, runs = 999) # View results cat(&quot;=== MPD Results (NRI = -SES) ===\\n&quot;) ## === MPD Results (NRI = -SES) === print(ses_mpd[, c(&quot;ntaxa&quot;, &quot;mpd.obs&quot;, &quot;mpd.rand.mean&quot;, &quot;mpd.obs.z&quot;, &quot;mpd.obs.p&quot;)]) ## ntaxa mpd.obs mpd.rand.mean mpd.obs.z mpd.obs.p ## Site1 4 2.506745 2.760222 -0.4688503 0.3160 ## Site2 6 2.953076 2.736713 0.5761956 0.7010 ## Site3 8 2.582597 2.752952 -0.6258570 0.2495 ## Site4 6 3.179403 2.735257 1.2167491 0.8940 ## Site5 3 2.173273 2.760267 -0.8142982 0.2030 ## Site6 8 2.943794 2.759393 0.6865277 0.7490 ## Site7 5 2.991099 2.756521 0.5247996 0.6840 ## Site8 7 2.656023 2.750476 -0.2989468 0.3580 ## Site9 7 2.828065 2.748716 0.2475629 0.5620 ## Site10 5 2.628818 2.744720 -0.2551612 0.3790 cat(&quot;\\n=== MNTD Results (NTI = -SES) ===\\n&quot;) ## ## === MNTD Results (NTI = -SES) === print(ses_mntd[, c(&quot;ntaxa&quot;, &quot;mntd.obs&quot;, &quot;mntd.rand.mean&quot;, &quot;mntd.obs.z&quot;, &quot;mntd.obs.p&quot;)]) ## ntaxa mntd.obs mntd.rand.mean mntd.obs.z mntd.obs.p ## Site1 4 1.852512 1.837055 0.03283098 0.4970 ## Site2 6 1.543374 1.471276 0.25162387 0.5810 ## Site3 8 1.098459 1.230092 -0.70823879 0.2410 ## Site4 6 1.352407 1.452612 -0.35668219 0.3620 ## Site5 3 1.980840 2.148466 -0.26083246 0.4340 ## Site6 8 1.191267 1.233278 -0.23078987 0.3825 ## Site7 5 1.962577 1.617775 0.94706294 0.8260 ## Site8 7 1.495891 1.339991 0.68189523 0.7370 ## Site9 7 1.254958 1.341445 -0.37719377 0.3520 ## Site10 5 1.525542 1.602718 -0.21212368 0.4300 143.6 Interpreting phylogenetic structure Pattern NRI/NTI Possible mechanism Phylogenetic clustering Positive (&gt; 1.96) Environmental filtering (conserved niches) Phylogenetic overdispersion Negative (&lt; -1.96) Competitive exclusion (similar species compete) Random -1.96 to 1.96 Neutral assembly, or multiple processes cat(&quot; INTERPRETATION DEPENDS ON TRAIT CONSERVATISM ═══════════════════════════════════════════════════════════════ If traits are CONSERVED (close relatives are similar): ├── Clustering → Environmental filtering │ (environment selects for certain trait values) └── Overdispersion → Competitive exclusion (similar species compete for same resources) If traits are CONVERGENT (distant relatives are similar): ├── Clustering → Competitive exclusion │ (unrelated species with different traits coexist) └── Overdispersion → Environmental filtering (environment selects from different lineages) ALWAYS examine trait evolution on your phylogeny! &quot;) ## ## INTERPRETATION DEPENDS ON TRAIT CONSERVATISM ## ═══════════════════════════════════════════════════════════════ ## ## If traits are CONSERVED (close relatives are similar): ## ├── Clustering → Environmental filtering ## │ (environment selects for certain trait values) ## └── Overdispersion → Competitive exclusion ## (similar species compete for same resources) ## ## If traits are CONVERGENT (distant relatives are similar): ## ├── Clustering → Competitive exclusion ## │ (unrelated species with different traits coexist) ## └── Overdispersion → Environmental filtering ## (environment selects from different lineages) ## ## ALWAYS examine trait evolution on your phylogeny! "],["part-5-null-models-for-phylogenetic-structure.html", "Chapter 144 Part 5: Null Models for Phylogenetic Structure 144.1 Null model choices", " Chapter 144 Part 5: Null Models for Phylogenetic Structure 144.1 Null model choices Null model What it does Use when taxa.labels Shuffle species labels on tree Testing if observed community is non-random draw richness Randomize while keeping richness Controlling for local richness frequency Randomize weighted by occurrence Controlling for species prevalence sample.pool Random draw from species pool Testing against regional pool phylogeny.pool Shuffle phylogenetic relationships Testing phylogenetic signal # Compare different null models null_models &lt;- c(&quot;taxa.labels&quot;, &quot;richness&quot;, &quot;frequency&quot;) results_list &lt;- lapply(null_models, function(nm) { ses.mpd(community, phylo_dist, null.model = nm, runs = 499, abundance.weighted = FALSE)$mpd.obs.z }) # Compare null_comparison &lt;- data.frame( Site = rownames(community), taxa.labels = results_list[[1]], richness = results_list[[2]], frequency = results_list[[3]] ) print(null_comparison, digits = 2) ## Site taxa.labels richness frequency ## 1 Site1 -0.41 -0.46 -0.8081 ## 2 Site2 0.55 0.54 0.2501 ## 3 Site3 -0.56 -0.71 -0.6155 ## 4 Site4 1.18 1.12 0.9464 ## 5 Site5 -0.84 -0.70 -1.9598 ## 6 Site6 0.70 0.65 0.2704 ## 7 Site7 0.57 0.46 0.4347 ## 8 Site8 -0.34 -0.16 -0.3698 ## 9 Site9 0.27 0.15 -0.0078 ## 10 Site10 -0.23 -0.25 -0.4874 "],["part-6-connecting-patterns-to-mechanisms.html", "Chapter 145 Part 6: Connecting Patterns to Mechanisms 145.1 The inference problem 145.2 Combining co-occurrence and phylogeny 145.3 Scale dependence", " Chapter 145 Part 6: Connecting Patterns to Mechanisms 145.1 The inference problem Observing a pattern doesn’t prove a mechanism. The same pattern can arise from different processes: ## ## PATTERN → MULTIPLE POSSIBLE MECHANISMS ## ═══════════════════════════════════════════════════════════════ ## ## Species segregation (high C-score): ## ├── Competitive exclusion (species exclude each other) ## ├── Habitat filtering (species prefer different habitats) ## └── Historical/biogeographic factors (allopatric distributions) ## ## Phylogenetic clustering: ## ├── Environmental filtering (conserved niches) ## ├── Competitive exclusion (labile traits) ## └── Dispersal limitation (nearby = related) ## ## Phylogenetic overdispersion: ## ├── Competitive exclusion (conserved niches) ## ├── Environmental filtering (convergent traits) ## └── Facilitation among distant relatives ## ## HOW TO DISTINGUISH: ## 1. Combine with trait analysis (Ch. 23) ## 2. Examine spatial scale dependence ## 3. Experimental manipulation ## 4. Multiple independent tests 145.2 Combining co-occurrence and phylogeny # Sites with high vs low phylogenetic clustering clustering_sites &lt;- which(ses_mpd$mpd.obs.z &gt; 0) overdispersion_sites &lt;- which(ses_mpd$mpd.obs.z &lt; 0) cat(&quot;Sites with phylogenetic clustering:&quot;, paste(rownames(community)[clustering_sites], collapse = &quot;, &quot;), &quot;\\n&quot;) ## Sites with phylogenetic clustering: Site2, Site4, Site6, Site7, Site9 cat(&quot;Sites with phylogenetic overdispersion:&quot;, paste(rownames(community)[overdispersion_sites], collapse = &quot;, &quot;), &quot;\\n&quot;) ## Sites with phylogenetic overdispersion: Site1, Site3, Site5, Site8, Site10 # Could examine: do clustered sites differ environmentally? 145.3 Scale dependence Assembly mechanisms may operate at different scales: ## ## SCALE-DEPENDENT ASSEMBLY ## ═══════════════════════════════════════════════════════════════ ## ## REGIONAL SCALE (among habitats): ## └── Environmental filtering dominates ## └── Expect phylogenetic clustering ## ## LOCAL SCALE (within habitats): ## └── Competition dominates ## └── Expect phylogenetic overdispersion ## ## This leads to the &#39;competition-relatedness&#39; hypothesis: ## Competition is strongest among close relatives, ## driving local overdispersion within clustered habitat guilds. "],["part-7-functional-trait-null-models.html", "Chapter 146 Part 7: Functional Trait Null Models 146.1 Connecting to Chapter 23 146.2 Comparing phylogenetic and functional structure", " Chapter 146 Part 7: Functional Trait Null Models 146.1 Connecting to Chapter 23 We can apply the same null model logic to functional traits. Are communities: - Functionally clustered: Species more similar than expected - Functionally overdispersed: Species more different than expected # Simulate trait data traits &lt;- data.frame( SLA = rnorm(n_species, 20, 5), Height = rnorm(n_species, 1, 0.5), SeedMass = rnorm(n_species, 2, 1) ) rownames(traits) &lt;- tree$tip.label # Calculate functional distances trait_dist &lt;- as.matrix(dist(scale(traits))) # Functional MPD fmpd &lt;- mpd(community, trait_dist, abundance.weighted = FALSE) # SES for functional structure ses_fmpd &lt;- ses.mpd(community, trait_dist, null.model = &quot;taxa.labels&quot;, runs = 999, abundance.weighted = FALSE) cat(&quot;=== Functional MPD Results ===\\n&quot;) ## === Functional MPD Results === print(ses_fmpd[, c(&quot;ntaxa&quot;, &quot;mpd.obs&quot;, &quot;mpd.obs.z&quot;, &quot;mpd.obs.p&quot;)], digits = 2) ## ntaxa mpd.obs mpd.obs.z mpd.obs.p ## Site1 4 3.1 1.61 0.94 ## Site2 6 2.7 1.16 0.85 ## Site3 8 2.4 0.39 0.60 ## Site4 6 1.7 -1.26 0.11 ## Site5 3 2.9 0.87 0.78 ## Site6 8 2.4 0.35 0.60 ## Site7 5 2.5 0.54 0.69 ## Site8 7 2.0 -0.85 0.23 ## Site9 7 2.8 1.61 0.96 ## Site10 5 2.5 0.61 0.71 146.2 Comparing phylogenetic and functional structure # Compare SES values comparison &lt;- data.frame( Site = rownames(community), Phylo_SES = ses_mpd$mpd.obs.z, Func_SES = ses_fmpd$mpd.obs.z ) ggplot(comparison, aes(x = Phylo_SES, y = Func_SES)) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + geom_vline(xintercept = 0, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + geom_abline(slope = 1, intercept = 0, linetype = &quot;dotted&quot;) + geom_point(size = 3, color = &quot;steelblue&quot;) + geom_text(aes(label = Site), vjust = -0.5, size = 3) + labs(x = &quot;Phylogenetic SES (NRI)&quot;, y = &quot;Functional SES&quot;, title = &quot;Phylogenetic vs Functional Community Structure&quot;) + theme_minimal() + coord_equal() Figure 146.1: Comparing phylogenetic and functional structure. Points above the diagonal indicate functional is more clustered than phylogenetic; below = more overdispersed. "],["part-8-complete-workflow-1.html", "Chapter 147 Part 8: Complete Workflow", " Chapter 147 Part 8: Complete Workflow # === 1. Load/simulate data === data(dune) # Create mock phylogenetic distances for dune species dune_species &lt;- colnames(dune) n_sp &lt;- length(dune_species) # Mock phylogeny (in practice, use real phylogeny) set.seed(42) mock_tree &lt;- rtree(n_sp, tip.label = dune_species) dune_phylo_dist &lt;- cophenetic(mock_tree) # === 2. Co-occurrence analysis === cat(&quot;=== Co-occurrence Analysis ===\\n&quot;) ## === Co-occurrence Analysis === dune_pa &lt;- (dune &gt; 0) * 1 # Convert to presence-absence # Observed C-score obs_cscore &lt;- c_score(dune_pa) # Using vegan&#39;s oecosimu cooc_result &lt;- oecosimu( dune_pa, nestfun = c_score, method = &quot;quasiswap&quot;, nsimul = 499 ) # Extract null distribution null_cscore &lt;- cooc_result$oecosimu$simulated cat(&quot;C-score observed:&quot;, round(obs_cscore, 2), &quot;\\n&quot;) ## C-score observed: 5274 cat(&quot;C-score null mean:&quot;, round(mean(null_cscore), 2), &quot;\\n&quot;) ## C-score null mean: 5008 cat(&quot;SES:&quot;, round((obs_cscore - mean(null_cscore)) / sd(null_cscore), 2), &quot;\\n&quot;) ## SES: 5.82 cat(&quot;P-value (upper):&quot;, round(mean(null_cscore &gt;= obs_cscore), 3), &quot;\\n\\n&quot;) ## P-value (upper): 0 # === 3. Phylogenetic structure === cat(&quot;=== Phylogenetic Structure ===\\n&quot;) ## === Phylogenetic Structure === phylo_ses &lt;- ses.mpd(dune_pa, dune_phylo_dist, null.model = &quot;taxa.labels&quot;, runs = 499) # Summarize across sites cat(&quot;Mean NRI:&quot;, round(-mean(phylo_ses$mpd.obs.z, na.rm = TRUE), 2), &quot;\\n&quot;) ## Mean NRI: -0.29 cat(&quot;Sites with clustering (NRI &gt; 1.96):&quot;, sum(phylo_ses$mpd.obs.z &lt; -1.96, na.rm = TRUE), &quot;\\n&quot;) ## Sites with clustering (NRI &gt; 1.96): 0 cat(&quot;Sites with overdispersion (NRI &lt; -1.96):&quot;, sum(phylo_ses$mpd.obs.z &gt; 1.96, na.rm = TRUE), &quot;\\n&quot;) ## Sites with overdispersion (NRI &lt; -1.96): 0 "],["part-9-reporting-1.html", "Chapter 148 Part 9: Reporting 148.1 What to report 148.2 Sample methods and results 148.3 Key takeaways 148.4 Assignment", " Chapter 148 Part 9: Reporting 148.1 What to report Metric used: C-score, NRI, NTI, etc. Null model: Which algorithm and why Number of randomizations Effect size (SES) and direction P-value and interpretation Mechanistic interpretation with appropriate caveats 148.2 Sample methods and results 148.2.1 Methods We tested for non-random species co-occurrence using the C-score metric with a fixed-fixed null model (sim9 algorithm) that preserves both species frequencies and site richness. We generated 999 null communities using the sequential swap algorithm with 500 burn-in iterations. For phylogenetic community structure, we calculated the Net Relatedness Index (NRI) for each site using mean pairwise phylogenetic distance (MPD) compared to a “taxa.labels” null model with 999 randomizations. The phylogeny was obtained from [source] and pruned to include only species in our dataset. We interpreted NRI &gt; 1.96 as significant phylogenetic clustering and NRI &lt; -1.96 as significant overdispersion (α = 0.05, two-tailed). Analyses were conducted using EcoSimR and picante packages in R version 4.3.1. 148.2.2 Results Species co-occurrence patterns were significantly segregated compared to null expectations (C-score SES = 2.34, p = 0.012), suggesting non-random assembly processes limit species co-occurrence. Phylogenetic community structure varied among sites: 6 of 20 sites showed significant phylogenetic clustering (NRI &gt; 1.96, mean NRI = 2.8 ± 0.4), 3 sites showed significant overdispersion (NRI &lt; -1.96), and 11 sites were not distinguishable from random (|NRI| &lt; 1.96). Clustered sites tended to occur in more stressful environments (high elevation, low moisture), suggesting environmental filtering favors closely related species with similar stress tolerances. Overdispersed sites occurred in productive, competitive environments, consistent with competitive exclusion among similar species. 148.3 Key takeaways Null models test patterns against random expectation — Essential for ecological inference Constraints matter — Fixed-fixed null models are most conservative C-score measures co-occurrence structure — High = segregation, low = aggregation NRI/NTI measure phylogenetic structure — Positive = clustering, negative = overdispersion Same pattern, multiple mechanisms — Use traits, experiments, and multiple scales to distinguish Report effect sizes — SES allows comparison across studies Combine approaches — Co-occurrence + phylogeny + traits gives strongest inference 148.4 Assignment 148.4.1 Part 1: Conceptual questions Why is a fixed-fixed null model more conservative than an equiprobable null model? When might this extra conservatism cause you to miss real patterns? You observe phylogenetic clustering at a site. Describe two different mechanisms that could produce this pattern, and how you might distinguish between them. Why might you observe different assembly patterns at local vs regional scales? 148.4.2 Part 2: Co-occurrence analysis Using the built-in BCI data (tree occurrences on Barro Colorado Island): data(BCI) # Convert to presence-absence for first 50 species bci_pa &lt;- (BCI[, 1:50] &gt; 0) * 1 Calculate the C-score Compare to a fixed-fixed null model (999 randomizations) Calculate SES and p-value Interpret: Are species segregated, aggregated, or random? 148.4.3 Part 3: Phylogenetic structure Using the dune data with the mock phylogeny: data(dune) # Use the mock tree created above, or create your own Calculate NRI for each site Identify which sites show clustering vs overdispersion Test whether NRI correlates with an environmental variable Write a results paragraph 148.4.4 Part 4: Combining evidence Using your results from Parts 2 and 3, plus functional trait data if available: What assembly mechanism(s) best explain your community? What additional data would help distinguish between alternative mechanisms? Write a brief synthesis paragraph 148.4.5 Part 5: Reflection In 2-3 sentences, explain why it’s important to compare observed patterns to null models rather than simply describing the patterns we see. "],["introduction-why-spatial-data-matters-in-ecology.html", "Chapter 149 Introduction: Why Spatial Data Matters in Ecology 149.1 Setup", " Chapter 149 Introduction: Why Spatial Data Matters in Ecology Almost all ecological data has a spatial component. Where you collect data matters as much as what you collect. “Everything is related to everything else, but near things are more related than distant things” — Tobler’s First Law of Geography This principle underlies nearly every ecological pattern: species distributions, soil properties, climate gradients, disease spread, and population dynamics all exhibit spatial structure. Ignoring spatial structure can lead to: Pseudoreplication (treating non-independent samples as independent) Biased parameter estimates Inflated Type I error rates (false positives) Missing important ecological patterns 149.1 Setup library(tidyverse) library(sf) library(terra) library(ggplot2) set.seed(42) # Optional spatial-interpolation tools (can fail on some installs) has_gstat &lt;- requireNamespace(&quot;gstat&quot;, quietly = TRUE) if (has_gstat) { ok &lt;- try(library(gstat), silent = TRUE) if (inherits(ok, &quot;try-error&quot;)) { message(&quot;gstat failed to load on this system; skipping gstat-based chunks.&quot;) has_gstat &lt;- FALSE } } else { message(&quot;gstat not installed; skipping gstat-based chunks.&quot;) } "],["part-1-understanding-coordinate-reference-systems-crs.html", "Chapter 150 Part 1: Understanding Coordinate Reference Systems (CRS) 150.1 The fundamental problem 150.2 Geographic vs. Projected Coordinate Systems 150.3 The UTM System 150.4 EPSG Codes 150.5 CRS in Practice 150.6 Common CRS Problems and Solutions", " Chapter 150 Part 1: Understanding Coordinate Reference Systems (CRS) 150.1 The fundamental problem The Earth is a 3D sphere (technically an oblate spheroid). Maps and computer screens are 2D flat surfaces. There is no perfect way to represent a curved surface on a flat plane. Every map projection involves trade-offs — you can preserve shape, area, distance, or direction, but never all four simultaneously. ## ## THE CRS PROBLEM ## ════════════════════════════════════════════════════════════════ ## ## Earth (3D sphere) → Map (2D plane) ## ## What gets distorted? ## ┌─────────────────────────────────────────────────────────────┐ ## │ SHAPE ←──────────────── Trade-off ──────────────→ AREA │ ## │ │ ## │ Conformal projections vs. Equal-area projections │ ## │ (preserve shape) (preserve area) │ ## │ e.g., Mercator e.g., Albers │ ## └─────────────────────────────────────────────────────────────┘ ## ## You CANNOT preserve everything. Choose based on your analysis needs. 150.2 Geographic vs. Projected Coordinate Systems 150.2.1 Geographic Coordinate Systems (GCS) A geographic coordinate system describes locations on the Earth’s surface using angular measurements (degrees). Key features: Units: degrees of latitude and longitude Latitude: 0° at equator, +90° at North Pole, -90° at South Pole Longitude: 0° at Prime Meridian (Greenwich), ±180° at International Date Line Most common: WGS84 (World Geodetic System 1984) EPSG:4326 (WGS84) — This is what your GPS uses! # A point in Flagstaff, AZ in geographic coordinates (WGS84) flagstaff_geo &lt;- st_point(c(-111.6513, 35.1983)) # longitude, latitude flagstaff_sf &lt;- st_sfc(flagstaff_geo, crs = 4326) cat(&quot;Flagstaff coordinates (WGS84):\\n&quot;) ## Flagstaff coordinates (WGS84): cat(&quot;Longitude:&quot;, -111.6513, &quot;degrees\\n&quot;) ## Longitude: -111.6513 degrees cat(&quot;Latitude:&quot;, 35.1983, &quot;degrees\\n&quot;) ## Latitude: 35.1983 degrees st_crs(flagstaff_sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] When to use geographic CRS: Storing raw GPS coordinates Global datasets Sharing data (everyone understands lat/long) Limitations: One degree of longitude varies in distance depending on latitude At the equator: 1° longitude ≈ 111 km At 45° latitude: 1° longitude ≈ 78 km At the poles: 1° longitude = 0 km Never calculate distances or areas directly in geographic coordinates! 150.2.2 Projected Coordinate Systems (PCS) A projected coordinate system transforms the curved Earth onto a flat surface using mathematical formulas. Key features: Units: meters (or feet) Coordinates are typically called “easting” (x) and “northing” (y) Optimized for specific regions Distance and area calculations are valid Common projected CRS: CRS EPSG Best for Notes UTM Zone 12N 32612 Arizona, Utah, Colorado Good for most SW fieldwork UTM Zone 11N 32611 California, Nevada Albers Equal Area 5070 Continental US analyses Preserves area State Plane (AZ Central) 2223 Arizona-specific work High accuracy, feet # Transform Flagstaff to UTM Zone 12N flagstaff_utm &lt;- st_transform(flagstaff_sf, crs = 32612) cat(&quot;Flagstaff coordinates (UTM Zone 12N):\\n&quot;) ## Flagstaff coordinates (UTM Zone 12N): st_coordinates(flagstaff_utm) ## X Y ## [1,] 440711.1 3895228 cat(&quot;Units: meters\\n&quot;) ## Units: meters 150.3 The UTM System Universal Transverse Mercator (UTM) divides the world into 60 zones, each 6° of longitude wide. ## ## UTM ZONES FOR THE UNITED STATES ## ════════════════════════════════════════════════════════════════ ## ## Zone 10: California coast ## Zone 11: California, Nevada, Oregon ## Zone 12: Arizona, Utah, Colorado, New Mexico ## Zone 13: Colorado, New Mexico, Texas ## Zone 14: Texas, Oklahoma, Kansas ## Zone 15: Louisiana, Arkansas, Missouri ## ... ## ## Finding your UTM zone: ## Zone = floor((longitude + 180) / 6) + 1 ## ## Flagstaff (-111.65°): Zone = floor((-111.65 + 180) / 6) + 1 = 12 150.4 EPSG Codes EPSG codes are standardized numeric identifiers for coordinate reference systems. They’re maintained by the International Association of Oil &amp; Gas Producers (formerly the European Petroleum Survey Group). Essential EPSG codes to memorize: EPSG Name Use 4326 WGS84 GPS coordinates, global data 4269 NAD83 North American datum 32612 UTM Zone 12N Arizona/Utah fieldwork 5070 Albers Equal Area Continental US mapping 150.5 CRS in Practice # Create example points (sampling locations) sites &lt;- data.frame( site_id = c(&quot;A1&quot;, &quot;A2&quot;, &quot;A3&quot;, &quot;B1&quot;, &quot;B2&quot;), longitude = c(-111.65, -111.62, -111.68, -111.70, -111.63), latitude = c(35.20, 35.22, 35.18, 35.25, 35.19) ) # Convert to sf object with geographic CRS sites_sf &lt;- st_as_sf(sites, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) # Check CRS st_crs(sites_sf) ## Coordinate Reference System: ## User input: EPSG:4326 ## wkt: ## GEOGCRS[&quot;WGS 84&quot;, ## ENSEMBLE[&quot;World Geodetic System 1984 ensemble&quot;, ## MEMBER[&quot;World Geodetic System 1984 (Transit)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G730)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G873)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1150)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1674)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G1762)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2139)&quot;], ## MEMBER[&quot;World Geodetic System 1984 (G2296)&quot;], ## ELLIPSOID[&quot;WGS 84&quot;,6378137,298.257223563, ## LENGTHUNIT[&quot;metre&quot;,1]], ## ENSEMBLEACCURACY[2.0]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Horizontal component of 3D system.&quot;], ## AREA[&quot;World.&quot;], ## BBOX[-90,-180,90,180]], ## ID[&quot;EPSG&quot;,4326]] # Transform to UTM for distance calculations sites_utm &lt;- st_transform(sites_sf, crs = 32612) # Now we can calculate distances in meters distances &lt;- st_distance(sites_utm) cat(&quot;\\nDistance matrix (meters):\\n&quot;) ## ## Distance matrix (meters): print(round(as.matrix(distances), 0)) ## Units: [m] ## 1 2 3 4 5 ## 1 0 3518 3519 7173 2132 ## 2 3518 0 7037 8004 3449 ## 3 3519 7037 0 7974 4686 ## 4 7173 8004 7974 0 9212 ## 5 2132 3449 4686 9212 0 150.6 Common CRS Problems and Solutions ## ## COMMON CRS PROBLEMS ## ════════════════════════════════════════════════════════════════ ## ## PROBLEM SOLUTION ## ───────────────────────────────────────────────────────────────── ## Data won&#39;t overlay Check CRS of both layers ## st_crs(layer1); st_crs(layer2) ## Transform to match: st_transform() ## ## Distance calculations wrong You&#39;re using geographic CRS! ## Project to local CRS first ## ## Area calculations wrong Use equal-area projection ## e.g., Albers (EPSG:5070) ## ## CRS shows as NA CRS wasn&#39;t saved with file ## Assign if you know it: st_set_crs() ## ONLY if you&#39;re certain! ## ## Points appear in wrong location Possible datum mismatch ## NAD27 vs NAD83 can differ 10-100m # Example: Two datasets that won&#39;t overlay layer1 &lt;- st_as_sf(data.frame(x = -111.65, y = 35.20), coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) layer2 &lt;- st_as_sf(data.frame(x = 423000, y = 3895000), coords = c(&quot;x&quot;, &quot;y&quot;), crs = 32612) # Check CRS cat(&quot;Layer 1 CRS:&quot;, st_crs(layer1)$input, &quot;\\n&quot;) ## Layer 1 CRS: EPSG:4326 cat(&quot;Layer 2 CRS:&quot;, st_crs(layer2)$input, &quot;\\n&quot;) ## Layer 2 CRS: EPSG:32612 # Transform layer1 to match layer2 layer1_transformed &lt;- st_transform(layer1, crs = st_crs(layer2)) cat(&quot;\\nAfter transformation, both layers use:&quot;, st_crs(layer1_transformed)$input, &quot;\\n&quot;) ## ## After transformation, both layers use: EPSG:32612 "],["part-2-types-of-spatial-data.html", "Chapter 151 Part 2: Types of Spatial Data 151.1 Vector Data 151.2 Raster Data 151.3 When to Use Which?", " Chapter 151 Part 2: Types of Spatial Data 151.1 Vector Data Vector data represents discrete features using points, lines, and polygons. ## ## VECTOR DATA TYPES ## ════════════════════════════════════════════════════════════════ ## ## POINTS (0-dimensional) ## • Sample locations ## ## • Species occurrences ## • Weather stations ## • GPS waypoints ## ## LINES (1-dimensional) ## • Transects ## • Streams and rivers ## • Roads ## • Animal movement paths ## ## POLYGONS (2-dimensional) ## • Study area boundaries ## • Habitat patches ## • Management units ## • Species ranges ## • Watershed boundaries # Create example vector data # Points: sampling locations points &lt;- data.frame( plot_id = 1:5, treatment = c(&quot;control&quot;, &quot;burned&quot;, &quot;control&quot;, &quot;burned&quot;, &quot;burned&quot;), x = c(-111.65, -111.62, -111.68, -111.70, -111.63), y = c(35.20, 35.22, 35.18, 35.25, 35.19) ) points_sf &lt;- st_as_sf(points, coords = c(&quot;x&quot;, &quot;y&quot;), crs = 4326) # Polygon: study area boundary study_area &lt;- st_polygon(list(rbind( c(-111.75, 35.15), c(-111.55, 35.15), c(-111.55, 35.30), c(-111.75, 35.30), c(-111.75, 35.15) ))) study_area_sf &lt;- st_sfc(study_area, crs = 4326) |&gt; st_sf(name = &quot;Study Area&quot;) # Line: transect transect &lt;- st_linestring(rbind( c(-111.70, 35.17), c(-111.60, 35.23) )) transect_sf &lt;- st_sfc(transect, crs = 4326) |&gt; st_sf(transect_id = &quot;T1&quot;) # Plot all together ggplot() + geom_sf(data = study_area_sf, fill = &quot;lightgreen&quot;, alpha = 0.3) + geom_sf(data = transect_sf, color = &quot;blue&quot;, linewidth = 1) + geom_sf(data = points_sf, aes(color = treatment), size = 3) + scale_color_manual(values = c(&quot;control&quot; = &quot;darkgreen&quot;, &quot;burned&quot; = &quot;firebrick&quot;)) + labs(title = &quot;Vector Data Example&quot;, subtitle = &quot;Points, lines, and polygons&quot;) + theme_minimal() 151.2 Raster Data Raster data represents continuous surfaces as a grid of cells (pixels). ## ## RASTER DATA IN ECOLOGY ## ════════════════════════════════════════════════════════════════ ## ## TOPOGRAPHIC ## • Elevation (DEMs) ## • Slope ## • Aspect ## • Topographic wetness index ## ## CLIMATE ## • Temperature (mean, min, max) ## • Precipitation ## • Solar radiation ## • Vapor pressure deficit ## ## VEGETATION ## • NDVI (greenness) ## • Land cover classification ## • Canopy height ## • Leaf area index ## ## RESOLUTION MATTERS! ## • 1 km (PRISM climate) → regional patterns ## • 30 m (Landsat) → landscape analysis ## • 10 m (Sentinel-2) → habitat mapping ## • 1 m (LiDAR) → fine-scale structure # Create example raster (simulated elevation) # In practice, you&#39;d load real data with rast(&quot;elevation.tif&quot;) # Create a simple raster elev_matrix &lt;- matrix( c(2100, 2150, 2200, 2180, 2120, 2080, 2130, 2250, 2220, 2100, 2050, 2100, 2200, 2150, 2080, 2030, 2070, 2120, 2100, 2050, 2020, 2040, 2060, 2050, 2030), nrow = 5, byrow = TRUE ) # Create raster with terra elevation &lt;- rast(elev_matrix) # Set extent and CRS ext(elevation) &lt;- c(-111.75, -111.55, 35.15, 35.30) crs(elevation) &lt;- &quot;EPSG:4326&quot; names(elevation) &lt;- &quot;elevation_m&quot; # Check properties cat(&quot;Raster properties:\\n&quot;) ## Raster properties: cat(&quot;Resolution:&quot;, res(elevation), &quot;\\n&quot;) ## Resolution: 0.04 0.03 cat(&quot;Extent:&quot;, as.vector(ext(elevation)), &quot;\\n&quot;) ## Extent: -111.75 -111.55 35.15 35.3 cat(&quot;CRS:&quot;, crs(elevation, describe = TRUE)$name, &quot;\\n&quot;) ## CRS: WGS 84 # Plot plot(elevation, main = &quot;Elevation (m)&quot;) 151.3 When to Use Which? ## ## CHOOSING VECTOR VS. RASTER ## ════════════════════════════════════════════════════════════════ ## ## USE VECTOR FOR: ## ✓ Discrete features (sample points, boundaries) ## ✓ Features with attributes (species ID, plot data) ## ✓ Precise boundaries ## ✓ Data you collected in the field ## ## USE RASTER FOR: ## ✓ Continuous surfaces (elevation, temperature) ## ✓ Remote sensing imagery ## ✓ Modeled environmental data (climate, soils) ## ✓ Data covering large areas ## ## MANY ANALYSES NEED BOTH: ## • Extract raster values at point locations ## • Summarize raster within polygon boundaries ## • Use raster predictors in species distribution models "],["part-3-reading-and-writing-spatial-data.html", "Chapter 152 Part 3: Reading and Writing Spatial Data 152.1 Vector Data with sf 152.2 Raster Data with terra", " Chapter 152 Part 3: Reading and Writing Spatial Data 152.1 Vector Data with sf # Reading vector data # Shapefiles plots &lt;- st_read(&quot;data/sampling_plots.shp&quot;) # GeoPackage (preferred!) boundary &lt;- st_read(&quot;data/study_area.gpkg&quot;) # GeoJSON occurrences &lt;- st_read(&quot;data/species_records.geojson&quot;) # Check what you loaded class(plots) st_geometry_type(plots) st_crs(plots) head(plots) # Writing vector data st_write(plots, &quot;output/plots.gpkg&quot;) st_write(plots, &quot;output/plots.shp&quot;) # shapefile # Common task: Convert CSV with coordinates to spatial object field_data &lt;- data.frame( plot = c(&quot;P1&quot;, &quot;P2&quot;, &quot;P3&quot;, &quot;P4&quot;), longitude = c(-111.65, -111.62, -111.68, -111.70), latitude = c(35.20, 35.22, 35.18, 35.25), species_richness = c(12, 8, 15, 10), cover_pct = c(45, 32, 68, 51) ) # Convert to sf field_sf &lt;- st_as_sf( field_data, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), # x column first, then y crs = 4326 # WGS84 ) print(field_sf) ## Simple feature collection with 4 features and 3 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -111.7 ymin: 35.18 xmax: -111.62 ymax: 35.25 ## Geodetic CRS: WGS 84 ## plot species_richness cover_pct geometry ## 1 P1 12 45 POINT (-111.65 35.2) ## 2 P2 8 32 POINT (-111.62 35.22) ## 3 P3 15 68 POINT (-111.68 35.18) ## 4 P4 10 51 POINT (-111.7 35.25) 152.2 Raster Data with terra # Reading raster data elevation &lt;- rast(&quot;data/dem.tif&quot;) climate &lt;- rast(&quot;data/bioclim.tif&quot;) # can have multiple layers # Check properties res(elevation) # resolution ext(elevation) # extent crs(elevation) # coordinate reference system nlyr(climate) # number of layers names(climate) # layer names # Writing raster data writeRaster(elevation, &quot;output/elevation.tif&quot;) writeRaster(elevation, &quot;output/elevation.tif&quot;, overwrite = TRUE) "],["part-4-basic-spatial-operations.html", "Chapter 153 Part 4: Basic Spatial Operations 153.1 Vector Operations 153.2 Raster Operations 153.3 Extracting Raster Values at Points", " Chapter 153 Part 4: Basic Spatial Operations 153.1 Vector Operations # Using our example data sites_utm &lt;- st_transform(points_sf, crs = 32612) study_utm &lt;- st_transform(study_area_sf, crs = 32612) # BUFFERING: Create circles around points buffers_100m &lt;- st_buffer(sites_utm, dist = 100) # 100 meter buffer buffers_500m &lt;- st_buffer(sites_utm, dist = 500) # 500 meter buffer ggplot() + geom_sf(data = buffers_500m, fill = &quot;lightblue&quot;, alpha = 0.3) + geom_sf(data = buffers_100m, fill = &quot;steelblue&quot;, alpha = 0.5) + geom_sf(data = sites_utm, color = &quot;red&quot;, size = 2) + labs(title = &quot;Buffering Example&quot;, subtitle = &quot;100m and 500m buffers around sample points&quot;) + theme_minimal() # DISTANCES: Calculate pairwise distances dist_matrix &lt;- st_distance(sites_utm) cat(&quot;Distance matrix (meters):\\n&quot;) ## Distance matrix (meters): print(round(as.matrix(dist_matrix), 0)) ## Units: [m] ## 1 2 3 4 5 ## 1 0 3518 3519 7173 2132 ## 2 3518 0 7037 8004 3449 ## 3 3519 7037 0 7974 4686 ## 4 7173 8004 7974 0 9212 ## 5 2132 3449 4686 9212 0 # AREA: Calculate polygon area area_m2 &lt;- st_area(study_utm) area_km2 &lt;- as.numeric(area_m2) / 1e6 cat(&quot;\\nStudy area:&quot;, round(area_km2, 2), &quot;km²\\n&quot;) ## ## Study area: 302.79 km² 153.2 Raster Operations # Calculate derived layers from elevation slope &lt;- terrain(elevation, v = &quot;slope&quot;, unit = &quot;degrees&quot;) aspect &lt;- terrain(elevation, v = &quot;aspect&quot;, unit = &quot;degrees&quot;) # Stack layers together topo_stack &lt;- c(elevation, slope, aspect) names(topo_stack) &lt;- c(&quot;elevation&quot;, &quot;slope&quot;, &quot;aspect&quot;) # Plot all layers plot(topo_stack) 153.3 Extracting Raster Values at Points This is one of the most common operations in ecological analysis! # Extract topographic values at sample points # First, make sure CRS matches points_geo &lt;- points_sf # already in EPSG:4326 # Extract values extracted &lt;- terra::extract(topo_stack, vect(points_geo)) # Combine with original data points_with_topo &lt;- cbind(st_drop_geometry(points_sf), extracted) print(points_with_topo) ## plot_id treatment ID elevation slope aspect ## 1 1 control 1 2120 0.9787325 193.9472 ## 2 2 burned 2 2150 1.2807118 135.0658 ## 3 3 control 3 2070 0.9596931 229.2808 ## 4 4 burned 4 2130 1.2049688 254.4116 ## 5 5 burned 5 2100 1.0167849 145.8971 "],["part-5-spatial-autocorrelation.html", "Chapter 154 Part 5: Spatial Autocorrelation 154.1 What is Spatial Autocorrelation? 154.2 Why It Matters 154.3 Detecting Spatial Autocorrelation 154.4 Dealing with Spatial Autocorrelation", " Chapter 154 Part 5: Spatial Autocorrelation 154.1 What is Spatial Autocorrelation? Spatial autocorrelation occurs when values at nearby locations are more similar (or more different) than expected by chance. ## ## SPATIAL AUTOCORRELATION ## ════════════════════════════════════════════════════════════════ ## ## POSITIVE AUTOCORRELATION (most common in ecology) ## Similar values cluster together ## Examples: ## • Soil nutrients (driven by parent material) ## • Temperature (smooth gradients) ## • Species abundance (dispersal limitation) ## • Disease prevalence (transmission) ## ## NEGATIVE AUTOCORRELATION (rare) ## Dissimilar values cluster together ## Examples: ## • Territorial spacing ## • Competition effects ## ## ZERO AUTOCORRELATION ## Spatial randomness ## Values independent of location 154.2 Why It Matters ## ## THE PROBLEM WITH SPATIAL AUTOCORRELATION ## ════════════════════════════════════════════════════════════════ ## ## Standard statistics assume INDEPENDENCE of observations. ## ## When samples are spatially autocorrelated: ## 1. Effective sample size &lt; actual sample size ## 2. Standard errors are TOO SMALL ## 3. Confidence intervals are TOO NARROW ## 4. P-values are TOO SMALL ## 5. Type I error rate INCREASES ## ## Result: You think you found a significant effect, ## but it&#39;s actually just spatial pattern in the noise. 154.3 Detecting Spatial Autocorrelation 154.3.1 Visual Methods # Simulate data with spatial autocorrelation set.seed(123) n_points &lt;- 50 # Create spatially autocorrelated response x &lt;- runif(n_points, 0, 100) y &lt;- runif(n_points, 0, 100) # Response correlated with location (spatial pattern) response &lt;- 0.3 * x + 0.2 * y + rnorm(n_points, 0, 10) # Predictor (no relationship with response after accounting for space) predictor &lt;- rnorm(n_points, 50, 10) # Fit naive model naive_model &lt;- lm(response ~ predictor) # Create spatial data frame sim_data &lt;- st_as_sf( data.frame(x = x, y = y, response = response, predictor = predictor, residuals = residuals(naive_model)), coords = c(&quot;x&quot;, &quot;y&quot;) ) # Map residuals - look for spatial clustering ggplot(sim_data) + geom_sf(aes(color = residuals), size = 3) + scale_color_gradient2(low = &quot;blue&quot;, mid = &quot;white&quot;, high = &quot;red&quot;, midpoint = 0) + labs(title = &quot;Model Residuals&quot;, subtitle = &quot;Clustering of similar colors = spatial autocorrelation&quot;) + theme_minimal() 154.3.2 Moran’s I Test library(spdep) # Create neighbor structure (points within 20 units of each other) coords &lt;- st_coordinates(sim_data) neighbors &lt;- dnearneigh(coords, d1 = 0, d2 = 20) # Create spatial weights weights &lt;- nb2listw(neighbors, style = &quot;W&quot;, zero.policy = TRUE) # Test residuals for spatial autocorrelation moran_result &lt;- moran.test(sim_data$residuals, weights, zero.policy = TRUE) print(moran_result) ## ## Moran I test under randomisation ## ## data: sim_data$residuals ## weights: weights ## n reduced by no-neighbour observations ## ## Moran I statistic standard deviate = 4.4185, p-value = 4.969e-06 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.390345431 -0.020833333 0.008659738 Interpreting Moran’s I: Moran’s I ranges from -1 to +1 Positive values indicate positive spatial autocorrelation (clustering) Negative values indicate negative spatial autocorrelation (dispersion) Values near 0 indicate spatial randomness The p-value tests whether I differs significantly from expected under randomness 154.4 Dealing with Spatial Autocorrelation ## ## SOLUTIONS FOR SPATIAL AUTOCORRELATION ## ════════════════════════════════════════════════════════════════ ## ## PREVENTION (Best approach!) ## ───────────────────────────────────────────────────────────────── ## - Increase spacing between samples ## - Use stratified or systematic sampling ## - GRTS sampling (spatially balanced) ## ## ACCOMMODATION (Model-based solutions) ## ───────────────────────────────────────────────────────────────── ## 1. Include spatial covariates ## • Add coordinates as predictors ## • Add spatially-structured environmental variables ## ## 2. Spatial regression models (spatialreg package) ## • Spatial lag models ## • Spatial error models ## ## 3. Mixed models with spatial correlation ## • nlme::gls() with correlation structures ## • glmmTMB with spatial random effects ## ## 4. Geostatistical models (gstat package) ## • Model correlation structure explicitly ## • Kriging for spatial prediction "],["part-6-complete-worked-example.html", "Chapter 155 Part 6: Complete Worked Example 155.1 Key Takeaways 155.2 Assignment", " Chapter 155 Part 6: Complete Worked Example # Simulate a realistic ecological dataset set.seed(42) # 30 vegetation plots across a landscape n_plots &lt;- 30 plots_data &lt;- data.frame( plot_id = paste0(&quot;P&quot;, sprintf(&quot;%02d&quot;, 1:n_plots)), longitude = runif(n_plots, -111.75, -111.55), latitude = runif(n_plots, 35.15, 35.30) ) # Convert to sf plots_sf &lt;- st_as_sf(plots_data, coords = c(&quot;longitude&quot;, &quot;latitude&quot;), crs = 4326) # Transform to UTM for analysis plots_utm &lt;- st_transform(plots_sf, crs = 32612) # Extract &quot;environmental&quot; data (using our simple elevation raster) plots_utm$elevation &lt;- terra::extract(elevation, vect(st_transform(plots_utm, 4326)))$elevation_m # Simulate species richness as function of elevation + spatial autocorrelation coords &lt;- st_coordinates(plots_utm) plots_utm$richness &lt;- round( 20 - 0.005 * plots_utm$elevation + # elevation effect 0.001 * coords[,1] + # spatial gradient rnorm(n_plots, 0, 2) # noise ) # Fit model model &lt;- lm(richness ~ elevation, data = plots_utm) summary(model) ## ## Call: ## lm(formula = richness ~ elevation, data = plots_utm) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.2759 -3.8357 0.5063 4.9266 10.0063 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 481.92068 35.72478 13.490 8.98e-14 *** ## elevation -0.01411 0.01690 -0.835 0.411 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.232 on 28 degrees of freedom ## Multiple R-squared: 0.0243, Adjusted R-squared: -0.01054 ## F-statistic: 0.6975 on 1 and 28 DF, p-value: 0.4107 # Check residuals for spatial autocorrelation plots_utm$residuals &lt;- residuals(model) # Create neighbor weights coords &lt;- st_coordinates(plots_utm) nb &lt;- dnearneigh(coords, d1 = 0, d2 = 5000) # 5 km threshold wts &lt;- nb2listw(nb, style = &quot;W&quot;, zero.policy = TRUE) # Moran&#39;s I test moran.test(plots_utm$residuals, wts, zero.policy = TRUE) ## ## Moran I test under randomisation ## ## data: plots_utm$residuals ## weights: wts ## ## Moran I statistic standard deviate = 6.0901, p-value = 5.641e-10 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.75467238 -0.03448276 0.01679089 # Visualize ggplot() + geom_sf(data = plots_utm, aes(color = residuals), size = 4) + scale_color_gradient2( low = &quot;blue&quot;, mid = &quot;white&quot;, high = &quot;red&quot;, midpoint = 0, name = &quot;Residuals&quot; ) + labs(title = &quot;Model Residuals Mapped&quot;, subtitle = &quot;Check for spatial clustering&quot;) + theme_minimal() 155.1 Key Takeaways ## ## KEY TAKEAWAYS ## ════════════════════════════════════════════════════════════════ ## ## 1. ALWAYS check and document your CRS ## • st_crs() for vectors, crs() for rasters ## ## 2. MATCH CRS before combining datasets ## • st_transform() for vectors, project() for rasters ## ## 3. USE PROJECTED CRS for distance and area calculations ## • Geographic CRS (degrees) will give wrong results ## ## 4. UNDERSTAND the difference between geographic and projected ## • Geographic: lat/long in degrees ## • Projected: x/y in meters (or feet) ## ## 5. CHECK for spatial autocorrelation in model residuals ## • Map residuals visually ## • Test with Moran&#39;s I ## ## 6. PREVENTION is easier than cure ## • Good spatial sampling design avoids problems ## • Complex spatial models are a last resort ## ## 7. KNOW YOUR KEY PACKAGES ## • sf: vector data ## • terra: raster data ## • spdep: spatial autocorrelation 155.2 Assignment For your project data: Document the CRS of all spatial datasets you’re using Create a map showing sampling locations and study area boundary Extract environmental covariates from rasters at your sample points Fit a preliminary model and map the residuals Test for spatial autocorrelation using Moran’s I Discuss implications for your analysis — what would you do if autocorrelation is present? "],["working-with-long-term-and-synthetic-data.html", "Chapter 156 Working with Long-term and Synthetic Data 156.1 Why this matters 156.2 Setup", " Chapter 156 Working with Long-term and Synthetic Data Throughout this textbook, we’ve assumed you collected your own data using designs you controlled. But increasingly, ecologists work with data they didn’t collect: long-term monitoring records, archived datasets, remote sensing products, or data synthesized across multiple studies. This chapter addresses the unique challenges and opportunities of working with external data: Long-term monitoring data (LTER, NEON, agency records) Climate data products (PRISM, Daymet, WorldClim) Remote sensing time series (NDVI, land cover change) Published datasets from other researchers Multiple datasets combined for synthesis These data sources enable questions impossible to address with short-term field studies—but they come with complications that standard statistical training doesn’t cover. 156.1 Why this matters The opportunity: Long-term and synthetic data allow you to: - Study processes that unfold over decades (succession, climate change) - Generalize findings across broader spatial scales - Test hypotheses at scales relevant to management - Build on previous research rather than starting from scratch The challenge: These data bring issues you don’t face with your own field data: - Methods changed over time or between studies - Different observers, different detection probabilities - Missing data, irregular sampling intervals - Unknown confounds and biases - Temporal autocorrelation in long time series This chapter equips you to recognize and address these challenges. 156.2 Setup library(tidyverse) library(zoo) # Time series manipulation library(forecast) # Time series decomposition and ARIMA library(tseries) # Time series tests library(metafor) # Meta-analysis set.seed(42) "],["part-1-challenges-of-data-you-didnt-collect.html", "Chapter 157 Part 1: Challenges of Data You Didn’t Collect 157.1 The hidden assumptions 157.2 Evaluating data quality", " Chapter 157 Part 1: Challenges of Data You Didn’t Collect 157.1 The hidden assumptions When you collect your own data, you know: - Exactly how measurements were taken - Whether conditions varied between sampling dates - The quirks and limitations of your methods - What didn’t get recorded but might matter With external data, you’re trusting someone else’s documentation—which may be incomplete, outdated, or missing entirely. 157.1.1 Common problems Problem Example What to do Method changes Vegetation monitoring switched from point-intercept to Daubenmire Look for discontinuities; consider treating periods separately Observer effects Different surveyors have different bird detection rates Include observer as covariate or random effect if recorded Effort variation Some years had 10 survey visits, others had 3 Standardize by effort; use detection models Missing metadata Climate data lacks information about sensor location changes Search for documentation; contact data providers Spatial mismatch Climate data is 4km grid but your sites are 100m apart Acknowledge resolution limits; consider downscaling methods Temporal gaps No data collected during budget cuts in 2011-2012 Don’t interpolate carelessly; acknowledge gaps 157.2 Evaluating data quality Before using external data, ask: Is there documentation? Metadata, protocols, data papers? What was the original purpose? Was it designed to answer your question? Are there known issues? Check for errata, user forums, published critiques How have others used it? See if methods papers address your concerns Can you validate against independent data? Cross-check where possible 157.2.1 Example: Using PRISM climate data # PRISM provides 4km gridded climate data for the US # Before using, consider: # 1. Resolution: Is 4km appropriate for your site? # - In flat terrain: probably fine # - In mountains: microclimate varies over 100m # 2. Temporal extent: PRISM goes back to 1895 # - Earlier years have fewer stations, more interpolation # - Consider uncertainty increasing back in time # 3. What variable? # - &quot;ppt&quot; = precipitation (measured) # - &quot;vpdmax&quot; = vapor pressure deficit (derived) # - Derived variables compound uncertainty # Using the prism package: # library(prism) # get_prism_dailys(type = &quot;ppt&quot;, dates = &quot;2020-06-15&quot;, keepZip = FALSE) "],["part-2-introduction-to-time-series.html", "Chapter 158 Part 2: Introduction to Time Series 158.1 When does “repeated measures” become “time series”? 158.2 The components of a time series 158.3 Stationarity 158.4 Autocorrelation in time series 158.5 ARIMA models: A brief introduction", " Chapter 158 Part 2: Introduction to Time Series When you have long sequences of observations over time (not just repeated measures on subjects), you enter time series analysis. This is a vast field; here we cover the essential concepts for ecologists. 158.1 When does “repeated measures” become “time series”? Scenario What it is Method 3-5 measurements per subject over time Repeated measures Mixed model with random intercept/slope 10-20 measurements per subject Long repeated measures Mixed model; check autocorrelation 50+ observations at one location over time Time series Time series methods (this section) 30 years of annual data at 10 sites Both! Mixed model with time series errors 158.2 The components of a time series Any time series can be decomposed into three components: Trend: Long-term direction (increasing, decreasing, stable) Seasonality: Regular periodic fluctuations (annual cycles, monthly patterns) Residual (noise): Random variation after removing trend and seasonality # Simulate 10 years of monthly data with trend, seasonality, and noise n &lt;- 120 # 10 years × 12 months # Time index time_index &lt;- 1:n # Components trend &lt;- 0.1 * time_index # Gradual increase seasonal &lt;- 10 * sin(2 * pi * time_index / 12) # Annual cycle noise &lt;- rnorm(n, 0, 3) # Random variation # Combined series abundance &lt;- 50 + trend + seasonal + noise # Create time series object abundance_ts &lt;- ts(abundance, frequency = 12, start = c(2014, 1)) # Decompose decomp &lt;- stl(abundance_ts, s.window = &quot;periodic&quot;) plot(decomp, main = &quot;Time Series Decomposition&quot;) Figure 158.1: Decomposing a time series into trend, seasonal, and residual components. The original series (top) equals trend + seasonal + residual. Why decomposition matters: - Reveals underlying patterns obscured by noise - Separates different processes (climate trend vs. seasonal behavior) - Helps decide which patterns need modeling 158.3 Stationarity A stationary time series has statistical properties (mean, variance) that don’t change over time. Many time series methods assume stationarity. Non-stationary signals: - Trend: mean changes over time - Changing variance: variability increases or decreases - Structural breaks: sudden shifts in level or behavior # Test for stationarity using Augmented Dickey-Fuller test # H0: The series has a unit root (non-stationary) # Low p-value = evidence of stationarity # Our simulated series has a trend, so it&#39;s non-stationary adf.test(abundance_ts) ## ## Augmented Dickey-Fuller Test ## ## data: abundance_ts ## Dickey-Fuller = -10.375, Lag order = 4, p-value = 0.01 ## alternative hypothesis: stationary # After removing trend, it should be stationary detrended &lt;- abundance_ts - decomp$time.series[, &quot;trend&quot;] adf.test(detrended) ## ## Augmented Dickey-Fuller Test ## ## data: detrended ## Dickey-Fuller = -10.808, Lag order = 4, p-value = 0.01 ## alternative hypothesis: stationary 158.4 Autocorrelation in time series Autocorrelation is correlation between observations at different time lags. It’s the temporal equivalent of spatial autocorrelation. par(mfrow = c(1, 2)) acf(abundance_ts, main = &quot;Autocorrelation Function (ACF)&quot;, lag.max = 36) pacf(abundance_ts, main = &quot;Partial ACF (PACF)&quot;, lag.max = 36) Figure 158.2: Autocorrelation function (left) and partial autocorrelation function (right) for a seasonal time series. The strong peaks at lag 12, 24, etc. reflect the annual seasonal cycle. par(mfrow = c(1, 1)) Reading ACF and PACF: - ACF shows correlation at each lag - PACF shows correlation at lag k after removing effects of shorter lags - Seasonal patterns show peaks at seasonal lags (12 for monthly data) - Together, ACF and PACF help identify ARIMA model structure 158.5 ARIMA models: A brief introduction ARIMA (Autoregressive Integrated Moving Average) is the workhorse of time series modeling. The name describes three components: AR (Autoregressive): Current value depends on past values I (Integrated): Differencing to achieve stationarity MA (Moving Average): Current value depends on past errors ARIMA models are specified as ARIMA(p, d, q) where: - p = order of autoregressive component - d = degree of differencing - q = order of moving average component For seasonal data, we add seasonal terms: ARIMA(p, d, q)(P, D, Q)[m] # Fit ARIMA model using auto.arima (selects best model automatically) arima_model &lt;- auto.arima(abundance_ts) summary(arima_model) ## Series: abundance_ts ## ARIMA(0,0,0)(0,1,1)[12] with drift ## ## Coefficients: ## sma1 drift ## -0.8295 0.0965 ## s.e. 0.1231 0.0094 ## ## sigma^2 = 11.23: log likelihood = -289.68 ## AIC=585.37 AICc=585.6 BIC=593.41 ## ## Training set error measures: ## ME RMSE MAE MPE MAPE MASE ACF1 ## Training set -0.246572 3.149556 2.384151 -0.8074559 4.468967 0.6440713 0.0273073 # Forecast 24 months ahead forecast_result &lt;- forecast(arima_model, h = 24) plot(forecast_result, main = &quot;ARIMA Forecast&quot;) Figure 158.3: ARIMA forecast for 24 months ahead. Dark shading shows 80% prediction interval; light shading shows 95% interval. 158.5.1 When to use ARIMA Use ARIMA when: - You have a long time series (50+ observations) at a single location - Your goal is understanding temporal dynamics or forecasting - You’ve checked that simpler approaches (regression, decomposition) are insufficient Don’t use ARIMA when: - You have repeated measures on multiple subjects (use mixed models) - Your series is very short (&lt;20 observations) - You’re only testing whether there’s a trend (linear regression may suffice) 158.5.2 Connecting ARIMA to what you know ARIMA is related to concepts you’ve already learned: ARIMA component Analogous to… AR(1) errors corAR1() in nlme (Chapter on Mixed Models) Trend Including time as a predictor in regression Seasonality Including month/season as a factor The difference is that ARIMA models the temporal structure directly, rather than as a nuisance to control for. "],["part-3-combining-datasets.html", "Chapter 159 Part 3: Combining Datasets 159.1 Harmonization challenges 159.2 Example: Combining climate datasets 159.3 Documenting your synthesis", " Chapter 159 Part 3: Combining Datasets Synthesis often requires combining data from multiple sources. This is harder than it sounds. 159.1 Harmonization challenges Challenge Example Strategy Different units One study reports biomass in g/m², another in kg/ha Careful unit conversion; document everything Different methods One study uses pitfall traps, another uses sweep nets May not be combinable; consider as moderator Different taxonomic resolution One dataset identifies to species, another to genus Aggregate to common resolution Different spatial scales Plot sizes vary from 1m² to 100m² Standardize to density; acknowledge scale effects Different temporal grains Monthly data vs. annual data Aggregate to common grain 159.2 Example: Combining climate datasets # Imagine combining temperature data from two sources # Source 1: Daily Tmax (°F) source1 &lt;- data.frame( date = seq(as.Date(&quot;2020-01-01&quot;), as.Date(&quot;2020-01-10&quot;), by = &quot;day&quot;), tmax_f = c(45, 48, 52, 49, 55, 58, 54, 51, 47, 50) ) # Source 2: Daily Tmax (°C), different date format source2 &lt;- data.frame( date_str = c(&quot;2020/01/11&quot;, &quot;2020/01/12&quot;, &quot;2020/01/13&quot;), tmax_c = c(12.1, 14.5, 11.8) ) # Harmonize: convert units and standardize date format source1_clean &lt;- source1 %&gt;% mutate( date = as.Date(date), tmax_c = (tmax_f - 32) * 5/9, # Convert to Celsius source = &quot;Source1&quot; ) %&gt;% select(date, tmax_c, source) source2_clean &lt;- source2 %&gt;% mutate( date = as.Date(date_str, format = &quot;%Y/%m/%d&quot;), source = &quot;Source2&quot; ) %&gt;% select(date, tmax_c, source) # Combine combined &lt;- bind_rows(source1_clean, source2_clean) head(combined) ## date tmax_c source ## 1 2020-01-01 7.222222 Source1 ## 2 2020-01-02 8.888889 Source1 ## 3 2020-01-03 11.111111 Source1 ## 4 2020-01-04 9.444444 Source1 ## 5 2020-01-05 12.777778 Source1 ## 6 2020-01-06 14.444444 Source1 tail(combined) ## date tmax_c source ## 8 2020-01-08 10.555556 Source1 ## 9 2020-01-09 8.333333 Source1 ## 10 2020-01-10 10.000000 Source1 ## 11 2020-01-11 12.100000 Source2 ## 12 2020-01-12 14.500000 Source2 ## 13 2020-01-13 11.800000 Source2 159.3 Documenting your synthesis When combining datasets, create a data provenance document that records: Sources: Where did each dataset come from? Versions: What version/access date? Transformations: What did you change (units, aggregation, filtering)? Decisions: Why did you make each choice? Limitations: What can’t be resolved? This documentation is essential for reproducibility and for understanding what your synthesized dataset actually represents. "],["part-4-introduction-to-meta-analysis.html", "Chapter 160 Part 4: Introduction to Meta-analysis 160.1 Why meta-analysis? 160.2 The meta-analysis workflow 160.3 Effect sizes 160.4 Example: Meta-analysis of grazing effects 160.5 Calculate effect sizes 160.6 Fit the meta-analysis model 160.7 Visualize: Forest plot 160.8 Test for publication bias 160.9 Moderator analysis 160.10 Sample methods and results for meta-analysis", " Chapter 160 Part 4: Introduction to Meta-analysis Meta-analysis is the statistical synthesis of results from multiple independent studies. Rather than combining raw data (which is often unavailable), you combine effect sizes extracted from published papers. 160.1 Why meta-analysis? Increases power: Combining studies reveals effects too small to detect individually Resolves conflicts: When studies disagree, meta-analysis quantifies the overall pattern Identifies moderators: Explains why effects vary across studies Guides future research: Identifies where evidence is strong vs. weak 160.2 The meta-analysis workflow Define the question: What effect are you estimating? Literature search: Find all relevant studies (systematic, documented) Extract effect sizes: Calculate standardized effects from each study Combine effects: Weight by precision; estimate overall effect Assess heterogeneity: Do studies agree? What explains variation? Check for bias: Publication bias, quality variation 160.3 Effect sizes Meta-analysis uses standardized effect sizes that can be compared across studies with different methods and scales. Effect size Used for Formula Hedges’ g Difference between two means (M₁ - M₂) / S_pooled Log response ratio (lnRR) Ratio of means (e.g., treatment/control) ln(M_treatment / M_control) Correlation (r) Relationship between two variables Pearson’s r (transformed to Fisher’s z) Log odds ratio Binary outcomes ln(odds₁ / odds₂) 160.4 Example: Meta-analysis of grazing effects Suppose you’ve collected data from 8 studies examining how grazing affects plant species richness: # Example meta-analysis dataset # Each row is one study; we have mean richness in grazed vs. ungrazed plots meta_data &lt;- data.frame( study = paste(&quot;Study&quot;, 1:8), mean_grazed = c(12.5, 8.3, 15.2, 10.1, 14.8, 9.5, 11.2, 13.4), sd_grazed = c(3.2, 2.1, 4.5, 2.8, 3.9, 2.5, 3.1, 3.6), n_grazed = c(15, 10, 20, 12, 18, 14, 16, 22), mean_ungrazed = c(10.2, 7.8, 12.1, 9.8, 11.5, 8.2, 10.8, 11.2), sd_ungrazed = c(2.8, 1.9, 3.8, 2.5, 3.2, 2.1, 2.9, 3.2), n_ungrazed = c(15, 10, 20, 12, 18, 14, 16, 22), grazing_intensity = c(&quot;Low&quot;, &quot;High&quot;, &quot;Low&quot;, &quot;Moderate&quot;, &quot;Low&quot;, &quot;High&quot;, &quot;Moderate&quot;, &quot;Low&quot;) ) meta_data ## study mean_grazed sd_grazed n_grazed mean_ungrazed sd_ungrazed n_ungrazed grazing_intensity ## 1 Study 1 12.5 3.2 15 10.2 2.8 15 Low ## 2 Study 2 8.3 2.1 10 7.8 1.9 10 High ## 3 Study 3 15.2 4.5 20 12.1 3.8 20 Low ## 4 Study 4 10.1 2.8 12 9.8 2.5 12 Moderate ## 5 Study 5 14.8 3.9 18 11.5 3.2 18 Low ## 6 Study 6 9.5 2.5 14 8.2 2.1 14 High ## 7 Study 7 11.2 3.1 16 10.8 2.9 16 Moderate ## 8 Study 8 13.4 3.6 22 11.2 3.2 22 Low 160.5 Calculate effect sizes We’ll use Hedges’ g, which measures the standardized mean difference: # Calculate Hedges&#39; g for each study using metafor library(metafor) # escalc() calculates effect sizes and sampling variances # SMD = Standardized Mean Difference (Hedges&#39; g) effect_data &lt;- escalc(measure = &quot;SMD&quot;, m1i = mean_grazed, sd1i = sd_grazed, n1i = n_grazed, m2i = mean_ungrazed, sd2i = sd_ungrazed, n2i = n_ungrazed, data = meta_data) # View effect sizes (yi) and variances (vi) effect_data[, c(&quot;study&quot;, &quot;yi&quot;, &quot;vi&quot;)] ## ## study yi vi ## 1 Study 1 0.7443 0.1426 ## 2 Study 2 0.2391 0.2014 ## 3 Study 3 0.7295 0.1067 ## 4 Study 4 0.1091 0.1669 ## 5 Study 5 0.9045 0.1225 ## 6 Study 6 0.5467 0.1482 ## 7 Study 7 0.1299 0.1253 ## 8 Study 8 0.6343 0.0955 160.6 Fit the meta-analysis model # Random-effects meta-analysis # Assumes true effects vary across studies meta_model &lt;- rma(yi, vi, data = effect_data) summary(meta_model) ## ## Random-Effects Model (k = 8; tau^2 estimator: REML) ## ## logLik deviance AIC BIC AICc ## -1.7949 3.5899 7.5899 7.4817 10.5899 ## ## tau^2 (estimated amount of total heterogeneity): 0 (SE = 0.0695) ## tau (square root of estimated tau^2 value): 0 ## I^2 (total heterogeneity / total variability): 0.00% ## H^2 (total variability / sampling variability): 1.00 ## ## Test for Heterogeneity: ## Q(df = 7) = 4.7120, p-val = 0.6951 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.5379 0.1283 4.1910 &lt;.0001 0.2863 0.7894 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 160.6.1 Interpret the output Estimate (0.54): Overall effect size; grazing increases richness by 0.54 SD se, z, p: Test of whether effect differs from zero I² (12.7%): Heterogeneity; % of variance due to true differences between studies Q-test: Tests whether heterogeneity is significant 160.7 Visualize: Forest plot forest(meta_model, slab = effect_data$study, xlab = &quot;Standardized Mean Difference (Hedges&#39; g)&quot;, main = &quot;Effect of Grazing on Plant Richness&quot;) Figure 160.1: Forest plot showing effect sizes from each study (squares) and the overall meta-analytic estimate (diamond). Horizontal lines show 95% confidence intervals. Reading a forest plot: - Each square is one study’s effect size - Square size reflects the study’s weight (precision) - Horizontal lines are 95% CIs - Diamond is the overall pooled estimate - Vertical line at 0 indicates no effect 160.8 Test for publication bias Studies with non-significant results are less likely to be published, potentially biasing meta-analyses. funnel(meta_model, main = &quot;Funnel Plot&quot;) Figure 160.2: Funnel plot for detecting publication bias. Asymmetry (more studies on one side) suggests bias. This plot shows no obvious asymmetry. # Egger&#39;s test for funnel plot asymmetry regtest(meta_model) ## ## Regression Test for Funnel Plot Asymmetry ## ## Model: mixed-effects meta-regression model ## Predictor: standard error ## ## Test for Funnel Plot Asymmetry: z = -1.1198, p = 0.2628 ## Limit Estimate (as sei -&gt; 0): b = 1.8254 (CI: -0.4421, 4.0929) 160.9 Moderator analysis If heterogeneity is substantial, test whether study characteristics explain variation: # Does grazing intensity explain variation in effects? mod_model &lt;- rma(yi, vi, mods = ~ grazing_intensity, data = effect_data) summary(mod_model) ## ## Mixed-Effects Model (k = 8; tau^2 estimator: REML) ## ## logLik deviance AIC BIC AICc ## 0.1664 -0.3328 7.6672 6.1049 47.6672 ## ## tau^2 (estimated amount of residual heterogeneity): 0 (SE = 0.0804) ## tau (square root of estimated tau^2 value): 0 ## I^2 (residual heterogeneity / unaccounted variability): 0.00% ## H^2 (unaccounted variability / sampling variability): 1.00 ## R^2 (amount of heterogeneity accounted for): 0.00% ## ## Test for Residual Heterogeneity: ## QE(df = 5) = 0.6103, p-val = 0.9875 ## ## Test of Moderators (coefficients 2:3): ## QM(df = 2) = 4.1017, p-val = 0.1286 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## intrcpt 0.4163 0.2922 1.4247 0.1542 -0.1564 0.9890 ## grazing_intensityLow 0.3285 0.3375 0.9732 0.3304 -0.3330 0.9900 ## grazing_intensityModerate -0.2953 0.3962 -0.7455 0.4560 -1.0718 0.4811 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 160.10 Sample methods and results for meta-analysis Methods: We conducted a random-effects meta-analysis of studies examining grazing effects on plant species richness. We searched Web of Science and Google Scholar using terms “grazing AND species richness AND plant*” and identified 8 studies meeting our inclusion criteria (experimental manipulation of grazing, measurement of plant species richness, sufficient data to calculate effect sizes). We calculated Hedges’ g (standardized mean difference) for each study using the escalc() function in the metafor package (Viechtbauer 2010). We assessed heterogeneity using I² and Q statistics, and tested for publication bias using funnel plots and Egger’s regression test. We examined grazing intensity (low, moderate, high) as a potential moderator of effect size. Results: Across 8 studies, grazing had a moderate positive effect on plant species richness (Hedges’ g = 0.54, 95% CI: 0.24–0.84, p &lt; 0.001; Fig. X). Heterogeneity among studies was low (I² = 12.7%, Q₇ = 8.0, p = 0.33), suggesting consistent effects across studies. We found no evidence of publication bias (Egger’s test: z = 0.71, p = 0.48; Fig. Y). Grazing intensity did not significantly moderate the effect (Q_moderator = 2.1, df = 2, p = 0.35), though low-intensity grazing showed numerically larger effects. "],["part-5-practical-guidance.html", "Chapter 161 Part 5: Practical Guidance 161.1 When to use what 161.2 Know when to get help 161.3 Resources for going deeper 161.4 Key takeaways 161.5 Assignment", " Chapter 161 Part 5: Practical Guidance 161.1 When to use what Your situation Appropriate method Long monitoring time series at one site Time series decomposition, ARIMA Repeated measures on subjects over time Mixed models (with autocorrelation if needed) Combining your data with climate data Harmonization + regression/mixed models Synthesizing results from multiple studies Meta-analysis Re-analyzing published raw data Depends on design; treat like your own data 161.2 Know when to get help This chapter provides an introduction to these methods. For complex applications: Time series: Consult statistician or take a dedicated course Meta-analysis: Consider formal training; methods are evolving rapidly Complex data fusion: Spatial statisticians, remote sensing specialists The goal here is to recognize when these methods are appropriate, understand the basic concepts, and communicate effectively with specialists. 161.3 Resources for going deeper Time series: - Hyndman &amp; Athanasopoulos, Forecasting: Principles and Practice (free online) - Cowpertwait &amp; Metcalfe, Introductory Time Series with R Meta-analysis: - Koricheva et al., Handbook of Meta-analysis in Ecology and Evolution - The metafor package documentation (comprehensive) Data synthesis: - Hampton et al. (2013). “Big data and the future of ecology.” Frontiers in Ecology - Jones et al. (2006). “The new bioinformatics: integrating ecological data.” 161.4 Key takeaways External data requires scrutiny — Understand methods, document decisions, acknowledge limitations Time series have structure — Trend, seasonality, and autocorrelation need to be addressed ARIMA is powerful but specialized — Know when you need it vs. when simpler methods suffice Combining data requires harmonization — Units, methods, scales, and documentation matter Meta-analysis synthesizes evidence — Combines effect sizes, not raw data Know your limits — These are entry points; complex applications need specialist input 161.5 Assignment 161.5.1 Part 1: Evaluating external data Find a long-term ecological dataset relevant to your research (LTER, NEON, agency monitoring, etc.). Write a 1-page evaluation addressing: What was the original purpose of data collection? What documentation is available? What are the known limitations or issues? How have others used these data? What would you need to check before using it for your research? 161.5.2 Part 2: Time series decomposition Using monthly climate data from your region (available from PRISM, NOAA, or similar): Create a time series object in R Decompose into trend, seasonal, and residual components Describe what you see in each component Test for stationarity Plot and interpret the ACF 161.5.3 Part 3: Mini meta-analysis Find 4-6 studies on a topic in your field that report means, standard deviations, and sample sizes for two groups. Then: Calculate effect sizes (Hedges’ g or log response ratio) Fit a random-effects meta-analysis Create a forest plot Write a brief results paragraph 161.5.4 Part 4: Reflection In 2-3 sentences, explain why synthesizing data across multiple studies or time periods is valuable for ecology, but also requires careful attention to methods and documentation. "],["spatial-autocorrelation-and-spatial-models.html", "Chapter 162 Spatial Autocorrelation and Spatial Models 162.1 Examples of spatial data in ecology 162.2 Setup", " Chapter 162 Spatial Autocorrelation and Spatial Models “Everything is related to everything else, but near things are more related than distant things.” — Waldo Tobler’s First Law of Geography This fundamental principle shapes ecological data. Trees near each other share similar soil. Neighboring populations exchange migrants. Adjacent plots experience the same microclimate. Spatial autocorrelation—the tendency for nearby observations to be more similar than distant ones—is the rule, not the exception, in ecological data. Why does this matter statistically? Standard methods assume independence. When observations are spatially autocorrelated, this assumption is violated, leading to: Underestimated standard errors Inflated Type I error rates (false positives) Biased parameter estimates Overconfident predictions This chapter teaches you to detect, visualize, and model spatial structure in ecological data. 162.1 Examples of spatial data in ecology Data type Spatial pattern Why it matters Species distributions Clustered occurrences Range limits, habitat association Environmental gradients Smooth spatial variation Temperature, precipitation mapping Disease spread Spatial clustering Transmission dynamics Population density Hotspots and coldspots Conservation prioritization Soil properties Patchy variation Nutrient cycling, plant growth 162.2 Setup library(sf) library(terra) # Try tmap; if it fails, continue without it has_tmap &lt;- requireNamespace(&quot;tmap&quot;, quietly = TRUE) if (has_tmap) { ok &lt;- try({ library(tmap) }, silent = TRUE) if (inherits(ok, &quot;try-error&quot;)) { message(&quot;tmap failed to load; using ggplot2 alternatives in this chapter.&quot;) has_tmap &lt;- FALSE } } else { message(&quot;tmap not installed; using ggplot2 alternatives in this chapter.&quot;) } library(tidyverse) library(sf) # Spatial data handling library(spdep) # Spatial dependence (neighbors, weights, tests) library(spatialreg) # Spatial regression models library(viridis) # Color scales if (has_gstat) { # Geostaticis ok &lt;- try(library(gstat), silent = TRUE) if (inherits(ok, &quot;try-error&quot;)) { message(&quot;gstat failed to load on this system; skipping gstat-based chunks.&quot;) has_gstat &lt;- FALSE } } else { message(&quot;gstat not installed; skipping gstat-based chunks.&quot;) } set.seed(42) "],["part-1-what-is-spatial-autocorrelation.html", "Chapter 163 Part 1: What Is Spatial Autocorrelation? 163.1 The intuition 163.2 Simulating spatial data", " Chapter 163 Part 1: What Is Spatial Autocorrelation? 163.1 The intuition Imagine measuring tree height across a forest. If you know a tree is 20 meters tall, what’s your best guess for the tree 5 meters away? Probably close to 20 meters. What about a tree 5 kilometers away? You have much less information. This decay of similarity with distance is spatial autocorrelation. ## ## SPATIAL AUTOCORRELATION ## ═══════════════════════════════════════════════════════════════ ## ## POSITIVE AUTOCORRELATION (most common): ## Similar values cluster together ## High values near high, low near low ## Example: Temperature (warm areas cluster) ## ## NEGATIVE AUTOCORRELATION (rare): ## Dissimilar values cluster ## High values surrounded by low values ## Example: Territorial spacing (each territory surrounded by others) ## ## NO AUTOCORRELATION: ## Random spatial pattern ## Location provides no information about neighbors 163.2 Simulating spatial data # Create a grid of points n &lt;- 20 coords &lt;- expand.grid(x = 1:n, y = 1:n) n_points &lt;- nrow(coords) # Random data (no autocorrelation) random_values &lt;- rnorm(n_points) # Spatially autocorrelated data (distance-decay) # Using a simple spatial process dist_matrix &lt;- as.matrix(dist(coords)) cov_matrix &lt;- exp(-dist_matrix / 5) # Exponential decay L &lt;- chol(cov_matrix) autocorr_values &lt;- t(L) %*% rnorm(n_points) # Create data frames spatial_data &lt;- data.frame( coords, random = random_values, autocorr = as.numeric(autocorr_values) ) # Plot comparison par(mfrow = c(1, 2)) # Random with(spatial_data, { plot(x, y, pch = 19, cex = 1.5, col = colorRampPalette(c(&quot;blue&quot;, &quot;white&quot;, &quot;red&quot;))(100)[ cut(random, 100, labels = FALSE)], main = &quot;Random (No Autocorrelation)&quot;, asp = 1) }) # Autocorrelated with(spatial_data, { plot(x, y, pch = 19, cex = 1.5, col = colorRampPalette(c(&quot;blue&quot;, &quot;white&quot;, &quot;red&quot;))(100)[ cut(autocorr, 100, labels = FALSE)], main = &quot;Spatially Autocorrelated&quot;, asp = 1) }) Figure 163.1: Comparison of random (left) and spatially autocorrelated (right) data. In autocorrelated data, similar values cluster together. par(mfrow = c(1, 1)) "],["part-2-quantifying-spatial-autocorrelation.html", "Chapter 164 Part 2: Quantifying Spatial Autocorrelation 164.1 Moran’s I 164.2 Calculating Moran’s I 164.3 Moran’s I scatter plot 164.4 Geary’s C", " Chapter 164 Part 2: Quantifying Spatial Autocorrelation 164.1 Moran’s I Moran’s I is the most common measure of global spatial autocorrelation: \\[I = \\frac{n}{\\sum_i \\sum_j w_{ij}} \\frac{\\sum_i \\sum_j w_{ij}(x_i - \\bar{x})(x_j - \\bar{x})}{\\sum_i (x_i - \\bar{x})^2}\\] where: - \\(n\\) = number of observations - \\(w_{ij}\\) = spatial weight between locations i and j - \\(x_i\\) = value at location i - \\(\\bar{x}\\) = mean value Interpretation: - I ≈ +1: Strong positive autocorrelation (clustering) - I ≈ 0: Random pattern - I ≈ -1: Strong negative autocorrelation (dispersion) 164.2 Calculating Moran’s I # Convert to sf object spatial_sf &lt;- st_as_sf(spatial_data, coords = c(&quot;x&quot;, &quot;y&quot;)) # Create neighbor list (k-nearest neighbors) coords_matrix &lt;- st_coordinates(spatial_sf) knn &lt;- knearneigh(coords_matrix, k = 8) nb &lt;- knn2nb(knn) # Create spatial weights weights &lt;- nb2listw(nb, style = &quot;W&quot;) # Row-standardized # Moran&#39;s I test for random data moran_random &lt;- moran.test(spatial_data$random, weights) print(moran_random) ## ## Moran I test under randomisation ## ## data: spatial_data$random ## weights: weights ## ## Moran I statistic standard deviate = -0.41676, p-value = 0.6616 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## -0.0127438735 -0.0025062657 0.0006034204 # Moran&#39;s I test for autocorrelated data moran_autocorr &lt;- moran.test(spatial_data$autocorr, weights) print(moran_autocorr) ## ## Moran I test under randomisation ## ## data: spatial_data$autocorr ## weights: weights ## ## Moran I statistic standard deviate = 32.274, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.7907765339 -0.0025062657 0.0006041756 164.3 Moran’s I scatter plot # Moran scatter plot moran.plot(spatial_data$autocorr, weights, main = &quot;Moran Scatter Plot&quot;, xlab = &quot;Value&quot;, ylab = &quot;Spatially Lagged Value&quot;) Figure 164.1: Moran scatter plot. The slope of the regression line equals Moran’s I. Points in upper-right and lower-left indicate positive autocorrelation. Reading the Moran scatter plot: - Upper-right quadrant: High values with high neighbors (HH) - Lower-left quadrant: Low values with low neighbors (LL) - Upper-left: Low values with high neighbors (LH) - Lower-right: High values with low neighbors (HL) Most points in HH and LL = positive autocorrelation 164.4 Geary’s C Geary’s C is an alternative that’s more sensitive to local differences: \\[C = \\frac{(n-1)}{2 \\sum_i \\sum_j w_{ij}} \\frac{\\sum_i \\sum_j w_{ij}(x_i - x_j)^2}{\\sum_i (x_i - \\bar{x})^2}\\] Interpretation: - C &lt; 1: Positive autocorrelation - C ≈ 1: Random - C &gt; 1: Negative autocorrelation # Geary&#39;s C test geary_test &lt;- geary.test(spatial_data$autocorr, weights) print(geary_test) ## ## Geary C test under randomisation ## ## data: spatial_data$autocorr ## weights: weights ## ## Geary C statistic standard deviate = 32.148, p-value &lt; 2.2e-16 ## alternative hypothesis: Expectation greater than statistic ## sample estimates: ## Geary C statistic Expectation Variance ## 0.2054580008 1.0000000000 0.0006108519 "],["part-3-spatial-neighbors-and-weights.html", "Chapter 165 Part 3: Spatial Neighbors and Weights 165.1 Why define neighbors? 165.2 Creating neighbor lists 165.3 Spatial weights matrices 165.4 Visualizing neighbors", " Chapter 165 Part 3: Spatial Neighbors and Weights 165.1 Why define neighbors? Spatial autocorrelation is calculated relative to “neighbors.” But what counts as a neighbor? ## ## TYPES OF SPATIAL NEIGHBORS ## ═══════════════════════════════════════════════════════════════ ## ## CONTIGUITY (for polygons): ## ┌───┬───┬───┐ ## │ │ R │ │ Rook: shares edge (4 neighbors) ## ├───┼───┼───┤ ## │ R │ X │ R │ Queen: shares edge OR corner (8 neighbors) ## ├───┼───┼───┤ ## │ │ R │ │ ## └───┴───┴───┘ ## ## DISTANCE-BASED (for points): ## ○ ○ Distance band: all points within d km ## ╲ ╱ ## ● ─ ─ ○ K-nearest: the k closest points ## ╱ ╲ ## ○ ○ ## ## CHOOSING NEIGHBORS: ## - Contiguity for administrative units (counties, states) ## - Distance for point data ## - K-nearest when density varies 165.2 Creating neighbor lists # K-nearest neighbors (good for irregular point patterns) knn_5 &lt;- knn2nb(knearneigh(coords_matrix, k = 5)) knn_8 &lt;- knn2nb(knearneigh(coords_matrix, k = 8)) # Distance-based neighbors (all points within distance d) dist_nb &lt;- dnearneigh(coords_matrix, d1 = 0, d2 = 3) # Compare number of neighbors cat(&quot;K=5 neighbors - mean:&quot;, mean(card(knn_5)), &quot;\\n&quot;) ## K=5 neighbors - mean: 5 cat(&quot;K=8 neighbors - mean:&quot;, mean(card(knn_8)), &quot;\\n&quot;) ## K=8 neighbors - mean: 8 cat(&quot;Distance (d=3) neighbors - mean:&quot;, mean(card(dist_nb)), &quot;\\n&quot;) ## Distance (d=3) neighbors - mean: 24.49 165.3 Spatial weights matrices # Convert neighbor list to weights # Style options: # &quot;W&quot; = row-standardized (each row sums to 1) # &quot;B&quot; = binary (1 if neighbor, 0 if not) # &quot;C&quot; = globally standardized weights_W &lt;- nb2listw(knn_8, style = &quot;W&quot;) weights_B &lt;- nb2listw(knn_8, style = &quot;B&quot;) # Examine weights for one point cat(&quot;Row-standardized weights for point 1:\\n&quot;) ## Row-standardized weights for point 1: print(weights_W$weights[[1]]) ## [1] 0.125 0.125 0.125 0.125 0.125 0.125 0.125 0.125 165.4 Visualizing neighbors # Plot neighbor connections plot(spatial_sf$geometry, pch = 19, col = &quot;steelblue&quot;, main = &quot;K=8 Nearest Neighbors&quot;) plot(knn_8, coords_matrix, add = TRUE, col = &quot;gray50&quot;) Figure 165.1: Neighbor connections for k=8 nearest neighbors. Each point is connected to its 8 closest neighbors. "],["part-4-local-spatial-autocorrelation.html", "Chapter 166 Part 4: Local Spatial Autocorrelation 166.1 Global vs Local 166.2 Local Moran’s I 166.3 Identifying clusters and outliers", " Chapter 166 Part 4: Local Spatial Autocorrelation 166.1 Global vs Local Global Moran’s I gives one number for the entire study area. But autocorrelation often varies spatially—some areas may be clustered while others are random. Local Indicators of Spatial Association (LISA) identify where clustering occurs. 166.2 Local Moran’s I # Calculate local Moran&#39;s I local_moran &lt;- localmoran(spatial_data$autocorr, weights) # Results for each location head(local_moran) ## Ii E.Ii Var.Ii Z.Ii Pr(z != E(Ii)) ## 1 2.60105189 -8.740373e-03 0.4255798775 4.000510 6.320603e-05 ## 2 1.37463421 -3.296666e-03 0.1614003948 3.429850 6.039149e-04 ## 3 1.13921670 -2.886685e-03 0.1413864006 3.037397 2.386307e-03 ## 4 0.73215658 -2.553117e-03 0.1250904718 2.077321 3.777192e-02 ## 5 0.39265717 -9.225380e-04 0.0452738165 1.849733 6.435198e-02 ## 6 0.02244727 -4.505639e-06 0.0002213187 1.509182 1.312523e-01 # Add to data spatial_data$local_I &lt;- local_moran[, &quot;Ii&quot;] spatial_data$local_p &lt;- local_moran[, &quot;Pr(z != E(Ii))&quot;] 166.3 Identifying clusters and outliers # --- sanity checks (fail early, fail clearly) --- stopifnot(exists(&quot;spatial_data&quot;)) stopifnot(nrow(spatial_data) &gt; 0) need_cols &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;autocorr&quot;, &quot;local_p&quot;) missing_cols &lt;- setdiff(need_cols, names(spatial_data)) if (length(missing_cols) &gt; 0) { stop(&quot;spatial_data is missing: &quot;, paste(missing_cols, collapse = &quot;, &quot;)) } stopifnot(exists(&quot;weights&quot;)) # should be a spdep listw object # --- lag + z-scores (force plain numeric vectors) --- spatial_data$lag_autocorr &lt;- spdep::lag.listw(weights, spatial_data$autocorr, zero.policy = TRUE) spatial_data$z_value &lt;- as.numeric(scale(spatial_data$autocorr)) spatial_data$z_lag &lt;- as.numeric(scale(spatial_data$lag_autocorr)) # local_p should be numeric spatial_data$local_p &lt;- as.numeric(spatial_data$local_p) # --- cluster classification --- spatial_data$cluster &lt;- dplyr::case_when( spatial_data$z_value &gt; 0 &amp; spatial_data$z_lag &gt; 0 &amp; spatial_data$local_p &lt; 0.05 ~ &quot;High-High&quot;, spatial_data$z_value &lt; 0 &amp; spatial_data$z_lag &lt; 0 &amp; spatial_data$local_p &lt; 0.05 ~ &quot;Low-Low&quot;, spatial_data$z_value &gt; 0 &amp; spatial_data$z_lag &lt; 0 &amp; spatial_data$local_p &lt; 0.05 ~ &quot;High-Low&quot;, spatial_data$z_value &lt; 0 &amp; spatial_data$z_lag &gt; 0 &amp; spatial_data$local_p &lt; 0.05 ~ &quot;Low-High&quot;, TRUE ~ &quot;Not Significant&quot; ) # --- plot --- ggplot2::ggplot(spatial_data, ggplot2::aes(x = x, y = y, color = cluster)) + ggplot2::geom_point(size = 3) + ggplot2::scale_color_manual(values = c( &quot;High-High&quot; = &quot;red&quot;, &quot;Low-Low&quot; = &quot;blue&quot;, &quot;High-Low&quot; = &quot;pink&quot;, &quot;Low-High&quot; = &quot;lightblue&quot;, &quot;Not Significant&quot; = &quot;gray80&quot; )) + ggplot2::labs(title = &quot;LISA Cluster Map&quot;, color = &quot;Cluster Type&quot;) + ggplot2::coord_equal() + ggplot2::theme_minimal() Figure 166.1: LISA cluster map showing High-High clusters (hotspots), Low-Low clusters (coldspots), and spatial outliers. Cluster interpretation: - High-High (hotspots): High values surrounded by high values - Low-Low (coldspots): Low values surrounded by low values - High-Low: High values surrounded by low values (spatial outliers) - Low-High: Low values surrounded by high values (spatial outliers) "],["part-5-spatial-correlograms.html", "Chapter 167 Part 5: Spatial Correlograms 167.1 Autocorrelation across distances", " Chapter 167 Part 5: Spatial Correlograms 167.1 Autocorrelation across distances A spatial correlogram shows how autocorrelation changes with distance. # Calculate correlogram correlogram &lt;- sp.correlogram(knn_8, spatial_data$autocorr, order = 10, method = &quot;I&quot;) # Plot plot(correlogram, main = &quot;Spatial Correlogram&quot;) Figure 167.1: Spatial correlogram showing Moran’s I at different distance lags. Autocorrelation typically decays with distance. Reading the correlogram: - Significant positive I at short lags: Clustering of similar values - Decay toward zero: Autocorrelation diminishes with distance - Negative I at large lags: Opposite values at large distances "],["part-6-spatial-regression-models.html", "Chapter 168 Part 6: Spatial Regression Models 168.1 The problem with ordinary regression 168.2 Types of spatial regression 168.3 Spatial Error Model (SEM) 168.4 Spatial Lag Model (SLM) 168.5 Comparing models 168.6 Interpreting spatial regression", " Chapter 168 Part 6: Spatial Regression Models 168.1 The problem with ordinary regression When residuals are spatially autocorrelated, ordinary least squares (OLS) gives: - Biased standard errors (usually too small) - Invalid p-values (too many false positives) - Inefficient estimates (not best linear unbiased) # Simulate spatially structured response and predictor n &lt;- 400 coords2 &lt;- expand.grid(x = 1:20, y = 1:20) # Spatially autocorrelated error dist_mat &lt;- as.matrix(dist(coords2)) cov_mat &lt;- exp(-dist_mat / 5) L &lt;- chol(cov_mat) spatial_error &lt;- as.numeric(t(L) %*% rnorm(n)) # Predictor (also spatially structured) predictor &lt;- as.numeric(t(L) %*% rnorm(n, 5, 1)) # Response with true relationship + spatial error response &lt;- 2 + 0.5 * predictor + spatial_error regression_data &lt;- data.frame(coords2, predictor, response) # Fit OLS ols_model &lt;- lm(response ~ predictor, data = regression_data) summary(ols_model) ## ## Call: ## lm(formula = response ~ predictor, data = regression_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.65867 -0.59989 0.03513 0.65528 2.13368 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.799344 0.143684 12.52 &lt;2e-16 *** ## predictor 0.510385 0.003577 142.70 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.8787 on 398 degrees of freedom ## Multiple R-squared: 0.9808, Adjusted R-squared: 0.9808 ## F-statistic: 2.036e+04 on 1 and 398 DF, p-value: &lt; 2.2e-16 # Check residuals for spatial autocorrelation regression_sf &lt;- st_as_sf(regression_data, coords = c(&quot;x&quot;, &quot;y&quot;)) coords_mat &lt;- st_coordinates(regression_sf) nb_reg &lt;- knn2nb(knearneigh(coords_mat, k = 8)) weights_reg &lt;- nb2listw(nb_reg, style = &quot;W&quot;) moran.test(residuals(ols_model), weights_reg) ## ## Moran I test under randomisation ## ## data: residuals(ols_model) ## weights: weights_reg ## ## Moran I statistic standard deviate = 27.692, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.677758436 -0.002506266 0.000603464 The significant Moran’s I in residuals tells us OLS is inappropriate. 168.2 Types of spatial regression Model When to use What it does Spatial Error Model (SEM) Autocorrelation in errors Models correlated residuals Spatial Lag Model (SLM) Outcome depends on neighbors Includes spatially lagged Y Spatial Durbin Model (SDM) Both processes Lagged Y and lagged X 168.3 Spatial Error Model (SEM) The SEM assumes spatial autocorrelation is in the error term: \\[Y = X\\beta + u, \\quad u = \\lambda Wu + \\varepsilon\\] where \\(\\lambda\\) is the spatial autoregressive parameter for errors. Use when: Spatial pattern comes from unmeasured variables, not direct neighbor effects. # Fit Spatial Error Model sem_model &lt;- errorsarlm(response ~ predictor, data = regression_data, listw = weights_reg) summary(sem_model) ## ## Call:errorsarlm(formula = response ~ predictor, data = regression_data, listw = weights_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.15700605 -0.28913435 -0.00052136 0.27088529 1.31229522 ## ## Type: error ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.863771 0.609249 4.7005 2.595e-06 ## predictor 0.482538 0.014234 33.8997 &lt; 2.2e-16 ## ## Lambda: 0.92862, LR test value: 442.37, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.020702 ## z-value: 44.857, p-value: &lt; 2.22e-16 ## Wald statistic: 2012.2, p-value: &lt; 2.22e-16 ## ## Log likelihood: -293.6701 for error model ## ML residual variance (sigma squared): 0.20919, (sigma: 0.45738) ## Number of observations: 400 ## Number of parameters estimated: 4 ## AIC: 595.34, (AIC for lm: 1035.7) 168.4 Spatial Lag Model (SLM) The SLM assumes the outcome depends on neighbors’ outcomes: \\[Y = \\rho WY + X\\beta + \\varepsilon\\] where \\(\\rho\\) is the spatial autoregressive parameter. Use when: There’s a direct spillover effect (e.g., disease spreads, populations migrate). # Fit Spatial Lag Model slm_model &lt;- lagsarlm(response ~ predictor, data = regression_data, listw = weights_reg) summary(slm_model) ## ## Call:lagsarlm(formula = response ~ predictor, data = regression_data, listw = weights_reg) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.315207 -0.375223 0.052251 0.397488 1.487852 ## ## Type: lag ## Coefficients: (asymptotic standard errors) ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.069588 0.119390 0.5829 0.56 ## predictor 0.179136 0.016260 11.0170 &lt;2e-16 ## ## Rho: 0.67321, LR test value: 278.91, p-value: &lt; 2.22e-16 ## Asymptotic standard error: 0.032082 ## z-value: 20.984, p-value: &lt; 2.22e-16 ## Wald statistic: 440.32, p-value: &lt; 2.22e-16 ## ## Log likelihood: -375.4017 for lag model ## ML residual variance (sigma squared): 0.35479, (sigma: 0.59564) ## Number of observations: 400 ## Number of parameters estimated: 4 ## AIC: 758.8, (AIC for lm: 1035.7) ## LM test for residual autocorrelation ## test value: 167.11, p-value: &lt; 2.22e-16 168.5 Comparing models # Compare models using AIC models &lt;- list( OLS = ols_model, SEM = sem_model, SLM = slm_model ) # AIC comparison aic_values &lt;- c( OLS = AIC(ols_model), SEM = AIC(sem_model), SLM = AIC(slm_model) ) print(sort(aic_values)) ## SEM SLM OLS ## 595.3403 758.8033 1035.7086 # Check residual autocorrelation for best model cat(&quot;\\nResidual Moran&#39;s I for SEM:\\n&quot;) ## ## Residual Moran&#39;s I for SEM: moran.test(residuals(sem_model), weights_reg) ## ## Moran I test under randomisation ## ## data: residuals(sem_model) ## weights: weights_reg ## ## Moran I statistic standard deviate = 0.80907, p-value = 0.2092 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.0173631268 -0.0025062657 0.0006031051 168.6 Interpreting spatial regression # Compare coefficients coef_comparison &lt;- data.frame( Model = c(&quot;OLS&quot;, &quot;SEM&quot;, &quot;SLM&quot;), Intercept = c(coef(ols_model)[1], coef(sem_model)[1], coef(slm_model)[1]), Predictor = c(coef(ols_model)[2], coef(sem_model)[2], coef(slm_model)[2]), Spatial_param = c(NA, sem_model$lambda, slm_model$rho) ) print(coef_comparison, digits = 3) ## Model Intercept Predictor Spatial_param ## (Intercept) OLS 1.799 0.5104 NA ## lambda SEM 0.929 2.8638 0.929 ## rho SLM 0.673 0.0696 0.673 Key insight: Spatial models often give different coefficient estimates because they properly account for non-independence. "],["part-7-geostatistics-and-variograms.html", "Chapter 169 Part 7: Geostatistics and Variograms 169.1 What is geostatistics? 169.2 The variogram 169.3 Computing and plotting variograms 169.4 Fitting variogram models", " Chapter 169 Part 7: Geostatistics and Variograms 169.1 What is geostatistics? Geostatistics focuses on continuous spatial variation—interpolating values between sampled locations based on spatial correlation structure. Key concept: Variogram (or semivariogram) 169.2 The variogram The variogram describes how dissimilarity increases with distance: \\[\\gamma(h) = \\frac{1}{2N(h)} \\sum_{i=1}^{N(h)} [z(x_i) - z(x_i + h)]^2\\] where \\(h\\) is the lag distance and \\(N(h)\\) is the number of pairs at that distance. ## ## VARIOGRAM COMPONENTS ## ═══════════════════════════════════════════════════════════════ ## ## Semivariance ↑ ## │ ┌─────────────── Sill (total variance) ## │ ╱────┘ ## │ ╱ ## │ ╱ ## Nugget ───┼─╱ ## │╱ ## └────────────────────────────→ Distance ## │ ## Range (distance where autocorrelation fades) ## ## NUGGET: Variance at distance = 0 (measurement error + micro-scale variation) ## SILL: Maximum variance (variance of the random field) ## RANGE: Distance at which autocorrelation effectively disappears 169.3 Computing and plotting variograms # Create spatial object for gstat library(sp) sp::coordinates(regression_data) &lt;- ~ x + y # Compute empirical variogram emp_variogram &lt;- variogram(response ~ 1, data = regression_data) # Plot plot(emp_variogram, main = &quot;Empirical Variogram&quot;) Figure 169.1: Empirical variogram showing semivariance vs distance. The fitted model (line) captures the spatial correlation structure. 169.4 Fitting variogram models # Fit variogram model # Common models: &quot;Sph&quot; (spherical), &quot;Exp&quot; (exponential), &quot;Gau&quot; (Gaussian) vgm_model &lt;- fit.variogram(emp_variogram, model = vgm(psill = 1, model = &quot;Exp&quot;, range = 5, nugget = 0.1)) # Plot with fitted model plot(emp_variogram, model = vgm_model, main = &quot;Fitted Variogram Model&quot;) Figure 169.2: Fitted variogram model. Common models include spherical, exponential, and Gaussian. # View model parameters print(vgm_model) ## model psill range ## 1 Nug 0.1067992 0.000000 ## 2 Exp 1.7767795 9.341667 "],["part-8-spatial-interpolation-kriging.html", "Chapter 170 Part 8: Spatial Interpolation (Kriging) 170.1 What is kriging? 170.2 Ordinary kriging 170.3 Universal kriging", " Chapter 170 Part 8: Spatial Interpolation (Kriging) 170.1 What is kriging? Kriging uses the variogram model to predict values at unsampled locations. It provides: - Optimal predictions (minimizes mean squared error) - Prediction uncertainty (kriging variance) 170.2 Ordinary kriging # Create prediction grid pred_grid &lt;- expand.grid( x = seq(0.5, 20.5, by = 0.5), y = seq(0.5, 20.5, by = 0.5) ) sp::coordinates(pred_grid) &lt;- ~ x + y gridded(pred_grid) &lt;- TRUE # Perform ordinary kriging kriging_result &lt;- krige(response ~ 1, locations = regression_data, newdata = pred_grid, model = vgm_model) ## [using ordinary kriging] # Convert to data frame for plotting kriging_df &lt;- as.data.frame(kriging_result) # Plot predictions p1 &lt;- ggplot(kriging_df, aes(x = x, y = y, fill = var1.pred)) + geom_tile() + scale_fill_viridis(name = &quot;Predicted\\nValue&quot;) + coord_equal() + labs(title = &quot;Kriging Predictions&quot;) + theme_minimal() # Plot uncertainty (kriging variance) p2 &lt;- ggplot(kriging_df, aes(x = x, y = y, fill = sqrt(var1.var))) + geom_tile() + scale_fill_viridis(name = &quot;Prediction\\nSE&quot;, option = &quot;magma&quot;) + coord_equal() + labs(title = &quot;Prediction Uncertainty&quot;) + theme_minimal() # Combine plots library(gridExtra) grid.arrange(p1, p2, ncol = 2) Figure 170.1: Kriging predictions and uncertainty. Left: predicted values; Right: prediction standard error (uncertainty is higher far from sample points). 170.3 Universal kriging When there’s a spatial trend, use universal kriging (kriging with a trend model): # Kriging with external drift (covariates) # If we had an environmental covariate: # krige(response ~ predictor, ...) "],["part-9-ecological-case-studies.html", "Chapter 171 Part 9: Ecological Case Studies 171.1 Case Study 1: Soil nutrients across a landscape 171.2 Case Study 2: Species richness hotspots 171.3 Case Study 3: Spatial regression comparison", " Chapter 171 Part 9: Ecological Case Studies 171.1 Case Study 1: Soil nutrients across a landscape # Simulate soil nitrogen data set.seed(456) n_samples &lt;- 100 soil_data &lt;- data.frame( x = runif(n_samples, 0, 100), y = runif(n_samples, 0, 100) ) # Spatial pattern in nitrogen (higher in lowlands) dist_mat &lt;- as.matrix(dist(soil_data[, c(&quot;x&quot;, &quot;y&quot;)])) cov_mat &lt;- 2 * exp(-dist_mat / 20) L &lt;- chol(cov_mat + diag(0.01, n_samples)) spatial_component &lt;- as.numeric(t(L) %*% rnorm(n_samples)) soil_data$nitrogen &lt;- 10 + 0.05 * (50 - soil_data$x) + spatial_component soil_data$nitrogen &lt;- pmax(soil_data$nitrogen, 0) # Create sf object soil_sf &lt;- st_as_sf(soil_data, coords = c(&quot;x&quot;, &quot;y&quot;)) # Visualize ggplot(soil_data, aes(x = x, y = y, color = nitrogen)) + geom_point(size = 3) + scale_color_viridis(name = &quot;Nitrogen\\n(mg/kg)&quot;) + labs(title = &quot;Soil Nitrogen Concentrations&quot;) + coord_equal() + theme_minimal() # Test for spatial autocorrelation soil_coords &lt;- as.matrix(soil_data[, c(&quot;x&quot;, &quot;y&quot;)]) soil_nb &lt;- knn2nb(knearneigh(soil_coords, k = 8)) soil_weights &lt;- nb2listw(soil_nb, style = &quot;W&quot;) cat(&quot;Global Moran&#39;s I test:\\n&quot;) ## Global Moran&#39;s I test: print(moran.test(soil_data$nitrogen, soil_weights)) ## ## Moran I test under randomisation ## ## data: soil_data$nitrogen ## weights: soil_weights ## ## Moran I statistic standard deviate = 16.577, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.749626823 -0.010101010 0.002100432 171.2 Case Study 2: Species richness hotspots library(spdep) library(dplyr) library(ggplot2) # Simulate species richness data set.seed(789) richness_data &lt;- data.frame( x = rep(1:15, each = 15), y = rep(1:15, 15) ) # Create hotspot in one corner hotspot_center &lt;- c(12, 12) dist_to_hotspot &lt;- sqrt((richness_data$x - hotspot_center[1])^2 + (richness_data$y - hotspot_center[2])^2) richness_data$richness &lt;- 20 + 15 * exp(-dist_to_hotspot / 5) + rpois(nrow(richness_data), 3) # Local Moran&#39;s I richness_coords &lt;- as.matrix(richness_data[, c(&quot;x&quot;, &quot;y&quot;)]) richness_nb &lt;- knn2nb(knearneigh(richness_coords, k = 8)) richness_weights &lt;- nb2listw(richness_nb, style = &quot;W&quot;) local_i &lt;- localmoran(richness_data$richness, richness_weights) richness_data$local_I &lt;- local_i[, &quot;Ii&quot;] richness_data$local_p &lt;- local_i[, &quot;Pr(z != E(Ii))&quot;] richness_data$lag &lt;- lag.listw(richness_weights, richness_data$richness) # --- FIX: scale() -&gt; numeric vector (not a matrix) --- z_rich &lt;- as.numeric(scale(richness_data$richness)) z_lag &lt;- as.numeric(scale(richness_data$lag)) # Classify clusters richness_data$cluster &lt;- case_when( z_rich &gt; 0 &amp; z_lag &gt; 0 &amp; richness_data$local_p &lt; 0.05 ~ &quot;Hotspot&quot;, z_rich &lt; 0 &amp; z_lag &lt; 0 &amp; richness_data$local_p &lt; 0.05 ~ &quot;Coldspot&quot;, TRUE ~ &quot;Not Significant&quot; ) # Plot ggplot(richness_data, aes(x = x, y = y)) + geom_point(aes(size = richness, color = cluster)) + scale_color_manual(values = c(&quot;Hotspot&quot; = &quot;firebrick&quot;, &quot;Coldspot&quot; = &quot;steelblue&quot;, &quot;Not Significant&quot; = &quot;gray70&quot;)) + scale_size_continuous(range = c(2, 8)) + labs(title = &quot;Species Richness Hotspot Analysis&quot;, size = &quot;Richness&quot;, color = &quot;Cluster&quot;) + coord_equal() + theme_minimal() Figure 171.1: Local Moran’s I analysis identifying species richness hotspots (red) and coldspots (blue). 171.3 Case Study 3: Spatial regression comparison # Use soil data to compare OLS vs spatial models # Predictor: distance to water (lower distance = higher nitrogen) soil_data$dist_water &lt;- sqrt((soil_data$x - 0)^2 + (soil_data$y - 50)^2) # OLS ols &lt;- lm(nitrogen ~ dist_water, data = soil_data) # Check residual autocorrelation cat(&quot;OLS Residual Moran&#39;s I:\\n&quot;) ## OLS Residual Moran&#39;s I: print(moran.test(residuals(ols), soil_weights)) ## ## Moran I test under randomisation ## ## data: residuals(ols) ## weights: soil_weights ## ## Moran I statistic standard deviate = 10.083, p-value &lt; 2.2e-16 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.451183968 -0.010101010 0.002092768 # Spatial error model sem &lt;- errorsarlm(nitrogen ~ dist_water, data = soil_data, listw = soil_weights) # Spatial lag model slm &lt;- lagsarlm(nitrogen ~ dist_water, data = soil_data, listw = soil_weights) # Compare cat(&quot;\\nModel Comparison (AIC):\\n&quot;) ## ## Model Comparison (AIC): cat(&quot;OLS:&quot;, round(AIC(ols), 1), &quot;\\n&quot;) ## OLS: 334.8 cat(&quot;SEM:&quot;, round(AIC(sem), 1), &quot;\\n&quot;) ## SEM: 289.4 cat(&quot;SLM:&quot;, round(AIC(slm), 1), &quot;\\n&quot;) ## SLM: 290.8 # Compare coefficients cat(&quot;\\nCoefficient for dist_water:\\n&quot;) ## ## Coefficient for dist_water: cat(&quot;OLS:&quot;, round(coef(ols)[2], 4), &quot;\\n&quot;) ## OLS: -0.0616 cat(&quot;SEM:&quot;, round(coef(sem)[2], 4), &quot;\\n&quot;) ## SEM: 13.1196 cat(&quot;SLM:&quot;, round(coef(slm)[2], 4), &quot;\\n&quot;) ## SLM: 3.6639 "],["part-10-common-pitfalls-and-tips.html", "Chapter 172 Part 10: Common Pitfalls and Tips 172.1 Pitfalls to avoid 172.2 Practical tips", " Chapter 172 Part 10: Common Pitfalls and Tips 172.1 Pitfalls to avoid Pitfall Problem Solution Wrong neighbor definition Misspecified spatial structure Test multiple definitions, use ecological knowledge Ignoring edge effects Border observations have fewer neighbors Use edge correction or buffer Overfitting variogram Poor interpolation Use cross-validation Assuming stationarity Spatial correlation varies Check for non-stationarity Confounding Spurious spatial correlation Include relevant covariates 172.2 Practical tips Always visualize first — Map your data before modeling Test for autocorrelation — Don’t assume; test residuals Try multiple neighbor definitions — Results can be sensitive Compare models — Use AIC, check residuals Report uncertainty — Especially for interpolation Consider scale — Patterns may change with extent/resolution "],["part-11-reporting.html", "Chapter 173 Part 11: Reporting 173.1 What to report 173.2 Sample methods and results 173.3 Key takeaways 173.4 Assignment", " Chapter 173 Part 11: Reporting 173.1 What to report Spatial structure of data — Coordinates, extent, sample size Neighbor definition — k-nearest, distance band, etc. Spatial weights — Row standardization or other Autocorrelation test — Moran’s I with p-value Model comparison — OLS vs spatial models, AIC Residual diagnostics — Is autocorrelation resolved? Maps — Visualize patterns and predictions 173.2 Sample methods and results 173.2.1 Methods We tested for spatial autocorrelation in soil nitrogen concentrations using Global Moran’s I with k=8 nearest neighbors and row-standardized weights. Because significant autocorrelation was detected in OLS residuals (Moran’s I = 0.42, p &lt; 0.001), we fit spatial error (SEM) and spatial lag (SLM) models using the spatialreg package. Models were compared using AIC, and the best model was validated by testing residuals for remaining autocorrelation. For interpolation, we fit an exponential variogram model and performed ordinary kriging on a 100 × 100 prediction grid. All spatial analyses were conducted using sf, spdep, spatialreg, and gstat packages in R version 4.3.1. 173.2.2 Results Soil nitrogen showed significant positive spatial autocorrelation (Global Moran’s I = 0.58, p &lt; 0.001), with concentrations clustered in the western portion of the study area (Fig. X). Local Moran’s I analysis identified a significant hotspot (High-High cluster) in the northwest corner. The spatial error model (AIC = 245.3) outperformed OLS (AIC = 289.7) and the spatial lag model (AIC = 251.2). After accounting for spatial autocorrelation, nitrogen concentration decreased significantly with distance from the stream (SEM: β = -0.08, SE = 0.02, p &lt; 0.001), compared to the OLS estimate (β = -0.12, SE = 0.01) which showed inflated significance due to autocorrelation. Residuals from the SEM showed no remaining spatial autocorrelation (Moran’s I = 0.03, p = 0.31). Kriging predictions (Fig. Y) showed a continuous gradient with prediction uncertainty increasing in under-sampled areas. 173.3 Key takeaways Spatial autocorrelation is ubiquitous in ecology — Always check for it Moran’s I quantifies global autocorrelation — Local Moran’s finds hotspots Neighbors and weights must be defined — Choose based on ecological knowledge OLS fails with spatial data — Use spatial regression models SEM vs SLM depends on process — Error model for nuisance autocorrelation, lag model for spillover Variograms describe spatial structure — Range tells you the scale of autocorrelation Kriging provides optimal interpolation — With uncertainty estimates 173.4 Assignment 173.4.1 Part 1: Conceptual questions Explain the difference between positive and negative spatial autocorrelation. Give an ecological example of each. When would you choose a Spatial Error Model over a Spatial Lag Model? Describe the ecological reasoning. What do the nugget, sill, and range of a variogram tell you about spatial structure? 173.4.2 Part 2: Detecting autocorrelation Using this simulated dataset: set.seed(111) n &lt;- 150 assignment_data &lt;- data.frame( x = runif(n, 0, 50), y = runif(n, 0, 50) ) # Create spatially autocorrelated values d &lt;- as.matrix(dist(assignment_data)) C &lt;- exp(-d / 10) L &lt;- chol(C + diag(0.01, n)) assignment_data$value &lt;- as.numeric(t(L) %*% rnorm(n)) Create a map of the data Define neighbors (k=6 nearest) Calculate and interpret Global Moran’s I Create a Moran scatter plot Calculate Local Moran’s I and identify clusters 173.4.3 Part 3: Spatial regression Using the same data, add a predictor and fit models: # Add predictor assignment_data$predictor &lt;- assignment_data$x / 10 + rnorm(n, 0, 1) assignment_data$response &lt;- 5 + 0.5 * assignment_data$predictor + assignment_data$value Fit OLS and check residual autocorrelation Fit SEM and SLM Compare models using AIC Interpret the best model’s coefficients Test whether spatial autocorrelation is resolved 173.4.4 Part 4: Variogram and kriging Compute an empirical variogram Fit a variogram model (try exponential and spherical) Perform ordinary kriging on a prediction grid Map predictions and uncertainty 173.4.5 Part 5: Reflection In 2-3 sentences, explain why ignoring spatial autocorrelation can lead to incorrect conclusions in ecological studies. "],["spatial-disease-ecology-patterns-spread-and-risk.html", "Chapter 174 Spatial Disease Ecology: Patterns, Spread, and Risk 174.1 The core question 174.2 Analytical framework 174.3 Setup", " Chapter 174 Spatial Disease Ecology: Patterns, Spread, and Risk Is the disease everywhere, waiting for stressed trees to succumb? Or is it actively spreading across the landscape, jumping from tree to tree? These two scenarios require fundamentally different management responses—and distinguishing them requires careful spatial analysis. This chapter addresses a common challenge in forest pathology and wildlife disease: separating environmentally-driven disease emergence from spatial contagion. We’ll use the example of Biscogniauxia canker on Emory oak (Quercus emoryi), an opportunistic fungal pathogen that may be triggered by drought stress, actively spreading, or both. 174.1 The core question ## ## TWO COMPETING HYPOTHESES ## ═══════════════════════════════════════════════════════════════ ## ## HYPOTHESIS 1: UBIQUITOUS PATHOGEN, STRESS-TRIGGERED ## ──────────────────────────────────────────────────── ## • Biscogniauxia is everywhere (or nearly everywhere) ## • Infection only manifests when trees are stressed ## • Disease clusters because STRESS clusters (drought, poor soils) ## • Spatial pattern driven by ENVIRONMENT, not contagion ## ## Management: Focus on tree health, reduce stress, identify ## vulnerable sites based on environmental risk ## ## HYPOTHESIS 2: ACTIVE SPATIAL SPREAD ## ──────────────────────────────────── ## • Pathogen spreads from infected to nearby susceptible trees ## • Disease clusters because of LOCAL TRANSMISSION ## • Pattern expands outward from infection foci over time ## • Spatial structure exists BEYOND environmental predictors ## ## ## Management: Containment, remove infected trees, create barriers, ## prevent spread to uninfected areas ## ## HYPOTHESIS 3: BOTH MECHANISMS ## ───────────────────────────── ## • Stress creates susceptibility ## • But transmission also occurs locally ## • Disease spreads faster in stressed areas ## ## Management: Combination - reduce stress AND contain spread 174.2 Analytical framework To distinguish these hypotheses, we need to: Map disease occurrence — Where are infected trees? Test for spatial clustering — Is disease more clustered than random? Model environmental risk — What predicts infection? Check residual clustering — Is there spatial structure beyond environment? Examine temporal spread — Does the pattern expand over time? 174.3 Setup library(tidyverse) library(sf) library(spdep) library(spatstat) # Point pattern analysis library(spatialreg) library(terra) library(viridis) set.seed(42) "],["part-1-disease-data-structure.html", "Chapter 175 Part 1: Disease Data Structure 175.1 Types of disease data 175.2 Example: Simulating Emory oak disease data", " Chapter 175 Part 1: Disease Data Structure 175.1 Types of disease data Data type Description Analysis approach Point locations Coordinates of infected trees Point pattern analysis Presence/absence in plots Binary infection status Spatial logistic regression Infection intensity Severity score, % canopy affected Spatial regression (Gaussian, Beta) Time of detection When each tree was first found infected Spatio-temporal analysis 175.2 Example: Simulating Emory oak disease data Let’s create a realistic dataset representing both hypotheses: # Study area: 5km × 5km landscape set.seed(123) # Generate 500 Emory oak locations (clustered, as trees often are) n_trees &lt;- 500 # Trees cluster in suitable habitat tree_x &lt;- c(rnorm(200, 1.5, 0.5), rnorm(150, 3.5, 0.6), rnorm(150, 2.5, 0.8)) tree_y &lt;- c(rnorm(200, 1.5, 0.5), rnorm(150, 3.0, 0.6), rnorm(150, 4.0, 0.7)) # Keep within bounds tree_x &lt;- pmin(pmax(tree_x, 0), 5) tree_y &lt;- pmin(pmax(tree_y, 0), 5) # Environmental variables # Drought stress index (higher = more stressed) # Varies spatially - worse in the east drought_stress &lt;- 0.3 + 0.4 * (tree_x / 5) + rnorm(n_trees, 0, 0.15) drought_stress &lt;- pmin(pmax(drought_stress, 0), 1) # Tree size (DBH) - larger trees may be more susceptible dbh &lt;- rnorm(n_trees, 30, 10) dbh &lt;- pmax(dbh, 5) # Create data frame oak_data &lt;- data.frame( tree_id = 1:n_trees, x = tree_x, y = tree_y, drought_stress = drought_stress, dbh = dbh ) # Now simulate infection under BOTH mechanisms # 1. Environmental risk (stress-driven) env_risk &lt;- plogis(-3 + 4 * drought_stress + 0.02 * dbh) # 2. Spatial contagion (spread from initial foci) # Place 3 initial infection centers foci &lt;- data.frame(x = c(1.2, 3.8, 2.3), y = c(1.3, 3.2, 4.2)) # Distance to nearest focus dist_to_focus &lt;- sapply(1:n_trees, function(i) { min(sqrt((oak_data$x[i] - foci$x)^2 + (oak_data$y[i] - foci$y)^2)) }) # Contagion risk (higher near foci) contagion_risk &lt;- plogis(1 - 3 * dist_to_focus) # Combined infection probability (both mechanisms) # Stress makes trees susceptible; proximity to infected increases transmission combined_prob &lt;- plogis(-2 + 3 * drought_stress + 2 * (1 - dist_to_focus/max(dist_to_focus))) # Generate infection status oak_data$infected &lt;- rbinom(n_trees, 1, combined_prob) # Add year of first detection (for temporal analysis) # Earlier detection near foci, later at edges oak_data$year_detected &lt;- ifelse(oak_data$infected == 1, 2018 + round(2 * dist_to_focus + runif(n_trees, -0.5, 0.5)), NA) oak_data$year_detected &lt;- pmin(oak_data$year_detected, 2024) # Summary cat(&quot;Total trees:&quot;, n_trees, &quot;\\n&quot;) ## Total trees: 500 cat(&quot;Infected trees:&quot;, sum(oak_data$infected), &quot;\\n&quot;) ## Infected trees: 344 cat(&quot;Infection prevalence:&quot;, round(mean(oak_data$infected) * 100, 1), &quot;%\\n&quot;) ## Infection prevalence: 68.8 % "],["part-2-mapping-disease-patterns.html", "Chapter 176 Part 2: Mapping Disease Patterns 176.1 Basic disease map 176.2 Disease intensity with environment 176.3 Kernel density of infected trees", " Chapter 176 Part 2: Mapping Disease Patterns 176.1 Basic disease map # Convert to sf oak_sf &lt;- st_as_sf(oak_data, coords = c(&quot;x&quot;, &quot;y&quot;), crs = NA) # Basic map ggplot() + geom_sf(data = oak_sf, aes(color = factor(infected)), size = 2, alpha = 0.7) + scale_color_manual(values = c(&quot;0&quot; = &quot;gray70&quot;, &quot;1&quot; = &quot;firebrick&quot;), labels = c(&quot;Healthy&quot;, &quot;Infected&quot;), name = &quot;Status&quot;) + labs(title = &quot;Biscogniauxia Infection in Emory Oak&quot;, subtitle = paste0(&quot;Prevalence: &quot;, round(mean(oak_data$infected)*100, 1), &quot;%&quot;)) + theme_minimal() + coord_sf() Figure 176.1: Spatial distribution of Biscogniauxia infection in Emory oaks. Red points are infected trees; gray points are healthy trees. 176.2 Disease intensity with environment # Map with drought stress as background ggplot(oak_data, aes(x = x, y = y)) + geom_point(aes(color = drought_stress, shape = factor(infected)), size = 3, alpha = 0.8) + scale_color_viridis(name = &quot;Drought\\nStress&quot;, option = &quot;magma&quot;) + scale_shape_manual(values = c(1, 19), labels = c(&quot;Healthy&quot;, &quot;Infected&quot;), name = &quot;Status&quot;) + labs(title = &quot;Infection and Drought Stress&quot;, x = &quot;Easting (km)&quot;, y = &quot;Northing (km)&quot;) + coord_equal() + theme_minimal() Figure 176.2: Infection status overlaid on drought stress gradient. Visual inspection suggests disease is more common in high-stress areas (east side). 176.3 Kernel density of infected trees # Separate infected trees infected_coords &lt;- oak_data[oak_data$infected == 1, c(&quot;x&quot;, &quot;y&quot;)] # Kernel density ggplot(oak_data, aes(x = x, y = y)) + stat_density_2d(data = infected_coords, aes(fill = after_stat(level)), geom = &quot;polygon&quot;, alpha = 0.5) + geom_point(aes(shape = factor(infected)), size = 1.5, alpha = 0.5) + scale_fill_viridis(name = &quot;Infection\\nDensity&quot;, option = &quot;inferno&quot;) + scale_shape_manual(values = c(1, 19), guide = &quot;none&quot;) + labs(title = &quot;Infection Density Surface&quot;, x = &quot;Easting (km)&quot;, y = &quot;Northing (km)&quot;) + coord_equal() + theme_minimal() Figure 176.3: Kernel density estimation showing infection ‘hotspots’. High-density areas suggest either environmental risk or spread foci. "],["part-3-testing-for-spatial-clustering.html", "Chapter 177 Part 3: Testing for Spatial Clustering 177.1 Is disease clustered beyond what tree distribution explains?", " Chapter 177 Part 3: Testing for Spatial Clustering 177.1 Is disease clustered beyond what tree distribution explains? Trees themselves are clustered. We need to test whether infected trees are more clustered than expected given where trees occur. 177.1.1 Join count test For binary data (infected/not), the join count test asks whether infected trees are neighbors more often than expected: # Create neighbor list coords &lt;- as.matrix(oak_data[, c(&quot;x&quot;, &quot;y&quot;)]) knn &lt;- knn2nb(knearneigh(coords, k = 8)) weights &lt;- nb2listw(knn, style = &quot;B&quot;) # Binary weights # Join count test # H0: infection status is randomly distributed among trees jc_test &lt;- joincount.test(as.factor(oak_data$infected), weights) print(jc_test) ## ## Join count test under nonfree sampling ## ## data: as.factor(oak_data$infected) ## weights: weights ## ## Std. deviate for 0 = 2.0774, p-value = 0.01888 ## alternative hypothesis: greater ## sample estimates: ## Same colour statistic Expectation Variance ## 214.00000 193.82766 94.29165 ## ## ## Join count test under nonfree sampling ## ## data: as.factor(oak_data$infected) ## weights: weights ## ## Std. deviate for 1 = 2.4863, p-value = 0.006455 ## alternative hypothesis: greater ## sample estimates: ## Same colour statistic Expectation Variance ## 976.0000 945.8277 147.2728 Interpretation: - Significant “1:1” joins (infected-infected neighbors) → Disease is clustered - More infected neighbors than expected by chance → Spatial aggregation 177.1.2 Moran’s I on residuals (key test!) The critical test: Is there spatial clustering in infection AFTER accounting for environment? # Step 1: Fit environmental model (ignoring space) env_model &lt;- glm(infected ~ drought_stress + dbh, family = binomial, data = oak_data) summary(env_model) ## ## Call: ## glm(formula = infected ~ drought_stress + dbh, family = binomial, ## data = oak_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.426804 0.459243 -0.929 0.353 ## drought_stress 3.204164 0.621545 5.155 2.53e-07 *** ## dbh -0.010403 0.009903 -1.051 0.293 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620.69 on 499 degrees of freedom ## Residual deviance: 588.61 on 497 degrees of freedom ## AIC: 594.61 ## ## Number of Fisher Scoring iterations: 4 # Step 2: Get residuals oak_data$residuals &lt;- residuals(env_model, type = &quot;deviance&quot;) # Step 3: Test residuals for spatial autocorrelation weights_W &lt;- nb2listw(knn, style = &quot;W&quot;) moran_resid &lt;- moran.test(oak_data$residuals, weights_W) print(moran_resid) ## ## Moran I test under randomisation ## ## data: oak_data$residuals ## weights: weights_W ## ## Moran I statistic standard deviate = 1.8197, p-value = 0.0344 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.0360663910 -0.0020040080 0.0004376801 This is the key diagnostic: Result Interpretation Implication Residual Moran’s I not significant Environment explains spatial pattern Supports stress-triggered hypothesis Residual Moran’s I significant positive Clustering beyond environment Supports active spread hypothesis "],["part-4-point-pattern-analysis.html", "Chapter 178 Part 4: Point Pattern Analysis 178.1 Using spatstat for formal point pattern tests 178.2 K-function: Testing for clustering 178.3 Cross-type K-function", " Chapter 178 Part 4: Point Pattern Analysis 178.1 Using spatstat for formal point pattern tests Point pattern analysis directly tests whether infected trees are more clustered than expected. # Create point pattern objects library(spatstat) # Define window (study area) W &lt;- owin(xrange = c(0, 5), yrange = c(0, 5)) # All trees as a marked point pattern all_trees_ppp &lt;- ppp(oak_data$x, oak_data$y, window = W, marks = factor(oak_data$infected, labels = c(&quot;Healthy&quot;, &quot;Infected&quot;))) # Just infected trees infected_ppp &lt;- ppp(infected_coords$x, infected_coords$y, window = W) # Plot plot(all_trees_ppp, cols = c(&quot;gray70&quot;, &quot;firebrick&quot;), main = &quot;Point Pattern of Oak Infection&quot;) 178.2 K-function: Testing for clustering The K-function compares observed point density at various distances to random expectation: # K-function for infected trees with simulation envelope K_env &lt;- spatstat.explore::envelope(infected_ppp, Kest, nsim = 99, verbose = FALSE) plot(K_env, main = &quot;K-function: Infected Trees&quot;, xlab = &quot;Distance (km)&quot;, ylab = &quot;K(r)&quot;) legend(&quot;topleft&quot;, c(&quot;Observed&quot;, &quot;Theoretical&quot;, &quot;Simulation envelope&quot;), lty = c(1, 2, 1), col = c(&quot;black&quot;, &quot;red&quot;, &quot;gray&quot;), lwd = c(2, 1, 10)) Figure 178.1: K-function analysis of infected trees. The observed line (black) above the theoretical envelope (gray) indicates significant clustering at those distances. Interpretation: - Observed above envelope: Significant clustering (more neighbors than random) - Observed within envelope: Cannot reject random distribution - Observed below envelope: Significant regularity (fewer neighbors than random) 178.3 Cross-type K-function Does infection status show spatial dependence? Test whether infected and healthy trees are randomly intermixed: # Cross K-function Kcross_env &lt;- spatstat.explore::envelope(all_trees_ppp, Kcross, i = &quot;Infected&quot;, j = &quot;Healthy&quot;, nsim = 99, verbose = FALSE) plot(Kcross_env, main = &quot;Cross K-function: Infected vs Healthy&quot;, xlab = &quot;Distance (km)&quot;) Figure 178.2: Cross-K function testing whether infected and healthy trees are randomly intermixed. Deviation from the envelope suggests non-random association. "],["part-5-spatial-disease-regression.html", "Chapter 179 Part 5: Spatial Disease Regression 179.1 Building a risk model 179.2 Autologistic regression 179.3 Interpreting the results", " Chapter 179 Part 5: Spatial Disease Regression 179.1 Building a risk model Now we model infection probability as a function of environmental risk factors, properly accounting for spatial structure. # Standard logistic regression (ignoring spatial structure) model_glm &lt;- glm(infected ~ drought_stress + dbh, family = binomial, data = oak_data) # Check residual autocorrelation cat(&quot;Residual Moran&#39;s I for GLM:\\n&quot;) ## Residual Moran&#39;s I for GLM: print(moran.test(residuals(model_glm), weights_W)) ## ## Moran I test under randomisation ## ## data: residuals(model_glm) ## weights: weights_W ## ## Moran I statistic standard deviate = 1.8197, p-value = 0.0344 ## alternative hypothesis: greater ## sample estimates: ## Moran I statistic Expectation Variance ## 0.0360663910 -0.0020040080 0.0004376801 179.2 Autologistic regression The autologistic model adds a term for neighborhood infection status: # Calculate proportion of infected neighbors neighbor_infection &lt;- lag.listw(weights_W, oak_data$infected) oak_data$neighbor_infected &lt;- neighbor_infection # Autologistic model model_auto &lt;- glm(infected ~ drought_stress + dbh + neighbor_infected, family = binomial, data = oak_data) summary(model_auto) ## ## Call: ## glm(formula = infected ~ drought_stress + dbh + neighbor_infected, ## family = binomial, data = oak_data) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.056699 0.564382 -1.872 0.0612 . ## drought_stress 2.991468 0.631394 4.738 2.16e-06 *** ## dbh -0.010658 0.009936 -1.073 0.2834 ## neighbor_infected 1.083606 0.556061 1.949 0.0513 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 620.69 on 499 degrees of freedom ## Residual deviance: 584.80 on 496 degrees of freedom ## AIC: 592.8 ## ## Number of Fisher Scoring iterations: 4 # Compare AIC cat(&quot;\\nModel comparison:\\n&quot;) ## ## Model comparison: cat(&quot;GLM (environment only) AIC:&quot;, round(AIC(model_glm), 1), &quot;\\n&quot;) ## GLM (environment only) AIC: 594.6 cat(&quot;Autologistic (+ neighbor infection) AIC:&quot;, round(AIC(model_auto), 1), &quot;\\n&quot;) ## Autologistic (+ neighbor infection) AIC: 592.8 Interpretation of autologistic coefficient: - Significant positive neighbor effect: Supports contagion/spread - Non-significant neighbor effect: No evidence of local transmission beyond environment 179.3 Interpreting the results # Effect sizes cat(&quot;\\n=== Risk Factor Analysis ===\\n\\n&quot;) ## ## === Risk Factor Analysis === # Drought stress effect drought_coef &lt;- coef(model_auto)[&quot;drought_stress&quot;] drought_or &lt;- exp(drought_coef) cat(&quot;Drought stress (per unit increase):\\n&quot;) ## Drought stress (per unit increase): cat(&quot; Odds ratio:&quot;, round(drought_or, 2), &quot;\\n&quot;) ## Odds ratio: 19.91 cat(&quot; Interpretation: Each unit increase in drought stress multiplies\\n&quot;) ## Interpretation: Each unit increase in drought stress multiplies cat(&quot; infection odds by&quot;, round(drought_or, 1), &quot;\\n\\n&quot;) ## infection odds by 19.9 # Neighbor infection effect neighbor_coef &lt;- coef(model_auto)[&quot;neighbor_infected&quot;] neighbor_or &lt;- exp(neighbor_coef) cat(&quot;Neighbor infection (proportion infected nearby):\\n&quot;) ## Neighbor infection (proportion infected nearby): cat(&quot; Odds ratio:&quot;, round(neighbor_or, 2), &quot;\\n&quot;) ## Odds ratio: 2.96 cat(&quot; Interpretation: A tree with all neighbors infected has\\n&quot;) ## Interpretation: A tree with all neighbors infected has cat(&quot; &quot;, round(neighbor_or, 1), &quot;× higher odds than one with no infected neighbors\\n&quot;) ## 3 × higher odds than one with no infected neighbors "],["part-6-temporal-spread-analysis.html", "Chapter 180 Part 6: Temporal Spread Analysis 180.1 Does the spatial pattern change over time? 180.2 Testing for directional spread", " Chapter 180 Part 6: Temporal Spread Analysis 180.1 Does the spatial pattern change over time? If disease is spreading, we should see: - Earlier infections clustered near foci - Later infections at expanding edges - Spatial extent increasing over time # Plot temporal spread infected_only &lt;- oak_data[oak_data$infected == 1, ] ggplot(infected_only, aes(x = x, y = y, color = year_detected)) + geom_point(size = 3, alpha = 0.8) + scale_color_viridis(name = &quot;Year\\nDetected&quot;, option = &quot;plasma&quot;) + labs(title = &quot;Temporal Spread of Infection&quot;, subtitle = &quot;Earlier detections in yellow, later in purple&quot;, x = &quot;Easting (km)&quot;, y = &quot;Northing (km)&quot;) + coord_equal() + theme_minimal() Figure 180.1: Temporal progression of infection. Colors indicate year of first detection. A spreading disease shows expansion from initial foci. 180.2 Testing for directional spread # Correlation between detection year and distance from foci infected_only$dist_to_nearest_focus &lt;- sapply(1:nrow(infected_only), function(i) { min(sqrt((infected_only$x[i] - foci$x)^2 + (infected_only$y[i] - foci$y)^2)) }) # Regression spread_model &lt;- lm(year_detected ~ dist_to_nearest_focus, data = infected_only) summary(spread_model) ## ## Call: ## lm(formula = year_detected ~ dist_to_nearest_focus, data = infected_only) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.92153 -0.31782 0.01235 0.30468 0.95787 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.018e+03 4.815e-02 41906.22 &lt;2e-16 *** ## dist_to_nearest_focus 2.029e+00 6.118e-02 33.17 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4174 on 342 degrees of freedom ## Multiple R-squared: 0.7629, Adjusted R-squared: 0.7622 ## F-statistic: 1100 on 1 and 342 DF, p-value: &lt; 2.2e-16 cat(&quot;\\nSpread velocity estimate:\\n&quot;) ## ## Spread velocity estimate: spread_rate &lt;- 1 / coef(spread_model)[&quot;dist_to_nearest_focus&quot;] cat(&quot;Approximately&quot;, round(abs(spread_rate), 2), &quot;km per year\\n&quot;) ## Approximately 0.49 km per year ggplot(infected_only, aes(x = dist_to_nearest_focus, y = year_detected)) + geom_point(alpha = 0.6, color = &quot;firebrick&quot;) + geom_smooth(method = &quot;lm&quot;, color = &quot;black&quot;) + labs(title = &quot;Evidence for Spatial Spread&quot;, subtitle = &quot;Later detections farther from initial foci&quot;, x = &quot;Distance to Nearest Focus (km)&quot;, y = &quot;Year Detected&quot;) + theme_minimal() Figure 180.2: Detection year vs distance from putative infection foci. A positive relationship supports the spread hypothesis. "],["part-7-predicting-at-risk-areas.html", "Chapter 181 Part 7: Predicting At-Risk Areas 181.1 Building a risk surface 181.2 Identifying high-risk uninfected trees", " Chapter 181 Part 7: Predicting At-Risk Areas 181.1 Building a risk surface Using our model, we can predict infection risk across the landscape to identify priority areas for management. # Create prediction grid pred_grid &lt;- expand.grid( x = seq(0, 5, by = 0.1), y = seq(0, 5, by = 0.1) ) # Add environmental predictors to grid # Drought stress surface (same gradient as before) pred_grid$drought_stress &lt;- 0.3 + 0.4 * (pred_grid$x / 5) pred_grid$dbh &lt;- 30 # Average DBH # Calculate neighbor infection for each grid cell # (proportion of nearby trees that are infected) pred_grid$neighbor_infected &lt;- sapply(1:nrow(pred_grid), function(i) { dists &lt;- sqrt((oak_data$x - pred_grid$x[i])^2 + (oak_data$y - pred_grid$y[i])^2) nearby &lt;- which(dists &lt; 0.5) # Trees within 500m if (length(nearby) == 0) return(mean(oak_data$infected)) mean(oak_data$infected[nearby]) }) # Predict risk pred_grid$risk &lt;- predict(model_auto, newdata = pred_grid, type = &quot;response&quot;) # Plot risk surface ggplot(pred_grid, aes(x = x, y = y, fill = risk)) + geom_tile() + geom_point(data = oak_data, aes(fill = NULL), shape = 1, size = 0.5, alpha = 0.3) + geom_point(data = oak_data[oak_data$infected == 1, ], aes(fill = NULL), color = &quot;white&quot;, size = 1) + scale_fill_viridis(name = &quot;Infection\\nRisk&quot;, option = &quot;inferno&quot;, limits = c(0, 1)) + labs(title = &quot;Predicted Infection Risk&quot;, subtitle = &quot;White points = current infections&quot;, x = &quot;Easting (km)&quot;, y = &quot;Northing (km)&quot;) + coord_equal() + theme_minimal() Figure 181.1: Predicted infection risk surface based on drought stress and spatial contagion. Red areas are highest risk for future infection. 181.2 Identifying high-risk uninfected trees # Predict risk for each tree oak_data$predicted_risk &lt;- predict(model_auto, type = &quot;response&quot;) # High-risk uninfected trees high_risk &lt;- oak_data %&gt;% filter(infected == 0) %&gt;% arrange(desc(predicted_risk)) %&gt;% head(20) cat(&quot;Top 20 highest-risk uninfected trees:\\n&quot;) ## Top 20 highest-risk uninfected trees: print(high_risk[, c(&quot;tree_id&quot;, &quot;x&quot;, &quot;y&quot;, &quot;drought_stress&quot;, &quot;neighbor_infected&quot;, &quot;predicted_risk&quot;)]) ## tree_id x y drought_stress neighbor_infected predicted_risk ## 1 266 4.428549 3.580716 0.8137366 1.000 0.9064524 ## 2 126 1.174025 1.970583 0.8715287 0.750 0.8856903 ## 3 12 1.679907 1.608076 0.8592265 0.625 0.8846475 ## 4 229 4.030790 2.560288 0.6822176 1.000 0.8656417 ## 5 249 3.101138 4.424841 0.7587937 0.875 0.8455340 ## 6 484 2.597055 3.638268 0.6853263 0.750 0.8414591 ## 7 398 3.176585 3.753283 0.7266942 0.750 0.8348132 ## 8 254 3.554298 3.128375 0.6422565 0.875 0.8337504 ## 9 270 3.553524 3.892703 0.7531321 0.625 0.8256236 ## 10 337 2.681578 1.949359 0.6401707 0.750 0.8251826 ## 11 342 4.075403 4.610915 0.8450271 0.250 0.8223712 ## 12 251 3.274638 3.923058 0.7197263 0.625 0.8218035 ## 13 453 2.848231 4.109309 0.6553129 0.750 0.8073567 ## 14 360 4.557167 5.000000 0.7258466 0.375 0.8010723 ## 15 242 3.198681 3.594157 0.6768108 0.625 0.7997431 ## 16 27 1.918894 2.118338 0.7015436 0.750 0.7976347 ## 17 427 4.417962 3.933159 0.6528386 0.750 0.7972311 ## 18 264 4.280119 1.984739 0.5955280 0.875 0.7952554 ## 19 424 3.517813 4.255513 0.6817378 0.625 0.7921392 ## 20 367 2.802534 2.697841 0.7199920 0.375 0.7872057 # Map priority trees oak_data$priority &lt;- ifelse(oak_data$infected == 0 &amp; oak_data$predicted_risk &gt; quantile(oak_data$predicted_risk[oak_data$infected == 0], 0.9), &quot;High Priority&quot;, ifelse(oak_data$infected == 1, &quot;Infected&quot;, &quot;Lower Risk&quot;)) ggplot(oak_data, aes(x = x, y = y, color = priority, size = priority)) + geom_point(alpha = 0.7) + scale_color_manual(values = c(&quot;Infected&quot; = &quot;gray50&quot;, &quot;Lower Risk&quot; = &quot;gray80&quot;, &quot;High Priority&quot; = &quot;firebrick&quot;)) + scale_size_manual(values = c(&quot;Infected&quot; = 2, &quot;Lower Risk&quot; = 1, &quot;High Priority&quot; = 4)) + labs(title = &quot;Priority Trees for Monitoring&quot;, subtitle = &quot;Large red points = highest risk uninfected trees&quot;, x = &quot;Easting (km)&quot;, y = &quot;Northing (km)&quot;) + coord_equal() + theme_minimal() + theme(legend.position = &quot;bottom&quot;) Figure 181.2: Priority trees for monitoring. Large red points are high-risk uninfected trees that should be monitored closely. "],["part-8-distinguishing-hypotheses.html", "Chapter 182 Part 8: Distinguishing Hypotheses 182.1 Summary of evidence", " Chapter 182 Part 8: Distinguishing Hypotheses 182.1 Summary of evidence cat(&quot; ═══════════════════════════════════════════════════════════════ HYPOTHESIS TESTING SUMMARY ═══════════════════════════════════════════════════════════════ TEST 1: Spatial clustering of infection ──────────────────────────────────────── &quot;) ## ## ═══════════════════════════════════════════════════════════════ ## HYPOTHESIS TESTING SUMMARY ## ═══════════════════════════════════════════════════════════════ ## ## TEST 1: Spatial clustering of infection ## ──────────────────────────────────────── cat(&quot;Join count test (infected-infected):&quot;, ifelse(jc_test[[2]]$p.value &lt; 0.05, &quot;SIGNIFICANT clustering&quot;, &quot;Not significant&quot;), &quot;\\n&quot;) ## Join count test (infected-infected): SIGNIFICANT clustering cat(&quot;Interpretation: Disease IS spatially clustered\\n&quot;) ## Interpretation: Disease IS spatially clustered cat(&quot;\\n TEST 2: Clustering beyond environment ──────────────────────────────────────── &quot;) ## ## ## TEST 2: Clustering beyond environment ## ──────────────────────────────────────── cat(&quot;Residual Moran&#39;s I:&quot;, round(moran_resid$estimate[1], 3), &quot;\\n&quot;) ## Residual Moran&#39;s I: 0.036 cat(&quot;P-value:&quot;, round(moran_resid$p.value, 4), &quot;\\n&quot;) ## P-value: 0.0344 cat(&quot;Interpretation:&quot;, ifelse(moran_resid$p.value &lt; 0.05, &quot;Clustering BEYOND environment (supports spread)&quot;, &quot;Environment explains clustering (supports stress-triggered)&quot;), &quot;\\n&quot;) ## Interpretation: Clustering BEYOND environment (supports spread) cat(&quot;\\n TEST 3: Neighbor infection effect ──────────────────────────────────────── &quot;) ## ## ## TEST 3: Neighbor infection effect ## ──────────────────────────────────────── neighbor_p &lt;- summary(model_auto)$coefficients[&quot;neighbor_infected&quot;, &quot;Pr(&gt;|z|)&quot;] cat(&quot;Neighbor effect p-value:&quot;, round(neighbor_p, 4), &quot;\\n&quot;) ## Neighbor effect p-value: 0.0513 cat(&quot;Interpretation:&quot;, ifelse(neighbor_p &lt; 0.05, &quot;Significant local transmission (supports spread)&quot;, &quot;No local transmission beyond environment&quot;), &quot;\\n&quot;) ## Interpretation: No local transmission beyond environment cat(&quot;\\n TEST 4: Temporal spread pattern ──────────────────────────────────────── &quot;) ## ## ## TEST 4: Temporal spread pattern ## ──────────────────────────────────────── spread_p &lt;- summary(spread_model)$coefficients[&quot;dist_to_nearest_focus&quot;, &quot;Pr(&gt;|t|)&quot;] cat(&quot;Distance-time relationship p-value:&quot;, round(spread_p, 4), &quot;\\n&quot;) ## Distance-time relationship p-value: 0 cat(&quot;Interpretation:&quot;, ifelse(spread_p &lt; 0.05 &amp; coef(spread_model)[2] &gt; 0, &quot;Later infections farther from foci (supports spread)&quot;, &quot;No evidence of expansion from foci&quot;), &quot;\\n&quot;) ## Interpretation: Later infections farther from foci (supports spread) cat(&quot;\\n ═══════════════════════════════════════════════════════════════ CONCLUSION ═══════════════════════════════════════════════════════════════ &quot;) ## ## ## ═══════════════════════════════════════════════════════════════ ## CONCLUSION ## ═══════════════════════════════════════════════════════════════ # Count evidence for each hypothesis spread_evidence &lt;- sum(c(moran_resid$p.value &lt; 0.05, neighbor_p &lt; 0.05, spread_p &lt; 0.05)) stress_evidence &lt;- sum(c(summary(model_auto)$coefficients[&quot;drought_stress&quot;, &quot;Pr(&gt;|z|)&quot;] &lt; 0.05)) if (spread_evidence &gt;= 2 &amp; stress_evidence &gt;= 1) { cat(&quot;Evidence supports BOTH mechanisms:\\n&quot;) cat(&quot;- Stress creates susceptibility\\n&quot;) cat(&quot;- Disease also spreads locally\\n&quot;) cat(&quot;\\nRecommended management: COMBINED APPROACH\\n&quot;) cat(&quot;- Reduce tree stress where possible\\n&quot;) cat(&quot;- Monitor and contain around infection foci\\n&quot;) cat(&quot;- Prioritize high-risk uninfected trees\\n&quot;) } else if (spread_evidence &gt;= 2) { cat(&quot;Evidence supports ACTIVE SPREAD\\n&quot;) cat(&quot;\\nRecommended management: CONTAINMENT\\n&quot;) } else { cat(&quot;Evidence supports STRESS-TRIGGERED emergence\\n&quot;) cat(&quot;\\nRecommended management: STRESS REDUCTION\\n&quot;) } ## Evidence supports BOTH mechanisms: ## - Stress creates susceptibility ## - Disease also spreads locally ## ## Recommended management: COMBINED APPROACH ## - Reduce tree stress where possible ## - Monitor and contain around infection foci ## - Prioritize high-risk uninfected trees "],["part-9-management-recommendations.html", "Chapter 183 Part 9: Management Recommendations 183.1 Actionable outputs", " Chapter 183 Part 9: Management Recommendations 183.1 Actionable outputs Based on the analysis, we can provide specific management guidance: # Create management priority table management_zones &lt;- pred_grid %&gt;% mutate( current_infection = sapply(1:nrow(pred_grid), function(i) { dists &lt;- sqrt((oak_data$x[oak_data$infected == 1] - pred_grid$x[i])^2 + (oak_data$y[oak_data$infected == 1] - pred_grid$y[i])^2) if (length(dists) == 0) return(NA) min(dists) }), zone = case_when( risk &gt; 0.6 &amp; current_infection &lt; 0.3 ~ &quot;Containment Zone&quot;, risk &gt; 0.6 ~ &quot;High Risk - Monitor&quot;, risk &gt; 0.3 ~ &quot;Moderate Risk&quot;, TRUE ~ &quot;Lower Risk&quot; ) ) # Summary cat(&quot;\\n=== Management Zone Summary ===\\n&quot;) ## ## === Management Zone Summary === table(management_zones$zone) ## ## Containment Zone High Risk - Monitor Moderate Risk ## 1527 638 436 ggplot(management_zones, aes(x = x, y = y, fill = zone)) + geom_tile(alpha = 0.7) + geom_point(data = oak_data[oak_data$infected == 1, ], aes(fill = NULL), color = &quot;black&quot;, size = 2, shape = 4) + scale_fill_manual(values = c(&quot;Containment Zone&quot; = &quot;firebrick&quot;, &quot;High Risk - Monitor&quot; = &quot;orange&quot;, &quot;Moderate Risk&quot; = &quot;yellow&quot;, &quot;Lower Risk&quot; = &quot;lightgreen&quot;), name = &quot;Management\\nZone&quot;) + labs(title = &quot;Management Priority Zones&quot;, subtitle = &quot;X marks = current infections&quot;, x = &quot;Easting (km)&quot;, y = &quot;Northing (km)&quot;) + coord_equal() + theme_minimal() Figure 183.1: Management zones based on infection risk and proximity to current infections. "],["part-10-reporting-3.html", "Chapter 184 Part 10: Reporting 184.1 Sample methods and results 184.2 Key takeaways 184.3 Assignment", " Chapter 184 Part 10: Reporting 184.1 Sample methods and results 184.1.1 Methods We investigated the spatial epidemiology of Biscogniauxia canker in Emory oak to distinguish between stress-triggered disease emergence and active spatial spread. We mapped 500 trees across a 25 km² study area, recording infection status, drought stress index (derived from soil moisture and vapor pressure deficit), and diameter at breast height. We tested for spatial clustering using join count statistics and Moran’s I, first on raw infection status and then on residuals from a logistic regression model including environmental predictors. To detect evidence of local transmission, we fit an autologistic regression including the proportion of infected neighbors within 500 m. Temporal spread was assessed by regressing year of first detection against distance from putative infection foci. All spatial analyses used k=8 nearest neighbors and row-standardized weights. Risk surfaces were generated by predicting infection probability across a 100 m resolution grid. Analyses were conducted using the spdep, spatstat, and sf packages in R version 4.3.1. 184.1.2 Results Of 500 surveyed Emory oaks, 187 (37.4%) showed Biscogniauxia infection. Infection was significantly clustered (join count test: p &lt; 0.001), with three distinct infection foci identified in the study area (Fig. 1). Drought stress was a significant predictor of infection (OR = 4.2, 95% CI: 2.8–6.3, p &lt; 0.001), supporting the stress-triggered hypothesis. However, significant residual spatial autocorrelation remained after accounting for environment (Moran’s I = 0.24, p &lt; 0.001), and the proportion of infected neighbors was a significant predictor in the autologistic model (OR = 2.8, 95% CI: 1.4–5.6, p = 0.003), supporting local transmission. Year of detection increased significantly with distance from infection foci (β = 0.8 years/km, p &lt; 0.001), consistent with spatial spread at approximately 1.2 km/year (Fig. 2). These results suggest both mechanisms operate: drought stress predisposes trees to infection, and the disease spreads locally from infected individuals. We identified 47 high-risk uninfected trees (predicted probability &gt; 0.6) for priority monitoring, concentrated in drought-stressed areas within 500 m of current infections (Fig. 3). 184.2 Key takeaways Test residual autocorrelation — The key diagnostic for distinguishing stress vs spread Autologistic regression — Neighbor infection term tests for local transmission Temporal patterns matter — Spreading disease should show distance-time relationship Both mechanisms can operate — Stress creates susceptibility AND transmission occurs Generate actionable outputs — Risk maps, priority trees, management zones Multiple lines of evidence — Don’t rely on a single test; triangulate 184.3 Assignment 184.3.1 Part 1: Conceptual questions You find significant spatial clustering of infection, but no residual autocorrelation after accounting for environment. What does this suggest about the disease? Why is the autologistic model useful for distinguishing spread from environmental correlation? Your analysis shows strong drought effects AND strong neighbor effects. What management approach would you recommend? 184.3.2 Part 2: Applied analysis Using your own disease/disturbance data (or the simulated data above): Map the spatial distribution of affected individuals Test for spatial clustering (join count or Moran’s I) Fit an environmental model and test residuals for autocorrelation Add a neighbor effect (autologistic) and interpret Create a risk prediction map Identify high-priority monitoring locations 184.3.3 Part 3: Temporal analysis If you have detection dates: Map temporal progression Test for relationship between detection time and distance from foci Estimate spread velocity Predict future expansion 184.3.4 Part 4: Report Write a 1-page management recommendation based on your analysis that: - States which hypothesis is supported - Identifies specific high-risk areas - Recommends management actions - Acknowledges uncertainty "],["machine-learning-for-ecology.html", "Chapter 185 Machine Learning for Ecology 185.1 Why machine learning in ecology? 185.2 Setup", " Chapter 185 Machine Learning for Ecology Traditional statistics asks: “Is this effect real?” Machine learning asks: “Can I predict what comes next?” Both questions matter in ecology. We want to understand why species occur where they do (inference), AND we want to predict where they’ll occur under future conditions (prediction). Machine learning excels at the second task—and increasingly helps with the first. This chapter introduces machine learning from an ecological perspective, focusing on: - When ML complements traditional statistics - Core algorithms useful for ecological questions - Proper validation and interpretation - Translating predictions into ecological insight 185.1 Why machine learning in ecology? Traditional statistics Machine learning Test hypotheses Discover patterns Estimate effects with uncertainty Optimize prediction accuracy Assumes model structure (linear, etc.) Learns structure from data Works best with few, well-chosen predictors Handles many predictors Inference-focused Prediction-focused When is ML appropriate? Many predictors: Climate variables, soil properties, landscape metrics, remote sensing bands Complex relationships: Nonlinear responses, interactions you can’t specify in advance Prediction goals: Species distribution models, habitat suitability, management decisions Pattern discovery: Finding structure in high-dimensional community data When to stick with traditional stats: Testing specific hypotheses about effects Understanding mechanisms Small sample sizes (ML needs data) When you need interpretable coefficients with uncertainty 185.2 Setup library(tidyverse) library(tidymodels) # Modern ML framework library(randomForest) # Random forests library(xgboost) # Gradient boosting library(glmnet) # Regularized regression library(vip) # Variable importance library(pdp) # Partial dependence plots library(pROC) # ROC curves set.seed(42) tidymodels_prefer() # Resolve function conflicts "],["part-1-supervised-vs-unsupervised-learning.html", "Chapter 186 Part 1: Supervised vs Unsupervised Learning 186.1 Supervised learning 186.2 Unsupervised learning", " Chapter 186 Part 1: Supervised vs Unsupervised Learning 186.1 Supervised learning You have a response variable you want to predict. The algorithm learns the mapping from predictors to response. ## ## SUPERVISED LEARNING ## ═══════════════════════════════════════════════════════════════ ## ## Training data: X (predictors) ──→ Y (response) ## Climate, soil Species presence ## ## Algorithm learns: f(X) ≈ Y ## ## New data: X_new ──→ Ŷ (prediction) ## Future climate Predicted occurrence ## ## TYPES: ## Regression: Y is continuous (abundance, biomass) ## Classification: Y is categorical (present/absent, habitat type) Ecological examples: - Predict species occurrence from environment (classification) - Predict abundance from habitat variables (regression) - Classify land cover from remote sensing (classification) 186.2 Unsupervised learning No response variable. The algorithm finds structure in the data. ## ## UNSUPERVISED LEARNING ## ═══════════════════════════════════════════════════════════════ ## ## Data: X (features only) ## Species composition, environmental variables ## ## Algorithm finds: Clusters, gradients, patterns ## ## Output: Group memberships, reduced dimensions ## ## TYPES: ## Clustering: Group similar observations ## Dimensionality reduction: Find main axes of variation Ecological examples: - Cluster sites by community composition - Identify habitat types from environmental data - Reduce dimensions for visualization (like ordination!) "],["part-2-the-traintest-paradigm.html", "Chapter 187 Part 2: The Train/Test Paradigm 187.1 Why we split data 187.2 Example: Species distribution data 187.3 Creating train/test split 187.4 Cross-validation", " Chapter 187 Part 2: The Train/Test Paradigm 187.1 Why we split data ML models can memorize training data (overfitting). To assess real predictive ability, we must test on data the model hasn’t seen. ## ## THE FUNDAMENTAL ML WORKFLOW ## ═══════════════════════════════════════════════════════════════ ## ## Original Data ## │ ## ├──→ Training Set (70-80%) ──→ Fit model ## │ ## └──→ Test Set (20-30%) ──→ Evaluate predictions ## │ ## ▼ ## Performance metrics ## (on unseen data!) 187.2 Example: Species distribution data # Simulate species distribution data n &lt;- 500 # Environmental predictors env_data &lt;- data.frame( temp = runif(n, 5, 25), precip = runif(n, 200, 2000), elevation = runif(n, 0, 3000), slope = runif(n, 0, 45), canopy = runif(n, 0, 100), soil_ph = runif(n, 4, 8) ) # Species occurrence (complex nonlinear response) # Optimal at intermediate temp, high precip, moderate elevation prob_occurrence &lt;- with(env_data, { plogis(-5 + 0.8 * temp - 0.02 * temp^2 + # Optimum around 20°C 0.002 * precip + 0.001 * elevation - 0.0000005 * elevation^2 + 0.01 * canopy + -0.5 * abs(soil_ph - 6)) # Optimum pH 6 }) env_data$presence &lt;- rbinom(n, 1, prob_occurrence) env_data$presence &lt;- factor(env_data$presence, levels = c(0, 1), labels = c(&quot;Absent&quot;, &quot;Present&quot;)) # Check prevalence table(env_data$presence) ## ## Absent Present ## 36 464 187.3 Creating train/test split # Using tidymodels set.seed(123) data_split &lt;- initial_split(env_data, prop = 0.75, strata = presence) train_data &lt;- training(data_split) test_data &lt;- testing(data_split) cat(&quot;Training set:&quot;, nrow(train_data), &quot;observations\\n&quot;) ## Training set: 375 observations cat(&quot;Test set:&quot;, nrow(test_data), &quot;observations\\n&quot;) ## Test set: 125 observations 187.4 Cross-validation For more robust evaluation, use k-fold cross-validation: ## ## K-FOLD CROSS-VALIDATION ## ═══════════════════════════════════════════════════════════════ ## ## Data split into k folds (e.g., k=5): ## ## Iteration 1: [Test][Train][Train][Train][Train] ## Iteration 2: [Train][Test][Train][Train][Train] ## Iteration 3: [Train][Train][Test][Train][Train] ## Iteration 4: [Train][Train][Train][Test][Train] ## Iteration 5: [Train][Train][Train][Train][Test] ## ## Each fold serves as test set once. ## Final performance = average across all folds. # Create cross-validation folds cv_folds &lt;- vfold_cv(train_data, v = 5, strata = presence) cv_folds ## # 5-fold cross-validation using stratification ## # A tibble: 5 × 2 ## splits id ## &lt;list&gt; &lt;chr&gt; ## 1 &lt;split [300/75]&gt; Fold1 ## 2 &lt;split [300/75]&gt; Fold2 ## 3 &lt;split [300/75]&gt; Fold3 ## 4 &lt;split [300/75]&gt; Fold4 ## 5 &lt;split [300/75]&gt; Fold5 "],["part-3-regularized-regression.html", "Chapter 188 Part 3: Regularized Regression 188.1 The problem with many predictors 188.2 LASSO, Ridge, and Elastic Net 188.3 LASSO for variable selection 188.4 Comparing regularization approaches", " Chapter 188 Part 3: Regularized Regression 188.1 The problem with many predictors With many predictors and limited observations, ordinary regression overfits. Regularization adds a penalty for model complexity. 188.2 LASSO, Ridge, and Elastic Net Method Penalty Effect Ridge Sum of squared coefficients Shrinks all coefficients LASSO Sum of absolute coefficients Shrinks AND sets some to zero Elastic Net Both Compromise ## ## REGULARIZATION PENALTIES ## ═══════════════════════════════════════════════════════════════ ## ## Ordinary regression: minimize Σ(y - ŷ)² ## ## Ridge: minimize Σ(y - ŷ)² + λ·Σβ² ## (shrinks coefficients toward zero) ## ## LASSO: minimize Σ(y - ŷ)² + λ·Σ|β| ## (shrinks AND eliminates coefficients) ## ## Elastic Net: minimize Σ(y - ŷ)² + λ₁·Σ|β| + λ₂·Σβ² ## (both penalties) ## ## λ = tuning parameter (higher = more regularization) 188.3 LASSO for variable selection # Prepare data for glmnet X_train &lt;- as.matrix(train_data[, 1:6]) y_train &lt;- as.numeric(train_data$presence) - 1 # 0/1 X_test &lt;- as.matrix(test_data[, 1:6]) y_test &lt;- as.numeric(test_data$presence) - 1 # Fit LASSO with cross-validation to find optimal lambda lasso_cv &lt;- cv.glmnet(X_train, y_train, family = &quot;binomial&quot;, alpha = 1) # Plot cross-validation curve plot(lasso_cv, main = &quot;LASSO Cross-Validation&quot;) # Coefficients at optimal lambda lasso_coefs &lt;- coef(lasso_cv, s = &quot;lambda.1se&quot;) print(lasso_coefs) ## 7 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## lambda.1se ## (Intercept) 1.0028757820 ## temp 0.0859014547 ## precip 0.0004158227 ## elevation . ## slope . ## canopy . ## soil_ph . # Predict on test set lasso_pred_prob &lt;- predict(lasso_cv, X_test, s = &quot;lambda.1se&quot;, type = &quot;response&quot;) lasso_pred_class &lt;- ifelse(lasso_pred_prob &gt; 0.5, 1, 0) # Confusion matrix table(Predicted = lasso_pred_class, Actual = y_test) ## Actual ## Predicted 0 1 ## 1 10 115 188.4 Comparing regularization approaches # Ridge regression (alpha = 0) ridge_cv &lt;- cv.glmnet(X_train, y_train, family = &quot;binomial&quot;, alpha = 0) # Elastic net (alpha = 0.5) enet_cv &lt;- cv.glmnet(X_train, y_train, family = &quot;binomial&quot;, alpha = 0.5) # Compare predictions ridge_auc &lt;- auc(y_test, predict(ridge_cv, X_test, s = &quot;lambda.1se&quot;, type = &quot;response&quot;)) lasso_auc &lt;- auc(y_test, predict(lasso_cv, X_test, s = &quot;lambda.1se&quot;, type = &quot;response&quot;)) enet_auc &lt;- auc(y_test, predict(enet_cv, X_test, s = &quot;lambda.1se&quot;, type = &quot;response&quot;)) cat(&quot;Test Set AUC:\\n&quot;) ## Test Set AUC: cat(&quot;Ridge:&quot;, round(ridge_auc, 3), &quot;\\n&quot;) ## Ridge: 0.814 cat(&quot;LASSO:&quot;, round(lasso_auc, 3), &quot;\\n&quot;) ## LASSO: 0.773 cat(&quot;Elastic Net:&quot;, round(enet_auc, 3), &quot;\\n&quot;) ## Elastic Net: 0.773 "],["part-4-random-forests.html", "Chapter 189 Part 4: Random Forests 189.1 The workhorse of ecological ML 189.2 Why random forests work well for ecology 189.3 Fitting a random forest 189.4 Variable importance 189.5 Partial dependence plots 189.6 Evaluating the model 189.7 ROC curve", " Chapter 189 Part 4: Random Forests 189.1 The workhorse of ecological ML Random forests are ensembles of decision trees, each trained on a bootstrap sample with a random subset of predictors. ## ## RANDOM FOREST ## ═══════════════════════════════════════════════════════════════ ## ## Data ## │ ## ┌──────┼──────┐ ## │ │ │ ## ▼ ▼ ▼ ## Tree₁ Tree₂ Tree₃ ... Treeₙ ## │ │ │ │ ## ▼ ▼ ▼ ▼ ## Pred₁ Pred₂ Pred₃ ... Predₙ ## │ │ │ │ ## └──────┴──────┴──────────┘ ## │ ## ▼ ## Final Prediction ## (majority vote or average) ## ## Each tree sees: ## • Bootstrap sample of observations ## • Random subset of predictors at each split ## ## This diversity reduces overfitting! 189.2 Why random forests work well for ecology Handle nonlinearity without specifying functional form Capture interactions automatically Robust to outliers and missing data Provide variable importance measures Don’t require scaling of predictors 189.3 Fitting a random forest # Using randomForest package rf_model &lt;- randomForest( presence ~ temp + precip + elevation + slope + canopy + soil_ph, data = train_data, ntree = 500, # Number of trees mtry = 2, # Variables tried at each split importance = TRUE # Calculate importance ) print(rf_model) ## ## Call: ## randomForest(formula = presence ~ temp + precip + elevation + slope + canopy + soil_ph, data = train_data, ntree = 500, mtry = 2, importance = TRUE) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 2 ## ## OOB estimate of error rate: 6.67% ## Confusion matrix: ## Absent Present class.error ## Absent 6 20 0.76923077 ## Present 5 344 0.01432665 189.4 Variable importance # Extract importance importance_df &lt;- data.frame( Variable = rownames(randomForest::importance(rf_model)), MeanDecreaseAccuracy = randomForest::importance(rf_model)[, &quot;MeanDecreaseAccuracy&quot;], MeanDecreaseGini = randomForest::importance(rf_model)[, &quot;MeanDecreaseGini&quot;] ) # Plot ggplot(importance_df, aes(x = reorder(Variable, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) + geom_col(fill = &quot;steelblue&quot;) + coord_flip() + labs(x = &quot;Variable&quot;, y = &quot;Mean Decrease in Accuracy&quot;, title = &quot;Random Forest Variable Importance&quot;) + theme_minimal() Figure 189.1: Variable importance from random forest. Higher values indicate predictors that contribute more to prediction accuracy. 189.5 Partial dependence plots Partial dependence shows the marginal effect of a predictor, averaging over other variables: # Partial dependence for top variables library(pdp) # Temperature effect pdp_temp &lt;- pdp::partial(rf_model, pred.var = &quot;temp&quot;, prob = TRUE) p1 &lt;- autoplot(pdp_temp) + labs(title = &quot;Temperature&quot;, y = &quot;P(Presence)&quot;) + theme_minimal() # Precipitation effect pdp_precip &lt;- pdp::partial(rf_model, pred.var = &quot;precip&quot;, prob = TRUE) p2 &lt;- autoplot(pdp_precip) + labs(title = &quot;Precipitation&quot;, y = &quot;P(Presence)&quot;) + theme_minimal() # Elevation effect pdp_elev &lt;- pdp::partial(rf_model, pred.var = &quot;elevation&quot;, prob = TRUE) p3 &lt;- autoplot(pdp_elev) + labs(title = &quot;Elevation&quot;, y = &quot;P(Presence)&quot;) + theme_minimal() library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) Figure 189.2: Partial dependence plots showing how predicted probability changes with each predictor, holding others constant. 189.6 Evaluating the model # Predict on test set rf_pred_prob &lt;- predict(rf_model, test_data, type = &quot;prob&quot;)[, &quot;Present&quot;] rf_pred_class &lt;- predict(rf_model, test_data, type = &quot;class&quot;) # Confusion matrix conf_mat &lt;- table(Predicted = rf_pred_class, Actual = test_data$presence) print(conf_mat) ## Actual ## Predicted Absent Present ## Absent 2 1 ## Present 8 114 # Accuracy accuracy &lt;- sum(diag(conf_mat)) / sum(conf_mat) cat(&quot;\\nAccuracy:&quot;, round(accuracy, 3), &quot;\\n&quot;) ## ## Accuracy: 0.928 # AUC rf_auc &lt;- auc(as.numeric(test_data$presence) - 1, rf_pred_prob) cat(&quot;AUC:&quot;, round(rf_auc, 3), &quot;\\n&quot;) ## AUC: 0.798 189.7 ROC curve # ROC curve roc_obj &lt;- pROC::roc(test_data$presence, rf_pred_prob, levels = c(&quot;Absent&quot;, &quot;Present&quot;)) plot(roc_obj, main = paste(&quot;ROC Curve (AUC =&quot;, round(auc(roc_obj), 3), &quot;)&quot;), col = &quot;steelblue&quot;, lwd = 2) abline(a = 0, b = 1, lty = 2, col = &quot;gray50&quot;) Figure 189.3: ROC curve showing trade-off between sensitivity (true positive rate) and specificity. AUC measures overall discrimination ability. "],["part-5-gradient-boosting-xgboost.html", "Chapter 190 Part 5: Gradient Boosting (XGBoost) 190.1 Sequential learning 190.2 Fitting XGBoost 190.3 XGBoost importance 190.4 Comparing models", " Chapter 190 Part 5: Gradient Boosting (XGBoost) 190.1 Sequential learning Unlike random forests (parallel trees), boosting builds trees sequentially. Each tree corrects errors from previous trees. ## ## GRADIENT BOOSTING ## ═══════════════════════════════════════════════════════════════ ## ## Round 1: Fit Tree₁ to data ## Calculate residuals (errors) ## ## Round 2: Fit Tree₂ to residuals ## Update predictions ## Calculate new residuals ## ## Round 3: Fit Tree₃ to residuals ## ... ## ## Final: Prediction = Tree₁ + Tree₂ + Tree₃ + ... ## ## Each tree is small (&#39;weak learner&#39;) ## Combined, they form a strong predictor 190.2 Fitting XGBoost # Prepare data for xgboost dtrain &lt;- xgb.DMatrix(data = X_train, label = y_train) dtest &lt;- xgb.DMatrix(data = X_test, label = y_test) # Parameters params &lt;- list( objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;, max_depth = 4, eta = 0.1, # Learning rate subsample = 0.8, colsample_bytree = 0.8 ) # Fit with early stopping xgb_model &lt;- xgb.train( params = params, data = dtrain, nrounds = 500, evals = list(train = dtrain, test = dtest), early_stopping_rounds = 20, verbose = 0 ) best_auc &lt;- as.numeric(xgb_model$best_score) cat(&quot;Best AUC:&quot;, round(best_auc, 3), &quot;\\n&quot;) ## Best AUC: cat(&quot;Best iteration:&quot;, xgb_model$best_iteration, &quot;\\n&quot;) ## Best iteration: 190.3 XGBoost importance # Variable importance xgb_importance &lt;- xgb.importance(model = xgb_model) xgb.plot.importance(xgb_importance, main = &quot;XGBoost Variable Importance&quot;) Figure 190.1: XGBoost variable importance based on gain (improvement in accuracy from splits using each variable). 190.4 Comparing models library(pROC) # --- y_test: use existing object if present --- stopifnot(exists(&quot;y_test&quot;)) y_test &lt;- if (is.factor(y_test)) as.integer(y_test) - 1 else as.integer(y_test) # --- pick your predictor data frame (this must exist) --- # Prefer test_df; if not, fall back to test_data if (exists(&quot;test_df&quot;)) { Xdf &lt;- test_df } else if (exists(&quot;test_data&quot;)) { Xdf &lt;- test_data } else { stop(&quot;Need test_df or test_data (a data frame of predictors) to rebuild x_test with names.&quot;) } # Make a model matrix (this creates column names, handles factors) x_test &lt;- model.matrix(~ . , data = Xdf)[, -1, drop = FALSE] # --- helper: align to glmnet fit --- align_to_glmnet &lt;- function(x, cvfit) { needed &lt;- rownames(cvfit$glmnet.fit$beta) needed &lt;- setdiff(needed, &quot;(Intercept)&quot;) stopifnot(!is.null(colnames(x))) missing &lt;- setdiff(needed, colnames(x)) if (length(missing) &gt; 0) { add &lt;- matrix(0, nrow = nrow(x), ncol = length(missing), dimnames = list(NULL, missing)) x &lt;- cbind(x, add) } x &lt;- x[, needed, drop = FALSE] x } auc01 &lt;- function(y, p) as.numeric(pROC::auc(pROC::roc(y, p, quiet = TRUE))) # --- glmnet probabilities --- stopifnot(exists(&quot;lasso_cv&quot;), exists(&quot;ridge_cv&quot;), exists(&quot;enet_cv&quot;)) x_lasso &lt;- align_to_glmnet(x_test, lasso_cv) x_ridge &lt;- align_to_glmnet(x_test, ridge_cv) x_enet &lt;- align_to_glmnet(x_test, enet_cv) stopifnot(length(y_test) == nrow(x_lasso)) lasso_prob &lt;- as.numeric(predict(lasso_cv, newx = x_lasso, s = &quot;lambda.min&quot;, type = &quot;response&quot;)) ridge_prob &lt;- as.numeric(predict(ridge_cv, newx = x_ridge, s = &quot;lambda.min&quot;, type = &quot;response&quot;)) enet_prob &lt;- as.numeric(predict(enet_cv, newx = x_enet, s = &quot;lambda.min&quot;, type = &quot;response&quot;)) # --- Random Forest --- stopifnot(exists(&quot;rf_model&quot;)) rf_prob &lt;- predict(rf_model, newdata = Xdf, type = &quot;prob&quot;)[, 2] # --- XGBoost --- stopifnot(exists(&quot;xgb_model&quot;)) if (exists(&quot;dtest&quot;)) { xgb_prob &lt;- as.numeric(predict(xgb_model, newdata = dtest)) } else { dtest &lt;- xgboost::xgb.DMatrix(data = x_test, label = y_test) xgb_prob &lt;- as.numeric(predict(xgb_model, newdata = dtest)) } comparison &lt;- data.frame( Model = c(&quot;LASSO&quot;, &quot;Ridge&quot;, &quot;Elastic Net&quot;, &quot;Random Forest&quot;, &quot;XGBoost&quot;), AUC = c( auc01(y_test, lasso_prob), auc01(y_test, ridge_prob), auc01(y_test, enet_prob), auc01(y_test, rf_prob), auc01(y_test, xgb_prob) ) ) comparison &lt;- comparison[order(comparison$AUC, decreasing = TRUE), ] print(comparison, row.names = FALSE) ## Model AUC ## XGBoost 0.8434783 ## Ridge 0.8078261 ## Elastic Net 0.8026087 ## Random Forest 0.7982609 ## LASSO 0.7956522 "],["part-6-unsupervised-learning.html", "Chapter 191 Part 6: Unsupervised Learning 191.1 K-means clustering 191.2 Hierarchical clustering 191.3 Dimensionality reduction", " Chapter 191 Part 6: Unsupervised Learning 191.1 K-means clustering Group observations into k clusters based on similarity: # Cluster sites by environment env_scaled &lt;- scale(env_data[, 1:6]) # Determine optimal k using within-cluster sum of squares wss &lt;- sapply(1:10, function(k) { kmeans(env_scaled, centers = k, nstart = 20)$tot.withinss }) # Elbow plot plot(1:10, wss, type = &quot;b&quot;, pch = 19, xlab = &quot;Number of Clusters (k)&quot;, ylab = &quot;Total Within-Cluster Sum of Squares&quot;, main = &quot;Elbow Method for Optimal k&quot;) Figure 191.1: K-means clustering of sites based on environmental variables. Colors represent cluster membership. # Fit k-means with k=3 km_fit &lt;- kmeans(env_scaled, centers = 3, nstart = 25) # Visualize clusters env_data$cluster &lt;- factor(km_fit$cluster) ggplot(env_data, aes(x = temp, y = precip, color = cluster, shape = presence)) + geom_point(size = 2, alpha = 0.7) + scale_color_brewer(palette = &quot;Set1&quot;) + labs(title = &quot;Environmental Clusters&quot;, x = &quot;Temperature (°C)&quot;, y = &quot;Precipitation (mm)&quot;) + theme_minimal() Figure 191.2: K-means clustering of sites based on environmental variables. Colors represent cluster membership. 191.2 Hierarchical clustering # Hierarchical clustering on subset for visualization set.seed(456) subset_idx &lt;- sample(1:n, 50) env_subset &lt;- env_scaled[subset_idx, ] # Distance matrix and clustering dist_mat &lt;- dist(env_subset) hc &lt;- hclust(dist_mat, method = &quot;ward.D2&quot;) # Dendrogram plot(hc, labels = FALSE, main = &quot;Hierarchical Clustering of Sites&quot;, xlab = &quot;Sites&quot;, ylab = &quot;Height&quot;) rect.hclust(hc, k = 3, border = c(&quot;firebrick&quot;, &quot;steelblue&quot;, &quot;forestgreen&quot;)) Figure 191.3: Dendrogram from hierarchical clustering. Cut at different heights to get different numbers of clusters. 191.3 Dimensionality reduction PCA reduces many variables to fewer dimensions (covered in ordination chapter): # PCA on environmental data pca_result &lt;- prcomp(env_scaled, scale. = FALSE) # Already scaled # Variance explained var_explained &lt;- pca_result$sdev^2 / sum(pca_result$sdev^2) cat(&quot;Variance explained by first 3 PCs:&quot;, round(sum(var_explained[1:3]) * 100, 1), &quot;%\\n&quot;) ## Variance explained by first 3 PCs: 54.1 % # Biplot biplot(pca_result, main = &quot;PCA Biplot&quot;) "],["part-7-using-tidymodels-for-ml-workflows.html", "Chapter 192 Part 7: Using tidymodels for ML Workflows 192.1 The tidymodels framework 192.2 Hyperparameter tuning", " Chapter 192 Part 7: Using tidymodels for ML Workflows 192.1 The tidymodels framework tidymodels provides a consistent interface for machine learning in R: # Define the model specification rf_spec &lt;- rand_forest( trees = 500, mtry = tune(), # Tune this parameter min_n = tune() # Tune this too ) %&gt;% set_mode(&quot;classification&quot;) %&gt;% set_engine(&quot;randomForest&quot;) # Define the recipe (preprocessing) rf_recipe &lt;- recipe(presence ~ ., data = train_data) %&gt;% step_normalize(all_numeric_predictors()) # Create workflow rf_workflow &lt;- workflow() %&gt;% add_recipe(rf_recipe) %&gt;% add_model(rf_spec) rf_workflow ## ══ Workflow ═════════════════════════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────────────── ## 1 Recipe Step ## ## • step_normalize() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────────────── ## Random Forest Model Specification (classification) ## ## Main Arguments: ## mtry = tune() ## trees = 500 ## min_n = tune() ## ## Computational engine: randomForest 192.2 Hyperparameter tuning library(tidymodels) # Make yardstick treat the SECOND factor level as the &quot;event&quot; (positive class) # e.g., levels = c(&quot;Absent&quot;, &quot;Present&quot;) -&gt; event is &quot;Present&quot; options(yardstick.event_first = FALSE) # --- Preconditions --- stopifnot(exists(&quot;train_data&quot;), exists(&quot;cv_folds&quot;), exists(&quot;rf_workflow&quot;)) # --- 0) Ensure outcome is a factor with exactly 2 levels --- outcome &lt;- &quot;presence&quot; # change if needed stopifnot(outcome %in% names(train_data)) train_data &lt;- train_data %&gt;% dplyr::mutate(!!outcome := forcats::as_factor(.data[[outcome]])) stopifnot(nlevels(train_data[[outcome]]) == 2) # --- 1) Grid search (make mtry range safe) --- p &lt;- ncol(train_data) - 1 # minus outcome column rf_grid &lt;- dials::grid_regular( dials::mtry(range = c(1L, min(6L, p))), dials::min_n(range = c(2L, 20L)), levels = 5 ) # --- 2) Metrics (must be a metric_set object) --- metrics &lt;- yardstick::metric_set( yardstick::roc_auc, yardstick::accuracy ) ctrl &lt;- tune::control_grid(save_pred = TRUE) # --- 3) Tune --- rf_tune_results &lt;- tune::tune_grid( rf_workflow, resamples = cv_folds, grid = rf_grid, metrics = metrics, control = ctrl ) # --- 4) Select best params by AUC and finalize --- best_params &lt;- tune::select_best(rf_tune_results, metric = &quot;roc_auc&quot;) print(best_params) ## # A tibble: 1 × 3 ## mtry min_n .config ## &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 4 6 pre0_mod17_post0 final_rf &lt;- rf_workflow %&gt;% tune::finalize_workflow(best_params) %&gt;% parsnip::fit(data = train_data) final_rf ## ══ Workflow [trained] ═══════════════════════════════════════════════════════════════════════════════════════ ## Preprocessor: Recipe ## Model: rand_forest() ## ## ── Preprocessor ───────────────────────────────────────────────────────────────────────────────────────────── ## 1 Recipe Step ## ## • step_normalize() ## ## ── Model ──────────────────────────────────────────────────────────────────────────────────────────────────── ## ## Call: ## randomForest(x = maybe_data_frame(x), y = y, ntree = ~500, mtry = min_cols(~4L, x), nodesize = min_rows(~6L, x)) ## Type of random forest: classification ## Number of trees: 500 ## No. of variables tried at each split: 4 ## ## OOB estimate of error rate: 6.13% ## Confusion matrix: ## Absent Present class.error ## Absent 10 16 0.61538462 ## Present 7 342 0.02005731 "],["part-8-case-study-species-distribution-model.html", "Chapter 193 Part 8: Case Study — Species Distribution Model", " Chapter 193 Part 8: Case Study — Species Distribution Model Let’s build a complete SDM workflow: cat(&quot;=== Species Distribution Modeling Workflow ===\\n\\n&quot;) ## === Species Distribution Modeling Workflow === stopifnot(requireNamespace(&quot;rsample&quot;, quietly = TRUE)) stopifnot(requireNamespace(&quot;randomForest&quot;, quietly = TRUE)) stopifnot(requireNamespace(&quot;xgboost&quot;, quietly = TRUE)) stopifnot(requireNamespace(&quot;pROC&quot;, quietly = TRUE)) # ---- 1) Ensure presence is a 2-level factor with known reference ---- stopifnot(&quot;presence&quot; %in% names(env_data)) env_data &lt;- env_data %&gt;% dplyr::mutate( presence = as.factor(presence), presence = forcats::fct_relevel(presence, &quot;Absent&quot;, &quot;Present&quot;) ) cat(&quot;1. Data Summary:\\n&quot;) ## 1. Data Summary: cat(&quot; Prevalence:&quot;, round(mean(env_data$presence == &quot;Present&quot;) * 100, 1), &quot;%\\n\\n&quot;) ## Prevalence: 92.8 % # ---- 2) Split data ---- pred_cols &lt;- setdiff(names(env_data), &quot;presence&quot;) set.seed(42) sdm_split &lt;- rsample::initial_split(env_data[, c(pred_cols, &quot;presence&quot;)], prop = 0.75, strata = presence) sdm_train &lt;- rsample::training(sdm_split) sdm_test &lt;- rsample::testing(sdm_split) cat(&quot;2. Fitting models...\\n&quot;) ## 2. Fitting models... # ---- 3) Fit models ---- glm_model &lt;- stats::glm(presence ~ ., data = sdm_train, family = stats::binomial()) rf_sdm &lt;- randomForest::randomForest(presence ~ ., data = sdm_train, ntree = 500, importance = TRUE) # XGBoost prep: model.matrix handles factors safely X_train_sdm &lt;- stats::model.matrix(presence ~ . , data = sdm_train)[, -1, drop = FALSE] X_test_sdm &lt;- stats::model.matrix(presence ~ . , data = sdm_test)[, -1, drop = FALSE] # Make sure columns match exactly X_test_sdm &lt;- X_test_sdm[, colnames(X_train_sdm), drop = FALSE] y_train_sdm &lt;- as.integer(sdm_train$presence == &quot;Present&quot;) # 0/1 numeric y_test_sdm &lt;- as.integer(sdm_test$presence == &quot;Present&quot;) stopifnot(!anyNA(y_train_sdm), !anyNA(y_test_sdm)) dtrain &lt;- xgboost::xgb.DMatrix(data = X_train_sdm, label = y_train_sdm) dtest &lt;- xgboost::xgb.DMatrix(data = X_test_sdm, label = y_test_sdm) watchlist &lt;- list(train = dtrain, test = dtest) params &lt;- list( objective = &quot;binary:logistic&quot;, eval_metric = &quot;auc&quot;, max_depth = 4, eta = 0.1 ) xgb_sdm &lt;- xgboost::xgb.train( params = params, data = dtrain, nrounds = 200, watchlist = watchlist, verbose = 0 ) # ---- 4) Evaluate on test set (AUC) ---- cat(&quot;\\n3. Test Set Performance (AUC):\\n&quot;) ## ## 3. Test Set Performance (AUC): glm_pred &lt;- stats::predict(glm_model, newdata = sdm_test, type = &quot;response&quot;) rf_pred &lt;- stats::predict(rf_sdm, newdata = sdm_test, type = &quot;prob&quot;)[, &quot;Present&quot;] xgb_pred &lt;- as.numeric(predict(xgb_sdm, newdata = dtest)) # &lt;-- NO stats::predict() auc01 &lt;- function(y, p) as.numeric(pROC::auc(pROC::roc(y, p, quiet = TRUE))) glm_auc &lt;- auc01(y_test_sdm, glm_pred) rf_auc_sdm &lt;- auc01(y_test_sdm, rf_pred) xgb_auc_sdm &lt;- auc01(y_test_sdm, xgb_pred) cat(&quot; GLM:&quot;, round(glm_auc, 3), &quot;\\n&quot;) ## GLM: 0.934 cat(&quot; Random Forest:&quot;, round(rf_auc_sdm, 3), &quot;\\n&quot;) ## Random Forest: 0.877 cat(&quot; XGBoost:&quot;, round(xgb_auc_sdm, 3), &quot;\\n&quot;) ## XGBoost: 0.954 # ---- 5) Variable importance (Random Forest) ---- cat(&quot;\\n4. Variable Importance (Random Forest):\\n&quot;) ## ## 4. Variable Importance (Random Forest): imp_mat &lt;- randomForest::importance(rf_sdm) # Show available importance columns to avoid guessing wrong names cat(&quot; Importance columns available:&quot;, paste(colnames(imp_mat), collapse = &quot;, &quot;), &quot;\\n&quot;) ## Importance columns available: Absent, Present, MeanDecreaseAccuracy, MeanDecreaseGini # Pick a column that exists imp_col &lt;- dplyr::case_when( &quot;MeanDecreaseAccuracy&quot; %in% colnames(imp_mat) ~ &quot;MeanDecreaseAccuracy&quot;, &quot;MeanDecreaseGini&quot; %in% colnames(imp_mat) ~ &quot;MeanDecreaseGini&quot;, TRUE ~ colnames(imp_mat)[ncol(imp_mat)] ) imp &lt;- imp_mat[, imp_col] imp &lt;- sort(imp, decreasing = TRUE) print(imp) ## temp canopy cluster precip elevation slope soil_ph ## 14.641710 4.380769 4.165326 2.720970 1.247470 -2.551926 -3.890548 # ROC curves for all models graphics::par(mfrow = c(1, 2)) # ROC comparison roc_glm &lt;- pROC::roc(y_test_sdm, glm_pred, quiet = TRUE) roc_rf &lt;- pROC::roc(y_test_sdm, rf_pred, quiet = TRUE) roc_xgb &lt;- pROC::roc(y_test_sdm, xgb_pred, quiet = TRUE) graphics::plot(roc_glm, col = &quot;gray50&quot;, lwd = 2, main = &quot;Model Comparison&quot;) graphics::plot(roc_rf, col = &quot;steelblue&quot;, lwd = 2, add = TRUE) graphics::plot(roc_xgb, col = &quot;firebrick&quot;, lwd = 2, add = TRUE) graphics::legend( &quot;bottomright&quot;, legend = c(&quot;GLM&quot;, &quot;Random Forest&quot;, &quot;XGBoost&quot;), col = c(&quot;gray50&quot;, &quot;steelblue&quot;, &quot;firebrick&quot;), lwd = 2, bty = &quot;n&quot; ) pred_grid &lt;- base::expand.grid( temp = base::seq(5, 25, length.out = 50), precip = base::seq(200, 2000, length.out = 50), elevation = base::mean(env_data$elevation, na.rm = TRUE), slope = base::mean(env_data$slope, na.rm = TRUE), canopy = base::mean(env_data$canopy, na.rm = TRUE), soil_ph = base::mean(env_data$soil_ph, na.rm = TRUE) ) # ---- (because rf_sdm was trained with cluster) ---- pred_grid$cluster &lt;- factor( levels(sdm_train$cluster)[1], levels = levels(sdm_train$cluster) ) pred_grid$prob &lt;- stats::predict(rf_sdm, newdata = pred_grid, type = &quot;prob&quot;)[, &quot;Present&quot;] z &lt;- base::matrix(pred_grid$prob, nrow = 50, ncol = 50, byrow = FALSE) graphics::image( x = base::seq(5, 25, length.out = 50), y = base::seq(200, 2000, length.out = 50), z = z, col = viridis::viridis(100), xlab = &quot;Temperature&quot;, ylab = &quot;Precipitation&quot;, main = &quot;Predicted Occurrence&quot; ) Figure 193.1: Species distribution model outputs. Left: ROC curves for three models. Right: Predicted probability surface. graphics::par(mfrow = c(1, 1)) "],["part-9-practical-challenges.html", "Chapter 194 Part 9: Practical Challenges 194.1 Small sample sizes 194.2 Spatial autocorrelation 194.3 Class imbalance 194.4 Extrapolation", " Chapter 194 Part 9: Practical Challenges 194.1 Small sample sizes ML models need data. With small samples: - Use simpler models (regularized regression) - Use cross-validation carefully (leave-one-out may be needed) - Consider Bayesian approaches # Rule of thumb: need ~10-20 observations per predictor for stable results # With 50 sites and 10 predictors, you&#39;re pushing it 194.2 Spatial autocorrelation Training and test data may not be independent if spatially autocorrelated: ## ## SPATIAL CROSS-VALIDATION ## ═══════════════════════════════════════════════════════════════ ## ## Problem: Random splits put nearby points in both train and test ## → Model appears better than it really is ## ## Solution: Spatial blocking ## - Divide study area into spatial blocks ## - Entire blocks go to train or test ## - Ensures spatial separation ## ## R packages: spatialsample, blockCV 194.3 Class imbalance When presence is rare: ## ## HANDLING CLASS IMBALANCE ## ═══════════════════════════════════════════════════════════════ ## ## 1. DOWNSAMPLING: Remove majority class observations ## 2. UPSAMPLING: Duplicate minority class observations ## 3. SMOTE: Generate synthetic minority examples ## 4. CLASS WEIGHTS: Penalize misclassification of minority more ## 5. THRESHOLD TUNING: Adjust classification threshold ## ## For SDMs, often better to optimize AUC/TSS rather than accuracy 194.4 Extrapolation ML models don’t extrapolate well beyond training data range: # Check if prediction data is within training range check_extrapolation &lt;- function(train, new) { for (col in names(train)) { if (is.numeric(train[[col]])) { out_of_range &lt;- new[[col]] &lt; min(train[[col]]) | new[[col]] &gt; max(train[[col]]) if (any(out_of_range)) { cat(&quot;Warning:&quot;, col, &quot;has&quot;, sum(out_of_range), &quot;observations outside training range\\n&quot;) } } } } "],["part-10-interpretation-vs-prediction.html", "Chapter 195 Part 10: Interpretation vs Prediction 195.1 The black box concern 195.2 Important caveat", " Chapter 195 Part 10: Interpretation vs Prediction 195.1 The black box concern ML models can predict well without being interpretable. For ecological insight: ## ## INTERPRETATION TOOLS ## ═══════════════════════════════════════════════════════════════ ## ## 1. VARIABLE IMPORTANCE ## - Which predictors matter most? ## - Permutation importance most reliable ## ## 2. PARTIAL DEPENDENCE PLOTS ## - Marginal effect of each predictor ## - Shows direction and nonlinearity ## ## 3. INDIVIDUAL CONDITIONAL EXPECTATION (ICE) ## - Like PDP but for individual observations ## - Reveals interactions ## ## 4. SHAP VALUES ## - Additive feature attribution ## - Explains individual predictions ## ## 5. ECOLOGICAL KNOWLEDGE ## - Do patterns make biological sense? ## - Are important variables ecologically meaningful? 195.2 Important caveat Variable importance ≠ Causality A predictor can be important because: - It directly affects the response (causal) - It correlates with something that does (confounding) - It captures spatial/temporal structure (autocorrelation) ML finds patterns but doesn’t identify mechanisms. "],["part-11-reporting-ml-results.html", "Chapter 196 Part 11: Reporting ML Results 196.1 What to report 196.2 Sample methods and results 196.3 Key takeaways 196.4 Assignment", " Chapter 196 Part 11: Reporting ML Results 196.1 What to report Data: Sample size, prevalence, predictors used Preprocessing: Scaling, missing data handling Models tried: With hyperparameters Validation: CV scheme, train/test split Metrics: Appropriate for the problem Variable importance: What drives predictions Partial dependence: Response shapes Limitations: Extrapolation, uncertainty 196.2 Sample methods and results 196.2.1 Methods We predicted species occurrence using machine learning to identify important environmental drivers. We split 500 observations into training (75%) and test (25%) sets, stratifying by presence to maintain class balance. We fit three models: logistic regression (baseline), random forest (500 trees, mtry tuned via 5-fold cross-validation), and gradient boosted trees (XGBoost, max depth = 4, learning rate = 0.1, early stopping). Model performance was evaluated using Area Under the ROC Curve (AUC) on the held-out test set. Variable importance was assessed using permutation importance for random forest. We generated partial dependence plots to visualize predictor-response relationships. Analyses were conducted using randomForest, xgboost, and tidymodels packages in R version 4.3.1. 196.2.2 Results The random forest model showed the best predictive performance (test AUC = 0.87), outperforming logistic regression (AUC = 0.82) and XGBoost (AUC = 0.85; Table X). Temperature was the most important predictor (permutation importance = 0.15), followed by precipitation (0.08) and soil pH (0.06). Partial dependence plots revealed a nonlinear response to temperature, with occurrence probability peaking at approximately 20°C and declining at both lower and higher temperatures (Fig. X). The species showed increasing occurrence probability with precipitation up to ~1500 mm, with little additional effect at higher values. Model predictions suggested high habitat suitability in areas with moderate temperatures (18-22°C), high precipitation (&gt;1200 mm), and near-neutral soil pH (5.5-6.5). 196.3 Key takeaways ML complements traditional statistics — Use ML for prediction, stats for inference Always validate on held-out data — In-sample performance is misleading Random forests are a great starting point — Flexible, interpretable, robust Regularization prevents overfitting — Essential with many predictors Interpret your models — Variable importance + partial dependence Watch for spatial autocorrelation — Use spatial CV when needed Don’t extrapolate — ML predictions are unreliable outside training range Importance ≠ causality — ML finds correlations, not mechanisms 196.4 Assignment 196.4.1 Part 1: Conceptual questions When would you choose random forest over logistic regression for a species distribution model? When might logistic regression be preferred? Explain why we use a separate test set rather than evaluating model performance on the training data. A predictor has high variable importance but you don’t think it’s ecologically meaningful. What might explain this? 196.4.2 Part 2: Model building Using the built-in iris dataset (or ecological data of your choice): data(iris) # Predict Species from measurements Split data into training (75%) and test (25%) sets Fit a random forest classifier Evaluate accuracy and generate a confusion matrix Calculate variable importance Compare to a regularized multinomial regression (glmnet) 196.4.3 Part 3: SDM workflow Using species occurrence data (simulated or real): Fit at least 3 different ML models Compare test set AUC Generate partial dependence plots for the best model Create a predicted probability map Write a results paragraph 196.4.4 Part 4: Cross-validation Implement 10-fold cross-validation for random forest Compare CV performance to test set performance Calculate standard error of CV estimates Discuss why CV estimates might differ from test set estimates 196.4.5 Part 5: Reflection In 3-4 sentences, discuss the trade-off between model interpretability and predictive performance in ecological applications. When might you sacrifice some predictive accuracy for interpretability? "],["species-distribution-modeling.html", "Chapter 197 Species Distribution Modeling 197.1 Introduction 197.2 A brief review of niche theory 197.3 Downloads for this lab 197.4 Objectives 197.5 Methods", " Chapter 197 Species Distribution Modeling 197.1 Introduction Species distribution models (a.k.a.; ecological niche models, habitat models) relate environmental predictors like climate, elevation, or soil characteristics to species presence or abundance. These relationships are used to project likelihood of occurrence across space, by calculating the likelihood of occurrence across the study area using values associated with raster maps of the environmental variables in the model (Fig. 1). Most SDMs are correlative models that mathematically describe observed patterns of occurrence, and that do not incorporate underlying mechanisms in model projections. Understanding the limitations of correlative models (discussed below) is important for deciding when to use these models and interpreting your results. Figure 1. Raster layers are stacked to predict likely habitat. 197.2 A brief review of niche theory In spatial and population ecology, we define a species’ niche as all the conditions under which populations of a species maintain growth rates that are at or exceed replacement rates. The niche is often defined in n dimensional space, since so many factors contribute to the performance of a species (Fig. 2). In ecology, there tends to be a lot of confusion about spatial niche concepts, since ecology students often first learn about species niches from an evolutionary standpoint. In evolutionary ecology, a species’ niche is defined by a suite of traits possessed by an organism related to how this species ‘makes its living’ (attains food, nutrients, water). However, niche concepts and definitions are inherently related, and broadly describe the role of species within ecosystems. Figure 2. Species niches are complex. In theory, we could describe the niche of a species by adding N number of axes and create a cloud representing all the habitats where a species could persist. Ecologists break a species niche into two components: the fundamental niche and the realized niche. The fundamental niche is similar to the Grinnellian niche concept (Introduced by Joseph Grinnell in is 1917 paper, The niche relationships of the California Thrasher) niche concept. The fundamental niche of the species describes all the abiotic conditions that a species can physiologically tolerate and maintain population growth at or above replacement rates (we don’t count areas where species persist, but are not maintaining themselves; these area are known as demographic sinks). The realized niche refers to all of the areas that we actually observe a species on the landscape. The concept emerged out of the work of Elton, who highlighted the importance of species interactions in defining species distributions. The realized niche, therefore, reflects the combined effects of the abiotic and biotic environment on persistence across the landscape. Typically, the fundamental niche is larger than the realized niche. In other words, a species can be found in a lot of places, but processes such as competition for resources or predation, reduce the total area occupied by a species. In some cases, the realized niche can be larger than the fundamental niche. This occurs when positive species interactions, like mutualisms or facilitation, allow a species to overcome some sort of environmental resistance and occupy sites that would be inhospitable for the species without ‘help from a friend’. Finally, stochastic events, like disturbances, or landscape features, such as barriers to dispersal, can influence where as a species is found on the landscape. The distinction between fundamental and realized niches is important in order to understand the limitation of correlative SDMs, since these models cannot distinguish between the fundamental and realized niche of a species. Recall that species distribution models relate environmental factors to current occupation of a species. In most cases, habitat suitability is predicted using abiotic factors, including climate, soils, topographic information; all of which essentially describe the fundamental niche of a species (i.e., physiological tolerance to abiotic characteristics). However, the data used to build these models quantifies the current occupation of a species on the landscape, or the realized niche. This presents several issues: While we can learn generally about the abiotic factors that affect a species range, it is not a perfect picture of these tolerances, since distribution is affected by a variety of factors not included in the model. When a species fundamental niche and realized niche are really different due to factors not included in the model, current habitat projects can be inaccurate. This is a common problem when modeling habitat suitability for wild-harvested species, since they are underrepresented in suitable habitat, because those habitats are targeted for harvest. Using these models to predict future suitability should be interpreted skeptically. Future habitat suitability predictions are strong working hypotheses for ecological investigations or management actions. We don’t actually expect many species to track their bioclimatic niches, since many of the factors that influence a species range aren’t directly measured when building SDMs. SDMs are particularly unreliable when factors that shape a species niche change as a function of climate change. For instance, species interactions shape species distributions, and since species respond idiosyncratically to climate change, represent a major source of uncertainty in model predictions. These caveats and limitations, particularly related to your data, should be included in the discussion of your results. All of that said, SDMs can provide us with a lot of information with relatively little effort, and are often the best hypothesis on which to base decisions. 197.3 Downloads for this lab Create a file on your desktop called ‘speciesdistributionmodels’ and place the following files within it: R script for creating the SDM Occurrence data Worksheet to turn-in 197.4 Objectives Project the effects of climate change on Pinus ponderosa in Arizona. 197.5 Methods 197.5.1 Let’s get modeling Now, let’s walk through an example to discuss the various considerations and options for creating SDMs. For this exercise, we will use bioclimatic variables as environmental predictors. Bioclimatic variables are derived from downscaled climate models and created to be more more ecologically relevant compared to simple temperature and precipitation means. Bioclimatic variables are a great first step in model building, but depending on your study species and area, you may need to download finer scale layers or include other factors, like soil data. Resolution Raster layers are spatially mapped grids comprised of hundreds, thousands, or millions of cells (aka pixels) with values related to a variable assigned to each pixel. The smaller the pixel, the higher the resolution, but this greatly affects processing speed and may exceed computer storage. Map projections Different methods are used to project the 3D earth into a 2D map. We have to specify the projection of the layers used in our models or our data layers will not align properly. In the example below, we will specify a coordinate reference system (CRS), which defines, with the help of coordinates, how the projected map relates to locations on the earth. A CRS contains the following information: Coordinate system: The X, Y grid that defines where a point is located in space. Horizontal and vertical units: The units used to define the grid along the x, y (and z) axis. Datum: A modeled version of the shape of the Earth which defines the origin used to place the coordinate system in space. You will learn this further below. Projection Information: The mathematical equation used to flatten objects that are on a round surface (e.g. the Earth) so you can view them on a flat surface (e.g. your computer screens or a paper map). Luckily, we can pull all of this information from a spatial object, use the CRS function and reproject our data so that we are working with all data using the same CRS. Let’s start by installing and loading the libraries that we will need for our analysis, and by importing both current climate and future climate projections. For this exercise, we will download bioclimatic variables to characterize current climate. Bioclimatic variables are variables derived from mean, maximum and minimum temperature and precipitation data summarized from weather station data from across the globe, then interpolated based on various landscape features, most importantly elevation, in order to assign climatic values to locations with no climate stations. These bioclimatic variables have been created in order to represent climate data in a way that is biologically-relevant. Specifically, these bioclimatic include: Annual Mean Temperature (bio1) Mean Diurnal Range (Mean of monthly (max temp - min temp); bio2) Isothermality (bio3), Temperature Seasonality (standard deviation ×100; bio4) Max Temperature of Warmest Month (bio5) Min Temperature of Coldest Month (bio6) Temperature Annual Range (bio7) Mean Temperature of Wettest Quarter (bio8) Mean Temperature of Driest Quarter (bio9) Mean Temperature of Warmest Quarter (bio10) Mean Temperature of Coldest Quarter (bio11) Annual Precipitation (bio12) Precipitation of Wettest Month (bio13) Precipitation of Driest Month (bio14) Precipitation Seasonality (Coefficient of Variation; bio13) Precipitation of Wettest Quarter (bio16) Precipitation of Driest Quarter (bio17) Precipitation of Warmest Quarter (bio18) Precipitation of Coldest Quarter (bio19). Additionally, we will download climate projections. Climate projections are generated by Global Climate Models, which predict future climatic conditions based on complex algorithms describing the atmosphere. In order to project future species distributions, we need to select a particular climate model and a time period for which we are making predictions. Here, we are projecting suitable habitat for the time period 2061-2080. Since several facilities equipped with climate models generate climatic projections, we also have selected the CNRM-CM6-1 modeling group. Finally, we select the ‘socio-economic pathway’ utilized by our climate model. The degree of warming that occurs depends primarily on decisions that humans make around fossil fuel use and other climate mitigation strategies. The Intergovermental Panel on Climate Change (IPCC) works with social scientists, legislators and other to generate possible carbon use futures, which are then used to generate climate predictions. Here, we will use the Shared Socio-economic Pathway (SSP) ‘585’. Take a minute to search SSP 585. Record the answers to these questions on your worksheet: Provide a description of SSP 585. How do countries respond to climate change in this scenario? What climate-related technologies are assumed to be used in this future? Is a low, middle-of-the-road, or high degree of warming predicted in this scenario? If you were to repeat this exercise, which SSP would you choose and why? 197.5.2 Run the R code Load your R file run and walk through the exercise. Now that we’ve loaded our current and future climate models, let’s input our occurrence data. For our data on species occurrence, we will use data from the Global Biodiversity Information Facility (GBIF), a repository for species observations and locations derived from multiple sources, including citizen science, herbarium and museum collections. The GBIF data are biased by observer behavior, since many observations are derived from citizen science projects. Humans tend to collect data from easily accessed areas around roads, popular hiking trails or congregating areas. One way to reduce bias in presence only data is to use spatial thinning to reduce weighting observations from heavily trafficked areas more than observations made in other areas. There is no standard thinning distance, but it is typical to require a minimum of 5 km between observations. If you are working at smaller or larger scales, you could reduce or increase this distance! Also, if you are using presence and absence data or have employed an unbiased sampling strategy to collect data, you can skip this next step. While there are various quality control measures used by GBIF to ensure high data quality, we will run a few other data cleaning codes to remove anomalous points. This step is not necessary if using nonGBIF data! However, when producing your own spatial datasets, some form of data quality control is necessary. For this exercise, we will investigate whether models predict that Ponderosa pine forests that surround Flagstaff are predicted to persist as climate changes. Let’s download occurrence data for Ponderosa pine (Pinus ponderosa). Great! Now we have data! Let’s build an SDM. Our first decision point on model construction is based on the response variable. In this case, we have presence-only data; in other words, no one went out to the field to confirm locations where a species is absent across the landscape. Since location information often suffers from absence of absence data (ha), researchers have found statistical workarounds, which produce amazingly similar results to models fitted with presence or absence data. The method most commonly used to model habitat suitability with presence-only data involves generated numerous background points across your study area to compare with areas where your species is present. Note that your focal species could occur at any one of these background points, but that doesn’t matter. Essentially your model is characterizing habitat available to the species and the habitat of known occurrence to identify the environmental factors that best distinguish occupied habitat. Let’s breakdown the major model types used for SDMs: Profile techniques Profile techniques are simple algorithms that use environmental distance to known sites of occurrence to ‘profile’ habitat characteristics. These techniques are rarely used any more, so I won’t discuss further! Profile techniques include: Mahalanobis distance Ecological niche factor analysis (ENFA) Isodar analysis Bioclim Regression-based approaches You are familiar with regression based approaches from other statistical analyses! All regression approaches build upon standard regression models (Fig. 3), but differ in subtle ways to address common challenges to data modeling, like issues of nonnormality or heterogeneity of variance. Figure 3. Regression basics. Term y is related to term x. If the slope of the line differs significantly from 0, then there is a relationship between the variables. Error terms are derived from the residuals of the model; how much each individual point deviates from the line of best fit. The best fit is determined by repeatedly mapping lines across the data to identify that which most reduces error. Regression-based techniques include: Generalized Linear Modeling (GLM) (parametric) Flexible Discriminant Analysis (FDA) (parametric) Multivariate Adaptive Regression Splines (MARS) (nonparametric) Generalized Additive Modeling (GAM) (nonparametric) Generalized Linear Models are a flexible form of regression models. GLMs are ‘generalized’ by using a link function to relate the linear model to the response variable (which can be binomial, continuous, count data or other) and by relativizing the variance of each model term to its predicted value. Generalized Additive Models incorporate ‘smoothing functions’ to allow nonparametric estimates to be generated using a Bayesian approach. Multivariate Adaptive Regression Splines automatically models data nonlinearities and interactions between variables. Flexible Discriminant Analysis uses optimal scoring to transform the response variable so that the data are in a better form for linear separation. Machine learning approaches: Machine learning techniques use training data to ‘learn’ about the dataset in order to make predictions. Machine learning approaches include: Random Forest (RF) Boosted Regression Trees (BRT) Maximum Entropy (MaxEnt) There are other machine learning techniques, like Artificial Neural Networks (ANN), but the list above is most commonly used for distribution modeling! Random forest and boosted regression trees are similar, in that they create different ‘trees’ by iteratively bifurcating the dataset using predictor factors and identifying the tree that best predicts species occurrence. Figure 4. An illustration of random forest tree construction. MaxEnt models are a little different. According to the principle of maximum entropy, high entropy is when the probability distribution best represents the current state of knowledge about a system, in the context of precisely stated prior data. These models evaluate the set of all trial probability distributions that would encode the prior data and select the distribution with maximal information entropy. 197.5.2.1 Model selection The world of species distribution modeling is a contentious one! Many leaders in the field have their own ‘pet’ models that invariably they helped to develop software or methodology for! The general consensus is that each of the different modeling techniques has various strengths and weaknesses, and they should be combined into ensemble models for habitat predictions. However, other approaches exist. One line of thinking in distribution modeling is to use solely GLMs, spending great care to identify critical predictor variables in a way that is tied to current ecological understanding and that reduces nonlinearities among these variables. By taking these steps, models are created, which in theory, should provide better inferential power for both current and future habitats. For presence only data, maximum entropy models are generally considered an excellent model choice. In my experience, there is no perfect model, rather model accuracy varies from species to species. For this reason, I typically build ensemble models to integrate the strengths of different model types. 197.5.2.2 Build an SDM Deal with environmental predictor colinearity In general, it is recommended to avoid having correlated features (variables that have different numbers, but are following the same pattern) in your dataset. Indeed, a group of highly correlated features will not bring additional information to our analyses, but will increase the complexity of the algorithm, thus increasing the risk of errors. Including highly correlated variables in models also, in essence, weights the correlated variables more than independent variables, again leading to less accurate model outputs. In other words, we need to remove highly correlated variables. We will do this by generating Variable Inflation Factor (VIF) values, a measure of collinearity, for all predictor variables. Then, we will remove one of the two correlated variables. Build the dataframe for the SDM Building the SDM, requires two additional steps. In the first, we assemble the final dataset to be used in the model. Using the sdmData function, we indicate the following: The column that contains presence data. The environmental predictors. Absence data or how to create background data. Specify model evaluation parameters When we build the final model using the SDM function, we specify replication. Replication is the method used to partition the dataset into training and test data. Ideally, we would have collected completely independent training and test datasets; however, I’ve never actually seen this done, except for researchers who are investigating SDM methods. Ninety nine point nine percent of the time, datasets are split into test and training datasets. As the names imply, training data are used to build the model, and then test data are used to measure how good our predictions are by quantifying how often our model correctly predicts presence or absence. Splitting or partitioning data into test and training datasets is often conducted several times, since outcomes may depend on the test or training data used to build and evaluate models. There are several methods to create training and test datasets. The three available in the package that we will use are subsampling (sub), crossvalidation (cv), bootstrapping (boot). For sub and boot, you must indicate what proportion of test and training data. A 30% test data, 70% training data split is common (test.percent=30). Finally, you will also the models how many times to repeat evaluations using the n equals code. This can eat up a lot of memory, so I typically use an n of 5. Choosing the evaluation model Crossvalidation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods, multiple rounds of crossvalidation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model’s predictive performance. Crossvalidation does not rely on random sampling, but rather splits the dataset into k unique subsets. This is the preferred method for spatial model evaluation and estimating generalization capability. Note that you have to select the number of ‘folds’ or data partitions, which is typically set at 5. Bootstrapping iteratively creates separate datasets from randomly sampling with replacement. Bootstrapping it is not as strong as crossvalidation when it is used for model validation, since it contains repeated elements in every subset. Bootstrapping is typically repeated 30 times in SDM model evaluation! Subsampling randomly splits the dataset into training and test datasets, but doesn’t maintain the independence of the datasets. In other words, due to random sampling, you might wind up with similar training and test datasets in each trial. For this reason, the more structured crossvalidation method is typically preferred. Build the SDM Once the data is appropriately compiled, we use the sdm function to build the actual model. Within this function, we specify: The column that contains presence or presence or absence The dataframe that we are using (d1) The types of models that we are using Replication type (cv), number of folds (5), how many times to repeat partitioning (1) THIS STEP WILL TAKE SOME TIME - JUST LET THE PROGRAM RUN! The model object (m1) tells you several things. First, it gives a brief summary of the model you ran. You can double check this to be sure that the model did what you told it to do. Here, everything seems fine: We ran a model for one species, we used two modeling methods, glm and maxent, we used cross_validation with 5 partitions. The model runs were successful (100% each). Finally, we are provided with 4 measures of model performance: AUC, COR, TSS, and Deviance. What types of models are we using to predict habitat suitability for Ponderosa pines (i.e., machine learning, regression, profile techniques)? 197.5.2.3 Model evaluation explained AUC stands for Area Under the Curve. AUC refers to a ROC plot, which plots sensitivity over 1 minus specificity. An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. AUC is desirable for SDM model evaluation for two main reasons: AUC is scale invariant. AUC is classification threshold invariant. It measures the quality of the model’s predictions irrespective of what classification threshold is chosen. This curve plots two parameters: True Positive Rate (Sensitivity): the proportion of presences correctly predicted as presence, False Positive Rate (1 minus Specificity): The specificity denotes the proportion of absences that are correctly predicted as absence, so the false positive rate indicates how many times the model predicted an occurrence when there was none. AUC ranges in value from 0 to 1. A model whose predictions are 100% wrong has an AUC of 0.0; one whose predictions are 100% correct has an AUC of 1.0. As a rule, an AUC &gt; 0.75 indicates a high performing model. SDMs predict a probability of occurrence across the landscape. Different thresholds can be used to create a cutoff to predict presences or absences. For instance, we could say that if there is a 90% chance or more that a pixel is suitable habitat, then we consider those areas as occupied. We want to identify a cutoff that maximizes true presences, while minimizes false positives (i.e., areas that you incorrectly say contain a population, but don’t). You can see that as you decrease the cutoff, from say 90% to 70%, then your likelihood of correctly predicting presences goes up, BUT so does your likelihood of false positives. So, let’s check out the ROC plot below. Looking at AUC, a common form of model performance assessment, which is the best performing model? Generally, there is strong agreement between the test and the training data. As you increase the cutoff threshold, the likelihood that you correctly assign presence goes up, but so does the false positive rate (Fig. 5). Note that on the far right hand side of each ROC plot, if the cutoff is high enough, you will have a 100% true positive rate, and a 100% false positive rate (the cutoff is so low, that all habitats are predicted to support the focal species). Alternatively, with a low enough cutoff (left hand side of the ROC plot), you won’t have any positives or any false positives! This AUC cutoff will be important for building ensemble models; explained below! Figure 5 This figure (lifted from an ARCGIS website) shows the relationship between omission rates and ROC plots. So if we want an Omission Rate that is slightly less than 10%, we can use 0.29 as the Cutoff instead, and in this case, we will pick up 20.48% background points as potential presence locations, which is also a good rate. Like AUC, True Skill Statistic (TSS) values are calculated across the range of possible thresholds for classifying model scores.The TSS similarly incorporates sensitivity and specificity comparing models against random, yielding values that range from negative 1 to positive 1, where positive 1 indicates perfect prediction and greater than or equal to 0 indicates a model that performs no better than random. TSS is typically considered a better indicator of model performance for presence only models. A TSS of 0.5 or higher indicates high model performance. Pearson correlation (COR) between the predicted likelihood of presence and the presence or absence testing data. Deviance Lastly, if a model is interpreted as estimating species’ probability of presence, rather than just giving an index of habitat suitability, then the model predictions can be evaluated using deviance, defined as 2 times the log probability of the test data. 197.5.2.4 Ensemble model assembly Finally, we will merge models into an ensemble model. You may want to exclude models that didn’t have high predictive performance. We will give higher weights to the models with higher accuracy, in this cause using the TSS score. 197.5.2.5 Investigating model components We can run code to look at the model components that best predict presence. According this figure, which climatic variable best predicts habitat suitability for Ponderosa pine? 197.5.2.6 Convert to presence or absense predictions We use the test statistics to identify a threshold that maximized true positives and reduced false negatives. In order to do this, we will create a new raster and populate it with predictions of presence, using that threshold. 197.5.2.7 Plotting and predictions Let’s take a quick look at the predictions we have created for the current time period. To predict response of your focal species to future climate, just plug the novel climate conditions into the model! Let’s run this model. Now, let’s plot this prediction against our original! First, we’ll take a zoomed out look, then we will focus into our region! Let’s convert to predicted occurrence and plot! According to these maps, how will the amount of suitable habitat for Pinus ponderosa in Flagstaff change if climate change continues along the SSP 585 projection? How certain are you of these projections? Why might these models NOT be accurate? What type of vegetation do you think might be more common around Flagstaff as climate changes? Submit your answers to the questions presented throughout this tutorial and the figures that you generated (Occurrence of Pinus ponderosa currently and in the future) to your TA. "],["occupancy-models-and-detectability.html", "Chapter 198 Occupancy Models and Detectability 198.1 The detection problem 198.2 When to use occupancy models 198.3 Setup", " Chapter 198 Occupancy Models and Detectability You survey a forest plot for owls. You don’t detect any. Does that mean owls are absent? Not necessarily. Maybe owls were present but silent. Maybe they were obscured by vegetation. Maybe you surveyed at the wrong time of day. Imperfect detection is the norm in ecological surveys—and ignoring it leads to biased estimates of species distributions and abundance. Occupancy models explicitly separate two processes: 1. Occupancy (ψ): Is the species truly present at a site? 2. Detection (p): Given the species is present, do we detect it? This chapter introduces occupancy modeling, from basic single-season models to dynamic multi-season models and N-mixture models for abundance. 198.1 The detection problem ## ## THE FUNDAMENTAL PROBLEM ## ═══════════════════════════════════════════════════════════════ ## ## What you OBSERVE: What&#39;s REAL: ## ## Detected Present AND Detected ## ↑ ↑ ## Not detected → Could be: Present but NOT detected ## OR ## Truly ABSENT ## ## Without accounting for detection: ## - Occupancy is UNDERESTIMATED ## - Range maps show &#39;false absences&#39; ## - Trends are BIASED (detection changes over time/space) ## ## The solution: REPEATED SURVEYS ## - Visit each site multiple times ## - Use detection/non-detection patterns to estimate p and ψ 198.2 When to use occupancy models Situation Use occupancy model? Rare or cryptic species ✅ Yes - detection often &lt; 100% Presence/absence surveys ✅ Yes - standard approach Camera trap data ✅ Yes - detection varies with effort eDNA surveys ✅ Yes - false negatives common Abundant, conspicuous species Maybe - if detection ≈ 1, simpler methods work Abundance estimation ✅ Use N-mixture models 198.3 Setup library(tidyverse) library(unmarked) # Core package for occupancy models library(AICcmodavg) # Model selection set.seed(42) "],["part-1-single-season-occupancy-models.html", "Chapter 199 Part 1: Single-Season Occupancy Models 199.1 The basic model 199.2 Simulating occupancy data 199.3 The detection history matrix 199.4 Fitting occupancy models with unmarked 199.5 Interpreting output", " Chapter 199 Part 1: Single-Season Occupancy Models 199.1 The basic model The single-season occupancy model estimates: - ψ (psi): Probability a site is occupied - p: Probability of detecting the species, given it’s present With repeated surveys at each site, we can separate these parameters. ## ## SINGLE-SEASON OCCUPANCY MODEL ## ═══════════════════════════════════════════════════════════════ ## ## Site i, Survey j: ## ## True state: z_i ~ Bernoulli(ψ) ## (site is occupied with probability ψ) ## ## Observation: y_ij | z_i ~ Bernoulli(z_i × p) ## (detected only if present AND detected) ## ## Detection histories tell us about p: ## ## Site 1: [1, 0, 1] → Detected, not detected, detected ## Species DEFINITELY present (we saw it) ## p &lt; 1 (we missed it once) ## ## Site 2: [0, 0, 0] → Never detected ## Species MIGHT be present (false absence) ## OR truly absent 199.2 Simulating occupancy data # Simulation parameters n_sites &lt;- 100 n_surveys &lt;- 4 true_psi &lt;- 0.6 # True occupancy probability true_p &lt;- 0.4 # True detection probability # Generate true occupancy states (latent) z_true &lt;- rbinom(n_sites, 1, true_psi) cat(&quot;True number of occupied sites:&quot;, sum(z_true), &quot;of&quot;, n_sites, &quot;\\n&quot;) ## True number of occupied sites: 54 of 100 # Generate detection histories det_hist &lt;- matrix(NA, n_sites, n_surveys) for (i in 1:n_sites) { for (j in 1:n_surveys) { det_hist[i, j] &lt;- rbinom(1, 1, z_true[i] * true_p) } } # What we would conclude without occupancy modeling naive_occupancy &lt;- mean(rowSums(det_hist) &gt; 0) cat(&quot;Naive occupancy (detected at least once):&quot;, round(naive_occupancy, 3), &quot;\\n&quot;) ## Naive occupancy (detected at least once): 0.43 cat(&quot;True occupancy:&quot;, true_psi, &quot;\\n&quot;) ## True occupancy: 0.6 199.3 The detection history matrix # View some detection histories head(det_hist, 10) ## [,1] [,2] [,3] [,4] ## [1,] 0 0 0 0 ## [2,] 0 0 0 0 ## [3,] 1 0 0 0 ## [4,] 0 0 0 0 ## [5,] 0 0 0 0 ## [6,] 1 1 1 1 ## [7,] 0 0 0 0 ## [8,] 0 0 1 1 ## [9,] 0 0 0 0 ## [10,] 0 0 0 0 # Summarize detection patterns det_patterns &lt;- apply(det_hist, 1, paste, collapse = &quot;&quot;) table(det_patterns) ## det_patterns ## 0000 0001 0010 0011 0100 0111 1000 1001 1010 1011 1100 1101 1110 1111 ## 57 12 5 5 3 1 2 1 2 4 3 2 1 2 199.4 Fitting occupancy models with unmarked # Create unmarked frame # unmarked requires detection history as a matrix umf &lt;- unmarkedFrameOccu(y = det_hist) # View structure summary(umf) ## unmarkedFrame Object ## ## 100 sites ## Maximum number of observations per site: 4 ## Mean number of observations per site: 4 ## Sites with at least one detection: 43 ## ## Tabulation of y observations: ## 0 1 ## 324 76 # Fit null model (constant ψ and p) occ_null &lt;- occu(~ 1 ~ 1, data = umf) # View results summary(occ_null) ## ## Call: ## occu(formula = ~1 ~ 1, data = umf) ## ## Occupancy (logit-scale): ## Estimate SE z P(&gt;|z|) ## 0.0318 0.257 0.124 0.901 ## ## Detection (logit-scale): ## Estimate SE z P(&gt;|z|) ## -0.515 0.193 -2.67 0.00752 ## ## AIC: 365.7504 ## Number of sites: 100 199.5 Interpreting output # Estimates are on logit scale - back-transform # Detection probability p_est &lt;- plogis(coef(occ_null, type = &quot;det&quot;)) cat(&quot;Estimated detection probability:&quot;, round(p_est, 3), &quot;\\n&quot;) ## Estimated detection probability: 0.374 cat(&quot;True detection probability:&quot;, true_p, &quot;\\n\\n&quot;) ## True detection probability: 0.4 # Occupancy probability psi_est &lt;- plogis(coef(occ_null, type = &quot;state&quot;)) cat(&quot;Estimated occupancy probability:&quot;, round(psi_est, 3), &quot;\\n&quot;) ## Estimated occupancy probability: 0.508 cat(&quot;True occupancy probability:&quot;, true_psi, &quot;\\n&quot;) ## True occupancy probability: 0.6 cat(&quot;Naive estimate:&quot;, round(naive_occupancy, 3), &quot;\\n&quot;) ## Naive estimate: 0.43 Key insight: The occupancy model correctly estimates ψ, while naive presence/absence underestimates it. "],["part-2-covariates-on-detection-and-occupancy.html", "Chapter 200 Part 2: Covariates on Detection and Occupancy 200.1 Why covariates matter 200.2 Adding site covariates 200.3 Adding observation covariates 200.4 Model selection", " Chapter 200 Part 2: Covariates on Detection and Occupancy 200.1 Why covariates matter Detection probability often varies with: - Survey conditions: Weather, time of day, observer - Effort: Survey duration, number of observers Occupancy probability often varies with: - Habitat: Vegetation type, elevation, patch size - Landscape: Connectivity, edge effects 200.2 Adding site covariates # Site covariates affecting occupancy elevation &lt;- rnorm(n_sites, mean = 1500, sd = 300) forest_cover &lt;- runif(n_sites, 0, 100) # Occupancy depends on covariates beta0 &lt;- -0.5 beta_elev &lt;- 0.002 beta_forest &lt;- 0.03 psi_cov &lt;- plogis(beta0 + beta_elev * (elevation - 1500) + beta_forest * (forest_cover - 50)) # Generate new occupancy states z_cov &lt;- rbinom(n_sites, 1, psi_cov) # Generate detection histories (same p for now) det_hist_cov &lt;- matrix(NA, n_sites, n_surveys) for (i in 1:n_sites) { for (j in 1:n_surveys) { det_hist_cov[i, j] &lt;- rbinom(1, 1, z_cov[i] * true_p) } } # Create site covariates data frame site_covs &lt;- data.frame( elev = scale(elevation)[,1], # Standardize! forest = scale(forest_cover)[,1] ) # Create unmarked frame with covariates umf_cov &lt;- unmarkedFrameOccu( y = det_hist_cov, siteCovs = site_covs ) # Fit model with occupancy covariates occ_cov &lt;- occu(~ 1 ~ elev + forest, data = umf_cov) summary(occ_cov) ## ## Call: ## occu(formula = ~1 ~ elev + forest, data = umf_cov) ## ## Occupancy (logit-scale): ## Estimate SE z P(&gt;|z|) ## (Intercept) -0.275 0.281 -0.978 0.3283 ## elev 0.396 0.260 1.525 0.1271 ## forest 0.702 0.291 2.414 0.0158 ## ## Detection (logit-scale): ## Estimate SE z P(&gt;|z|) ## -0.539 0.21 -2.56 0.0103 ## ## AIC: 322.5825 ## Number of sites: 100 200.3 Adding observation covariates # Observation-level covariates (vary by site AND survey) # Example: survey date affects detection survey_date &lt;- matrix( rep(1:n_surveys, each = n_sites) + rnorm(n_sites * n_surveys, 0, 0.5), n_sites, n_surveys ) # Standardize survey_date_scaled &lt;- scale(survey_date)[,] # Create unmarked frame with observation covariates umf_obs &lt;- unmarkedFrameOccu( y = det_hist_cov, siteCovs = site_covs, obsCovs = list(date = survey_date_scaled) ) # Fit model with detection covariate occ_det_cov &lt;- occu(~ date ~ elev + forest, data = umf_obs) summary(occ_det_cov) ## ## Call: ## occu(formula = ~date ~ elev + forest, data = umf_obs) ## ## Occupancy (logit-scale): ## Estimate SE z P(&gt;|z|) ## (Intercept) -0.268 0.284 -0.944 0.3453 ## elev 0.399 0.261 1.528 0.1264 ## forest 0.702 0.292 2.408 0.0161 ## ## Detection (logit-scale): ## Estimate SE z P(&gt;|z|) ## (Intercept) -0.5483 0.215 -2.556 0.0106 ## date 0.0374 0.161 0.232 0.8165 ## ## AIC: 324.5287 ## Number of sites: 100 200.4 Model selection # Fit candidate models m1 &lt;- occu(~ 1 ~ 1, data = umf_obs) # Null m2 &lt;- occu(~ 1 ~ elev, data = umf_obs) # Elevation only m3 &lt;- occu(~ 1 ~ forest, data = umf_obs) # Forest only m4 &lt;- occu(~ 1 ~ elev + forest, data = umf_obs) # Both m5 &lt;- occu(~ date ~ elev + forest, data = umf_obs) # + detection covariate # Model selection table models &lt;- list( &quot;null&quot; = m1, &quot;elev&quot; = m2, &quot;forest&quot; = m3, &quot;elev + forest&quot; = m4, &quot;full&quot; = m5 ) # AIC comparison aic_table &lt;- data.frame( Model = names(models), AIC = sapply(models, AIC) ) aic_table$deltaAIC &lt;- aic_table$AIC - min(aic_table$AIC) aic_table &lt;- aic_table[order(aic_table$AIC), ] print(aic_table) ## Model AIC deltaAIC ## elev + forest elev + forest 322.5825 0.0000000 ## forest forest 322.9743 0.3917751 ## full full 324.5287 1.9462053 ## elev elev 327.6392 5.0566867 ## null null 329.0570 6.4744766 "],["part-3-predictions-and-visualization.html", "Chapter 201 Part 3: Predictions and Visualization 201.1 Predicting occupancy 201.2 Mapping occupancy", " Chapter 201 Part 3: Predictions and Visualization 201.1 Predicting occupancy # Use best model best_model &lt;- occ_cov # Create prediction data frame pred_data &lt;- data.frame( elev = seq(-2, 2, length = 100), forest = 0 # Hold forest at mean ) # Predict occupancy pred_occ &lt;- predict(best_model, type = &quot;state&quot;, newdata = pred_data) pred_data$psi &lt;- pred_occ$Predicted pred_data$psi_lower &lt;- pred_occ$lower pred_data$psi_upper &lt;- pred_occ$upper ggplot(pred_data, aes(x = elev, y = psi)) + geom_ribbon(aes(ymin = psi_lower, ymax = psi_upper), fill = &quot;steelblue&quot;, alpha = 0.3) + geom_line(color = &quot;steelblue&quot;, size = 1.2) + labs(x = &quot;Elevation (standardized)&quot;, y = &quot;Occupancy Probability (ψ)&quot;, title = &quot;Predicted Occupancy vs Elevation&quot;) + ylim(0, 1) + theme_minimal() Figure 201.1: Predicted occupancy probability as a function of elevation (standardized). Shaded region shows 95% confidence interval. 201.2 Mapping occupancy # Predict for each site site_preds &lt;- predict(best_model, type = &quot;state&quot;) site_covs$predicted_psi &lt;- site_preds$Predicted # Sites with coordinates (simulated) site_covs$x &lt;- runif(n_sites, 0, 100) site_covs$y &lt;- runif(n_sites, 0, 100) site_covs$detected &lt;- rowSums(det_hist_cov) &gt; 0 ggplot(site_covs, aes(x = x, y = y)) + geom_point(aes(color = predicted_psi, shape = detected), size = 4) + scale_color_viridis_c(name = &quot;Predicted ψ&quot;, limits = c(0, 1)) + scale_shape_manual(values = c(1, 19), name = &quot;Detected&quot;) + labs(title = &quot;Predicted Occupancy Across Sites&quot;) + coord_equal() + theme_minimal() Figure 201.2: Map of predicted occupancy probability. Points are survey sites; filled points indicate species was detected. "],["part-4-model-goodness-of-fit.html", "Chapter 202 Part 4: Model Goodness-of-Fit 202.1 Checking model fit 202.2 Assessing detection probability", " Chapter 202 Part 4: Model Goodness-of-Fit 202.1 Checking model fit # MacKenzie-Bailey goodness-of-fit test # Uses parametric bootstrap # This takes a while, so we&#39;ll use fewer bootstraps for demonstration set.seed(123) gof_test &lt;- mb.gof.test(best_model, nsim = 100) print(gof_test) ## ## MacKenzie and Bailey goodness-of-fit for single-season occupancy model ## ## Pearson chi-square table: ## ## Cohort Observed Expected Chi-square ## 0000 0 63 62.92 0.00 ## 0001 0 3 4.09 0.29 ## 0010 0 5 4.09 0.20 ## 0011 0 2 2.39 0.06 ## 0100 0 5 4.09 0.20 ## 0101 0 3 2.39 0.16 ## 0110 0 1 2.39 0.81 ## 0111 0 2 1.39 0.26 ## 1000 0 3 4.09 0.29 ## 1001 0 4 2.39 1.09 ## 1100 0 6 2.39 5.46 ## 1110 0 1 1.39 0.11 ## 1111 0 2 0.81 1.73 ## ## Chi-square statistic = 15.8459 ## Number of bootstrap samples = 100 ## P-value = 0.21 ## ## Quantiles of bootstrapped statistics: ## 0% 25% 50% 75% 100% ## 3.3 10.3 13.0 15.2 26.6 ## ## Estimate of c-hat = 1.23 Interpretation: - p &gt; 0.05: Model fits adequately - p &lt; 0.05: Consider model misspecification - c-hat &gt; 1: Overdispersion present; use quasi-likelihood adjustments 202.2 Assessing detection probability # Extract estimated detection probability p_estimates &lt;- predict(best_model, type = &quot;det&quot;) cat(&quot;Mean detection probability:&quot;, round(mean(p_estimates$Predicted), 3), &quot;\\n&quot;) ## Mean detection probability: 0.369 cat(&quot;Range:&quot;, round(min(p_estimates$Predicted), 3), &quot;-&quot;, round(max(p_estimates$Predicted), 3), &quot;\\n&quot;) ## Range: 0.369 - 0.369 Rule of thumb: Detection probability should ideally be &gt; 0.2 for reliable occupancy estimates. Very low p means you need more surveys. "],["part-5-multi-season-dynamic-occupancy-models.html", "Chapter 203 Part 5: Multi-Season (Dynamic) Occupancy Models 203.1 Why multi-season models? 203.2 Simulating multi-season data 203.3 Fitting multi-season models 203.4 Derived parameters", " Chapter 203 Part 5: Multi-Season (Dynamic) Occupancy Models 203.1 Why multi-season models? Single-season models assume occupancy is static. But populations change: - Colonization (γ): Probability an unoccupied site becomes occupied - Extinction (ε): Probability an occupied site becomes unoccupied Dynamic models estimate these rates. ## ## DYNAMIC OCCUPANCY MODEL ## ═══════════════════════════════════════════════════════════════ ## ## Year t to Year t+1: ## ## If site WAS occupied: ## - Stays occupied with probability (1 - ε) ## - Becomes unoccupied with probability ε (extinction) ## ## If site WAS unoccupied: ## - Stays unoccupied with probability (1 - γ) ## - Becomes occupied with probability γ (colonization) ## ## This gives us: ## - ψ₁: Initial occupancy ## - γ: Colonization rate ## - ε: Extinction rate ## - p: Detection probability (can vary by year/survey) 203.2 Simulating multi-season data # Parameters n_sites &lt;- 80 n_years &lt;- 5 n_surveys &lt;- 3 psi1 &lt;- 0.5 # Initial occupancy gamma &lt;- 0.2 # Colonization epsilon &lt;- 0.15 # Extinction p &lt;- 0.5 # Detection # Initialize z &lt;- matrix(NA, n_sites, n_years) z[, 1] &lt;- rbinom(n_sites, 1, psi1) # Simulate dynamics for (t in 2:n_years) { for (i in 1:n_sites) { if (z[i, t-1] == 1) { # Occupied site: may go extinct z[i, t] &lt;- rbinom(1, 1, 1 - epsilon) } else { # Unoccupied site: may be colonized z[i, t] &lt;- rbinom(1, 1, gamma) } } } # Simulate detections y_array &lt;- array(NA, dim = c(n_sites, n_surveys, n_years)) for (t in 1:n_years) { for (i in 1:n_sites) { for (j in 1:n_surveys) { y_array[i, j, t] &lt;- rbinom(1, 1, z[i, t] * p) } } } # Check true occupancy over time true_occ &lt;- colMeans(z) cat(&quot;True occupancy by year:&quot;, round(true_occ, 3), &quot;\\n&quot;) ## True occupancy by year: 0.45 0.462 0.512 0.525 0.575 203.3 Fitting multi-season models # Format for unmarked # Need to create a list of detection matrices (one per year) y_list &lt;- lapply(1:n_years, function(t) y_array[, , t]) # Combine into single matrix (sites × surveys, across years) y_combined &lt;- do.call(cbind, y_list) # Create unmarked frame for multi-season umf_multi &lt;- unmarkedMultFrame( y = y_combined, numPrimary = n_years ) summary(umf_multi) ## unmarkedFrame Object ## ## 80 sites ## Maximum number of observations per site: 15 ## Mean number of observations per site: 15 ## Number of primary survey periods: 5 ## Number of secondary survey periods: 3 ## Sites with at least one detection: 64 ## ## Tabulation of y observations: ## 0 1 ## 905 295 # Fit dynamic occupancy model (colext) dyn_occ &lt;- colext( psiformula = ~ 1, # Initial occupancy gammaformula = ~ 1, # Colonization epsilonformula = ~ 1, # Extinction pformula = ~ 1, # Detection data = umf_multi ) summary(dyn_occ) ## ## Call: ## colext(psiformula = ~1, gammaformula = ~1, epsilonformula = ~1, ## pformula = ~1, data = umf_multi) ## ## Initial (logit-scale): ## Estimate SE z P(&gt;|z|) ## -0.369 0.26 -1.42 0.156 ## ## Colonization (logit-scale): ## Estimate SE z P(&gt;|z|) ## -1.04 0.212 -4.89 1.01e-06 ## ## Extinction (logit-scale): ## Estimate SE z P(&gt;|z|) ## -1.44 0.293 -4.91 9.3e-07 ## ## Detection (logit-scale): ## Estimate SE z P(&gt;|z|) ## -0.0544 0.106 -0.512 0.608 ## ## AIC: 1172.974 ## Number of sites: 80 # Back-transform parameters psi1_est &lt;- plogis(coef(dyn_occ, type = &quot;psi&quot;)) gamma_est &lt;- plogis(coef(dyn_occ, type = &quot;col&quot;)) epsilon_est &lt;- plogis(coef(dyn_occ, type = &quot;ext&quot;)) p_est &lt;- plogis(coef(dyn_occ, type = &quot;det&quot;)) cat(&quot;Parameter estimates:\\n&quot;) ## Parameter estimates: cat(&quot;Initial occupancy (ψ₁):&quot;, round(psi1_est, 3), &quot;(true:&quot;, psi1, &quot;)\\n&quot;) ## Initial occupancy (ψ₁): 0.409 (true: 0.5 ) cat(&quot;Colonization (γ):&quot;, round(gamma_est, 3), &quot;(true:&quot;, gamma, &quot;)\\n&quot;) ## Colonization (γ): 0.262 (true: 0.2 ) cat(&quot;Extinction (ε):&quot;, round(epsilon_est, 3), &quot;(true:&quot;, epsilon, &quot;)\\n&quot;) ## Extinction (ε): 0.192 (true: 0.15 ) cat(&quot;Detection (p):&quot;, round(p_est, 3), &quot;(true:&quot;, p, &quot;)\\n&quot;) ## Detection (p): 0.486 (true: 0.5 ) 203.4 Derived parameters # Equilibrium occupancy (if dynamics stabilize) psi_eq &lt;- gamma_est / (gamma_est + epsilon_est) cat(&quot;\\nEquilibrium occupancy:&quot;, round(psi_eq, 3), &quot;\\n&quot;) ## ## Equilibrium occupancy: 0.577 # Population growth rate (lambda) lambda &lt;- 1 - epsilon_est + gamma_est * (1/psi1_est - 1) cat(&quot;Approximate growth rate:&quot;, round(lambda, 3), &quot;\\n&quot;) ## Approximate growth rate: 1.186 "],["part-6-n-mixture-models-for-abundance.html", "Chapter 204 Part 6: N-Mixture Models for Abundance 204.1 Beyond presence/absence 204.2 Simulating count data 204.3 Fitting N-mixture models 204.4 Estimating site-specific abundance", " Chapter 204 Part 6: N-Mixture Models for Abundance 204.1 Beyond presence/absence Sometimes you count individuals, not just presence. N-mixture models estimate abundance while accounting for imperfect detection. ## ## N-MIXTURE MODEL ## ═══════════════════════════════════════════════════════════════ ## ## Site i, Survey j: ## ## True abundance: N_i ~ Poisson(λ) or NegBin(λ, θ) ## ## Observation: y_ij ~ Binomial(N_i, p) ## (detect each individual with probability p) ## ## Example: ## Site has N = 10 birds ## Detection p = 0.3 ## Survey 1: see 3 birds ## Survey 2: see 4 birds ## Survey 3: see 2 birds ## ## Naive estimate: max(3, 4, 2) = 4 ## N-mixture estimate: ≈ 10 (accounts for imperfect detection) 204.2 Simulating count data # Parameters n_sites &lt;- 60 n_surveys &lt;- 4 lambda &lt;- 8 # Mean abundance p &lt;- 0.4 # Detection probability per individual # Site covariate affecting abundance habitat_quality &lt;- rnorm(n_sites) lambda_site &lt;- exp(log(lambda) + 0.5 * habitat_quality) # True abundance at each site N_true &lt;- rpois(n_sites, lambda_site) # Counts (imperfect detection) counts &lt;- matrix(NA, n_sites, n_surveys) for (i in 1:n_sites) { for (j in 1:n_surveys) { counts[i, j] &lt;- rbinom(1, N_true[i], p) } } # Compare naive vs true cat(&quot;True total abundance:&quot;, sum(N_true), &quot;\\n&quot;) ## True total abundance: 518 cat(&quot;Max count total:&quot;, sum(apply(counts, 1, max)), &quot;\\n&quot;) ## Max count total: 283 cat(&quot;Mean count total:&quot;, sum(apply(counts, 1, mean)), &quot;\\n&quot;) ## Mean count total: 201.25 204.3 Fitting N-mixture models # Create site covariates site_covs_nmix &lt;- data.frame(habitat = scale(habitat_quality)[,1]) # Create unmarked frame for N-mixture umf_nmix &lt;- unmarkedFramePCount( y = counts, siteCovs = site_covs_nmix ) summary(umf_nmix) ## unmarkedFrame Object ## ## 60 sites ## Maximum number of observations per site: 4 ## Mean number of observations per site: 4 ## Sites with at least one detection: 60 ## ## Tabulation of y observations: ## 0 1 2 3 4 5 6 7 8 9 10 11 14 15 16 18 21 ## 29 51 40 36 24 14 13 9 9 3 4 2 2 1 1 1 1 ## ## Site-level covariates: ## habitat ## Min. :-2.0247 ## 1st Qu.:-0.5778 ## Median :-0.1609 ## Mean : 0.0000 ## 3rd Qu.: 0.5369 ## Max. : 3.1527 # Fit Poisson N-mixture model nmix_pois &lt;- pcount( ~ 1 ~ habitat, # detection ~ 1, abundance ~ habitat data = umf_nmix, K = 50 # Upper bound for integration (should be &gt; max possible N) ) summary(nmix_pois) ## ## Call: ## pcount(formula = ~1 ~ habitat, data = umf_nmix, K = 50) ## ## Abundance (log-scale): ## Estimate SE z P(&gt;|z|) ## (Intercept) 1.870 0.1231 15.2 4.32e-52 ## habitat 0.608 0.0435 14.0 2.45e-44 ## ## Detection (logit-scale): ## Estimate SE z P(&gt;|z|) ## -0.324 0.197 -1.64 0.101 ## ## AIC: 863.0937 ## Number of sites: 60 # Back-transform p_nmix &lt;- plogis(coef(nmix_pois, type = &quot;det&quot;)) cat(&quot;Estimated detection probability:&quot;, round(p_nmix, 3), &quot;\\n&quot;) ## Estimated detection probability: 0.42 cat(&quot;True detection probability:&quot;, p, &quot;\\n\\n&quot;) ## True detection probability: 0.4 # Abundance at average habitat lambda_mean &lt;- exp(coef(nmix_pois, type = &quot;state&quot;)[1]) cat(&quot;Estimated mean abundance (average habitat):&quot;, round(lambda_mean, 2), &quot;\\n&quot;) ## Estimated mean abundance (average habitat): 6.49 cat(&quot;True mean lambda:&quot;, lambda, &quot;\\n&quot;) ## True mean lambda: 8 204.4 Estimating site-specific abundance # Empirical Bayes estimates of N at each site N_estimates &lt;- unmarked::ranef(nmix_pois) # Extract posterior means N_post_mean &lt;- bup(N_estimates, stat = &quot;mean&quot;) # Compare to truth comparison &lt;- data.frame( True_N = N_true, Estimated_N = N_post_mean, Max_count = apply(counts, 1, max) ) head(comparison, 10) ## True_N Estimated_N Max_count ## 1 10 11.214448 7 ## 2 12 9.881568 5 ## 3 12 11.237675 8 ## 4 12 12.718365 7 ## 5 3 2.474030 2 ## 6 2 2.440070 2 ## 7 7 7.182223 4 ## 8 2 1.566757 1 ## 9 6 6.956566 4 ## 10 3 1.761250 1 ggplot(comparison, aes(x = True_N, y = Estimated_N)) + geom_point(alpha = 0.6) + geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;, color = &quot;red&quot;) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = &quot;steelblue&quot;) + labs(x = &quot;True Abundance&quot;, y = &quot;Estimated Abundance (N-mixture)&quot;, title = &quot;N-mixture Model Performance&quot;) + theme_minimal() Figure 204.1: Comparison of true abundance (x-axis) vs estimated abundance from N-mixture model (y-axis). The model corrects for imperfect detection. "],["part-7-survey-design-considerations.html", "Chapter 205 Part 7: Survey Design Considerations 205.1 How many surveys? 205.2 Sites vs surveys trade-off", " Chapter 205 Part 7: Survey Design Considerations 205.1 How many surveys? Detection probability determines how many surveys you need: # Probability of detecting species at least once # (cumulative detection probability) p_values &lt;- seq(0.1, 0.9, by = 0.1) n_surveys_options &lt;- 1:10 # Calculate P(detect at least once) = 1 - (1-p)^n design_table &lt;- expand.grid(p = p_values, n = n_surveys_options) design_table$p_star &lt;- 1 - (1 - design_table$p)^design_table$n # Reshape for viewing design_matrix &lt;- matrix( round(design_table$p_star, 3), nrow = length(p_values), ncol = length(n_surveys_options), dimnames = list(paste0(&quot;p=&quot;, p_values), paste0(&quot;n=&quot;, n_surveys_options)) ) print(design_matrix) ## n=1 n=2 n=3 n=4 n=5 n=6 n=7 n=8 n=9 n=10 ## p=0.1 0.1 0.19 0.271 0.344 0.410 0.469 0.522 0.570 0.613 0.651 ## p=0.2 0.2 0.36 0.488 0.590 0.672 0.738 0.790 0.832 0.866 0.893 ## p=0.3 0.3 0.51 0.657 0.760 0.832 0.882 0.918 0.942 0.960 0.972 ## p=0.4 0.4 0.64 0.784 0.870 0.922 0.953 0.972 0.983 0.990 0.994 ## p=0.5 0.5 0.75 0.875 0.938 0.969 0.984 0.992 0.996 0.998 0.999 ## p=0.6 0.6 0.84 0.936 0.974 0.990 0.996 0.998 0.999 1.000 1.000 ## p=0.7 0.7 0.91 0.973 0.992 0.998 0.999 1.000 1.000 1.000 1.000 ## p=0.8 0.8 0.96 0.992 0.998 1.000 1.000 1.000 1.000 1.000 1.000 ## p=0.9 0.9 0.99 0.999 1.000 1.000 1.000 1.000 1.000 1.000 1.000 Guideline: Aim for cumulative detection probability &gt; 0.95 ggplot(design_table, aes(x = n, y = p_star, color = factor(p))) + geom_line(size = 1) + geom_hline(yintercept = 0.95, linetype = &quot;dashed&quot;) + scale_color_viridis_d(name = &quot;Per-survey p&quot;) + labs(x = &quot;Number of Surveys&quot;, y = &quot;Cumulative Detection Probability&quot;, title = &quot;Survey Design: How Many Visits?&quot;) + theme_minimal() Figure 205.1: Cumulative detection probability as a function of number of surveys, for different per-survey detection probabilities. 205.2 Sites vs surveys trade-off # With fixed total effort, what&#39;s better: more sites or more surveys? # Simulate precision under different designs # Total effort = 120 site-visits designs &lt;- data.frame( n_sites = c(120, 60, 40, 30, 24, 20), n_surveys = c(1, 2, 3, 4, 5, 6) ) cat(&quot;Fixed total effort = 120 site-visits\\n\\n&quot;) ## Fixed total effort = 120 site-visits cat(&quot;With very low detection (p=0.2), more surveys helps separate ψ from p\\n&quot;) ## With very low detection (p=0.2), more surveys helps separate ψ from p cat(&quot;With high detection (p=0.7), more sites gives better precision on ψ\\n&quot;) ## With high detection (p=0.7), more sites gives better precision on ψ "],["part-8-reporting-occupancy-models.html", "Chapter 206 Part 8: Reporting Occupancy Models 206.1 What to report 206.2 Sample methods and results 206.3 Key takeaways 206.4 Assignment", " Chapter 206 Part 8: Reporting Occupancy Models 206.1 What to report Survey design: Sites, surveys per site, timing Detection covariates: What affected p Occupancy covariates: What affected ψ Parameter estimates: With standard errors Model selection: AIC table if multiple models Goodness-of-fit: MacKenzie-Bailey test Detection probability: Report p to justify survey effort Predictions: Maps or covariate response curves 206.2 Sample methods and results 206.2.1 Methods We surveyed 100 sites for Mexican spotted owl presence during the 2023 breeding season. Each site was visited 4 times between April and July, with visits separated by at least 7 days. At each visit, observers conducted 2-hour crepuscular surveys using playback calls. We recorded detection (1) or non-detection (0) for each survey, creating detection histories for each site. We fit single-season occupancy models using the unmarked package in R, estimating both occupancy probability (ψ) and detection probability (p). We modeled occupancy as a function of elevation and forest cover (both standardized), and detection as a function of survey date. We used AIC to select among candidate models and assessed goodness-of-fit using the MacKenzie-Bailey parametric bootstrap test (1000 iterations). Predicted occupancy was mapped across the study area using the best-supported model. 206.2.2 Results Occupancy probability for Mexican spotted owl averaged 0.48 (SE = 0.07) across the study area. The best-supported model included elevation as a predictor of occupancy (β = 0.82, SE = 0.21) and survey date as a predictor of detection. Occupancy increased with elevation, with predicted probability ranging from 0.25 at low-elevation sites to 0.78 at high-elevation sites (Fig. X). Detection probability averaged 0.42 (SE = 0.05) and decreased over the survey season (β = -0.15, SE = 0.06). With four surveys per site, cumulative detection probability was 0.89, providing adequate power to distinguish true absence from non-detection. The model showed adequate fit (χ² = 42.3, c-hat = 1.08, p = 0.23). Based on predicted occupancy, we identified 23 sites with ψ &gt; 0.7 as core habitat (Fig. Y). 206.3 Key takeaways Imperfect detection is the rule — Account for it or underestimate occupancy Repeated surveys are essential — They separate p from ψ Model both processes — Detection AND occupancy can have covariates Report detection probability — It justifies your survey effort Dynamic models track change — Colonization and extinction over time N-mixture models for counts — Abundance with imperfect detection Survey design matters — More surveys help with low detection Check model fit — MacKenzie-Bailey test for occupancy models 206.4 Assignment 206.4.1 Part 1: Conceptual questions You conduct 3 surveys at 50 sites and never detect a species at 20 sites. Does this mean those 20 sites are unoccupied? Explain. Your estimated detection probability is 0.15. What does this tell you about your survey design? How many surveys would you need for 95% cumulative detection? In an N-mixture model, how does accounting for imperfect detection change abundance estimates compared to using raw counts? 206.4.2 Part 2: Single-season occupancy Using the simulated data in this chapter (or your own data): Fit occupancy models with different covariate combinations Conduct model selection using AIC Check goodness-of-fit Generate and plot predictions Interpret the ecological meaning of your results 206.4.3 Part 3: Multi-season model Using multi-year survey data: Format data for colext() Fit a dynamic occupancy model Estimate colonization and extinction rates Calculate equilibrium occupancy Discuss what these dynamics mean biologically 206.4.4 Part 4: N-mixture model With count data (simulated or real): Fit a Poisson N-mixture model Fit a negative binomial model and compare Extract site-specific abundance estimates Compare to naive counts Discuss when N-mixture models are most valuable 206.4.5 Part 5: Reporting Write a complete methods and results section for an occupancy analysis of your choice, following the format in this chapter. "],["structural-equation-modeling-i-foundations-and-path-analysis.html", "Chapter 207 Structural Equation Modeling I: Foundations and Path Analysis 207.1 When should you use SEM? 207.2 How is SEM different from multiple regression? 207.3 Setup", " Chapter 207 Structural Equation Modeling I: Foundations and Path Analysis Ecological systems are complex. Species richness depends on habitat quality, which depends on disturbance, which depends on management—and management might also directly affect richness. These cascading, interconnected relationships can’t be captured in a single regression equation. Structural Equation Modeling (SEM) provides a framework for modeling entire systems of relationships simultaneously. It allows you to: Test hypothesized causal pathways Estimate direct AND indirect effects Incorporate latent (unmeasured) variables Assess how well your conceptual model fits the data This chapter covers the foundations: causal thinking, path diagrams, and fitting basic SEMs in R. 207.1 When should you use SEM? Use SEM when: You have multiple dependent variables that influence one another You want to estimate direct and indirect effects Your system includes mediating variables (A → B → C) You’re testing a theoretical model with multiple pathways You need to separate measurement error from true relationships SEM is ideal when a single regression model is too simplistic to capture the interdependent relationships in your system. 207.2 How is SEM different from multiple regression? Feature Multiple Regression SEM Number of equations One Many simultaneously Latent variables Not allowed Allowed Indirect effects Manual calculation Modeled explicitly Measurement error Ignored Can be modeled Model fit assessment R², AIC Chi-square, RMSEA, CFI 207.3 Setup library(tidyverse) library(lavaan) # Core SEM package library(semPlot) # SEM visualization library(dagitty) # DAG specification library(ggdag) # DAG visualization set.seed(42) "],["part-1-causal-thinking-and-dags.html", "Chapter 208 Part 1: Causal Thinking and DAGs 208.1 What is causality? 208.2 Directed Acyclic Graphs (DAGs) 208.3 Three fundamental structures 208.4 Summary: What to control for", " Chapter 208 Part 1: Causal Thinking and DAGs 208.1 What is causality? Causality means understanding the directional effect one variable has on another. In SEM, we explicitly model causal relationships—unlike regression where causality is implied but not specified. Judea Pearl’s Ladder of Causation: Observation — “What is?” (association) Intervention — “What if I do?” (action) Counterfactual — “What if I had done differently?” (imagination) We ascend the ladder by incorporating causal assumptions into our models. 208.2 Directed Acyclic Graphs (DAGs) A DAG is a visual representation of your causal assumptions: Nodes = variables Arrows = causal relationships (cause → effect) Acyclic = no feedback loops (arrows can’t circle back) # Define a simple causal chain chain_dag &lt;- dagify( SoilMoisture ~ CanopyCover, CanopyCover ~ Fire, labels = c( Fire = &quot;Fire\\nFrequency&quot;, CanopyCover = &quot;Canopy\\nCover&quot;, SoilMoisture = &quot;Soil\\nMoisture&quot; ), exposure = &quot;Fire&quot;, outcome = &quot;SoilMoisture&quot; ) ggdag(chain_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Causal Chain (Mediation)&quot;) Figure 208.1: A simple DAG showing fire frequency affecting canopy cover, which affects soil moisture. 208.3 Three fundamental structures Every DAG is built from three basic structures. Understanding them is essential for knowing what to control for. 208.3.1 1. Chains (Mediation) Structure: Cause → Mediator → Effect The mediator transmits the effect from cause to outcome. ggdag(chain_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Chain: Fire → Canopy → Soil Moisture&quot;) Figure 208.2: Chain structure: Fire affects soil moisture through canopy cover. Key insight: If you control for the mediator, you block the indirect path and underestimate the total effect. 208.3.2 2. Forks (Confounding) Structure: Common Cause → Variable A, Common Cause → Variable B A confounder creates spurious correlation between two variables. fork_dag &lt;- dagify( InvasiveCover ~ SoilType, NativeRichness ~ SoilType, labels = c( SoilType = &quot;Soil Type&quot;, InvasiveCover = &quot;Invasive\\nCover&quot;, NativeRichness = &quot;Native\\nRichness&quot; ), exposure = &quot;InvasiveCover&quot;, outcome = &quot;NativeRichness&quot; ) ggdag(fork_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Fork: Soil Type Confounds Invasive-Native Relationship&quot;) Figure 208.3: Fork structure: Soil type confounds the relationship between invasive cover and native richness. Key insight: Control for the confounder to block the spurious path. 208.3.3 3. Colliders Structure: Cause1 → Collider ← Cause2 A collider is influenced by two causes. Controlling for it opens a spurious path. collider_dag &lt;- dagify( PlantGrowth ~ SoilNitrogen + PathogenLoad, labels = c( SoilNitrogen = &quot;Soil N&quot;, PathogenLoad = &quot;Pathogens&quot;, PlantGrowth = &quot;Plant\\nGrowth&quot; ), exposure = &quot;SoilNitrogen&quot;, outcome = &quot;PathogenLoad&quot; ) ggdag(collider_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Collider: Don&#39;t Control for Plant Growth!&quot;) Figure 208.4: Collider structure: Plant growth is affected by both soil nitrogen and pathogens. Key insight: Never control for a collider—it creates bias where none existed. 208.4 Summary: What to control for Structure Control for it? Why Chain (mediator) ❌ (if estimating total effect) Blocks indirect path Fork (confounder) ✅ Blocks spurious path Collider ❌ Opens spurious path "],["part-2-building-a-conceptual-model.html", "Chapter 209 Part 2: Building a Conceptual Model 209.1 Meta-modeling your system 209.2 Case study: Pollinator response to vegetation management 209.3 Finding adjustment sets", " Chapter 209 Part 2: Building a Conceptual Model 209.1 Meta-modeling your system Before touching data, build a conceptual model: 209.1.1 1. Define your purpose Discovery: Exploring patterns for the first time Hypothesis testing: Evaluating specific predictions Prediction: Forecasting under new conditions 209.1.2 2. Identify variable roles Drivers: Variables that initiate change (treatments, environment) Mediators: Variables that transmit effects Responses: Outcome variables of interest 209.1.3 3. Determine scope Site-specific: Targeted insights for one location Generalizable: Testing broad ecological principles Make sure the pieces of your model are causal! Don’t throw in all variables just because you measured them—each should have a theorized role. 209.2 Case study: Pollinator response to vegetation management Let’s build a DAG for a real ecological question: How do different vegetation management treatments affect pollinators? Context: A utility company manages vegetation under powerlines using: - Mechanical removal - Herbicide application - Combined treatment - Untreated controls Question: Are treatment effects on pollinators mediated through plant community changes, or are there direct effects? # Define the DAG ivm_dag &lt;- dagify( # Treatment affects plant community PlantRichness ~ Treatment + Soil, PlantCover ~ Treatment + Soil, WoodyDebris ~ Treatment, # Context variables Treatment ~ Soil, # Pollinators respond to plants (and maybe treatment directly?) PollinatorAbund ~ PlantRichness + PlantCover + WoodyDebris, PollinatorRich ~ PlantRichness + PlantCover + WoodyDebris, # Labels for display labels = c( Treatment = &quot;Treatment&quot;, Soil = &quot;Soil Type&quot;, PlantRichness = &quot;Plant\\nRichness&quot;, PlantCover = &quot;Plant\\nCover&quot;, WoodyDebris = &quot;Woody\\nDebris&quot;, PollinatorAbund = &quot;Pollinator\\nAbundance&quot;, PollinatorRich = &quot;Pollinator\\nRichness&quot; ), exposure = &quot;Treatment&quot;, outcome = &quot;PollinatorAbund&quot; ) # Create node categories for coloring node_data &lt;- data.frame( name = c(&quot;Treatment&quot;, &quot;Soil&quot;, &quot;PlantRichness&quot;, &quot;PlantCover&quot;, &quot;WoodyDebris&quot;, &quot;PollinatorAbund&quot;, &quot;PollinatorRich&quot;), category = c(&quot;Treatment&quot;, &quot;Context&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Plant&quot;, &quot;Pollinator&quot;, &quot;Pollinator&quot;) ) # Plot with categories ggdag(ivm_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Hypothesized DAG: Pollinator Response to Vegetation Management&quot;) Figure 209.1: Conceptual DAG for pollinator response to integrated vegetation management. Treatment effects may be direct or mediated through plant community variables. 209.3 Finding adjustment sets An adjustment set is the set of variables you must control for to estimate a causal effect without bias. # What do we need to control for to estimate Treatment → Pollinator Abundance? adjustmentSets(ivm_dag, exposure = &quot;Treatment&quot;, outcome = &quot;PollinatorAbund&quot;) ## { Soil } Interpretation: To estimate the total effect of Treatment on Pollinator Abundance, we need to adjust for Soil (a confounder). We should NOT adjust for plant variables if we want the total effect (they’re mediators). "],["part-3-path-analysis-with-lavaan.html", "Chapter 210 Part 3: Path Analysis with lavaan 210.1 Example data: Post-fire plant recovery 210.2 The lavaan package 210.3 Step 1: Start simple 210.4 Step 2: Visualize the model 210.5 Step 3: Add mediation 210.6 Step 4: Calculate indirect effects", " Chapter 210 Part 3: Path Analysis with lavaan 210.1 Example data: Post-fire plant recovery We’ll use simulated data based on Keeley et al.’s study of chaparral recovery after fire. # Simulate Keeley-like data n &lt;- 90 # Exogenous variables age &lt;- runif(n, 1, 60) # Stand age (years since fire) distance &lt;- runif(n, 0.1, 10) # Distance to seed source (km) elev &lt;- runif(n, 100, 1500) # Elevation (m) abiotic &lt;- 0.5 * elev/100 + rnorm(n, 0, 1) # Abiotic stress index # Fire severity depends on age (older = more fuel = more severe) firesev &lt;- 2 + 0.08 * age + rnorm(n, 0, 1) firesev &lt;- pmax(firesev, 0) # Plant cover depends on fire severity and age cover &lt;- 80 - 3 * firesev + 0.5 * age + rnorm(n, 0, 8) cover &lt;- pmin(pmax(cover, 5), 100) # Heterogeneity depends on distance hetero &lt;- 3 - 0.2 * distance + rnorm(n, 0, 0.5) hetero &lt;- pmax(hetero, 0.5) # Species richness depends on cover, heterogeneity, abiotic rich &lt;- 20 + 0.3 * cover + 5 * hetero - 2 * abiotic + rnorm(n, 0, 5) rich &lt;- pmax(round(rich), 1) # Combine into data frame keeley &lt;- data.frame( age = age, distance = distance, elev = elev, abiotic = abiotic, firesev = firesev, cover = cover, hetero = hetero, rich = rich ) # Quick look head(keeley) ## age distance elev abiotic firesev cover hetero rich ## 1 54.520532 0.1154732 556.3568 2.038632 7.315950 90.87479 3.619867 59 ## 2 17.331870 6.4910978 547.6242 3.244637 2.971748 67.38241 2.551493 49 ## 3 3.621261 6.5125598 215.7519 1.525481 1.731083 88.29935 1.832440 55 ## 4 13.098696 7.0816857 1336.7063 7.054392 2.777620 75.49140 2.075912 45 ## 5 15.768973 3.6407198 644.0146 3.351855 5.154637 66.83416 2.073474 48 ## 6 49.097888 5.1848837 1361.4776 7.327790 3.230845 92.15629 1.180028 35 summary(keeley) ## age distance elev abiotic firesev cover ## Min. : 1.507 Min. :0.1155 Min. : 112.6 Min. :-1.281 Min. :0.7977 Min. : 51.59 ## 1st Qu.:14.561 1st Qu.:1.9608 1st Qu.: 446.7 1st Qu.: 2.407 1st Qu.:3.3972 1st Qu.: 73.61 ## Median :31.805 Median :4.4197 Median : 813.2 Median : 4.267 Median :4.5356 Median : 79.96 ## Mean :30.705 Mean :4.5342 Mean : 795.9 Mean : 4.187 Mean :4.5306 Mean : 79.98 ## 3rd Qu.:46.699 3rd Qu.:6.7659 3rd Qu.:1134.6 3rd Qu.: 6.030 3rd Qu.:5.8169 3rd Qu.: 87.92 ## Max. :59.345 Max. :9.8296 Max. :1495.5 Max. : 8.956 Max. :7.6958 Max. :100.00 ## hetero rich ## Min. :0.5122 Min. :24.00 ## 1st Qu.:1.5794 1st Qu.:40.25 ## Median :2.1392 Median :46.00 ## Mean :2.1849 Mean :46.73 ## 3rd Qu.:2.7237 3rd Qu.:53.75 ## Max. :3.6199 Max. :67.00 210.2 The lavaan package lavaan (Latent Variable Analysis) is the primary R package for SEM. Its syntax is similar to regression formulas: ## ## LAVAAN MODEL SYNTAX ## ═══════════════════════════════════════════════════════════════ ## ## Regression paths: y ~ x1 + x2 (y regressed on x1 and x2) ## Covariances: x1 ~~ x2 (covariance between x1 and x2) ## Latent variables: f =~ y1 + y2 (factor f measured by y1 and y2) ## Labeled parameters: y ~ b1*x1 (coefficient labeled &#39;b1&#39;) ## Defined parameters: ind := a * b (indirect effect) 210.3 Step 1: Start simple Let’s begin with a simple regression as SEM: # Simple model: cover depends on age model1 &lt;- &#39;cover ~ age&#39; # Fit the model fit1 &lt;- sem(model1, data = keeley) # Summary summary(fit1, standardized = TRUE, rsquare = TRUE) ## lavaan 0.6-21 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 2 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## cover ~ ## age 0.270 0.055 4.925 0.000 0.270 0.461 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .cover 82.832 12.348 6.708 0.000 82.832 0.788 ## ## R-Square: ## Estimate ## cover 0.212 Reading the output: Estimate: Unstandardized coefficient (on original scale) Std.Err: Standard error z-value: Test statistic P(&gt;|z|): P-value Std.lv / Std.all: Standardized coefficients 210.4 Step 2: Visualize the model semPaths(fit1, whatLabels = &quot;std&quot;, # Show standardized estimates edge.label.cex = 1.2, sizeMan = 10, style = &quot;ram&quot;) Figure 210.1: Path diagram showing the simple regression model. 210.5 Step 3: Add mediation Now let’s test whether fire severity mediates the effect of age on cover: # Mediation model model2 &lt;- &#39; firesev ~ age cover ~ firesev + age &#39; fit2 &lt;- sem(model2, data = keeley) summary(fit2, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE) ## lavaan 0.6-21 ended normally after 1 iteration ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 5 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 134.144 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -443.951 ## Loglikelihood unrestricted model (H1) -443.951 ## ## Akaike (AIC) 897.903 ## Bayesian (BIC) 910.402 ## Sample-size adjusted Bayesian (SABIC) 894.621 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## firesev ~ ## age 0.076 0.006 12.446 0.000 0.076 0.795 ## cover ~ ## firesev -4.236 0.836 -5.066 0.000 -4.236 -0.690 ## age 0.591 0.080 7.413 0.000 0.591 1.009 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .firesev 1.024 0.153 6.708 0.000 1.024 0.368 ## .cover 64.454 9.608 6.708 0.000 64.454 0.613 ## ## R-Square: ## Estimate ## firesev 0.632 ## cover 0.387 semPaths(fit2, whatLabels = &quot;std&quot;, edge.label.cex = 1.2, sizeMan = 10, style = &quot;ram&quot;, layout = &quot;tree&quot;) Figure 210.2: Path diagram showing the mediation model: age affects cover both directly and indirectly through fire severity. 210.6 Step 4: Calculate indirect effects # Label paths to calculate indirect effects model3 &lt;- &#39; # Direct paths (labeled) firesev ~ a*age cover ~ b*firesev + c*age # Defined parameters indirect := a * b # Indirect effect through firesev total := c + (a * b) # Total effect &#39; fit3 &lt;- sem(model3, data = keeley) # Get standardized solution with indirect effects standardizedSolution(fit3) %&gt;% filter(op == &quot;:=&quot;) %&gt;% select(lhs, est.std, se, pvalue) ## lhs est.std se pvalue ## 1 indirect -0.548 0.110 0 ## 2 total 0.461 0.079 0 Interpretation: - Direct effect (c): Age → Cover, controlling for fire severity - Indirect effect (a×b): Age → Fire Severity → Cover - Total effect: Direct + Indirect "],["part-4-model-fit-assessment.html", "Chapter 211 Part 4: Model Fit Assessment 211.1 Why assess fit? 211.2 Key fit indices 211.3 Modification indices", " Chapter 211 Part 4: Model Fit Assessment 211.1 Why assess fit? Unlike regression, SEM tests whether your entire model structure fits the data. A poor fit means your hypothesized causal structure may be wrong. 211.2 Key fit indices # Get fit measures fitMeasures(fit2, c(&quot;chisq&quot;, &quot;df&quot;, &quot;pvalue&quot;, &quot;cfi&quot;, &quot;tli&quot;, &quot;rmsea&quot;, &quot;srmr&quot;)) ## chisq df pvalue cfi tli rmsea srmr ## 0 0 NA 1 1 0 0 Index Good fit Interpretation Chi-square p-value &gt; 0.05 Model not significantly different from data CFI &gt; 0.95 Comparative fit index TLI &gt; 0.95 Tucker-Lewis index RMSEA &lt; 0.06 Root mean square error of approximation SRMR &lt; 0.08 Standardized root mean square residual Note: Just-identified models (df = 0) can’t be tested for fit—there’s no room for misfit! 211.3 Modification indices If fit is poor, modification indices suggest where the model might be improved: # Check modification indices modificationIndices(fit2, sort. = TRUE, minimum.value = 3) ## [1] lhs op rhs mi epc sepc.lv sepc.all sepc.nox ## &lt;0 rows&gt; (or 0-length row.names) Caution: Only add paths that make theoretical sense—don’t let statistics override ecology! "],["part-5-a-complete-example.html", "Chapter 212 Part 5: A Complete Example 212.1 Interpreting the results", " Chapter 212 Part 5: A Complete Example Let’s build a more complex model of species richness: # Complex model with multiple pathways model_rich &lt;- &#39; # Structural paths hetero ~ distance abiotic ~ elev firesev ~ age cover ~ firesev + age rich ~ cover + hetero + abiotic + distance # Covariance between exogenous variables distance ~~ elev distance ~~ age elev ~~ age &#39; fit_rich &lt;- sem(model_rich, data = keeley) # Summary summary(fit_rich, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE) ## lavaan 0.6-21 ended normally after 77 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 90 ## ## Model Test User Model: ## ## Test statistic 12.492 ## Degrees of freedom 16 ## P-value (Chi-square) 0.709 ## ## Model Test Baseline Model: ## ## Test statistic 486.420 ## Degrees of freedom 28 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.013 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2178.538 ## Loglikelihood unrestricted model (H1) -2172.292 ## ## Akaike (AIC) 4397.076 ## Bayesian (BIC) 4447.072 ## Sample-size adjusted Bayesian (SABIC) 4383.951 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.075 ## P-value H_0: RMSEA &lt;= 0.050 0.861 ## P-value H_0: RMSEA &gt;= 0.080 0.039 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.060 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## hetero ~ ## distance -0.184 NA -0.184 -0.735 ## abiotic ~ ## elev 0.005 NA 0.005 0.906 ## firesev ~ ## age 0.076 NA 0.076 0.795 ## cover ~ ## firesev -4.236 NA -4.236 -0.690 ## age 0.591 NA 0.591 1.009 ## rich ~ ## cover 0.334 NA 0.334 0.410 ## hetero 5.179 NA 5.179 0.456 ## abiotic -2.051 NA -2.051 -0.552 ## distance 0.168 NA 0.168 0.059 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## distance ~~ ## elev 90.371 NA 90.371 0.074 ## age -4.971 NA -4.971 -0.096 ## elev ~~ ## age -439.927 NA -439.927 -0.061 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .hetero 0.249 NA 0.249 0.460 ## .abiotic 0.906 NA 0.906 0.179 ## .firesev 1.024 NA 1.024 0.368 ## .cover 64.454 NA 64.454 0.613 ## .rich 21.883 NA 21.883 0.313 ## distance 8.683 NA 8.683 1.000 ## elev 171774.608 NA 171774.608 1.000 ## age 306.519 NA 306.519 1.000 ## ## R-Square: ## Estimate ## hetero 0.540 ## abiotic 0.821 ## firesev 0.632 ## cover 0.387 ## rich 0.687 # Visualize semPaths(fit_rich, whatLabels = &quot;std&quot;, edge.label.cex = 0.9, sizeMan = 8, style = &quot;ram&quot;, layout = &quot;tree2&quot;, rotation = 2) Figure 212.1: Complete path model showing multiple pathways affecting plant species richness after fire. 212.1 Interpreting the results # Extract standardized coefficients std_solution &lt;- standardizedSolution(fit_rich) %&gt;% filter(op == &quot;~&quot;) %&gt;% select(lhs, rhs, est.std, se, pvalue) %&gt;% arrange(lhs, desc(abs(est.std))) print(std_solution, digits = 3) ## lhs rhs est.std se pvalue ## 1 abiotic elev 0.906 NA NA ## 2 cover age 1.009 NA NA ## 3 cover firesev -0.690 NA NA ## 4 firesev age 0.795 NA NA ## 5 hetero distance -0.735 NA NA ## 6 rich abiotic -0.552 NA NA ## 7 rich hetero 0.456 NA NA ## 8 rich cover 0.410 NA NA ## 9 rich distance 0.059 NA NA Key findings: - Which pathways are significant? - What’s the relative importance of direct vs indirect effects? - Does the model fit the data well? "],["part-6-comparing-models.html", "Chapter 213 Part 6: Comparing Models 213.1 Nested models: Likelihood Ratio Test 213.2 Non-nested models: AIC/BIC", " Chapter 213 Part 6: Comparing Models 213.1 Nested models: Likelihood Ratio Test When one model is a subset of another (nested), use the likelihood ratio test: # Full model: direct effect of age on cover model_full &lt;- &#39; firesev ~ age cover ~ firesev + age &#39; fit_full &lt;- sem(model_full, data = keeley) # Reduced model: fully mediated (no direct age effect) model_reduced &lt;- &#39; firesev ~ age cover ~ firesev &#39; fit_reduced &lt;- sem(model_reduced, data = keeley) # Compare anova(fit_reduced, fit_full) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## fit_full 0 897.9 910.4 0.000 ## fit_reduced 1 938.8 948.8 42.896 42.896 0.68229 1 5.772e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Interpretation: If p &gt; 0.05, the simpler model fits equally well. 213.2 Non-nested models: AIC/BIC # Compare using information criteria data.frame( Model = c(&quot;Fully mediated&quot;, &quot;Partial mediation&quot;), AIC = c(AIC(fit_reduced), AIC(fit_full)), BIC = c(BIC(fit_reduced), BIC(fit_full)) ) ## Model AIC BIC ## 1 Fully mediated 938.7991 948.7984 ## 2 Partial mediation 897.9027 910.4017 Lower AIC/BIC = better fit (penalized for complexity) "],["part-7-assumptions-and-diagnostics.html", "Chapter 214 Part 7: Assumptions and Diagnostics 214.1 Key assumptions 214.2 Checking multivariate normality 214.3 Sample size guidelines 214.4 Robust estimation", " Chapter 214 Part 7: Assumptions and Diagnostics 214.1 Key assumptions Multivariate normality (for ML estimation) Correct model specification (all important paths included) No severe multicollinearity Sufficient sample size 214.2 Checking multivariate normality # Check univariate distributions keeley %&gt;% select(age, firesev, cover, rich) %&gt;% pivot_longer(everything()) %&gt;% ggplot(aes(x = value)) + geom_histogram(bins = 20, fill = &quot;steelblue&quot;, color = &quot;white&quot;) + facet_wrap(~name, scales = &quot;free&quot;) + theme_minimal() + labs(title = &quot;Distribution of Key Variables&quot;) 214.3 Sample size guidelines Minimum: 5-10 observations per estimated parameter Preferred: 20+ observations per parameter Rule of thumb: N &gt; 200 for complex models # Count parameters n_params &lt;- fitmeasures(fit_rich, &quot;npar&quot;) n_obs &lt;- nrow(keeley) cat(&quot;Estimated parameters:&quot;, n_params, &quot;\\n&quot;) ## Estimated parameters: 20 cat(&quot;Sample size:&quot;, n_obs, &quot;\\n&quot;) ## Sample size: 90 cat(&quot;Ratio:&quot;, round(n_obs / n_params, 1), &quot;observations per parameter\\n&quot;) ## Ratio: 4.5 observations per parameter 214.4 Robust estimation If normality is violated, use robust standard errors: # Satorra-Bentler correction fit_robust &lt;- sem(model_rich, data = keeley, estimator = &quot;MLM&quot;) # Robust ML summary(fit_robust, fit.measures = TRUE) ## lavaan 0.6-21 ended normally after 77 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 90 ## ## Model Test User Model: ## Standard Scaled ## Test Statistic 12.492 NA ## Degrees of freedom 16 16 ## P-value (Chi-square) 0.709 NA ## Scaling correction factor NA ## ## ## Model Test Baseline Model: ## ## Test statistic 486.420 473.421 ## Degrees of freedom 28 28 ## P-value 0.000 0.000 ## Scaling correction factor 1.027 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 NA ## Tucker-Lewis Index (TLI) 1.013 NA ## ## Robust Comparative Fit Index (CFI) NA ## Robust Tucker-Lewis Index (TLI) NA ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -2178.538 -2178.538 ## Loglikelihood unrestricted model (H1) -2172.292 -2172.292 ## ## Akaike (AIC) 4397.076 4397.076 ## Bayesian (BIC) 4447.072 4447.072 ## Sample-size adjusted Bayesian (SABIC) 4383.951 4383.951 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 NA ## 90 Percent confidence interval - lower 0.000 NA ## 90 Percent confidence interval - upper 0.075 NA ## P-value H_0: RMSEA &lt;= 0.050 0.861 NA ## P-value H_0: RMSEA &gt;= 0.080 0.039 NA ## ## Robust RMSEA NA ## 90 Percent confidence interval - lower NA ## 90 Percent confidence interval - upper NA ## P-value H_0: Robust RMSEA &lt;= 0.050 NA ## P-value H_0: Robust RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.060 0.060 ## ## Parameter Estimates: ## ## Standard errors Robust.sem ## Information Expected ## Information saturated (h1) model Structured ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## hetero ~ ## distance -0.184 NA ## abiotic ~ ## elev 0.005 NA ## firesev ~ ## age 0.076 NA ## cover ~ ## firesev -4.236 NA ## age 0.591 NA ## rich ~ ## cover 0.334 NA ## hetero 5.179 NA ## abiotic -2.051 NA ## distance 0.168 NA ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## distance ~~ ## elev 90.371 NA ## age -4.971 NA ## elev ~~ ## age -439.927 NA ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .hetero 0.249 NA ## .abiotic 0.906 NA ## .firesev 1.024 NA ## .cover 64.454 NA ## .rich 21.883 NA ## distance 8.683 NA ## elev 171774.608 NA ## age 306.519 NA "],["part-8-reporting-sem-results.html", "Chapter 215 Part 8: Reporting SEM Results 215.1 What to report 215.2 Sample methods and results 215.3 Key takeaways 215.4 Assignment", " Chapter 215 Part 8: Reporting SEM Results 215.1 What to report Sample size and any missing data handling Model specification (path diagram or equations) Estimation method (ML, robust, etc.) Fit indices (χ², df, p, CFI, RMSEA, SRMR) Path coefficients (unstandardized and standardized) R² for endogenous variables Indirect effects if testing mediation 215.2 Sample methods and results 215.2.1 Methods We tested hypothesized relationships among stand age, fire severity, plant cover, and species richness using structural equation modeling. Our conceptual model proposed that fire severity mediates the effect of stand age on vegetation cover, which in turn affects species richness along with habitat heterogeneity and abiotic stress. We estimated path coefficients using maximum likelihood in lavaan (Rosseel 2012). Model fit was assessed using chi-square test, CFI, RMSEA, and SRMR. Indirect effects were calculated as products of path coefficients with significance assessed via delta method standard errors. Analyses were conducted in R version 4.3.1. 215.2.2 Results The hypothesized path model showed acceptable fit to the data (χ² = 12.4, df = 8, p = 0.13; CFI = 0.97; RMSEA = 0.05; SRMR = 0.04; Fig. X). Stand age significantly increased fire severity (β = 0.42, p &lt; 0.001), which in turn reduced plant cover (β = -0.38, p &lt; 0.001). Plant cover positively predicted species richness (β = 0.35, p &lt; 0.001), along with habitat heterogeneity (β = 0.28, p = 0.003). The indirect effect of age on richness through fire severity and cover was significant (β = -0.06, p = 0.02), indicating partial mediation. The model explained 45% of variance in species richness. 215.3 Key takeaways SEM tests entire causal systems — Not just individual paths DAGs clarify causal assumptions — Build before you model Know your structures — Chains, forks, and colliders determine what to control Fit matters — Check χ², CFI, RMSEA, SRMR Label paths for indirect effects — Use := to define products Theory guides specification — Don’t add paths just because modification indices suggest them Sample size constraints — Need ~10-20 observations per parameter 215.4 Assignment 215.4.1 Part 1: Conceptual questions You’re studying how grazing affects plant diversity through soil compaction. Draw a DAG with grazing, compaction, and diversity. Is compaction a mediator, confounder, or collider? A colleague controls for plant biomass when estimating the effect of fertilizer on herbivore abundance. Plant biomass is affected by fertilizer AND affects herbivores. Is this appropriate? Why or why not? Your SEM has excellent fit (χ² p = 0.95, CFI = 1.0). Should you be suspicious? Why? 215.4.2 Part 2: Build a DAG For your own research question (or a hypothetical ecological system): Identify 4-6 variables and their hypothesized roles Create a DAG using dagitty Determine the adjustment set for your key causal question Discuss what variables you should and should NOT control for 215.4.3 Part 3: Fit a path model Using the keeley data: Propose a path model linking distance, heterogeneity, and richness Fit the model in lavaan Assess model fit Calculate any indirect effects Visualize with semPaths 215.4.4 Part 4: Model comparison Fit two competing models for the keeley data (one with a direct path, one without) Compare using LRT and AIC Interpret which model is preferred and why 215.4.5 Part 5: Reporting Write a complete methods and results paragraph for your path model from Part 3, following the format in this chapter. "],["structural-equation-modeling-ii-latent-variables-and-measurement-models.html", "Chapter 216 Structural Equation Modeling II: Latent Variables and Measurement Models 216.1 What is a latent variable? 216.2 Why use latent variables? 216.3 Setup", " Chapter 216 Structural Equation Modeling II: Latent Variables and Measurement Models In Part I, we modeled relationships among observed variables—things we directly measure. But many ecological concepts are latent: habitat quality, disturbance pressure, ecosystem health. We can’t measure these directly, but we can infer them from multiple indicators. This chapter extends SEM to include: Latent variables inferred from multiple indicators Confirmatory Factor Analysis (CFA) for measurement models Full structural models with both latent and observed variables Measurement error and why it matters 216.1 What is a latent variable? A latent variable is an unmeasured construct that we infer from multiple observed indicators. ## ## LATENT VARIABLES ## ═══════════════════════════════════════════════════════════════ ## ## Observable indicators Latent construct ## ↓ ↓ ## ┌─────────┐ ┌───────┐ ## │ Road │──────────────────│ │ ## │ density │ │ │ ## └─────────┘ │ │ ## ┌─────────┐ │Distur-│ ## │ Fire │──────────────────│ bance │ ## │ freq │ │Pressure│ ## └─────────┘ │ │ ## ┌─────────┐ │ │ ## │ Grazing │──────────────────│ │ ## │ intens. │ │ │ ## └─────────┘ └───────┘ ## (squares) (circle) ## Observed Latent ## ## We can&#39;t measure &#39;disturbance pressure&#39; directly, ## but we CAN infer it from road density, fire frequency, ## and grazing intensity. Ecological examples of latent variables: Habitat quality: Inferred from vegetation cover, food availability, water access Stress level: Inferred from cortisol, behavior, body condition Community stability: Inferred from temporal variability in multiple species 216.2 Why use latent variables? Theoretical accuracy: Many constructs are inherently unobservable Measurement error: Each indicator is imperfect; combining them reduces error Parsimony: One latent variable summarizes multiple indicators Better estimates: Separating true signal from measurement noise improves inference 216.3 Setup library(tidyverse) library(lavaan) library(semPlot) set.seed(42) "],["part-1-confirmatory-factor-analysis-cfa.html", "Chapter 217 Part 1: Confirmatory Factor Analysis (CFA) 217.1 What is CFA? 217.2 CFA syntax in lavaan 217.3 Example: Disturbance pressure 217.4 Fitting a CFA model 217.5 Interpreting CFA output 217.6 Identification in CFA", " Chapter 217 Part 1: Confirmatory Factor Analysis (CFA) 217.1 What is CFA? Confirmatory Factor Analysis tests whether observed variables load onto hypothesized latent factors. It’s the “measurement model” portion of SEM. Exploratory Factor Analysis (EFA): Discovers factor structure from data Confirmatory Factor Analysis (CFA): Tests whether data fit a pre-specified structure 217.2 CFA syntax in lavaan ## ## LAVAAN CFA SYNTAX ## ═══════════════════════════════════════════════════════════════ ## ## Define a latent factor: ## ## LatentFactor =~ indicator1 + indicator2 + indicator3 ## ## The &#39;=~&#39; operator means &#39;is measured by&#39; ## ## Example: ## ## HabitatQuality =~ veg_cover + food_avail + water_dist 217.3 Example: Disturbance pressure Let’s create data where three indicators reflect an underlying disturbance construct: # Simulate latent disturbance factor with three indicators n &lt;- 200 # True latent disturbance (unobserved) disturbance_true &lt;- rnorm(n, mean = 0, sd = 1) # Observed indicators (imperfect measures of disturbance) road_density &lt;- 0.7 * disturbance_true + rnorm(n, 0, 0.5) fire_frequency &lt;- 0.8 * disturbance_true + rnorm(n, 0, 0.4) grazing_intensity &lt;- 0.6 * disturbance_true + rnorm(n, 0, 0.6) # Combine disturbance_data &lt;- data.frame( road_density = road_density, fire_freq = fire_frequency, grazing = grazing_intensity ) # Check correlations (should be moderate-high) cor(disturbance_data) ## road_density fire_freq grazing ## road_density 1.0000000 0.7244333 0.5679056 ## fire_freq 0.7244333 1.0000000 0.6136867 ## grazing 0.5679056 0.6136867 1.0000000 217.4 Fitting a CFA model # Define CFA model cfa_model &lt;- &#39; # Latent factor definition Disturbance =~ road_density + fire_freq + grazing &#39; # Fit the model cfa_fit &lt;- cfa(cfa_model, data = disturbance_data) # Summary summary(cfa_fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-21 ended normally after 21 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 6 ## ## Number of observations 200 ## ## Model Test User Model: ## ## Test statistic 0.000 ## Degrees of freedom 0 ## ## Model Test Baseline Model: ## ## Test statistic 253.865 ## Degrees of freedom 3 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.000 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -599.757 ## Loglikelihood unrestricted model (H1) -599.757 ## ## Akaike (AIC) 1211.514 ## Bayesian (BIC) 1231.304 ## Sample-size adjusted Bayesian (SABIC) 1212.296 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.000 ## P-value H_0: RMSEA &lt;= 0.050 NA ## P-value H_0: RMSEA &gt;= 0.080 NA ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.000 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Disturbance =~ ## road_density 1.000 0.652 0.819 ## fire_freq 1.169 0.105 11.185 0.000 0.763 0.885 ## grazing 0.830 0.083 9.985 0.000 0.542 0.694 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .road_density 0.209 0.036 5.776 0.000 0.209 0.330 ## .fire_freq 0.161 0.044 3.707 0.000 0.161 0.217 ## .grazing 0.316 0.038 8.406 0.000 0.316 0.519 ## Disturbance 0.425 0.067 6.368 0.000 1.000 1.000 217.5 Interpreting CFA output Factor loadings: How strongly each indicator reflects the latent factor - Standardized loadings &gt; 0.5 indicate good indicators - Loadings &gt; 0.7 are excellent Model fit: Does the measurement model fit the data? - Same indices as path analysis (χ², CFI, RMSEA, SRMR) semPaths(cfa_fit, whatLabels = &quot;std&quot;, edge.label.cex = 1.2, sizeMan = 8, sizeLat = 10, style = &quot;ram&quot;, nCharNodes = 0, nCharEdges = 0) Figure 217.1: CFA diagram showing the latent Disturbance factor measured by three indicators. Numbers are standardized factor loadings. 217.6 Identification in CFA A CFA model must be identified (enough information to estimate all parameters). Rules for identification: Situation Identified? 3+ indicators per factor ✅ Yes 2 indicators, variances fixed ✅ Yes 1 indicator ❌ No (need to fix error variance) Setting the scale: The latent variable needs a scale. Options: 1. Fix first loading to 1.0 (default in lavaan) 2. Fix latent variance to 1.0 # Alternative: fix latent variance to 1 cfa_model_std &lt;- &#39; Disturbance =~ NA*road_density + fire_freq + grazing Disturbance ~~ 1*Disturbance # Fix variance to 1 &#39; cfa_fit_std &lt;- cfa(cfa_model_std, data = disturbance_data) standardizedSolution(cfa_fit_std) %&gt;% filter(op == &quot;=~&quot;) ## lhs op rhs est.std se z pvalue ci.lower ci.upper ## 1 Disturbance =~ road_density 0.819 0.037 22.262 0 0.747 0.891 ## 2 Disturbance =~ fire_freq 0.885 0.034 25.746 0 0.817 0.952 ## 3 Disturbance =~ grazing 0.694 0.044 15.800 0 0.608 0.780 "],["part-2-multiple-latent-factors.html", "Chapter 218 Part 2: Multiple Latent Factors 218.1 When constructs are multidimensional 218.2 Fitting a two-factor CFA 218.3 Factor correlations", " Chapter 218 Part 2: Multiple Latent Factors 218.1 When constructs are multidimensional Often we have multiple latent factors. For example, “ecosystem condition” might include: - Disturbance pressure - Resource availability - Habitat structure # Simulate two latent factors n &lt;- 250 # Factor 1: Disturbance disturbance &lt;- rnorm(n) road &lt;- 0.7 * disturbance + rnorm(n, 0, 0.5) fire &lt;- 0.8 * disturbance + rnorm(n, 0, 0.4) grazing &lt;- 0.6 * disturbance + rnorm(n, 0, 0.6) # Factor 2: Resources (negatively correlated with disturbance) resources &lt;- -0.3 * disturbance + rnorm(n, 0, 0.8) food &lt;- 0.75 * resources + rnorm(n, 0, 0.5) water &lt;- 0.70 * resources + rnorm(n, 0, 0.5) cover &lt;- 0.65 * resources + rnorm(n, 0, 0.5) # Combine multi_factor_data &lt;- data.frame( road = road, fire = fire, grazing = grazing, food = food, water = water, cover = cover ) 218.2 Fitting a two-factor CFA # Two-factor CFA model cfa_2factor &lt;- &#39; # Factor definitions Disturbance =~ road + fire + grazing Resources =~ food + water + cover # Factor correlation (estimated by default) # Disturbance ~~ Resources &#39; cfa_2fit &lt;- cfa(cfa_2factor, data = multi_factor_data) summary(cfa_2fit, standardized = TRUE, fit.measures = TRUE) ## lavaan 0.6-21 ended normally after 25 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 13 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 4.055 ## Degrees of freedom 8 ## P-value (Chi-square) 0.852 ## ## Model Test Baseline Model: ## ## Test statistic 691.398 ## Degrees of freedom 15 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.011 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1548.263 ## Loglikelihood unrestricted model (H1) -1546.236 ## ## Akaike (AIC) 3122.526 ## Bayesian (BIC) 3168.305 ## Sample-size adjusted Bayesian (SABIC) 3127.094 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.041 ## P-value H_0: RMSEA &lt;= 0.050 0.971 ## P-value H_0: RMSEA &gt;= 0.080 0.002 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.014 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Disturbance =~ ## road 1.000 0.805 0.841 ## fire 1.072 0.066 16.171 0.000 0.862 0.921 ## grazing 0.912 0.065 13.987 0.000 0.733 0.773 ## Resources =~ ## food 1.000 0.589 0.732 ## water 0.997 0.099 10.083 0.000 0.587 0.771 ## cover 0.986 0.098 10.109 0.000 0.581 0.779 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Disturbance ~~ ## Resources -0.182 0.039 -4.623 0.000 -0.385 -0.385 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .road 0.268 0.036 7.404 0.000 0.268 0.293 ## .fire 0.133 0.033 4.011 0.000 0.133 0.152 ## .grazing 0.363 0.040 9.131 0.000 0.363 0.403 ## .food 0.301 0.038 7.952 0.000 0.301 0.465 ## .water 0.235 0.034 7.004 0.000 0.235 0.405 ## .cover 0.218 0.032 6.792 0.000 0.218 0.393 ## Disturbance 0.647 0.083 7.812 0.000 1.000 1.000 ## Resources 0.347 0.058 6.000 0.000 1.000 1.000 semPaths(cfa_2fit, whatLabels = &quot;std&quot;, edge.label.cex = 1.0, sizeMan = 7, sizeLat = 9, style = &quot;ram&quot;, layout = &quot;tree2&quot;, rotation = 2) Figure 218.1: Two-factor CFA model with Disturbance and Resources as correlated latent factors. 218.3 Factor correlations # Extract factor correlation lavInspect(cfa_2fit, what = &quot;cor.lv&quot;) ## Dstrbn Resrcs ## Disturbance 1.000 ## Resources -0.385 1.000 Interpretation: The correlation between latent factors tells you how related the constructs are. High correlation (&gt; 0.8) might suggest they’re actually one factor. "],["part-3-full-structural-models.html", "Chapter 219 Part 3: Full Structural Models 219.1 Combining measurement and structural models 219.2 Example: Disturbance effects on biodiversity 219.3 Extracting results", " Chapter 219 Part 3: Full Structural Models 219.1 Combining measurement and structural models A full SEM has two components: Measurement model: How latent variables relate to indicators (CFA) Structural model: How latent variables relate to each other ## ## FULL STRUCTURAL EQUATION MODEL ## ═══════════════════════════════════════════════════════════════ ## ## MEASUREMENT MODEL (CFA) STRUCTURAL MODEL (Path) ## ↓ ↓ ## ┌─────────────────┐ ┌─────────────────┐ ## │ road ─┐ │ │ │ ## │ │ │ │ Disturbance │ ## │ fire ─┼→ Disturb│──────────────│ │ │ ## │ │ │ │ ▼ │ ## │ graz ─┘ │ │ Richness ←────│── Observed ## └─────────────────┘ │ │ │ predictors ## │ ▼ │ ## │ Abundance │ ## └─────────────────┘ ## ## Left side: HOW we measure latent constructs ## Right side: HOW latent constructs relate to outcomes 219.2 Example: Disturbance effects on biodiversity # Add outcome variables multi_factor_data$species_richness &lt;- -0.5 * disturbance + 0.6 * resources + rnorm(n, 0, 0.4) + 15 multi_factor_data$total_abundance &lt;- -0.3 * disturbance + 0.8 * resources + rnorm(n, 0, 0.5) + 100 # Full SEM with latent and observed variables full_sem &lt;- &#39; # Measurement model Disturbance =~ road + fire + grazing Resources =~ food + water + cover # Structural model species_richness ~ Disturbance + Resources total_abundance ~ Disturbance + Resources + species_richness &#39; full_fit &lt;- sem(full_sem, data = multi_factor_data) summary(full_fit, standardized = TRUE, rsquare = TRUE, fit.measures = TRUE) ## lavaan 0.6-21 ended normally after 35 iterations ## ## Estimator ML ## Optimization method NLMINB ## Number of model parameters 20 ## ## Number of observations 250 ## ## Model Test User Model: ## ## Test statistic 15.393 ## Degrees of freedom 16 ## P-value (Chi-square) 0.496 ## ## Model Test Baseline Model: ## ## Test statistic 1269.746 ## Degrees of freedom 28 ## P-value 0.000 ## ## User Model versus Baseline Model: ## ## Comparative Fit Index (CFI) 1.000 ## Tucker-Lewis Index (TLI) 1.001 ## ## Loglikelihood and Information Criteria: ## ## Loglikelihood user model (H0) -1949.328 ## Loglikelihood unrestricted model (H1) -1941.631 ## ## Akaike (AIC) 3938.656 ## Bayesian (BIC) 4009.085 ## Sample-size adjusted Bayesian (SABIC) 3945.683 ## ## Root Mean Square Error of Approximation: ## ## RMSEA 0.000 ## 90 Percent confidence interval - lower 0.000 ## 90 Percent confidence interval - upper 0.056 ## P-value H_0: RMSEA &lt;= 0.050 0.911 ## P-value H_0: RMSEA &gt;= 0.080 0.002 ## ## Standardized Root Mean Square Residual: ## ## SRMR 0.017 ## ## Parameter Estimates: ## ## Standard errors Standard ## Information Expected ## Information saturated (h1) model Structured ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Disturbance =~ ## road 1.000 0.805 0.841 ## fire 1.052 0.060 17.629 0.000 0.847 0.905 ## grazing 0.936 0.064 14.733 0.000 0.754 0.794 ## Resources =~ ## food 1.000 0.599 0.744 ## water 0.977 0.085 11.450 0.000 0.585 0.769 ## cover 0.959 0.084 11.477 0.000 0.574 0.770 ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## species_richness ~ ## Disturbance -0.689 0.056 -12.313 0.000 -0.555 -0.579 ## Resources 0.776 0.083 9.366 0.000 0.465 0.485 ## total_abundance ~ ## Disturbance -0.444 0.094 -4.740 0.000 -0.357 -0.378 ## Resources 0.910 0.135 6.724 0.000 0.545 0.577 ## species_rchnss 0.048 0.106 0.448 0.654 0.048 0.048 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## Disturbance ~~ ## Resources -0.186 0.040 -4.677 0.000 -0.385 -0.385 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .road 0.267 0.032 8.384 0.000 0.267 0.292 ## .fire 0.159 0.026 6.099 0.000 0.159 0.181 ## .grazing 0.333 0.036 9.266 0.000 0.333 0.369 ## .food 0.289 0.033 8.901 0.000 0.289 0.446 ## .water 0.237 0.028 8.508 0.000 0.237 0.409 ## .cover 0.226 0.027 8.473 0.000 0.226 0.406 ## .species_rchnss 0.196 0.026 7.611 0.000 0.196 0.214 ## .total_abundanc 0.256 0.031 8.258 0.000 0.256 0.287 ## Disturbance 0.648 0.081 7.993 0.000 1.000 1.000 ## Resources 0.359 0.055 6.467 0.000 1.000 1.000 ## ## R-Square: ## Estimate ## road 0.708 ## fire 0.819 ## grazing 0.631 ## food 0.554 ## water 0.591 ## cover 0.594 ## species_rchnss 0.786 ## total_abundanc 0.713 semPaths(full_fit, whatLabels = &quot;std&quot;, edge.label.cex = 0.8, sizeMan = 6, sizeLat = 8, style = &quot;ram&quot;, layout = &quot;tree2&quot;, rotation = 2, nCharNodes = 6) Figure 219.1: Full SEM showing how latent Disturbance and Resources affect species richness and abundance. 219.3 Extracting results pe &lt;- lavaan::parameterEstimates(full_fit, standardized = TRUE) pe %&gt;% dplyr::filter(op == &quot;~&quot;) %&gt;% dplyr::select(lhs, rhs, est, se, pvalue, std.all) %&gt;% dplyr::arrange(lhs) ## lhs rhs est se pvalue std.all ## 1 species_richness Disturbance -0.689 0.056 0.000 -0.579 ## 2 species_richness Resources 0.776 0.083 0.000 0.485 ## 3 total_abundance Disturbance -0.444 0.094 0.000 -0.378 ## 4 total_abundance Resources 0.910 0.135 0.000 0.577 ## 5 total_abundance species_richness 0.048 0.106 0.654 0.048 "],["part-4-measurement-error-in-predictors.html", "Chapter 220 Part 4: Measurement Error in Predictors 220.1 Why measurement error matters 220.2 Modeling measurement error 220.3 Comparing models with and without error correction", " Chapter 220 Part 4: Measurement Error in Predictors 220.1 Why measurement error matters Standard regression assumes predictors are measured without error. This is rarely true in ecology: GPS locations have uncertainty Species counts have observation error Environmental sensors have measurement error Consequence: Measurement error in predictors biases coefficients toward zero (“attenuation bias”). 220.2 Modeling measurement error In SEM, we can explicitly model measurement error by treating observed variables as indicators of “true” latent variables: # Simulate data with known measurement error n &lt;- 150 # True values true_temperature &lt;- rnorm(n, mean = 20, sd = 5) true_precipitation &lt;- rnorm(n, mean = 1000, sd = 200) # Measured with error obs_temp &lt;- true_temperature + rnorm(n, 0, 2) # ±2°C error obs_precip &lt;- true_precipitation + rnorm(n, 0, 100) # ±100mm error # Response depends on TRUE values species_count &lt;- 20 + 0.5 * true_temperature + 0.01 * true_precipitation + rnorm(n, 0, 3) error_data &lt;- data.frame( temp = obs_temp, precip = obs_precip, species = species_count ) 220.3 Comparing models with and without error correction # Standard regression (ignores measurement error) lm_naive &lt;- lm(species ~ temp + precip, data = error_data) cat(&quot;Standard regression (ignoring measurement error):\\n&quot;) ## Standard regression (ignoring measurement error): summary(lm_naive)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 21.536715578 1.518865049 14.179479 2.636730e-29 ## temp 0.454424397 0.044418113 10.230610 6.939820e-19 ## precip 0.009097557 0.001138806 7.988678 3.639224e-13 # SEM with single indicators (still ignores error, but same framework) sem_naive &lt;- &#39; species ~ temp + precip &#39; fit_naive &lt;- sem(sem_naive, data = error_data) To properly account for measurement error with single indicators, you need: 1. Multiple indicators, OR 2. External information about error variance # If you KNOW the error variance, you can fix it # Error variance = 4 for temp (SD = 2, variance = 4) # Error variance = 10000 for precip (SD = 100, variance = 10000) sem_error_known &lt;- &#39; # Latent &quot;true&quot; variables with single indicators TrueTemp =~ 1*temp TruePrecip =~ 1*precip # Fix error variances based on known measurement error temp ~~ 4*temp precip ~~ 10000*precip # Structural model uses latent (error-corrected) variables species ~ TrueTemp + TruePrecip &#39; fit_error &lt;- sem(sem_error_known, data = error_data) cat(&quot;\\nSEM with known error variances:\\n&quot;) ## ## SEM with known error variances: parameterEstimates(fit_error) %&gt;% filter(op == &quot;~&quot;) %&gt;% select(lhs, rhs, est, se, pvalue) ## lhs rhs est se pvalue ## 1 species TrueTemp 0.530 0.052 0 ## 2 species TruePrecip 0.012 0.001 0 "],["part-5-model-identification-deep-dive.html", "Chapter 221 Part 5: Model Identification Deep Dive 221.1 The identification problem 221.2 Key identification rules 221.3 Checking identification", " Chapter 221 Part 5: Model Identification Deep Dive 221.1 The identification problem A model is identified if there’s a unique solution for all parameters. Under-identified models can’t be estimated. 221.2 Key identification rules T-rule: Number of free parameters ≤ unique elements in covariance matrix # Number of unique covariances for p variables p &lt;- 6 # Number of observed variables n_unique &lt;- p * (p + 1) / 2 cat(&quot;With&quot;, p, &quot;variables, you have&quot;, n_unique, &quot;unique variances/covariances\\n&quot;) ## With 6 variables, you have 21 unique variances/covariances Three-indicator rule: Each latent factor needs ≥3 indicators (with uncorrelated errors) Two-indicator rule: Okay with 2 indicators IF: - Multiple factors exist, OR - You constrain some parameters 221.3 Checking identification # lavaan will warn about identification problems # Let&#39;s try an under-identified model underid_model &lt;- &#39; LatentFactor =~ x1 + x2 # Only 2 indicators &#39; # Simulate simple data underid_data &lt;- data.frame( x1 = rnorm(100), x2 = rnorm(100) ) # This should give a warning or error tryCatch({ underid_fit &lt;- cfa(underid_model, data = underid_data) summary(underid_fit) }, error = function(e) { cat(&quot;Error:&quot;, e$message, &quot;\\n&quot;) }, warning = function(w) { cat(&quot;Warning:&quot;, w$message, &quot;\\n&quot;) }) ## Warning: lavaan-&gt;lav_model_vcov(): ## Could not compute standard errors! The information matrix could not be inverted. This may be a symptom ## that the model is not identified. "],["part-6-advanced-model-assessment.html", "Chapter 222 Part 6: Advanced Model Assessment 222.1 Comparing nested CFA models 222.2 Reliability measures 222.3 Average Variance Extracted (AVE)", " Chapter 222 Part 6: Advanced Model Assessment 222.1 Comparing nested CFA models # One-factor model (all indicators load on one factor) one_factor &lt;- &#39; General =~ road + fire + grazing + food + water + cover &#39; one_fit &lt;- cfa(one_factor, data = multi_factor_data) # Two-factor model (from before) two_fit &lt;- cfa_2fit # Compare anova(one_fit, two_fit) ## ## Chi-Squared Difference Test ## ## Df AIC BIC Chisq Chisq diff RMSEA Df diff Pr(&gt;Chisq) ## two_fit 8 3122.5 3168.3 4.0549 ## one_fit 9 3320.8 3363.1 204.3286 200.27 0.8928 1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 # AIC comparison data.frame( Model = c(&quot;One-factor&quot;, &quot;Two-factor&quot;), AIC = c(AIC(one_fit), AIC(two_fit)), BIC = c(BIC(one_fit), BIC(two_fit)) ) ## Model AIC BIC ## 1 One-factor 3320.800 3363.057 ## 2 Two-factor 3122.526 3168.305 222.2 Reliability measures Composite reliability measures how well indicators capture the latent factor: # Calculate composite reliability manually std_solution &lt;- standardizedSolution(cfa_2fit) # For Disturbance factor dist_loadings &lt;- std_solution %&gt;% filter(lhs == &quot;Disturbance&quot;, op == &quot;=~&quot;) %&gt;% pull(est.std) # Composite reliability = (Σλ)² / [(Σλ)² + Σθ] sum_loadings &lt;- sum(dist_loadings) sum_error_var &lt;- sum(1 - dist_loadings^2) # Error = 1 - λ² comp_reliability &lt;- sum_loadings^2 / (sum_loadings^2 + sum_error_var) cat(&quot;Composite reliability for Disturbance:&quot;, round(comp_reliability, 3), &quot;\\n&quot;) ## Composite reliability for Disturbance: 0.883 # Guideline: &gt; 0.7 is acceptable 222.3 Average Variance Extracted (AVE) # AVE = average variance explained by the factor ave &lt;- mean(dist_loadings^2) cat(&quot;AVE for Disturbance:&quot;, round(ave, 3), &quot;\\n&quot;) ## AVE for Disturbance: 0.718 # Guideline: &gt; 0.5 means the factor explains more variance than error "],["part-7-reporting-latent-variable-models.html", "Chapter 223 Part 7: Reporting Latent Variable Models 223.1 What to report for CFA 223.2 What to report for full SEM 223.3 Sample methods and results 223.4 Key takeaways 223.5 Assignment", " Chapter 223 Part 7: Reporting Latent Variable Models 223.1 What to report for CFA Model specification: Which indicators load on which factors Sample size and estimation method Model fit: χ², df, p, CFI, RMSEA, SRMR Factor loadings: Standardized, with significance Factor correlations (if multiple factors) Reliability: Composite reliability, AVE 223.2 What to report for full SEM All of the above, plus: Structural paths: Between latent and/or observed variables R² for endogenous variables Indirect effects if applicable 223.3 Sample methods and results 223.3.1 Methods We used structural equation modeling to test how latent disturbance pressure and resource availability affect biodiversity outcomes. Disturbance pressure was measured via three indicators: road density (km/km²), fire frequency (fires/decade), and grazing intensity (AUM/ha). Resource availability was measured via food availability index, water proximity, and vegetation cover. We first evaluated the measurement model using confirmatory factor analysis, assessing fit with chi-square test, CFI (&gt;0.95), RMSEA (&lt;0.06), and SRMR (&lt;0.08). Factor loadings &gt;0.5 were considered acceptable. We then fit the full structural model with species richness and total abundance as outcomes. Maximum likelihood estimation was used with robust standard errors (MLR) to account for non-normality. Analyses were conducted using lavaan version 0.6-12 (Rosseel 2012) in R version 4.3.1. 223.3.2 Results The two-factor measurement model showed good fit (χ² = 11.2, df = 8, p = 0.19; CFI = 0.98; RMSEA = 0.04; SRMR = 0.03). All indicators loaded significantly on their hypothesized factors with standardized loadings ranging from 0.65 to 0.82 (Table X). The latent factors were negatively correlated (r = -0.31, p = 0.003), indicating that high-disturbance sites had lower resource availability. Composite reliability exceeded 0.70 for both factors (Disturbance: 0.78; Resources: 0.74). In the structural model, disturbance negatively predicted species richness (β = -0.42, p &lt; 0.001) while resources had a positive effect (β = 0.53, p &lt; 0.001). Together, the latent factors explained 58% of variance in species richness. Total abundance was positively predicted by both resources (β = 0.61, p &lt; 0.001) and richness (β = 0.35, p &lt; 0.001), with a weaker negative effect of disturbance (β = -0.18, p = 0.04). 223.4 Key takeaways Latent variables represent unmeasured constructs — Inferred from multiple indicators CFA tests measurement models — Do indicators reflect hypothesized factors? Need 3+ indicators per factor — For identification (or constraints with fewer) Measurement error can be modeled — Separates true signal from noise Full SEM combines measurement + structure — Both in one framework Report reliability — Composite reliability and AVE Factor loadings &gt; 0.5 — Indicate acceptable indicators 223.5 Assignment 223.5.1 Part 1: Conceptual questions What’s the difference between EFA and CFA? When would you use each? You have two indicators for a latent factor but the model won’t converge. What are your options? Why might using a latent variable (with multiple indicators) give different results than using a single observed variable? 223.5.2 Part 2: Build a CFA Simulate data for a latent “Habitat Quality” factor with three indicators: - Vegetation cover - Canopy height - Understory density Create correlated indicators that reflect an underlying construct Fit a one-factor CFA Report fit indices and factor loadings Calculate composite reliability 223.5.3 Part 3: Two-factor CFA Extend Part 2 to include a second latent factor “Predation Risk” measured by: - Predator abundance - Distance to cover - Visibility Fit the two-factor model Estimate the factor correlation Compare to a one-factor model using LRT and AIC Interpret: Are these distinct constructs or one underlying dimension? 223.5.4 Part 4: Full SEM Add outcome variables (prey abundance, nest success) to your measurement model: Specify a full SEM where Habitat Quality and Predation Risk predict outcomes Fit the model and assess fit Report structural path coefficients Calculate R² for outcome variables Write a complete results paragraph 223.5.5 Part 5: Reflection In 2-3 sentences, discuss when it’s appropriate to use a single observed variable versus constructing a latent variable from multiple indicators. What are the trade-offs? "],["structural-equation-modeling-iii-piecewise-sem.html", "Chapter 224 Structural Equation Modeling III: Piecewise SEM 224.1 When to use piecewise vs covariance-based SEM 224.2 Setup", " Chapter 224 Structural Equation Modeling III: Piecewise SEM Covariance-based SEM (Chapters 23-24) is powerful but demanding: it requires large samples, multivariate normality, and can’t easily handle hierarchical data or non-Gaussian responses. When your ecological data violate these assumptions—as field data often do—piecewise SEM offers a flexible alternative. Piecewise SEM decomposes a full SEM into a series of separate regression models, each estimated independently. This allows you to: Use GLMs for count, binary, or proportion data Include random effects for hierarchical/nested designs Work with smaller sample sizes Handle non-normal distributions The trade-off: piecewise SEM cannot model latent variables or correlated errors as elegantly as covariance-based SEM. 224.1 When to use piecewise vs covariance-based SEM Feature Covariance-Based (lavaan) Piecewise (piecewiseSEM) Data assumptions Multivariate normality Flexible (GLM families) Model structure All paths simultaneously Separate (G)LMs Fit assessment χ², RMSEA, CFI, SRMR D-separation, Fisher’s C Sample size N &gt; 200 recommended Works with smaller N Hierarchical data Difficult Easy (mixed models) Latent variables ✅ Supported ❌ Not supported Non-Gaussian responses Limited ✅ Full GLM support Rule of thumb: - Use lavaan for large samples, latent variables, measurement models - Use piecewiseSEM for ecological field data with mixed models, GLMs, or small samples 224.2 Setup library(tidyverse) library(piecewiseSEM) library(lme4) library(nlme) library(dagitty) library(ggdag) library(emmeans) set.seed(42) "],["part-1-d-separation-and-model-fit.html", "Chapter 225 Part 1: D-Separation and Model Fit 225.1 The logic of piecewise SEM 225.2 What is D-separation? 225.3 Example: Testing D-separation 225.4 Fitting piecewise SEM 225.5 Understanding the output 225.6 Detecting missing paths", " Chapter 225 Part 1: D-Separation and Model Fit 225.1 The logic of piecewise SEM Instead of fitting one big covariance matrix, piecewise SEM: Fits each endogenous variable as a separate regression Tests whether the model’s implied conditional independencies hold Combines these tests into an overall fit statistic (Fisher’s C) 225.2 What is D-separation? D-separation (directed separation) identifies pairs of variables that should be conditionally independent given your model structure. If your model is correct, these “missing paths” should show no significant relationship. ## ## D-SEPARATION LOGIC ## ═══════════════════════════════════════════════════════════════ ## ## Your DAG implies certain conditional independencies: ## ## X → M → Y ## ## This DAG implies: X is independent of Y, given M ## Written: X ⊥ Y | M ## ## D-sep test: Regress Y ~ X + M ## - If X is NOT significant → independence holds → model OK ## - If X IS significant → independence violated → missing path! ## ## The BASIS SET is all implied independencies. ## Fisher&#39;s C combines all d-sep tests into one fit statistic. 225.3 Example: Testing D-separation # A model with a testable independence (full mediation) mediation_dag &lt;- dagify( y ~ m, m ~ x, exposure = &quot;x&quot;, outcome = &quot;y&quot; ) # Implied independencies impliedConditionalIndependencies(mediation_dag) ## x _||_ y | m This says: “y and x should be independent, given m.” If we find a significant direct effect of x on y after controlling for m, our model is wrong. 225.4 Fitting piecewise SEM # Simulate data matching the mediation DAG n &lt;- 100 x &lt;- rnorm(n) m &lt;- 0.6 * x + rnorm(n, 0, 0.8) y &lt;- 0.7 * m + rnorm(n, 0, 0.7) # No direct x effect sim_data &lt;- data.frame(x = x, m = m, y = y) # Fit piecewise SEM psem_model &lt;- psem( lm(m ~ x, data = sim_data), lm(y ~ m, data = sim_data) # Note: x is NOT included ) # Model summary summary(psem_model) ## | | | 0% | |===================================================================================================| 100% ## ## Structural Equation Model of psem_model ## ## Call: ## m ~ x ## y ~ m ## ## AIC ## 444.570 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## y ~ x + ... coef 97 -1.5815 0.117 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 2.546 with P-value = 0.111 and on 1 degrees of freedom ## Fisher&#39;s C = 4.291 with P-value = 0.117 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## m x 0.6217 0.0701 98 8.8651 0 0.6671 *** ## y m 0.6706 0.0740 98 9.0569 0 0.6750 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## m none 0.45 ## y none 0.46 225.5 Understanding the output # D-separation tests (basis set) dSep(psem_model) ## | | | 0% | |===================================================================================================| 100% ## Independ.Claim Test.Type DF Crit.Value P.Value ## 1 y ~ x + ... coef 97 -1.581466 0.1170268 # Fisher&#39;s C statistic fisherC(psem_model) ## Fisher.C df P.Value ## 1 4.291 2 0.117 Interpretation: - Fisher’s C p-value &gt; 0.05: Model is not rejected; implied independencies hold - Fisher’s C p-value &lt; 0.05: Model is rejected; missing paths may be needed 225.6 Detecting missing paths # Simulate data WITH a direct effect y_direct &lt;- 0.7 * m + 0.4 * x + rnorm(n, 0, 0.7) sim_data2 &lt;- data.frame(x = x, m = m, y = y_direct) # Fit the WRONG model (missing the direct path) psem_wrong &lt;- psem( lm(m ~ x, data = sim_data2), lm(y ~ m, data = sim_data2) ) # Check fit - should show poor fit summary(psem_wrong) ## | | | 0% | |===================================================================================================| 100% ## ## Structural Equation Model of psem_wrong ## ## Call: ## m ~ x ## y ~ m ## ## AIC ## 441.522 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## y ~ x + ... coef 97 5.5012 0 *** ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 27.155 with P-value = 0 and on 1 degrees of freedom ## Fisher&#39;s C = 29.996 with P-value = 0 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## m x 0.6217 0.0701 98 8.8651 0 0.6671 *** ## y m 1.0206 0.0729 98 13.9962 0 0.8164 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## m none 0.45 ## y none 0.67 The significant d-sep test (p &lt; 0.05) tells us we’re missing the x → y path! # Fit the CORRECT model (include direct path) psem_correct &lt;- psem( lm(m ~ x, data = sim_data2), lm(y ~ m + x, data = sim_data2) ) summary(psem_correct) ## ## Structural Equation Model of psem_correct ## ## Call: ## m ~ x ## y ~ m + x ## ## AIC ## 416.368 ## ## --- ## Tests of directed separation: ## ## No independence claims present. Tests of directed separation not possible. ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0 with P-value = 1 and on 0 degrees of freedom ## Fisher&#39;s C = NA with P-value = NA and on 0 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## m x 0.6217 0.0701 98 8.8651 0 0.6671 *** ## y m 0.7054 0.0859 97 8.2118 0 0.5643 *** ## y x 0.4404 0.0801 97 5.5012 0 0.3780 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## m none 0.45 ## y none 0.75 "],["part-2-working-with-real-ecological-data.html", "Chapter 226 Part 2: Working with Real Ecological Data 226.1 Example: Post-fire recovery (Keeley data) 226.2 Building the model 226.3 Visualizing the model 226.4 Extracting results", " Chapter 226 Part 2: Working with Real Ecological Data 226.1 Example: Post-fire recovery (Keeley data) # Load keeley data from piecewiseSEM data(keeley) # Examine structure str(keeley) ## &#39;data.frame&#39;: 90 obs. of 8 variables: ## $ distance: num 53.4 37 53.7 53.7 52 ... ## $ elev : int 1225 60 200 200 970 970 950 740 170 190 ... ## $ abiotic : num 60.7 40.9 51 61.2 46.7 ... ## $ age : int 40 25 15 15 23 24 35 14 45 35 ... ## $ hetero : num 0.757 0.491 0.844 0.691 0.546 ... ## $ firesev : num 3.5 4.05 2.6 2.9 4.3 4 4.8 4.8 7.25 6.2 ... ## $ cover : num 1.039 0.478 0.949 1.195 1.298 ... ## $ rich : int 51 31 71 64 68 34 39 66 25 31 ... head(keeley) ## distance elev abiotic age hetero firesev cover rich ## 1 53.40900 1225 60.67103 40 0.757065 3.50 1.0387974 51 ## 2 37.03745 60 40.94291 25 0.491340 4.05 0.4775924 31 ## 3 53.69565 200 50.98805 15 0.844485 2.60 0.9489357 71 ## 4 53.69565 200 61.15633 15 0.690847 2.90 1.1949002 64 ## 5 51.95985 970 46.66807 23 0.545628 4.30 1.2981890 68 ## 6 51.95985 970 39.82357 24 0.652895 4.00 1.1734866 34 226.2 Building the model We hypothesize: 1. Fire severity depends on stand age 2. Plant cover depends on fire severity and age 3. Species richness depends on cover, heterogeneity, and abiotic stress 4. Heterogeneity depends on distance to seed source 5. Abiotic stress depends on elevation # Fit individual models mod_firesev &lt;- lm(firesev ~ age, data = keeley) mod_cover &lt;- lm(cover ~ firesev + age, data = keeley) mod_hetero &lt;- lm(hetero ~ distance, data = keeley) mod_abiotic &lt;- lm(abiotic ~ elev, data = keeley) mod_rich &lt;- lm(rich ~ cover + hetero + abiotic, data = keeley) # Combine into piecewise SEM keeley_psem &lt;- psem( mod_firesev, mod_cover, mod_hetero, mod_abiotic, mod_rich, data = keeley ) # Summary summary(keeley_psem) ## | | | 0% | |====== | 6% | |============ | 12% | |================= | 18% | |======================= | 24% | |============================= | 29% | |=================================== | 35% | |========================================= | 41% | |=============================================== | 47% | |==================================================== | 53% | |========================================================== | 59% | |================================================================ | 65% | |====================================================================== | 71% | |============================================================================ | 76% | |================================================================================== | 82% | |======================================================================================= | 88% | |============================================================================================= | 94% | |===================================================================================================| 100% ## ## Structural Equation Model of keeley_psem ## ## Call: ## firesev ~ age ## cover ~ firesev + age ## hetero ~ distance ## abiotic ~ elev ## rich ~ cover + hetero + abiotic ## ## AIC ## 1532.900 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## hetero ~ age + ... coef 87 0.0043 0.9966 ## abiotic ~ age + ... coef 87 -1.6563 0.1013 ## rich ~ age + ... coef 85 -1.2670 0.2086 ## firesev ~ distance + ... coef 87 -1.6793 0.0967 ## cover ~ distance + ... coef 86 1.0381 0.3021 ## abiotic ~ distance + ... coef 87 3.4557 0.0009 *** ## rich ~ distance + ... coef 85 3.1710 0.0021 ** ## firesev ~ elev + ... coef 87 -1.7071 0.0914 ## cover ~ elev + ... coef 86 2.1381 0.0353 * ## hetero ~ elev + ... coef 87 0.0744 0.9409 ## rich ~ elev + ... coef 85 0.0601 0.9522 ## hetero ~ firesev + ... coef 86 0.4923 0.6237 ## abiotic ~ firesev + ... coef 86 -1.0966 0.2759 ## rich ~ firesev + ... coef 84 -1.3692 0.1746 ## hetero ~ cover + ... coef 85 -2.7889 0.0065 ** ## abiotic ~ cover + ... coef 85 -0.4618 0.6454 ## abiotic ~ hetero + ... coef 86 1.3199 0.1904 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 46.713 with P-value = 0 and on 17 degrees of freedom ## Fisher&#39;s C = 74.204 with P-value = 0 and on 34 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## firesev age 0.0597 0.0125 88 4.7781 0.0000 0.4539 *** ## cover firesev -0.0672 0.0204 87 -3.2965 0.0014 -0.3502 ** ## cover age -0.0048 0.0027 87 -1.8018 0.0750 -0.1914 ## hetero distance 0.0045 0.0013 88 3.4593 0.0008 0.3460 *** ## abiotic elev 0.0097 0.0030 88 3.2549 0.0016 0.3278 ** ## rich cover 17.0138 3.7923 86 4.4864 0.0000 0.3573 *** ## rich hetero 55.3686 10.8212 86 5.1167 0.0000 0.4209 *** ## rich abiotic 0.6847 0.1607 86 4.2609 0.0001 0.3481 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## firesev none 0.21 ## cover none 0.22 ## hetero none 0.12 ## abiotic none 0.11 ## rich none 0.49 226.3 Visualizing the model plot(keeley_psem) Figure 226.1: Path diagram from piecewise SEM showing relationships among fire, vegetation, and diversity variables. 226.4 Extracting results # Standardized coefficients coefs(keeley_psem, standardize = &quot;scale&quot;) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 firesev age 0.0597 0.0125 88 4.7781 0.0000 0.4539 *** ## 2 cover firesev -0.0672 0.0204 87 -3.2965 0.0014 -0.3502 ** ## 3 cover age -0.0048 0.0027 87 -1.8018 0.0750 -0.1914 ## 4 hetero distance 0.0045 0.0013 88 3.4593 0.0008 0.3460 *** ## 5 abiotic elev 0.0097 0.0030 88 3.2549 0.0016 0.3278 ** ## 6 rich cover 17.0138 3.7923 86 4.4864 0.0000 0.3573 *** ## 7 rich hetero 55.3686 10.8212 86 5.1167 0.0000 0.4209 *** ## 8 rich abiotic 0.6847 0.1607 86 4.2609 0.0001 0.3481 *** # R-squared for each endogenous variable rsquared(keeley_psem) ## Response family link method R.squared ## 1 firesev gaussian identity none 0.2059938 ## 2 cover gaussian identity none 0.2201874 ## 3 hetero gaussian identity none 0.1197074 ## 4 abiotic gaussian identity none 0.1074539 ## 5 rich gaussian identity none 0.4868856 "],["part-3-glms-in-piecewise-sem.html", "Chapter 227 Part 3: GLMs in Piecewise SEM 227.1 Why GLMs matter 227.2 Example: Binary response 227.3 Example: Count response 227.4 Standardizing GLM coefficients", " Chapter 227 Part 3: GLMs in Piecewise SEM 227.1 Why GLMs matter Many ecological responses are non-Gaussian: - Counts: Species richness, abundance (Poisson, negative binomial) - Binary: Presence/absence (binomial) - Proportions: Cover, survival (beta, binomial) Piecewise SEM handles these naturally. 227.2 Example: Binary response # Simulate presence/absence data n &lt;- 150 habitat_quality &lt;- rnorm(n) connectivity &lt;- rnorm(n) local_cover &lt;- 0.5 * habitat_quality + rnorm(n, 0, 0.8) # Presence depends on cover and connectivity presence_prob &lt;- plogis(-1 + 0.8 * local_cover + 0.5 * connectivity) presence &lt;- rbinom(n, 1, presence_prob) glm_data &lt;- data.frame( habitat = habitat_quality, connect = connectivity, cover = local_cover, presence = presence ) # Fit piecewise SEM with GLM glm_psem &lt;- psem( lm(cover ~ habitat, data = glm_data), glm(presence ~ cover + connect, family = binomial, data = glm_data), data = glm_data ) summary(glm_psem) ## | | | 0% | |================================================== | 50% | |===================================================================================================| 100% ## ## Structural Equation Model of glm_psem ## ## Call: ## cover ~ habitat ## presence ~ cover + connect ## ## AIC ## 509.165 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## presence ~ habitat + ... coef 146 0.9869 0.3237 ## cover ~ connect + ... coef 147 -0.0007 0.9995 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.982 with P-value = 0.612 and on 2 degrees of freedom ## Fisher&#39;s C = 2.257 with P-value = 0.689 and on 4 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## cover habitat 0.4541 0.0590 148 7.6991 0.0000 0.5348 *** ## presence cover 0.8841 0.2382 147 3.7112 0.0002 0.3908 *** ## presence connect 0.4954 0.2185 147 2.2672 0.0234 0.2374 * ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## cover none 0.29 ## presence nagelkerke 0.19 227.3 Example: Count response # Simulate count data (species richness) n &lt;- 150 productivity &lt;- rnorm(n, 10, 2) disturbance &lt;- rnorm(n) biomass &lt;- 2 + 0.3 * productivity - 0.5 * disturbance + rnorm(n, 0, 1) biomass &lt;- pmax(biomass, 0.1) # Richness as Poisson count richness_lambda &lt;- exp(1 + 0.1 * biomass + 0.05 * productivity) richness &lt;- rpois(n, richness_lambda) count_data &lt;- data.frame( productivity = productivity, disturbance = disturbance, biomass = biomass, richness = richness ) # Piecewise SEM with Poisson GLM count_psem &lt;- psem( lm(biomass ~ productivity + disturbance, data = count_data), glm(richness ~ biomass + productivity, family = poisson, data = count_data), data = count_data ) summary(count_psem) ## | | | 0% | |===================================================================================================| 100% ## ## Structural Equation Model of count_psem ## ## Call: ## biomass ~ productivity + disturbance ## richness ~ biomass + productivity ## ## AIC ## 1111.917 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## richness ~ disturbance + ... coef 146 -0.4638 0.6428 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.215 with P-value = 0.643 and on 1 degrees of freedom ## Fisher&#39;s C = 0.884 with P-value = 0.643 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## biomass productivity 0.2917 0.0335 147 8.6971 0.0000 0.5526 *** ## biomass disturbance -0.3647 0.0763 147 -4.7789 0.0000 -0.3037 *** ## richness biomass 0.1695 0.0304 147 5.5718 0.0000 0.4672 *** ## richness productivity 0.0221 0.0162 147 1.3666 0.1717 0.1156 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## biomass none 0.41 ## richness nagelkerke 0.45 227.4 Standardizing GLM coefficients # Default: uses Latent Theoretic (LT) approach coefs(count_psem, standardize = &quot;scale&quot;) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 biomass productivity 0.2917 0.0335 147 8.6971 0.0000 0.5526 *** ## 2 biomass disturbance -0.3647 0.0763 147 -4.7789 0.0000 -0.3037 *** ## 3 richness biomass 0.1695 0.0304 147 5.5718 0.0000 0.4672 *** ## 4 richness productivity 0.0221 0.0162 147 1.3666 0.1717 0.1156 "],["part-4-mixed-models-in-piecewise-sem.html", "Chapter 228 Part 4: Mixed Models in Piecewise SEM 228.1 Handling hierarchical data 228.2 Fitting mixed models in psem 228.3 Understanding R² for mixed models", " Chapter 228 Part 4: Mixed Models in Piecewise SEM 228.1 Handling hierarchical data Ecological data are often nested: plots within sites, observations within individuals. Mixed models account for this non-independence. # Simulate hierarchical data n_sites &lt;- 15 n_plots &lt;- 6 n_total &lt;- n_sites * n_plots # Site-level random effects site_id &lt;- rep(1:n_sites, each = n_plots) site_effect &lt;- rep(rnorm(n_sites, 0, 1), each = n_plots) # Fixed effects treatment &lt;- rep(c(0, 1), length.out = n_total) soil_moisture &lt;- rnorm(n_total) # Response with site random effect plant_cover &lt;- 30 + 10 * treatment + 5 * soil_moisture + site_effect + rnorm(n_total, 0, 5) mixed_data &lt;- data.frame( site = factor(site_id), treatment = factor(treatment), soil = soil_moisture, cover = plant_cover ) 228.2 Fitting mixed models in psem stopifnot(requireNamespace(&quot;nlme&quot;, quietly = TRUE)) stopifnot(exists(&quot;mixed_data&quot;)) mixed_data$site &lt;- as.factor(mixed_data$site) m1 &lt;- nlme::lme( fixed = cover ~ treatment + soil, random = ~ 1 | site, data = mixed_data, na.action = na.exclude ) cat(&quot;\\n=== Model overview ===\\n&quot;) ## ## === Model overview === print(m1) ## Linear mixed-effects model fit by REML ## Data: mixed_data ## Log-restricted-likelihood: -270.9223 ## Fixed: cover ~ treatment + soil ## (Intercept) treatment1 soil ## 31.235422 8.774527 5.409938 ## ## Random effects: ## Formula: ~1 | site ## (Intercept) Residual ## StdDev: 1.445597 4.913832 ## ## Number of Observations: 90 ## Number of Groups: 15 cat(&quot;\\n=== Path coefficients (fixed effects) ===\\n&quot;) ## ## === Path coefficients (fixed effects) === sm &lt;- summary(m1) print(sm$tTable) ## Value Std.Error DF t-value p-value ## (Intercept) 31.235422 0.8226699 73 37.968352 8.122294e-50 ## treatment1 8.774527 1.0360402 73 8.469292 1.847257e-12 ## soil 5.409938 0.5275254 73 10.255311 8.610437e-16 228.3 Understanding R² for mixed models # R-squared decomposition rsquared(mixed_psem) ## Response family link method Marginal Conditional ## 1 cover gaussian identity none 0.6602457 0.6873083 Interpretation: - Marginal R²: Variance explained by fixed effects only - Conditional R²: Variance explained by fixed + random effects "],["part-5-nonlinearity-and-interactions.html", "Chapter 229 Part 5: Nonlinearity and Interactions 229.1 Polynomial terms 229.2 Interaction terms", " Chapter 229 Part 5: Nonlinearity and Interactions 229.1 Polynomial terms Many ecological relationships are nonlinear: # Simulate unimodal response to nitrogen n &lt;- 100 nitrogen &lt;- runif(n, 0, 10) productivity &lt;- 5 + 3 * nitrogen - 0.3 * nitrogen^2 + rnorm(n, 0, 1) herbivory &lt;- 0.5 * productivity + rnorm(n, 0, 0.5) nonlin_data &lt;- data.frame( N = nitrogen, prod = productivity, herb = herbivory ) # Center to reduce collinearity nonlin_data$N_c &lt;- scale(nonlin_data$N, scale = FALSE)[,1] nonlin_data$N2_c &lt;- nonlin_data$N_c^2 # Check correlation before and after centering cat(&quot;Correlation N vs N²:&quot;, round(cor(nonlin_data$N, nonlin_data$N^2), 3), &quot;\\n&quot;) ## Correlation N vs N²: 0.969 cat(&quot;Correlation N_c vs N²_c:&quot;, round(cor(nonlin_data$N_c, nonlin_data$N2_c), 3), &quot;\\n&quot;) ## Correlation N_c vs N²_c: -0.183 # Fit nonlinear piecewise SEM nonlin_psem &lt;- psem( lm(prod ~ N_c + N2_c, data = nonlin_data), lm(herb ~ prod, data = nonlin_data), N_c %~~% N2_c, data = nonlin_data ) summary(nonlin_psem) ## | | | 0% | |================================================== | 50% | |===================================================================================================| 100% ## ## Structural Equation Model of nonlin_psem ## ## Call: ## prod ~ N_c + N2_c ## herb ~ prod ## N_c ~~ N2_c ## ## AIC ## 432.958 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## herb ~ N_c + ... coef 97 -0.4932 0.6230 ## herb ~ N2_c + ... coef 97 0.4243 0.6723 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0.26 with P-value = 0.878 and on 2 degrees of freedom ## Fisher&#39;s C = 1.741 with P-value = 0.783 and on 4 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## prod N_c -0.3932 0.0367 97 -10.7040 0.0000 -0.4147 *** ## prod N2_c -0.3047 0.013 97 -23.4503 0.0000 -0.9084 *** ## herb prod 0.5079 0.0189 98 26.9320 0.0000 0.9386 *** ## ~~N_c ~~N2_c -0.1831 - 98 -1.8434 0.0683 -0.1831 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## prod none 0.86 ## herb none 0.88 229.2 Interaction terms # Center predictors for interaction keeley$age_c &lt;- scale(keeley$age, scale = FALSE)[,1] keeley$firesev_c &lt;- scale(keeley$firesev, scale = FALSE)[,1] # Piecewise SEM with interaction int_psem &lt;- psem( lm(firesev_c ~ age_c, data = keeley), lm(cover ~ firesev_c * age_c, data = keeley), data = keeley ) summary(int_psem) ## ## Structural Equation Model of int_psem ## ## Call: ## firesev_c ~ age_c ## cover ~ firesev_c * age_c ## ## AIC ## 362.993 ## ## --- ## Tests of directed separation: ## ## No independence claims present. Tests of directed separation not possible. ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 0 with P-value = 1 and on 0 degrees of freedom ## Fisher&#39;s C = NA with P-value = NA and on 0 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## firesev_c age_c 0.0597 0.0125 88 4.7781 0.0000 0.4539 *** ## cover firesev_c -0.0684 0.0203 86 -3.3752 0.0011 -0.3561 ** ## cover age_c -0.0050 0.0027 86 -1.8810 0.0634 -0.1985 ## cover firesev_c:age_c -0.0021 0.0014 86 -1.5263 0.1306 -0.1438 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## firesev_c none 0.21 ## cover none 0.24 "],["part-6-categorical-predictors.html", "Chapter 230 Part 6: Categorical Predictors 230.1 Treatment effects in SEM 230.2 Post-hoc comparisons", " Chapter 230 Part 6: Categorical Predictors 230.1 Treatment effects in SEM # Simulate experiment with 4 treatment levels n &lt;- 120 treatment &lt;- factor(rep(c(&quot;Control&quot;, &quot;Low&quot;, &quot;Medium&quot;, &quot;High&quot;), each = 30)) # Effects trt_effects &lt;- c(Control = 0, Low = 2, Medium = 5, High = 8) plant_biomass &lt;- 20 + trt_effects[treatment] + rnorm(n, 0, 3) insect_abundance &lt;- 10 + 0.5 * plant_biomass + rnorm(n, 0, 2) cat_data &lt;- data.frame( treatment = treatment, biomass = plant_biomass, insects = insect_abundance ) # Fit piecewise SEM with categorical predictor cat_psem &lt;- psem( lm(biomass ~ treatment, data = cat_data), lm(insects ~ biomass, data = cat_data), data = cat_data ) summary(cat_psem) ## | | | 0% | |===================================================================================================| 100% ## ## Structural Equation Model of cat_psem ## ## Call: ## biomass ~ treatment ## insects ~ biomass ## ## AIC ## 1100.441 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## insects ~ treatment + ... anova 3 2.2819 0.0829 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 6.939 with P-value = 0.074 and on 3 degrees of freedom ## Fisher&#39;s C = 4.981 with P-value = 0.083 and on 2 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## biomass treatment - - 3 57.1594 0 - *** ## biomass treatment = Control 19.1408 0.5076 116 37.7107 0 - *** ## biomass treatment = High 21.5063 0.5076 116 42.3713 0 - *** ## biomass treatment = Low 23.9045 0.5076 116 47.0961 0 - *** ## biomass treatment = Medium 28.1334 0.5076 116 55.4278 0 - *** ## insects biomass 0.4754 0.042 118 11.3129 0 0.7213 *** ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## biomass none 0.60 ## insects none 0.52 230.2 Post-hoc comparisons # Extract the biomass model biomass_mod &lt;- cat_psem[[1]] # Estimated marginal means emmeans(biomass_mod, specs = ~ treatment) ## treatment emmean SE df lower.CL upper.CL ## Control 19.1 0.508 116 18.1 20.1 ## High 21.5 0.508 116 20.5 22.5 ## Low 23.9 0.508 116 22.9 24.9 ## Medium 28.1 0.508 116 27.1 29.1 ## ## Confidence level used: 0.95 # Pairwise comparisons pairs(emmeans(biomass_mod, specs = ~ treatment)) ## contrast estimate SE df t.ratio p.value ## Control - High -2.37 0.718 116 -3.296 0.0070 ## Control - Low -4.76 0.718 116 -6.636 &lt;0.0001 ## Control - Medium -8.99 0.718 116 -12.528 &lt;0.0001 ## High - Low -2.40 0.718 116 -3.341 0.0061 ## High - Medium -6.63 0.718 116 -9.232 &lt;0.0001 ## Low - Medium -4.23 0.718 116 -5.891 &lt;0.0001 ## ## P value adjustment: tukey method for comparing a family of 4 estimates "],["part-7-complete-ecological-example.html", "Chapter 231 Part 7: Complete Ecological Example 231.1 Case study: Vegetation management and pollinators 231.2 Build and fit the SEM 231.3 Extract key results", " Chapter 231 Part 7: Complete Ecological Example 231.1 Case study: Vegetation management and pollinators # Simulate IVM dataset set.seed(123) n &lt;- 80 # Experimental design treatment &lt;- factor(rep(c(&quot;Control&quot;, &quot;Herbicide&quot;, &quot;Mechanical&quot;, &quot;Combined&quot;), each = 20)) soil_type &lt;- factor(sample(c(&quot;Sandy&quot;, &quot;Clay&quot;, &quot;Loam&quot;), n, replace = TRUE)) cattle_present &lt;- rbinom(n, 1, 0.3) # Treatment effects on plant community trt_rich &lt;- c(Control = 0, Herbicide = -3, Mechanical = -2, Combined = -4) trt_cover &lt;- c(Control = 0, Herbicide = -15, Mechanical = -10, Combined = -20) plant_richness &lt;- 25 + trt_rich[treatment] + ifelse(soil_type == &quot;Loam&quot;, 3, 0) + -2 * cattle_present + rnorm(n, 0, 3) plant_cover &lt;- 70 + trt_cover[treatment] + ifelse(soil_type == &quot;Clay&quot;, -5, 0) + -5 * cattle_present + rnorm(n, 0, 8) plant_height &lt;- 40 + 0.3 * plant_cover + rnorm(n, 0, 5) # Pollinator responses poll_rich &lt;- rpois(n, exp(1.5 + 0.02 * plant_richness + 0.01 * plant_cover)) poll_abund &lt;- rpois(n, exp(2 + 0.015 * plant_cover + 0.01 * plant_height)) ivm_data &lt;- data.frame( treatment = treatment, soil = soil_type, cattle = cattle_present, plant_rich = plant_richness, plant_cover = plant_cover, plant_height = plant_height, poll_rich = poll_rich, poll_abund = poll_abund ) 231.2 Build and fit the SEM # Define the DAG ivm_dag &lt;- dagify( plant_rich ~ treatment + soil + cattle, plant_cover ~ treatment + soil + cattle, plant_height ~ plant_cover + treatment, poll_rich ~ plant_rich + plant_cover, poll_abund ~ plant_cover + plant_height, exposure = &quot;treatment&quot;, outcome = &quot;poll_abund&quot; ) ggdag(ivm_dag, text = TRUE) + theme_dag() + labs(title = &quot;IVM Effects on Pollinators&quot;) Figure 231.1: Hypothesized DAG for pollinator response to vegetation management. # Fit piecewise SEM ivm_psem &lt;- psem( lm(plant_rich ~ treatment + soil + cattle, data = ivm_data), lm(plant_cover ~ treatment + soil + cattle, data = ivm_data), lm(plant_height ~ plant_cover + treatment, data = ivm_data), glm(poll_rich ~ plant_rich + plant_cover, family = poisson, data = ivm_data), glm(poll_abund ~ plant_cover + plant_height, family = poisson, data = ivm_data), data = ivm_data ) summary(ivm_psem) ## | | | 0% | |======== | 8% | |=============== | 15% | |======================= | 23% | |============================== | 31% | |====================================== | 38% | |============================================== | 46% | |===================================================== | 54% | |============================================================= | 62% | |===================================================================== | 69% | |============================================================================ | 77% | |==================================================================================== | 85% | |=========================================================================================== | 92% | |===================================================================================================| 100% ## ## Structural Equation Model of ivm_psem ## ## Call: ## plant_rich ~ treatment + soil + cattle ## plant_cover ~ treatment + soil + cattle ## plant_height ~ plant_cover + treatment ## poll_rich ~ plant_rich + plant_cover ## poll_abund ~ plant_cover + plant_height ## ## AIC ## 2420.998 ## ## --- ## Tests of directed separation: ## ## Independ.Claim Test.Type DF Crit.Value P.Value ## poll_rich ~ treatment + ... anova 3 0.6916 0.5600 ## poll_abund ~ treatment + ... anova 3 1.5357 0.2123 ## plant_height ~ soil + ... anova 2 0.1514 0.8597 ## poll_rich ~ soil + ... anova 2 0.1324 0.8762 ## poll_abund ~ soil + ... anova 2 0.1577 0.8544 ## plant_height ~ cattle + ... coef 74 -0.8903 0.3762 ## poll_rich ~ cattle + ... coef 76 -0.7225 0.4700 ## poll_abund ~ cattle + ... coef 76 -2.3418 0.0192 * ## plant_cover ~ plant_rich + ... coef 72 -0.3159 0.7530 ## plant_height ~ plant_rich + ... coef 71 -0.2568 0.7981 ## poll_abund ~ plant_rich + ... coef 70 -0.2093 0.8342 ## poll_rich ~ plant_height + ... coef 73 0.4974 0.6189 ## poll_abund ~ poll_rich + ... coef 75 0.6476 0.5172 ## ## -- ## Global goodness-of-fit: ## ## Chi-Squared = 16.188 with P-value = 0.705 and on 20 degrees of freedom ## Fisher&#39;s C = 20.171 with P-value = 0.783 and on 26 degrees of freedom ## ## --- ## Coefficients: ## ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## plant_rich cattle -2.8999 0.7651 73 -3.7901 0.0003 - *** ## plant_rich treatment - - 3 5.5183 0.0018 - ** ## plant_rich treatment = Mechanical 20.5757 0.6773 73 30.3797 0.0000 - *** ## plant_rich treatment = Control 22.3005 0.6665 73 33.4606 0.0000 - *** ## plant_rich treatment = Combined 23.6663 0.7004 73 33.7873 0.0000 - *** ## plant_rich treatment = Herbicide 23.8744 0.6754 73 35.3465 0.0000 - *** ## plant_rich soil - - 2 7.5724 0.0010 - ** ## plant_rich soil = Clay 21.3071 0.6128 73 34.7705 0.0000 - *** ## plant_rich soil = Sandy 22.1803 0.5924 73 37.4438 0.0000 - *** ## plant_rich soil = Loam 24.3253 0.5877 73 41.3936 0.0000 - *** ## plant_cover cattle -6.5066 2.4176 73 -2.6913 0.0088 - ** ## plant_cover treatment - - 3 18.0191 0.0000 - *** ## plant_cover treatment = Mechanical 45.015 2.14 73 21.0348 0.0000 - *** ## plant_cover treatment = Control 51.5211 2.1059 73 24.4656 0.0000 - *** ## plant_cover treatment = Herbicide 55.7958 2.1342 73 26.1438 0.0000 - *** ## plant_cover treatment = Combined 65.8636 2.2132 73 29.7592 0.0000 - *** ## plant_cover soil - - 2 5.6800 0.0051 - ** ## plant_cover soil = Clay 49.5813 1.9362 73 25.6069 0.0000 - *** ## plant_cover soil = Sandy 56.6312 1.8717 73 30.2567 0.0000 - *** ## plant_cover soil = Loam 57.4341 1.8568 73 30.9313 0.0000 - *** ## plant_height plant_cover 0.3149 0.0583 75 5.4045 0.0000 - *** ## plant_height treatment - - 3 1.0393 0.3802 - ## plant_height treatment = Control 55.2303 1.1578 75 47.7024 0.0000 - *** ## plant_height treatment = Mechanical 56.4677 1.2582 75 44.8811 0.0000 - *** ## plant_height treatment = Combined 57.6786 1.313 75 43.9282 0.0000 - *** ## plant_height treatment = Herbicide 57.9807 1.142 75 50.7705 0.0000 - *** ## poll_rich plant_rich 0.0073 0.0094 77 0.7844 0.4328 0.0816 ## poll_rich plant_cover 0.0122 0.0026 77 4.6474 0.0000 0.4729 *** ## poll_abund plant_cover 0.0118 0.0021 77 5.6020 0.0000 0.6108 *** ## poll_abund plant_height 0.0074 0.004 77 1.8439 0.0652 0.2058 ## ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 ## ## --- ## Individual R-squared: ## ## Response method R.squared ## plant_rich none 0.39 ## plant_cover none 0.50 ## plant_height none 0.45 ## poll_rich nagelkerke 0.39 ## poll_abund nagelkerke 0.76 # Clean coefficient table ctab_clean &lt;- ctab %&gt;% dplyr::mutate( Estimate_num = suppressWarnings(as.numeric(Estimate)), P_num = suppressWarnings(as.numeric(P.Value)) ) %&gt;% dplyr::filter(!is.na(Estimate_num)) # significance stars sigstars &lt;- function(p){ ifelse(p &lt; 0.001, &quot;***&quot;, ifelse(p &lt; 0.01, &quot;**&quot;, ifelse(p &lt; 0.05, &quot;*&quot;, &quot;&quot;))) } edges &lt;- transform( ctab_clean, label = sprintf(&quot;%.2f%s&quot;, Estimate_num, sigstars(P_num)) ) dot &lt;- paste0( &quot;digraph psem {\\n&quot;, &quot;rankdir=LR;\\n&quot;, &quot;node [shape=box, style=rounded];\\n&quot;, paste0( &quot; \\&quot;&quot;, edges$Predictor, &quot;\\&quot; -&gt; \\&quot;&quot;, edges$Response, &quot;\\&quot; [label=\\&quot;&quot;, edges$label, &quot;\\&quot;];\\n&quot;, collapse = &quot;&quot; ), &quot;}\\n&quot; ) DiagrammeR::grViz(dot) 231.3 Extract key results # Standardized coefficients coefs(ivm_psem, standardize = &quot;scale&quot;) ## Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate ## 1 plant_rich cattle -2.8999 0.7651 73 -3.7901 0.0003 - *** ## 2 plant_rich treatment - - 3 5.5183 0.0018 - ** ## 3 plant_rich treatment = Mechanical 20.5757 0.6773 73 30.3797 0.0000 - *** ## 4 plant_rich treatment = Control 22.3005 0.6665 73 33.4606 0.0000 - *** ## 5 plant_rich treatment = Combined 23.6663 0.7004 73 33.7873 0.0000 - *** ## 6 plant_rich treatment = Herbicide 23.8744 0.6754 73 35.3465 0.0000 - *** ## 7 plant_rich soil - - 2 7.5724 0.0010 - ** ## 8 plant_rich soil = Clay 21.3071 0.6128 73 34.7705 0.0000 - *** ## 9 plant_rich soil = Sandy 22.1803 0.5924 73 37.4438 0.0000 - *** ## 10 plant_rich soil = Loam 24.3253 0.5877 73 41.3936 0.0000 - *** ## 11 plant_cover cattle -6.5066 2.4176 73 -2.6913 0.0088 - ** ## 12 plant_cover treatment - - 3 18.0191 0.0000 - *** ## 13 plant_cover treatment = Mechanical 45.015 2.14 73 21.0348 0.0000 - *** ## 14 plant_cover treatment = Control 51.5211 2.1059 73 24.4656 0.0000 - *** ## 15 plant_cover treatment = Herbicide 55.7958 2.1342 73 26.1438 0.0000 - *** ## 16 plant_cover treatment = Combined 65.8636 2.2132 73 29.7592 0.0000 - *** ## 17 plant_cover soil - - 2 5.6800 0.0051 - ** ## 18 plant_cover soil = Clay 49.5813 1.9362 73 25.6069 0.0000 - *** ## 19 plant_cover soil = Sandy 56.6312 1.8717 73 30.2567 0.0000 - *** ## 20 plant_cover soil = Loam 57.4341 1.8568 73 30.9313 0.0000 - *** ## 21 plant_height plant_cover 0.3149 0.0583 75 5.4045 0.0000 - *** ## 22 plant_height treatment - - 3 1.0393 0.3802 - ## 23 plant_height treatment = Control 55.2303 1.1578 75 47.7024 0.0000 - *** ## 24 plant_height treatment = Mechanical 56.4677 1.2582 75 44.8811 0.0000 - *** ## 25 plant_height treatment = Combined 57.6786 1.313 75 43.9282 0.0000 - *** ## 26 plant_height treatment = Herbicide 57.9807 1.142 75 50.7705 0.0000 - *** ## 27 poll_rich plant_rich 0.0073 0.0094 77 0.7844 0.4328 0.0816 ## 28 poll_rich plant_cover 0.0122 0.0026 77 4.6474 0.0000 0.4729 *** ## 29 poll_abund plant_cover 0.0118 0.0021 77 5.6020 0.0000 0.6108 *** ## 30 poll_abund plant_height 0.0074 0.004 77 1.8439 0.0652 0.2058 # R-squared values rsquared(ivm_psem) ## Response family link method R.squared ## 1 plant_rich gaussian identity none 0.3927781 ## 2 plant_cover gaussian identity none 0.5012544 ## 3 plant_height gaussian identity none 0.4454089 ## 4 poll_rich poisson log nagelkerke 0.3917529 ## 5 poll_abund poisson log nagelkerke 0.7645490 "],["part-8-reporting-piecewise-sem.html", "Chapter 232 Part 8: Reporting Piecewise SEM 232.1 What to report 232.2 Sample methods and results 232.3 Key takeaways 232.4 Assignment", " Chapter 232 Part 8: Reporting Piecewise SEM 232.1 What to report Model specification: Path diagram or equations Sample size and data structure Model types: lm, glm, lme for each equation D-separation tests: Basis set results Fisher’s C: Statistic, df, p-value Path coefficients: Standardized with p-values R²: For each endogenous variable 232.2 Sample methods and results 232.2.1 Methods We used piecewise structural equation modeling to test hypothesized pathways linking vegetation management treatments to pollinator communities through plant community mediators. The model structure was specified a priori based on ecological theory (Fig. X). Plant community variables (richness, cover, height) were modeled with linear regression including treatment, soil type, and cattle presence as predictors. Pollinator richness and abundance were modeled with Poisson GLMs given their count distribution. Model fit was assessed using Shipley’s test of directed separation (d-sep), with Fisher’s C statistic used to evaluate overall model fit. Standardized path coefficients were calculated using the latent-theoretic approach. We considered Fisher’s C p &gt; 0.05 as indicating acceptable model fit. Analyses were conducted using piecewiseSEM version 2.3.0 (Lefcheck 2016) in R version 4.3.1. 232.2.2 Results The piecewise SEM showed good fit to the data (Fisher’s C = 8.4, df = 6, p = 0.21), indicating that the hypothesized path structure adequately captured the relationships among variables. Vegetation management significantly affected plant community composition: combined mechanical + herbicide treatment reduced plant species richness by 4.2 species (β = -0.38, p &lt; 0.001) and cover by 18% (β = -0.42, p &lt; 0.001) compared to controls (Table X). These plant community changes propagated to pollinators: plant richness positively predicted pollinator richness (β = 0.35, p = 0.002), while plant cover positively predicted pollinator abundance (β = 0.41, p &lt; 0.001). The model explained 45% of variance in pollinator richness and 52% in pollinator abundance. Post-hoc comparisons revealed that all active treatments significantly reduced plant cover relative to controls, with combined treatment showing the strongest effects (Tukey HSD, all p &lt; 0.01). 232.3 Key takeaways Piecewise SEM = flexibility — GLMs, mixed models, small samples D-separation tests missing paths — Fisher’s C combines into overall fit p &gt; 0.05 means good fit — Model structure is supported Use GLMs for non-normal data — Counts, binary, proportions Mixed models for hierarchical data — Random effects handled naturally Center polynomials and interactions — Reduces collinearity No latent variables — Use lavaan if you need them 232.4 Assignment 232.4.1 Part 1: Conceptual questions What does Fisher’s C test? How is it different from the chi-square test in covariance-based SEM? You fit a piecewise SEM and get Fisher’s C = 15.2, df = 4, p = 0.004. What does this tell you? When would you choose piecewise SEM over lavaan? Give three specific scenarios. 232.4.2 Part 2: Basic piecewise SEM Using the keeley data: Propose a different path structure than the one in this chapter Fit the model with psem() Check d-sep tests and Fisher’s C Extract standardized coefficients Plot the path diagram 232.4.3 Part 3: GLM in piecewise SEM Using simulated or real data with a count response: Create a piecewise SEM with at least one Poisson GLM Check model fit Interpret the standardized coefficients Discuss how interpretation differs from linear models 232.4.4 Part 4: Mixed model SEM Using hierarchical data (simulated or real): Fit a piecewise SEM with random effects using lme() Compare marginal vs conditional R² Discuss what the random effects capture 232.4.5 Part 5: Reporting Write a complete methods and results paragraph for your model from Part 2, following the format in this chapter. "],["causal-inference-and-directed-acyclic-graphs.html", "Chapter 233 Causal Inference and Directed Acyclic Graphs 233.1 Why causal inference matters in ecology 233.2 Setup", " Chapter 233 Causal Inference and Directed Acyclic Graphs Correlation is not causation. You’ve heard this a thousand times. But then what is causation? And how do we move from observing patterns to making causal claims? This chapter introduces the modern framework for causal inference based on Directed Acyclic Graphs (DAGs). DAGs provide a visual and mathematical language for: Stating causal assumptions explicitly Identifying confounders that bias estimates Determining which variables to control for (and which NOT to) Understanding when causal inference is possible from observational data This foundation is essential for structural equation modeling (Chapters 23-25) and for designing studies that can answer causal questions. 233.1 Why causal inference matters in ecology Ecologists constantly ask causal questions: Does nitrogen addition cause increased productivity? Does fragmentation cause species loss? Does warming cause earlier phenology? But most ecological data are observational. We can’t randomly assign temperatures to ecosystems or fragmentation levels to landscapes. Without experiments, how do we make causal claims? The answer: We need to understand the structure of causal relationships—what causes what, what confounds what—before we can extract causal information from data. 233.2 Setup library(tidyverse) library(dagitty) # DAG specification and analysis library(ggdag) # DAG visualization library(ggplot2) set.seed(42) "],["part-1-from-correlation-to-causation.html", "Chapter 234 Part 1: From Correlation to Causation 234.1 The ladder of causation 234.2 Why correlation ≠ causation", " Chapter 234 Part 1: From Correlation to Causation 234.1 The ladder of causation Judea Pearl describes three levels of causal reasoning: ## ## THE LADDER OF CAUSATION (Pearl) ## ═══════════════════════════════════════════════════════════════ ## ## RUNG 1: ASSOCIATION (Seeing) ## Question: &#39;What is?&#39; ## Example: &#39;Do plants with more nitrogen have higher biomass?&#39; ## Method: Observe correlations in data ## ## RUNG 2: INTERVENTION (Doing) ## Question: &#39;What if I do?&#39; ## Example: &#39;If I ADD nitrogen, will biomass increase?&#39; ## Method: Experiments or causal inference from observational data ## ## RUNG 3: COUNTERFACTUAL (Imagining) ## Question: &#39;What if I had done differently?&#39; ## Example: &#39;Would this plant have survived if I hadn&#39;t added nitrogen?&#39; ## Method: Requires causal model + individual-level reasoning ## ## Most statistics operates at Rung 1. ## Causal inference aims for Rung 2. ## DAGs help us climb the ladder. 234.2 Why correlation ≠ causation Three reasons a correlation between X and Y might exist: X causes Y (direct causation) Y causes X (reverse causation) Z causes both X and Y (confounding) # Create three DAGs showing different causal structures # 1. X causes Y dag1 &lt;- dagify(Y ~ X) # 2. Y causes X dag2 &lt;- dagify(X ~ Y) # 3. Confounding dag3 &lt;- dagify(X ~ Z, Y ~ Z) p1 &lt;- ggdag(dag1) + theme_dag() + labs(title = &quot;X causes Y&quot;) p2 &lt;- ggdag(dag2) + theme_dag() + labs(title = &quot;Y causes X&quot;) p3 &lt;- ggdag(dag3) + theme_dag() + labs(title = &quot;Z confounds X and Y&quot;) library(gridExtra) grid.arrange(p1, p2, p3, ncol = 3) Figure 234.1: Three reasons X and Y might be correlated, only one of which represents X causing Y. The challenge: Data alone cannot distinguish these scenarios. We need causal assumptions about the system. "],["part-2-what-is-a-dag.html", "Chapter 235 Part 2: What is a DAG? 235.1 Directed Acyclic Graphs 235.2 Reading a DAG 235.3 Creating DAGs in R", " Chapter 235 Part 2: What is a DAG? 235.1 Directed Acyclic Graphs A DAG is a diagram representing causal relationships: Nodes: Variables (measured or unmeasured) Arrows: Direct causal effects (cause → effect) Directed: Arrows have direction (no bidirectional arrows) Acyclic: No feedback loops (can’t follow arrows back to start) # Simple ecological DAG eco_dag &lt;- dagify( Growth ~ Metabolism + Activity, Metabolism ~ Temperature, Activity ~ Temperature, labels = c( Growth = &quot;Growth&quot;, Metabolism = &quot;Metabolism&quot;, Activity = &quot;Activity&quot;, Temperature = &quot;Temperature&quot; ) ) ggdag(eco_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Temperature Effects on Growth&quot;) Figure 235.1: A simple DAG showing that temperature affects both metabolism and activity, which both affect growth. 235.2 Reading a DAG ## ## DAG TERMINOLOGY ## ═══════════════════════════════════════════════════════════════ ## ## PARENT: A variable with an arrow pointing TO another variable ## Temperature is a parent of Metabolism ## ## CHILD: A variable with an arrow pointing FROM another variable ## Metabolism is a child of Temperature ## ## ANCESTOR: Any variable you can reach by following arrows backward ## Temperature is an ancestor of Growth ## ## DESCENDANT: Any variable you can reach by following arrows forward ## Growth is a descendant of Temperature ## ## PATH: Any route connecting two variables (ignoring arrow direction) ## Temperature → Metabolism → Growth is a path ## ## DIRECTED PATH: A path following arrow directions ## Temperature → Metabolism → Growth is directed ## ## BACKDOOR PATH: A path that enters a variable through an incoming arrow ## (We&#39;ll return to this crucial concept) 235.3 Creating DAGs in R # Using dagitty syntax simple_dag &lt;- dagify( # Each line specifies: outcome ~ causes Y ~ X + Z, X ~ Z, # Specify which variable is exposure and outcome exposure = &quot;X&quot;, outcome = &quot;Y&quot; ) # View the DAG structure print(simple_dag) ## dag { ## X [exposure] ## Y [outcome] ## Z ## X -&gt; Y ## Z -&gt; X ## Z -&gt; Y ## } # What are the paths between X and Y? paths(simple_dag, from = &quot;X&quot;, to = &quot;Y&quot;) ## $paths ## [1] &quot;X -&gt; Y&quot; &quot;X &lt;- Z -&gt; Y&quot; ## ## $open ## [1] TRUE TRUE "],["part-3-the-three-fundamental-structures.html", "Chapter 236 Part 3: The Three Fundamental Structures 236.1 Structure 1: Chains (Mediation) 236.2 Structure 2: Forks (Confounding) 236.3 Structure 3: Colliders 236.4 Summary: The three structures", " Chapter 236 Part 3: The Three Fundamental Structures Every DAG, no matter how complex, is built from three basic structures. Understanding these is the key to causal inference. 236.1 Structure 1: Chains (Mediation) A → B → C B mediates the effect of A on C. chain_dag &lt;- dagify( SoilMoisture ~ CanopyCover, CanopyCover ~ Fire, labels = c( Fire = &quot;Fire&quot;, CanopyCover = &quot;Canopy\\nCover&quot;, SoilMoisture = &quot;Soil\\nMoisture&quot; ), exposure = &quot;Fire&quot;, outcome = &quot;SoilMoisture&quot; ) ggdag(chain_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Chain: Mediation&quot;) Figure 236.1: Chain structure: Fire affects soil moisture through its effect on canopy cover. Key property: A and C are dependent (correlated) marginally, but independent conditional on B. # Demonstrate chain independence n &lt;- 1000 Fire &lt;- rnorm(n) CanopyCover &lt;- -0.8 * Fire + rnorm(n, 0, 0.5) SoilMoisture &lt;- 0.6 * CanopyCover + rnorm(n, 0, 0.5) chain_data &lt;- data.frame(Fire, CanopyCover, SoilMoisture) # Marginal correlation: Fire and SoilMoisture cat(&quot;Correlation Fire ~ SoilMoisture (marginal):&quot;, round(cor(Fire, SoilMoisture), 3), &quot;\\n&quot;) ## Correlation Fire ~ SoilMoisture (marginal): -0.653 # Partial correlation: Fire and SoilMoisture | CanopyCover residual_fire &lt;- residuals(lm(Fire ~ CanopyCover, data = chain_data)) residual_soil &lt;- residuals(lm(SoilMoisture ~ CanopyCover, data = chain_data)) cat(&quot;Correlation Fire ~ SoilMoisture | CanopyCover:&quot;, round(cor(residual_fire, residual_soil), 3), &quot;\\n&quot;) ## Correlation Fire ~ SoilMoisture | CanopyCover: 0.012 Implication: If you control for the mediator (CanopyCover), you block the causal path and can’t estimate the total effect of Fire on SoilMoisture. 236.2 Structure 2: Forks (Confounding) A ← B → C B is a common cause (confounder) of both A and C. fork_dag &lt;- dagify( InvasiveCover ~ SoilType, NativeRichness ~ SoilType, labels = c( SoilType = &quot;Soil\\nType&quot;, InvasiveCover = &quot;Invasive\\nCover&quot;, NativeRichness = &quot;Native\\nRichness&quot; ), exposure = &quot;InvasiveCover&quot;, outcome = &quot;NativeRichness&quot; ) ggdag(fork_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Fork: Confounding&quot;) Figure 236.2: Fork structure: Soil type confounds the relationship between invasive cover and native richness. Key property: A and C are dependent marginally (spuriously!), but independent conditional on B. # Demonstrate fork dependence SoilType &lt;- rnorm(n) # Continuous for simplicity InvasiveCover &lt;- 0.7 * SoilType + rnorm(n, 0, 0.5) NativeRichness &lt;- -0.5 * SoilType + rnorm(n, 0, 0.5) fork_data &lt;- data.frame(SoilType, InvasiveCover, NativeRichness) # Marginal correlation: spurious! cat(&quot;Correlation Invasive ~ Native (marginal):&quot;, round(cor(InvasiveCover, NativeRichness), 3), &quot;\\n&quot;) ## Correlation Invasive ~ Native (marginal): -0.589 # Partial correlation: controlling for confounder residual_inv &lt;- residuals(lm(InvasiveCover ~ SoilType, data = fork_data)) residual_nat &lt;- residuals(lm(NativeRichness ~ SoilType, data = fork_data)) cat(&quot;Correlation Invasive ~ Native | SoilType:&quot;, round(cor(residual_inv, residual_nat), 3), &quot;\\n&quot;) ## Correlation Invasive ~ Native | SoilType: 0.007 Implication: You MUST control for the confounder to get an unbiased estimate of the causal effect (which in this case is zero—there’s no direct effect of Invasive on Native). 236.3 Structure 3: Colliders A → B ← C B is caused by both A and C. B is called a collider because two arrows “collide” at it. collider_dag &lt;- dagify( PlantGrowth ~ SoilN + Pathogens, labels = c( SoilN = &quot;Soil N&quot;, Pathogens = &quot;Pathogens&quot;, PlantGrowth = &quot;Plant\\nGrowth&quot; ), exposure = &quot;SoilN&quot;, outcome = &quot;Pathogens&quot; ) ggdag(collider_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Collider: Don&#39;t Control!&quot;) Figure 236.3: Collider structure: Plant growth is influenced by both soil nitrogen and pathogen load. Key property: A and C are independent marginally, but become dependent conditional on B! This is counterintuitive but crucial. # Demonstrate collider behavior SoilN &lt;- rnorm(n) Pathogens &lt;- rnorm(n) # Independent of SoilN! PlantGrowth &lt;- 0.6 * SoilN - 0.5 * Pathogens + rnorm(n, 0, 0.5) collider_data &lt;- data.frame(SoilN, Pathogens, PlantGrowth) # Marginal correlation: should be ~0 (independent) cat(&quot;Correlation SoilN ~ Pathogens (marginal):&quot;, round(cor(SoilN, Pathogens), 3), &quot;\\n&quot;) ## Correlation SoilN ~ Pathogens (marginal): -0.022 # Partial correlation: conditioning on collider creates spurious association! residual_n &lt;- residuals(lm(SoilN ~ PlantGrowth, data = collider_data)) residual_p &lt;- residuals(lm(Pathogens ~ PlantGrowth, data = collider_data)) cat(&quot;Correlation SoilN ~ Pathogens | PlantGrowth:&quot;, round(cor(residual_n, residual_p), 3), &quot;\\n&quot;) ## Correlation SoilN ~ Pathogens | PlantGrowth: 0.521 Implication: Controlling for a collider creates a spurious association where none exists. This is called collider bias or selection bias. 236.4 Summary: The three structures Structure Form Marginal Conditional on middle Chain A → B → C Dependent Independent Fork A ← B → C Dependent Independent Collider A → B ← C Independent Dependent ## ## THE GOLDEN RULES ## ═══════════════════════════════════════════════════════════════ ## ## ✅ CONTROL FOR forks (confounders) ## → Removes spurious association ## ## ❌ DON&#39;T CONTROL FOR chains (mediators) ## → Blocks the causal path you want to estimate ## ## ❌ DON&#39;T CONTROL FOR colliders ## → Creates spurious association where none exists "],["part-4-paths-and-d-separation.html", "Chapter 237 Part 4: Paths and D-Separation 237.1 What is a path? 237.2 When is a path blocked? 237.3 Example: Multiple paths", " Chapter 237 Part 4: Paths and D-Separation 237.1 What is a path? A path is any route connecting two variables, regardless of arrow direction. Paths can be: Open: Information flows through the path (creates association) Blocked: Information cannot flow (no association through this path) 237.2 When is a path blocked? ## ## PATH BLOCKING RULES ## ═══════════════════════════════════════════════════════════════ ## ## A path is BLOCKED if: ## ## 1. It contains a CHAIN (A → B → C) or FORK (A ← B → C) ## where B is conditioned on (controlled for) ## ## 2. It contains a COLLIDER (A → B ← C) ## where B is NOT conditioned on ## AND no descendant of B is conditioned on ## ## A path is OPEN if it is not blocked. ## ## D-SEPARATION: ## Two variables are &#39;d-separated&#39; (conditionally independent) ## if ALL paths between them are blocked. 237.3 Example: Multiple paths complex_dag &lt;- dagify( Y ~ X + Z + W, X ~ Z, W ~ Z, labels = c( X = &quot;Treatment&quot;, Y = &quot;Outcome&quot;, Z = &quot;Confounder&quot;, W = &quot;Mediator&quot; ), exposure = &quot;X&quot;, outcome = &quot;Y&quot; ) ggdag(complex_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Multiple Paths Between X and Y&quot;) Figure 237.1: A DAG with multiple paths between treatment and outcome. Some paths are causal, others are confounding. # Find all paths between X and Y all_paths &lt;- paths(complex_dag, from = &quot;X&quot;, to = &quot;Y&quot;) print(all_paths) ## $paths ## [1] &quot;X -&gt; Y&quot; &quot;X &lt;- Z -&gt; W -&gt; Y&quot; &quot;X &lt;- Z -&gt; Y&quot; ## ## $open ## [1] TRUE TRUE TRUE "],["part-5-the-backdoor-criterion.html", "Chapter 238 Part 5: The Backdoor Criterion 238.1 Causal vs non-causal paths 238.2 Finding adjustment sets 238.3 More complex example", " Chapter 238 Part 5: The Backdoor Criterion 238.1 Causal vs non-causal paths To estimate the causal effect of X on Y, we need to: Keep open the causal (directed) paths from X to Y Block all non-causal (backdoor) paths A backdoor path is any path that enters X through an arrow pointing INTO X. ## ## BACKDOOR PATHS ## ═══════════════════════════════════════════════════════════════ ## ## Consider: Z → X → Y ## Z → Y ## ## Causal path: X → Y (what we want to estimate) ## Backdoor path: X ← Z → Y (confounding path) ## ## The backdoor path &#39;sneaks in the back door&#39; of X. ## It creates non-causal association between X and Y. ## ## To isolate the causal effect, we must BLOCK the backdoor. 238.2 Finding adjustment sets An adjustment set is a set of variables that, when controlled for, blocks all backdoor paths while leaving causal paths open. # Simple confounding example confound_dag &lt;- dagify( Y ~ X + Z, X ~ Z, exposure = &quot;X&quot;, outcome = &quot;Y&quot; ) # What do we need to adjust for? adjustmentSets(confound_dag, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) ## { Z } ggdag_adjustment_set(confound_dag) + theme_dag() + labs(title = &quot;Adjustment Set for X → Y&quot;) Figure 238.1: Adjustment sets highlighted: controlling for Z blocks the backdoor path. 238.3 More complex example # Multiple confounders multi_dag &lt;- dagify( Y ~ X + A + B, X ~ A + C, A ~ C, B ~ C, exposure = &quot;X&quot;, outcome = &quot;Y&quot; ) # Find minimal adjustment sets adj_sets &lt;- adjustmentSets(multi_dag, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) print(adj_sets) ## { A, B } ## { A, C } ggdag_adjustment_set(multi_dag) + theme_dag() + labs(title = &quot;Adjustment Sets for Complex DAG&quot;) Figure 238.2: A more complex DAG with multiple potential confounders. dagitty identifies the minimal adjustment sets needed. "],["part-6-common-pitfalls.html", "Chapter 239 Part 6: Common Pitfalls 239.1 Pitfall 1: Controlling for a mediator 239.2 Pitfall 2: Controlling for a collider 239.3 Pitfall 3: Controlling for a descendant of a collider 239.4 Pitfall 4: Table 2 Fallacy", " Chapter 239 Part 6: Common Pitfalls 239.1 Pitfall 1: Controlling for a mediator mediator_dag &lt;- dagify( Y ~ M, M ~ X, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, labels = c(X = &quot;Treatment&quot;, M = &quot;Mediator&quot;, Y = &quot;Outcome&quot;) ) # If we control for M, we block the only path! ggdag_paths(mediator_dag, shadow = TRUE) + theme_dag() + labs(title = &quot;Don&#39;t Control for Mediator!&quot;, subtitle = &quot;Controlling for M gives effect estimate of 0&quot;) Figure 239.1: Pitfall: Controlling for mediator M blocks the causal path we’re trying to estimate. When is this a problem? You want to know: “Does fertilizer affect yield?” The path is: Fertilizer → Plant Size → Yield If you control for plant size, you’re asking: “Does fertilizer affect yield for plants of the same size?” This removes most of the effect! 239.2 Pitfall 2: Controlling for a collider # Example: Selection bias in graduate school # Smart students AND hard-working students get admitted # But among admitted students, smart and hard-working appear negatively correlated collider_example &lt;- dagify( Admitted ~ Smart + HardWorking, GPA ~ Smart + HardWorking, exposure = &quot;Smart&quot;, outcome = &quot;GPA&quot; ) # If we only look at admitted students (condition on Admitted)... cat(&quot;Paths between Smart and HardWorking:\\n&quot;) ## Paths between Smart and HardWorking: paths(collider_example, from = &quot;Smart&quot;, to = &quot;HardWorking&quot;) ## $paths ## [1] &quot;Smart -&gt; Admitted &lt;- HardWorking&quot; &quot;Smart -&gt; GPA &lt;- HardWorking&quot; ## ## $open ## [1] FALSE FALSE The problem: Among admitted students, smart students who aren’t hard-working still got in (because they’re smart), so there’s a negative correlation between intelligence and work ethic—but ONLY in the selected sample! 239.3 Pitfall 3: Controlling for a descendant of a collider descendant_dag &lt;- dagify( M ~ X + U, # M is collider D ~ M, # D is descendant of collider Y ~ X + U, exposure = &quot;X&quot;, outcome = &quot;Y&quot; ) ggdag(descendant_dag) + theme_dag() + labs(title = &quot;Descendant of Collider&quot;, subtitle = &quot;Controlling for D partially opens M&#39;s collider path&quot;) Figure 239.2: Pitfall: Controlling for a descendant of a collider also opens the collider path. Rule: Don’t control for descendants of colliders either! 239.4 Pitfall 4: Table 2 Fallacy Including all variables in a regression and interpreting each coefficient causally. ## ## THE TABLE 2 FALLACY ## ═══════════════════════════════════════════════════════════════ ## ## A regression model: ## Y ~ X + Z + W + V ## ## Each coefficient has a DIFFERENT causal interpretation: ## - X coefficient: effect of X controlling for Z, W, V ## - Z coefficient: effect of Z controlling for X, W, V ## ... ## ## But the correct adjustment set DIFFERS for each variable! ## ## To estimate X → Y: might need to control for Z only ## To estimate W → Y: might need to control for X and Z ## ## Including everything gives biased estimates for most coefficients. "],["part-7-building-your-own-dag.html", "Chapter 240 Part 7: Building Your Own DAG 240.1 Step-by-step process 240.2 Example: Grazing effects on plant diversity", " Chapter 240 Part 7: Building Your Own DAG 240.1 Step-by-step process ## ## HOW TO BUILD A DAG ## ═══════════════════════════════════════════════════════════════ ## ## STEP 1: Define your causal question ## - What exposure (X) and outcome (Y) are you interested in? ## ## STEP 2: List all relevant variables ## - What might affect X? ## - What might affect Y? ## - What might affect both? ## ## STEP 3: Draw arrows based on causal knowledge ## - Use domain expertise, literature, biological reasoning ## - Arrow means &#39;directly causes&#39; (not just correlates) ## ## STEP 4: Check for cycles ## - DAGs must be acyclic (no feedback loops) ## - If you have feedback, consider a time-indexed DAG ## ## STEP 5: Identify unmeasured variables ## - Include them! They affect what you can estimate ## ## STEP 6: Find adjustment sets ## - Use dagitty to identify what to control for ## ## STEP 7: Assess feasibility ## - Can you measure the variables in the adjustment set? ## - If not, causal inference may not be possible 240.2 Example: Grazing effects on plant diversity # Build a DAG for grazing → diversity grazing_dag &lt;- dagify( # Diversity depends on... Diversity ~ Grazing + SoilN + Moisture + Competition, # Competition depends on... Competition ~ Grazing + SoilN + Moisture, # Grazing intensity depends on... Grazing ~ Rancher + Forage, # Forage depends on... Forage ~ Moisture + SoilN, # Moisture and SoilN might share common cause Moisture ~ Topography, SoilN ~ Topography, # Labels labels = c( Diversity = &quot;Plant\\nDiversity&quot;, Grazing = &quot;Grazing\\nIntensity&quot;, SoilN = &quot;Soil N&quot;, Moisture = &quot;Moisture&quot;, Competition = &quot;Competition&quot;, Rancher = &quot;Rancher\\nDecision&quot;, Forage = &quot;Forage\\nAvailability&quot;, Topography = &quot;Topography&quot; ), exposure = &quot;Grazing&quot;, outcome = &quot;Diversity&quot; ) ggdag(grazing_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;DAG: Grazing Effects on Plant Diversity&quot;) Figure 240.1: A DAG for understanding grazing effects on plant diversity. # What do we need to control for? adjustment_sets &lt;- adjustmentSets(grazing_dag, exposure = &quot;Grazing&quot;, outcome = &quot;Diversity&quot;, type = &quot;minimal&quot;) cat(&quot;Minimal adjustment sets:\\n&quot;) ## Minimal adjustment sets: print(adjustment_sets) ## { Moisture, SoilN } ## { Forage } Interpretation: To estimate the causal effect of Grazing on Diversity, we need to control for Forage (and its causes) but NOT for Competition (which is a mediator). "],["part-8-when-causal-inference-fails.html", "Chapter 241 Part 8: When Causal Inference Fails 241.1 Unmeasured confounding 241.2 Alternatives when DAG assumptions fail", " Chapter 241 Part 8: When Causal Inference Fails 241.1 Unmeasured confounding unmeasured_dag &lt;- dagify( Y ~ X + U, X ~ U, latent = &quot;U&quot;, exposure = &quot;X&quot;, outcome = &quot;Y&quot;, labels = c(X = &quot;Observed\\nExposure&quot;, Y = &quot;Outcome&quot;, U = &quot;Unmeasured\\nConfounder&quot;) ) ggdag(unmeasured_dag, text = FALSE, use_labels = &quot;label&quot;) + theme_dag() + labs(title = &quot;Unmeasured Confounding&quot;, subtitle = &quot;Cannot identify causal effect without measuring U&quot;) Figure 241.1: Unmeasured confounding: U affects both X and Y but cannot be measured. # Check if effect is identifiable adj &lt;- adjustmentSets(unmeasured_dag, exposure = &quot;X&quot;, outcome = &quot;Y&quot;) cat(&quot;Adjustment sets:&quot;, ifelse(length(adj) == 0, &quot;NONE - effect not identifiable&quot;, paste(adj, collapse = &quot;, &quot;)), &quot;\\n&quot;) ## Adjustment sets: NONE - effect not identifiable 241.2 Alternatives when DAG assumptions fail Problem Potential solutions Unmeasured confounding Instrumental variables, regression discontinuity Can’t measure mediator Sensitivity analysis Feedback loops Time-series analysis, dynamic models Complex interactions Targeted learning, causal forests "],["part-9-connecting-dags-to-analysis.html", "Chapter 242 Part 9: Connecting DAGs to Analysis 242.1 DAGs inform statistical models 242.2 Example: Complete workflow", " Chapter 242 Part 9: Connecting DAGs to Analysis 242.1 DAGs inform statistical models ## ## FROM DAG TO STATISTICAL MODEL ## ═══════════════════════════════════════════════════════════════ ## ## DAG Analysis: ## 1. Draw DAG based on domain knowledge ## 2. Identify adjustment set for your causal question ## 3. Check for unmeasured confounders ## ## Statistical Analysis: ## 4. Include adjustment variables as covariates ## 5. DO NOT include mediators or colliders ## 6. Fit appropriate model (regression, GLM, etc.) ## 7. Interpret coefficient on exposure as causal effect ## (given DAG assumptions are correct) ## ## Key insight: The DAG justifies your model specification. ## Without a DAG, covariate selection is arbitrary. 242.2 Example: Complete workflow # Question: Does nitrogen addition increase plant biomass? # Step 1: Build DAG nitrogen_dag &lt;- dagify( Biomass ~ Nitrogen + Water + Light + Herbivory, Herbivory ~ Nitrogen + Biomass, # Herbivores attracted to high-N plants Light ~ Biomass, # Self-shading Water ~ SoilType, Nitrogen ~ SoilType + Treatment, # Treatment is our manipulation exposure = &quot;Nitrogen&quot;, outcome = &quot;Biomass&quot; ) # Step 2: Find adjustment set adj_set &lt;- adjustmentSets(nitrogen_dag, exposure = &quot;Nitrogen&quot;, outcome = &quot;Biomass&quot;) cat(&quot;To estimate N → Biomass, control for:&quot;, paste(unlist(adj_set), collapse = &quot;, &quot;), &quot;\\n&quot;) ## To estimate N → Biomass, control for: Water, SoilType # Step 3: Check what NOT to control for cat(&quot;\\nDO NOT control for:\\n&quot;) ## ## DO NOT control for: cat(&quot;- Herbivory (collider + descendant)\\n&quot;) ## - Herbivory (collider + descendant) cat(&quot;- Light (descendant of outcome)\\n&quot;) ## - Light (descendant of outcome) # Step 4: Simulate data following DAG structure n &lt;- 200 SoilType &lt;- rnorm(n) Treatment &lt;- rbinom(n, 1, 0.5) Water &lt;- 0.6 * SoilType + rnorm(n, 0, 0.5) Nitrogen &lt;- 0.3 * SoilType + 2 * Treatment + rnorm(n, 0, 0.5) Biomass &lt;- 0.8 * Nitrogen + 0.5 * Water - 0.3 * SoilType + rnorm(n, 0, 1) Herbivory &lt;- 0.4 * Nitrogen + 0.3 * Biomass + rnorm(n, 0, 0.5) dag_data &lt;- data.frame(SoilType, Treatment, Water, Nitrogen, Biomass, Herbivory) # Step 5: Fit correct model (control for adjustment set) correct_model &lt;- lm(Biomass ~ Nitrogen + Water, data = dag_data) # Compare to naive model (everything) naive_model &lt;- lm(Biomass ~ Nitrogen + Water + SoilType + Herbivory, data = dag_data) # And to treatment model (ignoring confounding) treatment_model &lt;- lm(Biomass ~ Nitrogen, data = dag_data) cat(&quot;True effect of Nitrogen: 0.8\\n\\n&quot;) ## True effect of Nitrogen: 0.8 cat(&quot;Estimates:\\n&quot;) ## Estimates: cat(&quot;Correct model (adjust for Water):&quot;, round(coef(correct_model)[&quot;Nitrogen&quot;], 3), &quot;\\n&quot;) ## Correct model (adjust for Water): 0.713 cat(&quot;Naive model (adjust for everything):&quot;, round(coef(naive_model)[&quot;Nitrogen&quot;], 3), &quot;\\n&quot;) ## Naive model (adjust for everything): 0.205 cat(&quot;Unadjusted model:&quot;, round(coef(treatment_model)[&quot;Nitrogen&quot;], 3), &quot;\\n&quot;) ## Unadjusted model: 0.761 The correct model recovers the true effect because we controlled for the right variables based on the DAG. "],["part-10-reporting-causal-analysis.html", "Chapter 243 Part 10: Reporting Causal Analysis 243.1 What to report 243.2 Sample methods paragraph 243.3 Key takeaways 243.4 Assignment", " Chapter 243 Part 10: Reporting Causal Analysis 243.1 What to report Your causal question: What effect are you trying to estimate? The DAG: Show it! Justify arrow presence/absence Adjustment set: What variables you controlled for and why What you didn’t control for: And why (mediators, colliders) Assumptions: What must be true for your estimate to be causal? Sensitivity: How robust is your conclusion to unmeasured confounding? 243.2 Sample methods paragraph We used directed acyclic graphs (DAGs) to identify the minimal adjustment set for estimating the causal effect of nitrogen addition on plant biomass. Based on ecological knowledge and prior literature, we constructed a DAG including nitrogen availability, soil moisture, soil type, herbivory, and light availability as potentially relevant variables (Fig. X). Analysis of the DAG using the dagitty package in R revealed that controlling for soil moisture was sufficient to block all backdoor paths between nitrogen and biomass, while avoiding conditioning on colliders (herbivory, which is affected by both nitrogen and biomass) or mediators. We therefore fit a linear regression of biomass on nitrogen addition with soil moisture as the only covariate. Our causal interpretation assumes no unmeasured confounders beyond those in the DAG—particularly, we assume no unmeasured variables that affect both nitrogen availability and biomass independently of the pathways we modeled. 243.3 Key takeaways Correlation ≠ causation — But DAGs help us move from one to the other Three structures: Chains (mediate), Forks (confound), Colliders (don’t control!) Backdoor paths create confounding — Must be blocked for causal estimates Adjustment sets tell you what to control — dagitty does the work Not everything should be controlled — Mediators and colliders cause bias DAGs encode assumptions — They make your causal reasoning explicit Unmeasured confounding breaks causal inference — Include unmeasured variables in your DAG DAGs justify your statistical model — Without them, covariate selection is arbitrary 243.4 Assignment 243.4.1 Part 1: Identify structures For each scenario, identify whether the middle variable is a chain, fork, or collider: Temperature → Metabolic Rate → Activity Level Soil Quality → Crop Yield, Soil Quality → Pest Pressure Pollution → Fish Health ← Fishing Pressure 243.4.2 Part 2: Build a DAG For your research question (or a hypothetical one): List all variables that might be relevant Draw arrows based on causal relationships Identify your exposure and outcome Use dagitty to find the adjustment set Explain what variables you should and shouldn’t control for 243.4.3 Part 3: Collider bias simulation Create a simulation demonstrating collider bias: Generate X and Y as independent variables Create Z that depends on both X and Y (collider) Show that X and Y are uncorrelated marginally Show that X and Y become correlated when conditioning on Z Explain in words why this happens 243.4.4 Part 4: Analyze a DAG Consider this ecological scenario: You want to know if invasive plants reduce native bee diversity. You have data on invasive plant cover, native plant diversity, floral resources, pesticide use, and land use intensity. Draw a plausible DAG for this system Identify backdoor paths Find the minimal adjustment set Discuss what happens if you control for floral resources (is it a mediator?) 243.4.5 Part 5: Reflection In 3-4 sentences, explain why it’s important to draw a DAG before running your regression analysis. What problems can arise from selecting covariates based on statistical criteria (like stepwise selection) rather than causal reasoning? "],["hierarchical-and-bayesian-thinking.html", "Chapter 244 Hierarchical and Bayesian Thinking 244.1 Why this matters for ecologists 244.2 Setup", " Chapter 244 Hierarchical and Bayesian Thinking You measure plant growth in 15 plots across 5 sites. Each site has different soils, microclimates, and histories. Should you analyze all 300 plants as independent observations? Should you average within sites and analyze just 5 data points? Neither feels right. Hierarchical models offer a principled middle ground: they recognize that plants within sites share something in common while still using all your data. Combined with Bayesian inference, they provide: Proper handling of nested/grouped data Natural incorporation of prior knowledge Full uncertainty quantification Principled borrowing of information across groups This chapter introduces both hierarchical thinking and Bayesian inference, showing how they combine to solve common ecological problems. 244.1 Why this matters for ecologists Ecological data are almost always hierarchical: - Observations within plots within sites within regions - Repeated measures on individuals over time - Species within communities within ecosystems - Studies within meta-analyses Ignoring this structure leads to: - Pseudoreplication: Treating non-independent observations as independent - Lost information: Averaging away within-group variation - Overconfident inference: Underestimating uncertainty 244.2 Setup library(tidyverse) library(brms) # Bayesian regression with Stan library(tidybayes) # Tidy tools for Bayesian models library(bayesplot) # Posterior visualization library(rstanarm) # Alternative Bayesian regression library(lme4) # Frequentist comparison set.seed(42) # Set brms options for faster sampling in examples options(brms.backend = &quot;cmdstanr&quot;) "],["part-1-two-philosophies-of-statistics.html", "Chapter 245 Part 1: Two Philosophies of Statistics 245.1 Frequentist thinking 245.2 Bayesian thinking 245.3 A simple example", " Chapter 245 Part 1: Two Philosophies of Statistics 245.1 Frequentist thinking The approach you’ve learned so far: ## ## FREQUENTIST PHILOSOPHY ## ═══════════════════════════════════════════════════════════════ ## ## Parameters are FIXED but unknown constants. ## Data are RANDOM samples from a population. ## ## Key questions: ## &#39;If I repeated this experiment infinitely, ## what would the sampling distribution look like?&#39; ## ## Inference tools: ## - P-values: P(data this extreme | H₀ true) ## - Confidence intervals: 95% of such intervals contain true value ## - Maximum likelihood: Find parameter that maximizes P(data | θ) ## ## Limitations: ## - Can&#39;t say &#39;probability parameter is in this range&#39; ## - No natural way to incorporate prior knowledge ## - Uncertainty only about sampling, not parameters 245.2 Bayesian thinking A different philosophy: ## ## BAYESIAN PHILOSOPHY ## ═══════════════════════════════════════════════════════════════ ## ## Parameters are RANDOM variables with probability distributions. ## Data are FIXED (we observed what we observed). ## ## Key questions: ## &#39;Given what I observed, what do I believe about the parameter?&#39; ## ## The fundamental equation (Bayes&#39; theorem): ## ## P(θ | data) = P(data | θ) × P(θ) ## ───────────────────── ## P(data) ## ## Posterior = Likelihood × Prior ## ───────────────── ## Normalizing constant ## ## What each term means: ## P(θ) Prior: What we believed before seeing data ## P(data | θ) Likelihood: How probable is data given θ ## P(θ | data) Posterior: What we believe after seeing data 245.3 A simple example You’re studying a rare plant. Before surveying, you estimate ~30% of suitable habitat patches are occupied. You survey 20 patches and find the species in 12. Frequentist approach: # Proportion and confidence interval n &lt;- 20 successes &lt;- 12 p_hat &lt;- successes / n # 95% CI using normal approximation se &lt;- sqrt(p_hat * (1 - p_hat) / n) ci_freq &lt;- c(p_hat - 1.96 * se, p_hat + 1.96 * se) cat(&quot;Frequentist estimate:&quot;, round(p_hat, 3), &quot;\\n&quot;) ## Frequentist estimate: 0.6 cat(&quot;95% CI:&quot;, round(ci_freq, 3), &quot;\\n&quot;) ## 95% CI: 0.385 0.815 Bayesian approach: # Prior: Beta distribution centered on 0.3 # Beta(3, 7) has mean = 3/(3+7) = 0.3 prior_a &lt;- 3 prior_b &lt;- 7 # Posterior: Beta(prior_a + successes, prior_b + failures) post_a &lt;- prior_a + successes post_b &lt;- prior_b + (n - successes) # Posterior mean and 95% credible interval post_mean &lt;- post_a / (post_a + post_b) ci_bayes &lt;- qbeta(c(0.025, 0.975), post_a, post_b) cat(&quot;Bayesian posterior mean:&quot;, round(post_mean, 3), &quot;\\n&quot;) ## Bayesian posterior mean: 0.5 cat(&quot;95% credible interval:&quot;, round(ci_bayes, 3), &quot;\\n&quot;) ## 95% credible interval: 0.325 0.675 # Visualize theta &lt;- seq(0, 1, length = 200) plot_data &lt;- data.frame( theta = rep(theta, 3), density = c( dbeta(theta, prior_a, prior_b), dbeta(theta, successes + 1, n - successes + 1), # Likelihood (uniform prior) dbeta(theta, post_a, post_b) ), distribution = rep(c(&quot;Prior&quot;, &quot;Likelihood&quot;, &quot;Posterior&quot;), each = length(theta)) ) ggplot(plot_data, aes(x = theta, y = density, color = distribution)) + geom_line(size = 1.2) + scale_color_manual(values = c(&quot;Prior&quot; = &quot;steelblue&quot;, &quot;Likelihood&quot; = &quot;gray50&quot;, &quot;Posterior&quot; = &quot;firebrick&quot;)) + labs(x = &quot;Occupancy probability (θ)&quot;, y = &quot;Density&quot;, title = &quot;Bayesian Updating&quot;, subtitle = &quot;Posterior = Prior × Likelihood (normalized)&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) Figure 245.1: Comparison of prior, likelihood, and posterior. The posterior combines prior knowledge with observed data. Key difference: The Bayesian credible interval is a direct probability statement: “There’s a 95% probability the true occupancy is between 0.42 and 0.76.” The frequentist CI doesn’t mean this! "],["part-2-priors.html", "Chapter 246 Part 2: Priors 246.1 What are priors? 246.2 Choosing priors 246.3 Prior sensitivity", " Chapter 246 Part 2: Priors 246.1 What are priors? Priors encode what you know (or assume) before seeing the data: ## ## TYPES OF PRIORS ## ═══════════════════════════════════════════════════════════════ ## ## INFORMATIVE PRIORS ## Based on previous studies, expert knowledge, or constraints ## Example: Survival probability is probably 0.7-0.9 based on similar species ## ## WEAKLY INFORMATIVE PRIORS ## Rule out implausible values but remain vague ## Example: Effect size is probably between -10 and 10 (on some scale) ## ## FLAT/UNIFORM PRIORS ## All values equally likely (rarely truly appropriate) ## Example: Uniform(0, 1) for a probability ## ## REGULARIZING PRIORS ## Pull estimates toward zero to prevent overfitting ## Example: Normal(0, 1) for regression coefficients 246.2 Choosing priors theta &lt;- seq(0, 1, length = 200) priors_df &lt;- data.frame( theta = rep(theta, 4), density = c( dbeta(theta, 1, 1), # Flat dbeta(theta, 2, 2), # Weakly informative (center) dbeta(theta, 8, 2), # Informative (high survival) dbeta(theta, 2, 8) # Informative (low survival) ), prior = rep(c(&quot;Flat: Beta(1,1)&quot;, &quot;Weakly informative: Beta(2,2)&quot;, &quot;Informative (high): Beta(8,2)&quot;, &quot;Informative (low): Beta(2,8)&quot;), each = length(theta)) ) ggplot(priors_df, aes(x = theta, y = density, color = prior)) + geom_line(size = 1.2) + labs(x = &quot;Survival probability&quot;, y = &quot;Prior density&quot;, title = &quot;Prior Choices for Survival Probability&quot;) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) Figure 246.1: Different priors for a survival probability. Choice should reflect actual prior knowledge. 246.3 Prior sensitivity Good practice: Check if conclusions change with different reasonable priors. # Test sensitivity of our occupancy example priors_to_test &lt;- list( &quot;Flat: Beta(1,1)&quot; = c(1, 1), &quot;Weak: Beta(2,2)&quot; = c(2, 2), &quot;Informative: Beta(3,7)&quot; = c(3, 7), &quot;Strong: Beta(6,14)&quot; = c(6, 14) ) sensitivity &lt;- data.frame( Prior = names(priors_to_test), Prior_mean = sapply(priors_to_test, function(x) x[1]/(x[1]+x[2])), Posterior_mean = sapply(priors_to_test, function(x) { (x[1] + successes) / (x[1] + x[2] + n) }), CI_lower = sapply(priors_to_test, function(x) { qbeta(0.025, x[1] + successes, x[2] + n - successes) }), CI_upper = sapply(priors_to_test, function(x) { qbeta(0.975, x[1] + successes, x[2] + n - successes) }) ) knitr::kable(sensitivity, digits = 3, caption = &quot;Prior sensitivity analysis for occupancy estimation&quot;) Table 246.1: Prior sensitivity analysis for occupancy estimation Prior Prior_mean Posterior_mean CI_lower CI_upper Flat: Beta(1,1) Flat: Beta(1,1) 0.5 0.591 0.384 0.782 Weak: Beta(2,2) Weak: Beta(2,2) 0.5 0.583 0.385 0.768 Informative: Beta(3,7) Informative: Beta(3,7) 0.3 0.500 0.325 0.675 Strong: Beta(6,14) Strong: Beta(6,14) 0.3 0.450 0.301 0.604 With 20 observations, results are reasonably robust to prior choice. With fewer data, priors matter more. "],["part-3-hierarchical-models.html", "Chapter 247 Part 3: Hierarchical Models 247.1 The partial pooling insight 247.2 Why hierarchical models work", " Chapter 247 Part 3: Hierarchical Models 247.1 The partial pooling insight Consider estimating species richness at 8 sites, each surveyed 3 times: # Simulate hierarchical data n_sites &lt;- 8 n_obs_per_site &lt;- 3 # True site-level means (drawn from population) true_mu &lt;- 25 # Population mean true_sigma_site &lt;- 5 # Between-site SD true_sigma_obs &lt;- 3 # Within-site SD site_means &lt;- rnorm(n_sites, true_mu, true_sigma_site) # Generate observations richness_data &lt;- data.frame( site = rep(1:n_sites, each = n_obs_per_site), obs = rep(1:n_obs_per_site, n_sites), richness = unlist(lapply(site_means, function(m) rnorm(n_obs_per_site, m, true_sigma_obs))) ) # Site summaries site_summary &lt;- richness_data %&gt;% group_by(site) %&gt;% summarise( mean_richness = mean(richness), sd_richness = sd(richness), n = n() ) print(site_summary) ## # A tibble: 8 × 4 ## site mean_richness sd_richness n ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 1 35.1 3.17 3 ## 2 2 22.8 5.66 3 ## 3 3 27.0 1.48 3 ## 4 4 24.4 6.71 3 ## 5 5 24.8 2.68 3 ## 6 6 27.1 3.59 3 ## 7 7 31.0 3.40 3 ## 8 8 25.0 2.15 3 Three approaches to estimation: # Approach 1: Complete pooling (ignore sites) complete_pool &lt;- mean(richness_data$richness) # Approach 2: No pooling (estimate each site separately) no_pool &lt;- site_summary$mean_richness # Approach 3: Partial pooling (hierarchical model) library(lme4) partial_pool_model &lt;- lmer(richness ~ 1 + (1|site), data = richness_data) partial_pool &lt;- coef(partial_pool_model)$site[,1] # Compare comparison &lt;- data.frame( site = 1:n_sites, true = site_means, complete_pooling = complete_pool, no_pooling = no_pool, partial_pooling = partial_pool ) knitr::kable(comparison, digits = 2, caption = &quot;Comparison of estimation approaches&quot;) Table 247.1: Comparison of estimation approaches site true complete_pooling no_pooling partial_pooling 1 31.85 27.16 35.12 32.58 2 22.18 27.16 22.80 24.18 3 26.82 27.16 27.03 27.07 4 28.16 27.16 24.39 25.27 5 27.02 27.16 24.76 25.53 6 24.47 27.16 27.15 27.15 7 32.56 27.16 31.00 29.78 8 24.53 27.16 25.05 25.72 ggplot(comparison, aes(x = factor(site))) + geom_point(aes(y = true), shape = 4, size = 4, color = &quot;black&quot;) + geom_point(aes(y = no_pooling), color = &quot;steelblue&quot;, size = 3) + geom_point(aes(y = partial_pooling), color = &quot;firebrick&quot;, size = 3) + geom_hline(yintercept = complete_pool, linetype = &quot;dashed&quot;, color = &quot;gray50&quot;) + labs(x = &quot;Site&quot;, y = &quot;Estimated richness&quot;, title = &quot;Shrinkage Toward the Grand Mean&quot;, subtitle = &quot;× = true, blue = no pooling, red = partial pooling, dashed = complete pooling&quot;) + theme_minimal() Figure 247.1: Shrinkage in hierarchical models. Partial pooling estimates (red) are pulled toward the grand mean compared to no-pooling estimates (blue). The shrinkage principle: Partial pooling “shrinks” extreme estimates toward the grand mean. Sites with little data are shrunk more. This reduces overfitting and improves predictions. 247.2 Why hierarchical models work ## ## HIERARCHICAL MODEL STRUCTURE ## ═══════════════════════════════════════════════════════════════ ## ## Level 1 (Observations): ## y_ij ~ Normal(μ_j, σ_within) ## ## Each observation i in group j is drawn from group j&#39;s distribution ## ## Level 2 (Groups): ## μ_j ~ Normal(μ_grand, σ_between) ## ## Each group&#39;s mean comes from a population distribution ## ## WHAT THIS DOES: ## 1. Borrows strength: Small groups borrow information from large groups ## 2. Reduces overfitting: Extreme estimates pulled toward mean ## 3. Quantifies variation: Separates within- vs between-group variance ## 4. Handles imbalance: Works with unequal group sizes ## ## KEY INSIGHT: ## Groups are not &#39;fixed&#39; independent estimates. ## They&#39;re drawn from a common distribution. ## This CONNECTS them and allows information sharing. "],["part-4-bayesian-regression-with-brms.html", "Chapter 248 Part 4: Bayesian Regression with brms 248.1 A simple Bayesian regression 248.2 Understanding the output 248.3 Visualizing posteriors 248.4 Posterior predictive checks", " Chapter 248 Part 4: Bayesian Regression with brms 248.1 A simple Bayesian regression # Simulate data n &lt;- 100 x &lt;- rnorm(n, 10, 2) y &lt;- 5 + 2 * x + rnorm(n, 0, 3) simple_data &lt;- data.frame(x = x, y = y) # Fit Bayesian linear regression # (Using default priors for demonstration) bayes_lm &lt;- brm( y ~ x, data = simple_data, family = gaussian(), chains = 4, iter = 2000, warmup = 1000, seed = 42, silent = 2 ) # Model summary summary(bayes_lm) ## Family: gaussian ## Links: mu = identity ## Formula: y ~ x ## Data: simple_data (Number of observations: 100) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 4.02 1.43 1.22 6.85 1.00 4075 2769 ## x 2.09 0.14 1.81 2.37 1.00 4178 2745 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 2.71 0.20 2.36 3.14 1.00 3819 2828 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 248.2 Understanding the output # Extract posterior samples posterior &lt;- as_draws_df(bayes_lm) # Summarize slope parameter slope_summary &lt;- posterior %&gt;% summarise( mean = mean(b_x), sd = sd(b_x), q2.5 = quantile(b_x, 0.025), q97.5 = quantile(b_x, 0.975), prob_positive = mean(b_x &gt; 0) ) cat(&quot;Slope posterior summary:\\n&quot;) ## Slope posterior summary: print(slope_summary) ## # A tibble: 1 × 5 ## mean sd q2.5 q97.5 prob_positive ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.09 0.142 1.81 2.37 1 cat(&quot;\\nInterpretation: There&#39;s a&quot;, round(slope_summary$prob_positive * 100, 1), &quot;% probability the slope is positive.\\n&quot;) ## ## Interpretation: There&#39;s a 100 % probability the slope is positive. 248.3 Visualizing posteriors # Posterior density plots mcmc_areas(bayes_lm, pars = c(&quot;b_Intercept&quot;, &quot;b_x&quot;, &quot;sigma&quot;), prob = 0.95) + labs(title = &quot;Posterior Distributions&quot;) Figure 248.1: Posterior distributions for regression parameters. The shaded region shows 95% credible interval. 248.4 Posterior predictive checks # Does the model generate data that looks like our data? pp_check(bayes_lm, ndraws = 100) + labs(title = &quot;Posterior Predictive Check&quot;) Figure 248.2: Posterior predictive check: Simulated data (light blue) should resemble observed data (dark blue). "],["part-5-bayesian-hierarchical-models.html", "Chapter 249 Part 5: Bayesian Hierarchical Models 249.1 Fitting hierarchical models with brms 249.2 Extracting group-level estimates 249.3 Comparing to frequentist", " Chapter 249 Part 5: Bayesian Hierarchical Models 249.1 Fitting hierarchical models with brms # Use our richness data bayes_hier &lt;- brm( richness ~ 1 + (1|site), data = richness_data, family = gaussian(), chains = 4, iter = 2000, warmup = 1000, seed = 42, silent = 2 ) summary(bayes_hier) ## Family: gaussian ## Links: mu = identity ## Formula: richness ~ 1 + (1 | site) ## Data: richness_data (Number of observations: 24) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Multilevel Hyperparameters: ## ~site (Number of levels: 8) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 3.47 1.70 0.48 7.19 1.00 871 765 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 27.10 1.56 24.03 30.31 1.00 1410 1070 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 4.28 0.81 2.97 6.21 1.00 1474 2121 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 249.2 Extracting group-level estimates # Site-level effects site_effects &lt;- rstanarm::ranef(bayes_hier)$site print(site_effects) ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 4.87187203 2.944088 -0.2169671 10.735161 ## 2 -2.59448261 2.362045 -7.6365728 1.603508 ## 3 -0.04887910 2.205301 -4.4592181 4.305277 ## 4 -1.63696433 2.275266 -6.3561226 2.519939 ## 5 -1.41992721 2.267517 -6.2841064 2.875705 ## 6 0.04432292 2.157409 -4.5988997 4.274163 ## 7 2.32386134 2.327584 -1.7098838 7.266762 ## 8 -1.29302543 2.229977 -6.0125701 2.706105 # Full site-level estimates (intercept + site effect) site_estimates &lt;- coef(bayes_hier)$site print(site_estimates) ## , , Intercept ## ## Estimate Est.Error Q2.5 Q97.5 ## 1 31.97035 2.776866 26.31897 37.12473 ## 2 24.50399 2.141818 20.20430 28.65039 ## 3 27.04959 1.970213 23.21582 30.99669 ## 4 25.46151 2.071780 21.33438 29.44188 ## 5 25.67855 2.027674 21.69422 29.58916 ## 6 27.14280 1.951986 23.25377 30.97508 ## 7 29.42234 2.119718 25.50364 33.70189 ## 8 25.80545 2.033270 21.59278 29.71626 249.3 Comparing to frequentist # Frequentist version freq_hier &lt;- lmer(richness ~ 1 + (1|site), data = richness_data) # Compare estimates comparison_hier &lt;- data.frame( site = 1:n_sites, true = site_means, frequentist = coef(freq_hier)$site[,1], bayesian = site_estimates[, , 1][, &quot;Estimate&quot;] ) cat(&quot;Comparison of hierarchical model estimates:\\n&quot;) ## Comparison of hierarchical model estimates: print(comparison_hier, digits = 2) ## site true frequentist bayesian ## 1 1 32 33 32 ## 2 2 22 24 25 ## 3 3 27 27 27 ## 4 4 28 25 25 ## 5 5 27 26 26 ## 6 6 24 27 27 ## 7 7 33 30 29 ## 8 8 25 26 26 "],["part-6-ecological-example---species-richness.html", "Chapter 250 Part 6: Ecological Example - Species Richness 250.1 A more realistic model 250.2 Setting priors 250.3 Visualizing effects 250.4 Model diagnostics", " Chapter 250 Part 6: Ecological Example - Species Richness 250.1 A more realistic model # Simulate a richer dataset # Species richness depends on habitat quality, varies by region n_regions &lt;- 6 n_sites_per_region &lt;- 8 n_total &lt;- n_regions * n_sites_per_region # Region effects region_effects &lt;- rnorm(n_regions, 0, 3) # Generate data eco_data &lt;- data.frame( region = factor(rep(1:n_regions, each = n_sites_per_region)), site = 1:n_total, habitat_quality = runif(n_total, 0, 10), area = runif(n_total, 1, 100) ) # True model: richness ~ habitat + log(area) + region eco_data$richness &lt;- with(eco_data, { 15 + # Intercept 2.5 * habitat_quality + # Habitat effect 5 * log(area) + # Area effect region_effects[as.numeric(region)] + # Region random effect rnorm(n_total, 0, 4) # Residual }) # Make counts eco_data$richness &lt;- round(pmax(eco_data$richness, 1)) head(eco_data) ## region site habitat_quality area richness ## 1 1 1 5.489711 61.878228 55 ## 2 1 2 9.994402 13.796732 50 ## 3 1 3 8.608261 75.215781 55 ## 4 1 4 1.486222 59.183419 45 ## 5 1 5 9.697451 33.570134 46 ## 6 1 6 9.645848 8.153995 51 250.2 Setting priors # Check default priors get_prior(richness ~ habitat_quality + log(area) + (1|region), data = eco_data, family = gaussian()) ## prior class coef group resp dpar nlpar lb ub tag source ## (flat) b default ## (flat) b habitat_quality (vectorized) ## (flat) b logarea (vectorized) ## student_t(3, 45, 7.4) Intercept default ## student_t(3, 0, 7.4) sd 0 default ## student_t(3, 0, 7.4) sd region 0 (vectorized) ## student_t(3, 0, 7.4) sd Intercept region 0 (vectorized) ## student_t(3, 0, 7.4) sigma 0 default # Set custom priors my_priors &lt;- c( prior(normal(15, 10), class = Intercept), # Grand mean ~15 species prior(normal(0, 5), class = b), # Regression coefficients prior(exponential(0.1), class = sd), # Random effect SD prior(exponential(0.1), class = sigma) # Residual SD ) # Fit model with custom priors eco_model &lt;- brm( richness ~ habitat_quality + log(area) + (1|region), data = eco_data, family = gaussian(), prior = my_priors, chains = 4, iter = 2000, warmup = 1000, seed = 42, silent = 2 ) summary(eco_model) ## Family: gaussian ## Links: mu = identity ## Formula: richness ~ habitat_quality + log(area) + (1 | region) ## Data: eco_data (Number of observations: 48) ## Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1; ## total post-warmup draws = 4000 ## ## Multilevel Hyperparameters: ## ~region (Number of levels: 6) ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sd(Intercept) 2.47 1.55 0.29 6.64 1.00 597 455 ## ## Regression Coefficients: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## Intercept 16.35 2.86 10.52 21.89 1.00 1420 851 ## habitat_quality 2.03 0.21 1.62 2.45 1.00 2400 2483 ## logarea 5.18 0.63 3.94 6.40 1.00 3555 2250 ## ## Further Distributional Parameters: ## Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS ## sigma 3.92 0.45 3.17 4.91 1.00 2347 2630 ## ## Draws were sampled using sample(hmc). For each parameter, Bulk_ESS ## and Tail_ESS are effective sample size measures, and Rhat is the potential ## scale reduction factor on split chains (at convergence, Rhat = 1). 250.3 Visualizing effects # Conditional effects plots plot(conditional_effects(eco_model), points = TRUE) Figure 250.1: Posterior distributions for habitat quality and area effects on species richness. Figure 250.2: Posterior distributions for habitat quality and area effects on species richness. 250.4 Model diagnostics # Trace plots (should look like &quot;hairy caterpillars&quot;) mcmc_trace(eco_model, pars = c(&quot;b_Intercept&quot;, &quot;b_habitat_quality&quot;, &quot;b_logarea&quot;)) Figure 250.3: MCMC trace plots showing convergence. Chains should mix well and look like ‘fuzzy caterpillars’. # Check R-hat values (should be &lt; 1.01) brms::rhat(eco_model) ## b_Intercept b_habitat_quality b_logarea sd_region__Intercept sigma Intercept ## 1.003321 1.001878 1.001024 1.004918 1.000429 1.004091 ## r_region[1,Intercept] r_region[2,Intercept] r_region[3,Intercept] r_region[4,Intercept] r_region[5,Intercept] r_region[6,Intercept] ## 1.001981 1.003732 1.003433 1.002677 1.003016 1.003358 ## lprior lp__ ## 1.001485 1.003358 "],["part-7-model-comparison.html", "Chapter 251 Part 7: Model Comparison 251.1 Bayesian model comparison", " Chapter 251 Part 7: Model Comparison 251.1 Bayesian model comparison # Fit alternative models model_null &lt;- brm(richness ~ 1 + (1|region), data = eco_data, family = gaussian(), chains = 4, iter = 2000, silent = 2) model_habitat &lt;- brm(richness ~ habitat_quality + (1|region), data = eco_data, family = gaussian(), chains = 4, iter = 2000, silent = 2) model_area &lt;- brm(richness ~ log(area) + (1|region), data = eco_data, family = gaussian(), chains = 4, iter = 2000, silent = 2) model_full &lt;- eco_model # Already fit above # Leave-one-out cross-validation loo_null &lt;- rstanarm::loo(model_null) loo_habitat &lt;- rstanarm::loo(model_habitat) loo_area &lt;- rstanarm::loo(model_area) loo_full &lt;- rstanarm::loo(model_full) # Compare models loo_compare(loo_null, loo_habitat, loo_area, loo_full) ## elpd_diff se_diff ## model_full 0.0 0.0 ## model_habitat -23.4 8.3 ## model_area -27.9 6.8 ## model_null -35.3 8.2 Interpretation: Lower ELPD (expected log predictive density) difference = better model. The full model is best. "],["part-8-predictions-and-uncertainty.html", "Chapter 252 Part 8: Predictions and Uncertainty 252.1 Posterior predictions 252.2 Predictions for new groups", " Chapter 252 Part 8: Predictions and Uncertainty 252.1 Posterior predictions # Predict for new data new_data &lt;- data.frame( habitat_quality = c(2, 5, 8), area = c(10, 50, 100), region = factor(rep(&quot;1&quot;, 3)) # Existing region ) # Predictions with uncertainty predictions &lt;- posterior_predict(eco_model, newdata = new_data) # Summarize pred_summary &lt;- data.frame( habitat = new_data$habitat_quality, area = new_data$area, mean = colMeans(predictions), lower = apply(predictions, 2, quantile, 0.025), upper = apply(predictions, 2, quantile, 0.975) ) knitr::kable(pred_summary, digits = 1, caption = &quot;Predicted species richness with 95% credible intervals&quot;) Table 252.1: Predicted species richness with 95% credible intervals habitat area mean lower upper 2 10 32.9 24.4 41.2 5 50 47.3 39.2 55.5 8 100 57.0 48.6 65.2 252.2 Predictions for new groups # Predict for a NEW region (not in training data) new_region_data &lt;- data.frame( habitat_quality = 5, area = 50, region = factor(&quot;new&quot;) # New region ) # This uses the estimated between-region variance pred_new_region &lt;- posterior_predict(eco_model, newdata = new_region_data, allow_new_levels = TRUE) cat(&quot;Prediction for new region:\\n&quot;) ## Prediction for new region: cat(&quot;Mean:&quot;, round(mean(pred_new_region), 1), &quot;\\n&quot;) ## Mean: 47.2 cat(&quot;95% CI:&quot;, round(quantile(pred_new_region, c(0.025, 0.975)), 1), &quot;\\n&quot;) ## 95% CI: 38.5 55.6 Note: Predictions for new groups have wider uncertainty because we don’t know that group’s random effect. "],["part-9-when-to-go-bayesian.html", "Chapter 253 Part 9: When to Go Bayesian 253.1 Advantages of Bayesian approach 253.2 When frequentist is fine 253.3 Practical considerations", " Chapter 253 Part 9: When to Go Bayesian 253.1 Advantages of Bayesian approach ## ## WHEN BAYESIAN METHODS SHINE ## ═══════════════════════════════════════════════════════════════ ## ## 1. PRIOR INFORMATION EXISTS ## - Previous studies inform expectations ## - Expert knowledge is relevant ## - Physical/biological constraints exist ## ## 2. COMPLEX HIERARCHICAL STRUCTURES ## - Nested random effects ## - Crossed random effects ## - Non-standard grouping structures ## ## 3. SMALL SAMPLE SIZES ## - Priors provide regularization ## - Prevents extreme estimates ## - Better handling of sparse data ## ## 4. UNCERTAINTY QUANTIFICATION ## - Full posterior distributions ## - Probability statements about parameters ## - Propagating uncertainty to predictions ## ## 5. COMPLEX MODELS ## - Nonlinear relationships ## - Custom likelihoods ## - Multi-response models 253.2 When frequentist is fine Large sample sizes with standard designs Simple models with established theory When speed is essential (Bayesian can be slow) When audiences expect frequentist results 253.3 Practical considerations ## ## PRACTICAL BAYESIAN TIPS ## ═══════════════════════════════════════════════════════════════ ## ## 1. START WITH FREQUENTIST ## Fit lm/glm/lmer first to understand your data ## ## 2. CHECK DEFAULT PRIORS ## brms defaults are often reasonable, but verify ## ## 3. DIAGNOSE CONVERGENCE ## - R-hat &lt; 1.01 ## - No divergences ## - Effective sample size adequate ## ## 4. DO POSTERIOR PREDICTIVE CHECKS ## Does the model generate data like yours? ## ## 5. SENSITIVITY ANALYSIS ## Do conclusions change with different priors? ## ## 6. REPORT APPROPRIATELY ## - Posterior means and credible intervals ## - Not p-values! "],["part-10-reporting-bayesian-analyses.html", "Chapter 254 Part 10: Reporting Bayesian Analyses 254.1 What to report 254.2 Sample methods and results 254.3 Key takeaways 254.4 Assignment", " Chapter 254 Part 10: Reporting Bayesian Analyses 254.1 What to report Model specification: Formula, priors, likelihood Software: Package and version (brms, Stan version) MCMC settings: Chains, iterations, warmup Convergence diagnostics: R-hat, divergences, ESS Posterior summaries: Means, SDs, credible intervals Model checks: Posterior predictive checks Model comparison: LOO-CV, WAIC if multiple models 254.2 Sample methods and results 254.2.1 Methods We analyzed species richness using Bayesian hierarchical regression implemented in brms version 2.20.0 (Bürkner 2017), which interfaces with Stan for MCMC sampling. We modeled richness as a function of habitat quality (continuous, 0-10 scale) and log-transformed patch area (ha), with random intercepts for region (n = 6). We specified weakly informative priors: Normal(15, 10) for the intercept reflecting typical richness values in similar systems, Normal(0, 5) for fixed effect coefficients, and Exponential(0.1) for standard deviation parameters. We ran 4 chains for 2000 iterations each (1000 warmup), yielding 4000 posterior samples. Convergence was assessed via R-hat statistics (all &lt; 1.01), trace plots, and absence of divergent transitions. Model fit was evaluated using posterior predictive checks. We compared candidate models using leave-one-out cross-validation (LOO-CV). 254.2.2 Results Species richness increased significantly with both habitat quality (β = 2.42, 95% CI: 2.05–2.79) and log patch area (β = 5.12, 95% CI: 4.21–6.03). The probability that both effects are positive exceeded 99.9%. Regional variation was substantial (SD = 2.89, 95% CI: 1.48–5.73), indicating that regions differ in baseline richness beyond what predictors explain. The model explained 76% of variance in richness (Bayesian R² = 0.76, 95% CI: 0.67–0.83). Posterior predictive checks showed adequate fit, with simulated data closely matching observed richness distributions. LOO-CV strongly favored the full model over reduced alternatives (ΔELPD &gt; 15 for all comparisons). 254.3 Key takeaways Bayesian = probability statements about parameters — What we actually want! Hierarchical models handle nested data — Partial pooling is the middle ground Priors encode prior knowledge — Not arbitrary; check sensitivity Shrinkage improves estimates — Especially for small groups brms makes Bayesian accessible — R formula syntax you know Always check convergence — R-hat, trace plots, divergences Posterior predictive checks — Does the model fit your data? Report fully — Priors, MCMC settings, diagnostics 254.4 Assignment 254.4.1 Part 1: Conceptual questions In your own words, explain the difference between a 95% confidence interval and a 95% credible interval. Why does partial pooling (hierarchical modeling) produce better estimates than either complete pooling or no pooling? When would you choose an informative prior over a weakly informative prior? Give an ecological example. 254.4.2 Part 2: Prior exploration For a study of survival probability (bounded 0-1): Specify three different Beta priors (informative, weakly informative, flat) Simulate data with true p = 0.7 and n = 15 Calculate the posterior under each prior Plot and compare the posteriors Discuss: How much do priors matter with this sample size? 254.4.3 Part 3: Fit a Bayesian hierarchical model Using data of your choice (or simulated data): Fit a frequentist mixed model with lmer Fit the equivalent Bayesian model with brms Compare the estimates Check convergence diagnostics Generate posterior predictive checks 254.4.4 Part 4: Model comparison Using your model from Part 3: Fit at least 2 alternative models Compare using LOO-CV Interpret which model is best and why Discuss whether the “best” model is actually good (check PPCs) 254.4.5 Part 5: Reporting Write a complete methods and results section for your Bayesian analysis, following the format in this chapter. "],["population-sensation.html", "Chapter 255 Population sensation 255.1 Population characteristics 255.2 Planning a census 255.3 Life cycle diagrams 255.4 Population demographic models 255.5 Integral projection models 255.6 Population dynamics 255.7 Chapter summary 255.8 Test your knowledge 255.9 Assignment", " Chapter 255 Population sensation Populations contain multitudes - numerous interacting individuals of the same species with slightly different genetic backgrounds, sharing genes, colonizing new areas, responding to environmental stimuli. Populations are hot-pots of evolution and essential for understanding species viability, especially when species are rare. Population-level studies have contributed fundamental ecological knowledge in evolution, range dynamics, and species conservation. Now that we are all jazzed about populations, let’s learn about how to describe them and their dynamics quantitative! May your knowledge acquisition be exponential! 255.1 Population characteristics In population ecology, we commonly describe populations in terms of size and structure. Size is simply the number of individuals in a population and structure refers to the proportion of individuals across age or size classes. For rare species or species with low population sizes, population size can be directly estimated through a census. Just as it seems, a census is when each individual is counted. We often collect information pertinent to understanding demographic vital rates of individuals that are censused - just like we do in the U.S. census. For plants, rather than collect information on religious background or age as we would for populations of humans, we typically record the size of the plant and other salient characteristics derived from an understanding of the species’ breeding system, dispersal mechanism, life history, and ecology. When species are abundant, we use other methods to estimate population size, such as density (i.e., how many individuals occur within a particular area) or sub-sampling (i.e., collecting census data within a plot). If a species if cryptic, meaning hard to observe (as is the case for most animals), we use mark-recapture or occupancy modeling to estimate population size and structure. 255.2 Planning a census Censuses are the ideal method for collecting demographic data, since it allows in-depth and comprehensive examination of the population (and it’s dynamics!). Let’s discuss some considerations for establishing a census protocol. When conducting a census, you want to plan on capturing critical life events and ensure that you are able to capture those life events through time (for multiple years). Let’s look at an example and establish a plan to census individuals within a population of this hypothetical species. The figure above shows a typical phenological pattern for an understory plant species in the eastern deciduous forest. These plants are dormant throughout the winter when temperatures are cold enough to freeze plant tissue. All plants, adults and new seedlings, emerge in the spring, and grow to their full size for the summer over a few week period in early spring. Then, they begin flowering and seed development in late summer, then disperse seeds into the fall. The first census indicated in this figure takes place after emergence in the spring once plants have reached their final size for the growing season. The census is planned to occur as early as possible post-growth in order to assess recruitment (new seedlings that enter the population), since seedlings tend to start to die-off through time. Seeds of this species require an 18-month stratification period prior to germination. In order to census across all members of the population, including seeds, we need to include the second census to quantify the number of seeds present at census 1, since counting seeds post-dispersal is almost impossible. Note that we could capture individuals across all important classes, adult plants, seedlings and seeds, if we conducted a single census at time point 2. Why not conduct a single census? In this case, the researchers were interested in causes of mortality of plants, particularly new germinants, across the growing season. By conducting the census in the spring, they were able to identify all plants that emerged that year and note if those plants were lost during the growing season and in many cases ascribe a reason for the mortality of the plant. 255.3 Life cycle diagrams In the above example, the census was timed to measure the performance and fate of important classes within the population. A useful tool for developing census protocols and demographic models is called a life cycle diagram. A life cycle diagram indicates important life stages and possible transitions among those stages. Continuing with our example above, we develop the following life history diagram. Here, you will note several important demographic characteristics of this species. First, on the right hand side of the diagram, you can see that the researchers have classified plants into several groups: new seedlings, &gt;1 year old seedlings, juveniles, small adults and large adults. These categories were selected because individuals within the categories share similar rates of reproduction and survival (vital rates). On the left hand side of the diagram, you will see that this species forms a seedbank and that seeds within this seed back persist around 45 months. 255.4 Population demographic models Population demographic models allow us to predict population trajectories, evaluate management strategies, understand extinction risk, and identify key life stages for conservation. Population demographic models come in several forms. We will take a deep dive into Integral Projection Models (IPMs), but understanding the various forms of population models will help you understand IPMs better! 255.4.1 Unstructured Models Unstructured models refer to population growth models that do not distinguish among individuals within a population. In these models, all individuals are treated as equivalent, meaning they are assumed to have the same probabilities of survival and reproduction, regardless of differences in age, size, or life stage. Because unstructured models treat all individuals as identical, they are best suited for simple or theoretical questions and often serve as a foundation for more complex, structured population models that account for age, size, or stage. The most basic population model describes a population experiencing exponential growth. Exponential growth—the idea that populations increase multiplicatively rather than additively — is foundational to Darwin’s development of the Theory of Evolution by Natural Selection. Darwin recognized that organisms have the capacity to produce far more offspring than are needed to replace themselves, and yet the world is not overrun by offspring (for example, ladybugs). From this insight, he reasoned that not all individuals can survive and reproduce, and that selection must therefore operate strongly and continuously. This differential survival and reproduction provides the mechanism by which natural selection can drive evolutionary change over time. Since the concept of multiplicative growth is central to the equation for exponential growth rate, let’s compare additive versus exponential growth. Additive growth means a population increases by a fixed number of individuals each time step, regardless of how large the population already is. You can think of it as: “The population gains the same number each year.” Example: Start with 100 individuals Add 10 individuals each year Population over time: Year 0: 100; Year 1: 110; Year 2: 120; Year 3: 130 Mathematically, this looks like: \\[ N_{t+1} = N_t + b \\] where b is a constant number added each time step. Multiplicative growth means the population increases by a proportion of its current size each time step. You can think of it as: “The population grows by a percentage.” Example: Start with 100 individuals Grow by 10% per year Population over time: Year 0: 100; Year 1: 110; Year 2: 121; Year 3: 133 Additive growth increases a population by a fixed number, while multiplicative growth increases a population by a proportion of its current size—making growth faster as populations get larger. Mathematically, this looks like: \\[ N_{t+1} = \\lambda N_t \\] or in continuous time: \\[ \\frac{dN}{dt} = rN \\] This form of growth is biologically realistic over short time scales and is the foundation of population ecology and evolutionary theory. 255.4.1.1 Exponential Growth: Unlimited resources, constant per capita growth When to use: Short-term projections, invasive species establishment Key assumption: No limits to growth Equations: The continuous-time exponential growth model describes how a population grows when resources are unlimited and the per-capita growth rate remains constant. \\[ N(t) = N_0 e^{rt} \\] where: \\(N(t)\\) is the population size at time \\(t\\) \\(N_0\\) is the initial population size (at \\(t = 0\\)) \\(r\\) is the intrinsic rate of increase \\(e\\) is the base of the natural logarithm \\(t\\) is time or more simply: \\[ N_{t+1} = \\lambda N_t \\] Given that it is exceedingly rare that there are no limits to population growth, population ecologist rarely use exponential growth models (I’ve never used this model for research). 255.4.1.2 Logistic Growth: Density-dependent regulation In real ecosystems, resources are finite, so populations cannot grow exponentially forever. The logistic growth model captures this reality by slowing population growth as population size approaches carrying capacity—the maximum number of individuals an environment can support over time. When to use: Populations approaching carrying capacity Key assumption: Negative density dependence, as populations get larger, per capita (per individual) growth decreases. The logistic growth model is written as: \\[ \\frac{dN}{dt} = rN\\left(1 - \\frac{N}{K}\\right) \\] where: \\(N\\) is population size \\(r\\) is the intrinsic rate of increase \\(K\\) is the carrying capacity When population size (\\(N\\)) is small relative to carrying capacity (\\(K\\)), the term \\(\\left(1 - \\frac{N}{K}\\right)\\) is close to 1 and population growth is approximately exponential. As \\(N\\) approaches \\(K\\), this term approaches 0, slowing growth until the population stabilizes. Note that the logistic growth equation is essentially the continuous exponential growth equation with a penalty for large populations! 255.4.2 Structured Models For most macroecological applications—such as modeling population growth in plants, tigers, or mice—unstructured models are inappropriate because individuals differ substantially in their probabilities of survival and reproduction depending on their age, size, or life stage. To address this, age-structured population models were developed in the mid-20th century. In 1945, Patrick Leslie introduced the Leslie matrix, a discrete-time, age-structured model designed to track populations divided into age classes. Leslie’s work was motivated largely by human demography and vertebrate populations, where age is relatively easy to determine and closely tied to survival and reproduction. The Leslie matrix provided a powerful framework for predicting population growth rates, stable age distributions, and sensitivity to changes in survival or fecundity. However, for many organisms—particularly plants and invertebrates—age is difficult or impossible to measure, and demographic rates are often more strongly associated with developmental stage or size than chronological age. In response, Lefkovitch (1965) extended Leslie’s framework to create the stage-structured matrix model, which allows individuals to transition among life stages rather than progress strictly by age. This innovation made structured population modeling broadly applicable across ecological systems, especially for organisms with complex life cycles or variable growth rates. Hal Caswell formalized, unified, and generalized structured population models into a coherent mathematical and ecological framework. His book is the cornerstone reference for population modeling, especially for ecology: Caswell, H. (2001; updated 2019). Matrix Population Models: Construction, Analysis, and Interpretation. William F. Morris and Daniel F. Doak further tuned population analysis for conservation, focusing on viability analyses, in their seminal book: Morris, W. F., &amp; Doak, D. F. (2002). Quantitative Conservation Biology. These are great references and citations to use in your research! 255.4.2.1 Age-structured models (Leslie matrix) Structure: Discrete age classes When to use: Species with clearly defined age classes Long-lived organisms where age strongly predicts survival and reproduction Examples: Mammals Because demographic rates in many ecological systems depend more on size or stage than on age, we will walk through a matrix model example below using a stage-structured (Lefkovitch) matrix. The mathematics underlying this approach is essentially identical to that of age-structured models. 255.4.2.2 Stage-structured models (Lefkovitch matrix) Structure: Discrete life stages (e.g., juvenile, subadult, adult) When to use: Age is difficult or impossible to determine Demographic rates depend more on stage or size than age Examples: Plants (seedling, sapling, adult) Insects with metamorphosis Amphibians Using matrices to model population dynamics allows us to track the fate of individuals in a population within a single, unified framework. To build a demographic model, population ecologists first describe the species’ life cycle, identifying groups of individuals that share similar vital rates, such as survival, growth, and reproduction. These groups—often called stages—form the structure of the matrix. Each column represents individuals currently in a given stage, and each row represents the stage they may transition into during the next time step. The values in the matrix describe the probabilities of survival, transitions among stages, and reproduction. 255.4.2.2.1 Example: A stage-structured (Lefkovitch) matrix Let’s look at a matrix for a hypothetical plant population. We’ve broken the population into three stages Juveniles, Subadults, and Adults. The adults are the only stage able to reproduce. We start with a population consisting of 100 juveniles, 50 subadults, 20 adults. Let’s transform this life cycle diagram into a matrix! To do this, you would have marked the plants in this population and conducted a census. You would then be able to quantify all the possible transitions, marked by arrows connecting stages in the life cycle diagram. Load programs: Create a life cycle diagram: # Create life cycle diagram for the plant example grViz(&quot; digraph plant_lifecycle { # Graph attributes graph [rankdir = LR, fontsize = 12] # Node definitions with labels node [shape = circle, style = filled, fillcolor = lightblue, fontname = Helvetica, fontsize = 14] Seeds [label = &#39;Seeds\\n(S)&#39;] Juveniles [label = &#39;Juveniles\\n(J)&#39;] Adults [label = &#39;Adults\\n(A)&#39;] # Edge definitions with labels showing transition probabilities Seeds -&gt; Seeds [label = &#39;0.05\\n(stay seed)&#39;, fontsize = 10] Seeds -&gt; Juveniles [label = &#39;0.10\\n(germinate)&#39;, fontsize = 10] Juveniles -&gt; Juveniles [label = &#39;0.30\\n(stay juv)&#39;, fontsize = 10] Juveniles -&gt; Adults [label = &#39;0.50\\n(mature)&#39;, fontsize = 10] Adults -&gt; Adults [label = &#39;0.90\\n(survive)&#39;, fontsize = 10] Adults -&gt; Seeds [label = &#39;15.0\\n(fecundity)&#39;, fontsize = 10, color = red, penwidth = 2] } &quot;) Let’s transform this into a matrix! A &lt;- matrix(c(0.1, 0.3, 0, # Juveniles column 0, 0.5, 0.2, # Subadults column 0, 0, 0.8), # Adults column (with fecundity) nrow = 3, ncol = 3, byrow = FALSE) # Let&#39;s look at the matrix library(tibble) library(knitr) # Name stages stages &lt;- c(&quot;Juvenile&quot;, &quot;Subadult&quot;, &quot;Adult&quot;) # Convert matrix to a tidy table A_df &lt;- as.data.frame(A) colnames(A_df) &lt;- stages rownames(A_df) &lt;- stages # Display nicely kable(A_df, digits = 2, caption = &quot;Stage-structured (Lefkovitch) projection matrix&quot;) Table 255.1: Stage-structured (Lefkovitch) projection matrix Juvenile Subadult Adult Juvenile 0.1 0.0 0.0 Subadult 0.3 0.5 0.0 Adult 0.0 0.2 0.8 # Starting population vector n0 &lt;- c(100, 50, 20) # 100 juveniles, 50 subadults, 20 adults # Project one time step n1 &lt;- A %*% n0 Here, the population vector lists the number of individuals in each stage at time \\(t\\), and matrix multiplication projects the population forward one time step to \\(t + 1\\). To see what is happening biologically, we can write out the matrix multiplication explicitly for one time step: The number of juveniles at the next time step is calculated as: \\[ n(J, t+1) = 0.1(100) = 10 \\] This means that 10% of the 100 juveniles present at time \\(t\\) survive and remain in the juvenile stage. The number of subadults at the next time step is calculated as: \\[ n(S, t+1) = 0.3(100) + 0.5(50) = 55 \\] This includes contributions from two processes: 30% of juveniles grow into subadults 50% of subadults survive and remain subadults The number of adults at the next time step is calculated as: \\[ n(A, t+1) = 0.2(50) + 0.8(20) = 26 \\] This reflects: Maturation of subadults into adults Survival of existing adults 255.4.2.2.1.1 Total population size and growth The total population size at time \\(t\\) is the sum of individuals across all stages: \\[ N_t = 100 + 50 + 20 = 170 \\] The total population size at time \\(t+1\\) is: \\[ N_{t+1} = 10 + 55 + 26 = 91 \\] The absolute change in population size is: \\[ \\Delta N = N_{t+1} - N_t = 91 - 170 = -79 \\] This indicates that the population declined by 79 individuals over one time step. The proportional population growth rate over one time step is: \\[ \\lambda = \\frac{N_{t+1}}{N_t} = \\frac{91}{170} \\approx 0.54 \\] A value of \\(\\lambda &lt; 1\\) indicates that, under these demographic rates, the population is declining. Et viola! That is basic matrix math! Want to learn how to multiply matrices? Leslie matrix population projection example (YouTube) These short-term, time-dependent dynamics are summarized using a suite of metrics collectively referred to as transient dynamics. These measures capture how populations behave before reaching long-term equilibrium, and we will explore them in more detail shortly. 255.4.2.2.1.2 Population growth rate (λ): the dominant eigenvalue After many time steps (repeat the process above), a structured population tends to settle into a stable pattern in which the relative proportions of individuals in each stage remain constant, even though the total population size may be changing. The rate at which the population grows or declines at this stable structure is denoted by \\(\\lambda\\) (lambda). Transient dynamics capture how populations behave over short time scales and can be strongly influenced by the initial mix of life stages. While these short-term responses are often ecologically important, they can be difficult to compare across populations because they depend on where the population starts. By contrast, describes the long-term growth or decline of a population once it has settled into a stable stage distribution. Because is determined by the underlying life cycle rather than initial conditions, it provides a consistent and interpretable measure of population viability and long-term persistence. Conceptual definition: λ is the long-term, proportional population growth rate implied by the matrix. • $\\lambda &gt; 1$: population grows • $\\lambda = 1$: population is stable • $\\lambda &lt; 1$: population declines Mathematically, \\(\\lambda\\) is the dominant eigenvalue of the projection matrix: \\[ \\mathbf{A}\\mathbf{n} = \\lambda \\mathbf{n} \\] In practice, \\(\\lambda\\) can be interpreted as the ratio of population size between successive time steps once the population has reached its stable stage distribution: \\[ \\lambda = \\frac{N_{t+1}}{N_t} \\] Mathematically, is the dominant eigenvalue of the projection matrix. When the matrix is repeatedly multiplied by a population vector, the population eventually grows or declines at a constant proportional rate, and that rate is given by . What is an eigenvalue? Think of a matrix as a machine that reshuffles individuals among life stages. Most starting populations change shape as they pass through this machine. Eventually, the population settles into a stable mix of juveniles, subadults, and adults. After that point, the matrix redistributes individuals in the same proportions each time step, so the population simply grows or shrinks without changing its stage structure. The number that tells us how much larger or smaller the population gets each step is called the dominant eigenvalue, . Let me show you in terms of matrix structure: 255.4.2.2.1.2.1 The projection matrix (structure) The A or transition matrix (as we call the matrix the summarizes transitions / dynamics), consists of these parts: Rows = stage at t+1 Columns = stage at t Entries = survival, growth, reproduction 255.4.2.2.1.2.2 Eigenvectors = stage structure An eigenvector describes a pattern of stage abundances: Dominant eigenvector → stable stage distribution Subdominant eigenvectors → transient dynamics (oscillations, overshoot, lag) 255.4.2.2.1.2.3 Eigenvalues = scaling rates Eigenvalues tell us how fast those patterns grow or decay. Dominant eigenvalue (_1) → long-term growth rate Subdominant eigenvalues (_2, _3) → how fast transients fade 255.4.2.2.1.2.4 Why “dominant” matters The dominant eigenvalue is called “dominant” because it eventually overwhelms all other dynamics. No matter how a population starts, its long-term behavior is governed by this single value. This matrix-based framework provides the foundation for sensitivity analyses, elasticities, and more advanced models such as Integral Projection Models (IPMs), which extend the same logic to continuous size variables. 255.5 Integral projection models When building standard matrix population models, stages are defined based on our understanding of which groups within a population exhibit similar vital rates. In reality, however, vital rates often vary continuously among individuals rather than falling into discrete categories. As a result, stage boundaries are frequently imposed by binning individuals into classes that may be somewhat arbitrary. In some cases this binning is practical or biologically justified—for example, when land managers conduct rapid censuses by counting only large, reproductively mature individuals. In many systems, however, discrete stages are approximations rather than natural divisions of demographic variation. Instead of binning individuals into discrete stages, Integral Projection Models (IPMs) describe population dynamics using a continuous state variable, most commonly a measure of individual size (such as height, diameter, or biomass in plants). In IPMs, vital rates—such as survival, growth, and reproduction—are modeled as functions of this state variable using statistical models fit to data. Conceptually, IPMs extend stage-structured matrix models by replacing discrete stage classes with a continuous size axis. This allows IPMs to capture fine-scale demographic variation without requiring arbitrary size bins. One useful way to think about an IPM is as an extremely fine-grained matrix model. Instead of a small matrix with a handful of discrete stages, an IPM can be viewed as a very large matrix in which individuals are divided into many tiny size classes, each with correspondingly small transition probabilities. In practice, this “infinite matrix” is represented mathematically by a kernel that combines survival, growth, and reproduction processes. This kernel plays the same conceptual role as a projection matrix in a matrix population model: it maps the population’s size distribution at time t to the distribution at time t+1. As in matrix models, repeated application of the kernel leads to a stable size distribution and a long-term population growth rate, . Mathematically, IPMs are built from a kernel that combines survival, growth, and reproduction processes. This kernel plays the same role as a projection matrix in a matrix population model: it maps the population’s size distribution at time \\(t\\) to the distribution at time \\(t+1\\). As in matrix models, repeated application of the kernel leads to a stable size distribution and a long-term population growth rate, \\(\\lambda\\). Because IPMs preserve the biological logic of matrix models while allowing demographic rates to vary smoothly with size, they are particularly well suited for plant populations and other organisms in which size, rather than age or stage, best predicts survival and reproduction. Since my lab uses IPMs for most demographic analyses, we will explore these models in detail in the next chapter! 255.6 Population dynamics Once a demographic model has been constructed—whether as a stage-structured matrix or an Integral Projection Model—it can be used to quantify a wide range of population-level properties. These quantities describe not only whether a population is growing or declining, but also why it behaves the way it does and which demographic processes matter most. Together, these quantities are referred to as population dynamics, and they provide insight into long-term growth, short-term responses to disturbance, and the sensitivity of populations to changes in survival, growth, or reproduction. 255.6.1 Population growth rate The most fundamental quantity derived from a projection matrix is the long-term population growth rate, \\(\\lambda\\). As discussed above, \\(\\lambda\\) is the dominant eigenvalue of the matrix and describes whether a population is expected to grow (\\(\\lambda &gt; 1\\)), decline (\\(\\lambda &lt; 1\\)), or remain stable (\\(\\lambda = 1\\)) once it reaches a stable structure. We calculated this above! 255.6.2 Stable stage distribution Associated with \\(\\lambda\\) is the stable stage distribution, given by the dominant right eigenvector of the projection matrix. This distribution describes the long-term proportion of individuals in each stage, independent of initial population structure. 255.6.3 Reproductive value The reproductive value of each stage, given by the dominant left eigenvector, describes the relative contribution of individuals in different stages to future population growth. Stages with high reproductive value have a disproportionate influence on population dynamics. 255.6.4 Sensitivities: absolute effects Sensitivity analysis quantifies how much \\(\\lambda\\) would change in response to a small absolute change in a given matrix element. Sensitivities answer the question: If this vital rate increased slightly, how much would population growth change? Let \\(\\mathbf{A}\\) be a population projection matrix with dominant eigenvalue \\(\\lambda\\), right eigenvector \\(\\mathbf{w}\\) (stable stage distribution), and left eigenvector \\(\\mathbf{v}\\) (reproductive value), scaled such that: \\[ \\mathbf{v}^\\top \\mathbf{w} = 1 \\] The sensitivity of population growth rate \\(\\lambda\\) to a matrix element \\(a_{ij}\\) is defined as: \\[ s_{ij} = \\frac{\\partial \\lambda}{\\partial a_{ij}} \\] For matrix population models, sensitivities can be calculated as: \\[ s_{ij} = v_i w_j \\] Sensitivity measures how much \\(\\lambda\\) changes in response to a small absolute change in a vital rate. 255.6.5 Elasticities: proportional effects Because vital rates are measured on different scales, sensitivities can be difficult to compare directly. Elasticity analysis rescales sensitivities to measure the proportional change in \\(\\lambda\\) resulting from a proportional change in a matrix element. Elasticities answer the question: Which life-history processes does population growth depend on most, relative to their current values? Since elasticities allow us to compare across species, reveal life-history strategies, and can be useful for management prioritization, we use and report elasticities more frequently than sensitivities. Because matrix elements are measured on different scales, sensitivities are often rescaled to obtain elasticities, which measure proportional effects. The elasticity of \\(\\lambda\\) with respect to matrix element \\(a_{ij}\\) is: \\[ e_{ij} = \\frac{a_{ij}}{\\lambda} \\frac{\\partial \\lambda}{\\partial a_{ij}} \\] Substituting the sensitivity expression gives: \\[ e_{ij} = \\frac{a_{ij}}{\\lambda} v_i w_j \\] The sum of all elasticities equals 1: \\[ \\sum_{i,j} e_{ij} = 1 \\] Together, sensitivities and elasticities reveal how population growth is partitioned among survival, growth, and reproduction. For example, long-lived species often show high elasticity to adult survival, whereas short-lived or fast-growing species tend to show higher elasticity to reproduction. 255.6.6 Beyond asymptotic dynamics: transient behavior While \\(\\lambda\\), stable stage distributions, and elasticities describe long-term behavior, populations often experience important short-term dynamics before equilibrium is reached. These transient dynamics capture temporary amplification, decline, or oscillations driven by initial population structure or disturbance. Let’s go over some common metrics to describe transient dynamics! Let: \\(\\mathbf{n}_0\\) be an initial population vector \\(\\mathbf{A}\\) be the projection matrix \\(\\lambda\\) be the dominant eigenvalue 255.6.6.1 Reactivity (short-term amplification) Reactivity measures the maximum possible proportional population growth over one time step, relative to long-term growth: \\[ \\rho_1 = \\max_{\\mathbf{n}_0} \\frac{\\|\\mathbf{A}\\mathbf{n}_0\\|}{\\|\\mathbf{n}_0\\|} \\] Interpretation: \\(\\rho_1 &gt; \\lambda\\) indicates transient amplification \\(\\rho_1 &lt; \\lambda\\) indicates immediate decline 255.6.6.2 Maximum amplification over time The maximum amplification over all time steps is: \\[ \\rho_{\\max} = \\max_{t \\ge 1} \\frac{\\|\\mathbf{A}^t \\mathbf{n}_0\\|}{\\|\\mathbf{n}_0\\|} \\] This quantity captures the largest possible transient increase in population size that can occur before long-term (asymptotic) behavior dominates. Interpretation: Maximum amplification describes the strongest short-term population response that can arise purely from population structure, even when long-term growth is stable or declining. A population with \\(\\lambda \\le 1\\) may still experience substantial temporary growth if its initial stage structure aligns with highly productive or resilient stages. In applied contexts, high values of \\(\\rho_{\\max}\\) indicate the potential for short-term booms following disturbance, management action, or unusual recruitment events, even if long-term persistence is not guaranteed. 255.6.6.3 Inertia Inertia describes how long the influence of initial population structure persists before convergence to stable growth: \\[ \\text{Inertia} = \\frac{\\|\\mathbf{A}^t \\mathbf{n}_0\\|}{\\lambda^t \\|\\mathbf{n}_0\\|} \\quad \\text{as } t \\to \\infty \\] Interpretation: Values greater than 1 indicate persistent amplification Values less than 1 indicate persistent suppression 255.6.6.4 Damping ratio (rate of convergence) The damping ratio quantifies how quickly transient dynamics decay and is defined as: \\[ \\text{Damping ratio} = \\frac{|\\lambda_1|}{|\\lambda_2|} \\] where: \\(\\lambda_1\\) is the dominant eigenvalue \\(\\lambda_2\\) is the subdominant eigenvalue (second largest magnitude) Interpretation: Larger values indicate faster convergence to the stable stage distribution Values close to 1 indicate long-lasting transient dynamics 255.7 Chapter summary In this chapter, we explored a progression of demographic models used to describe and analyze population dynamics. We began with simple, unstructured models of exponential and logistic growth, which capture fundamental principles of population increase and resource limitation. While these models provide important conceptual foundations, they assume that all individuals in a population are demographically identical. We then introduced structured population models, which explicitly account for differences among individuals based on age, stage, or size. Age-structured (Leslie) and stage-structured (Lefkovitch) matrix models describe how individuals transition among life stages and how these transitions determine population growth. From these models, we can calculate key quantities such as the population growth rate (\\(\\lambda\\)), stable stage distribution, and reproductive value. Building on this framework, we examined how sensitivities and elasticities identify which vital rates have the greatest influence on population growth, and how transient dynamics describe short-term population responses before long-term equilibrium is reached. Metrics such as reactivity, maximum amplification, inertia, and the damping ratio reveal how populations can temporarily grow, decline, or overshoot even when long-term growth is stable or negative. Finally, we introduced Integral Projection Models (IPMs), which extend matrix models by treating size as a continuous variable rather than dividing individuals into discrete stages. IPMs preserve the logic of matrix models while avoiding arbitrary stage boundaries, making them particularly well suited for plant populations and other systems where vital rates vary smoothly with size. Together, these approaches provide a powerful toolkit for understanding how individual-level processes scale up to population-level dynamics, and for identifying the demographic mechanisms that drive population growth, decline, and resilience. 255.8 Test your knowledge What is the key difference between unstructured population models and structured population models? Why is exponential growth considered a multiplicative process rather than an additive one? In a stage-structured matrix model, what biological processes are represented by: diagonal matrix elements? off-diagonal matrix elements? What does the dominant eigenvalue (\\(\\lambda\\)) represent in a population projection matrix, and why is it useful for comparing populations? What is the stable stage distribution, and how does it differ from the population’s initial stage structure? Explain why sensitivities and elasticities can lead to different conclusions about which life stages matter most for population growth. A population has \\(\\lambda &lt; 1\\) but exhibits high maximum amplification. What does this tell you about its short-term versus long-term dynamics? Why might transient dynamics be especially important for populations experiencing disturbance, management interventions, or rapid environmental change? Give one ecological situation where a stage-structured matrix model is appropriate and one where an Integral Projection Model would be preferable. Explain why sensitivities and elasticities can lead to different conclusions about which life stages matter most for population growth. A population has \\(\\lambda &lt; 1\\) but exhibits high maximum amplification. What does this tell you about its short-term versus long-term dynamics? Why might transient dynamics be especially important for populations experiencing disturbance, management interventions, or rapid environmental change? Give one ecological situation where a stage-structured matrix model is appropriate and one where an Integral Projection Model would be preferable. Suppose a population has: high elasticity to adult survival low elasticity to reproduction What type of life-history strategy does this suggest, and how might this influence management priorities? Two populations have the same \\(\\lambda\\), but one has a much lower damping ratio than the other. What does this imply about their transient dynamics? Why might focusing only on long-term population growth (\\(\\lambda\\)) be misleading in some ecological or conservation contexts? How do matrix models and IPMs help bridge individual-level processes (survival, growth, reproduction) with population-level outcomes? 255.9 Assignment Create a life cycle diagram for a species that you would like to examine. This species may be a plant or animal, but you should choose a system for which life stages or size classes are biologically meaningful. Life cycle diagrams are simplified representations of population structure and demographic processes. For Integral Projection Models (IPMs) in particular, these diagrams are often highly reduced, because size is treated as a continuous variable rather than divided into discrete stages. For example, a perennial plant life cycle diagram might include: A single size-based individual stage (rather than multiple discrete stages) Survival and growth within that size continuum Reproduction producing new individuals (e.g., seedlings) 255.9.1 Your task Choose a focal species and briefly describe its life history (1–2 sentences). Identify the key life stages or size classes relevant to its survival, growth, and reproduction. Draw a life cycle diagram that includes: All stages or size classes Arrows showing possible transitions among stages Arrows representing survival, growth, and reproduction Clearly label all stages and arrows. 255.9.2 Guidelines Your diagram does not need to include numerical values. Focus on biological realism rather than mathematical detail. You may draw your diagram by hand, digitally, or using software (e.g., PowerPoint, Illustrator, R). 255.9.3 What to submit A clear image or PDF of your life cycle diagram A figure legend (3–5 sentences) for your figure, describing how it reflects the species’ biology This figure can be included in a manuscript and will set you up to complete the next activity. "],["integral-population-models.html", "Chapter 256 Integral Population Models 256.1 Case Study: Modeling an Endangered Hawaiian Plant 256.2 Designing a Demographic Census: Timing and Measurements 256.3 Packages and approaches to creating IPMs 256.4 Building an IPM: Structuring linear models, variable transformations, and dealing with outliers 256.5 Constructing a transition matrix 256.6 Building an IPM: Boundary points, mesh points, and step size 256.7 Model Evaluation: Eviction 256.8 Estimating population dynamics parameters 256.9 Calculating confidence intervals 256.10 Putting this all together in code form! 256.11 Test your knowledge 256.12 Assignment", " Chapter 256 Integral Population Models 256.1 Case Study: Modeling an Endangered Hawaiian Plant For our applied example, we’ll build a population model for Silene lanceolata (lanceolate catchfly), a small woody shrub endemic to the Hawaiian Islands. This species is federally listed as threatened/endangered and faces multiple conservation challenges that make it an ideal candidate for demographic modeling. 256.1.1 Why model S. lanceolata populations? Hawaiian plant species like S. lanceolata occur in small, isolated populations that are particularly vulnerable to extinction. At Pōhakuloa Training Area (PTA) on Hawaii Island, these native plants face a perfect storm of threats: Invasive plant competition: Fountain grass (Cenchrus setaceus) and fireweed (Senecio madagascariensis) outcompete native species for water, light, and space Altered fire regimes: Invasive grasses produce fine fuels that dramatically increase fire frequency—a disturbance to which Hawaiian plants have no evolutionary adaptations Climate change: Increasing drought frequency threatens plants already stressed by competition and fire Small population sizes: Limited numbers mean that small changes in vital rates (survival, growth, reproduction) can have large consequences for extinction risk 256.1.2 The management question: Conservation managers need to decide how to allocate limited resources. Should they prioritize invasive plant control? Focus on fire suppression? Protect certain life stages? Population models allow us to project population trajectories under different management scenarios and identify which vital rates have the greatest influence on population growth. In this example, we’ll use demographic data collected from field studies at PTA to build an Integral Projection Model (IPM) that links individual plant size to survival, growth, and reproduction. This will allow us to evaluate how management actions might influence S. lanceolata persistence. 256.2 Designing a Demographic Census: Timing and Measurements Before we dive into building the population model, let’s review how demographic data are collected. The design of a census protocol involves several critical decisions that directly impact the quality and interpretability of your population projections. 256.2.1 Principle 1: Census Timing Must Align with the Organism’s Life Cycle For demographic models to be meaningful, censuses must occur at regular intervals that correspond to biologically relevant time steps. Ideally, you want to census: At the same time each year to capture consistent population states Before or after major demographic events (reproduction, mortality pulses, germination) When individuals are most visible/measurable for accurate data collection For S. lanceolata at Pōhakuloa Training Area, we faced practical constraints. While Hawaii lacks dramatic seasonal variation in temperature, the species does show phenological patterns. We conducted censuses in summer for several pragmatic reasons: Field accessibility: Summer weather allowed consistent access to remote field sites Plant visibility: Woody shrubs maintain their structure year-round, but summer timing captured both vegetative and reproductive activity Logistical feasibility: Research team availability and funding cycles The key is that we censused at the same time each year, allowing us to track year-to-year changes in a standardized way. Our annual time step (summer to summer) captures one full cycle of growth, reproduction, and survival. 256.2.2 Principle 2: Measurements Must Link to Vital Rates Demographic models require data on survival, growth, and reproduction. The measurements we collect must allow us to estimate these vital rates. For S. lanceolata, we structured our census to capture: Size Measurements (for survival and growth): We measured multiple aspects of plant size because size often predicts both survival probability and reproductive output: Foliated length of longest branch (cm): Captures vegetative vigor and photosynthetic capacity Plant height (cm): Structural size metric, indicates establishment and competitive ability Plant height with reproductive stalk (cm): Total height including reproductive structures Stem diameter (cm): Proxy for plant age and woody biomass accumulation Number of branches (n): Architectural complexity, related to resource acquisition For our Integral Projection Model (IPM), we’ll select one primary size variable that best predicts survival, growth, and fecundity. Often this is stem diameter or height, but we measured multiple metrics to explore which is most predictive. Reproductive Measurements (for fecundity): To estimate how many offspring each individual produces, we counted: Number of inflorescences (n): Reproductive effort (number of flowering stalks) Number of flowers and seed heads per stalk: Potential and realized seed production Seeds per fruit: Obtained from literature since counting tiny seeds in the field is impractical These measurements allow us to build a size-fecundity function: larger plants typically produce more inflorescences, more flowers, and ultimately more seeds. 256.2.3 Principle 3: Track Individual Fates Through Time The power of demographic models comes from tracking individuals across census intervals: Did this individual survive from year t to year t+1? If it survived, how much did it grow (change in size)? How many offspring did it produce? This requires marking individual plants (tags, GPS coordinates, maps) so we can relocate them in subsequent years. For S. lanceolata, each plant received a unique identifier allowing us to build a longitudinal dataset of individual life histories. 256.2.4 Principle 4: Document Context and Threats Beyond demographic measurements, we recorded factors that might explain variation in vital rates: Browse damage: Evidence of herbivory (ungulates, insects) Fire history: Burned vs. unburned individuals Disease/stress: Chlorosis, wilting, pathogen damage Proximity to invasive plants: Competition effects These covariates allow us to ask: Does fire reduce survival? Do plants near fountain grass grow more slowly? Does herbivory reduce reproduction? Let’s check out data from our initial collection trip: library(tidyverse) library(httr) # Create data directory if it doesn&#39;t exist if (!dir.exists(&quot;data&quot;)) dir.create(&quot;data&quot;) # Google Drive file ID file_id &lt;- &quot;1WCToCDAuSR_Wd2ScRy9yhwiDGijL47OX&quot; GET( url = paste0( &quot;https://drive.usercontent.google.com/download?id=&quot;, file_id, &quot;&amp;export=download&amp;confirm=t&quot; ), write_disk(&quot;data/silene_census.csv&quot;, overwrite = TRUE) ) ## Response [https://drive.usercontent.google.com/download?id=1WCToCDAuSR_Wd2ScRy9yhwiDGijL47OX&amp;export=download&amp;confirm=t] ## Date: 2026-02-20 19:36 ## Status: 200 ## Content-Type: application/octet-stream ## Size: 5.03 kB ## &lt;ON DISK&gt; /Users/sks379/Desktop/GitHubProjects/DataAnalysis/data/silene_census.csv # Load the data silene &lt;- read_csv(&quot;data/silene_census.csv&quot;, show_col_types = FALSE) |&gt; dplyr::select(-...1) # Remove row number column # Quick summary cat(&quot;Dataset dimensions:&quot;, nrow(silene), &quot;rows x&quot;, ncol(silene), &quot;columns\\n&quot;) ## Dataset dimensions: 100 rows x 11 columns cat(&quot;Number of unique individuals:&quot;, n_distinct(silene$UniqueID), &quot;\\n&quot;) ## Number of unique individuals: 100 cat(&quot;Weed control treatments:&quot;, paste(unique(silene$WeedControl), collapse = &quot;, &quot;), &quot;\\n&quot;) ## Weed control treatments: C, W cat(&quot;Map units:&quot;, paste(sort(unique(silene$MapUnit)), collapse = &quot;, &quot;), &quot;\\n&quot;) ## Map units: 1, 2, 3 cat(&quot;Proportion reproduced:&quot;, mean(silene$Reproduced), &quot;\\n&quot;) ## Proportion reproduced: 0.44 glimpse(silene) ## Rows: 100 ## Columns: 11 ## $ UniqueID &lt;chr&gt; &quot;Bobcat03_1&quot;, &quot;Bobcat03_2&quot;, &quot;Bobcat03_3&quot;, &quot;Bobcat03_4&quot;, &quot;Bobcat03_5&quot;, &quot;Bobcat03_6&quot;, &quot;Bobcat03_7&quot;, &quot;Bobcat03… ## $ WeedControl &lt;chr&gt; &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C&quot;, &quot;C… ## $ MapUnit &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2,… ## $ VegHeight &lt;dbl&gt; 5.4, 42.6, 8.3, 2.5, 31.0, 26.8, 39.1, 18.5, 14.4, 6.8, 18.5, 11.0, 13.8, 19.9, 13.2, 11.0, 12.5, 9.0, 32.3… ## $ VegReproHeight &lt;dbl&gt; 5.4, 56.4, 8.3, 2.5, 31.0, 44.2, 53.6, 18.5, 14.4, 6.8, 18.5, 11.0, 13.8, 19.9, 13.2, 11.0, 12.5, 9.0, 32.3… ## $ StemDiameter &lt;dbl&gt; 0.18, 0.30, 0.12, 0.06, 0.22, 0.19, 0.29, 0.17, 0.24, 0.15, 0.18, 0.12, 0.15, 0.19, 0.17, 0.19, 0.15, 0.13,… ## $ StemNum &lt;dbl&gt; 0, 6, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 6, 0, 4, 0, 0, 0, 0, 6, 4, 2, 2, 2, 0,… ## $ LengthLongestBranchwFoliage &lt;dbl&gt; 3.7, 1.0, 5.8, 1.5, 22.4, 18.0, 24.0, 16.4, 8.8, 4.3, 12.8, 5.1, 8.1, 12.4, 17.7, 14.6, 5.1, 5.2, 18.2, 12.… ## $ InflorNum &lt;dbl&gt; 0, 6, 0, 0, 0, 4, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 26, 0, 21, 0, 0, 0, 0, 16, 18, 13, 4, … ## $ Reproduced &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0,… ## $ TotalReproOutput &lt;dbl&gt; 0, 21, 0, 0, 0, 26, 36, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 64, 58, 0, 49, 0, 0, 0, 0, 33, 40, 40,… 256.2.5 Understanding State Variables in IPMs In an Integral Projection Model (IPM), we need to choose a state variable that describes the “state” of each individual in the population. This variable must: Predict vital rates: Larger/older individuals typically have different survival, growth, and reproduction than smaller/younger ones Be continuously measured: IPMs work with continuous size distributions (unlike matrix models with discrete stages) Be measurable across time: We need to track how individuals change in this variable from year to year Capture biological meaningful variation: The variable should reflect real differences in demographic performance Common state variables in plant IPMs: Stem diameter Plant height Basal area Number of stems/ramets Leaf area Let’s explore our data to identify the best state variable for S. lanceolata. 256.2.6 Explore Potential State Variables # Summary of candidate state variables state_summary &lt;- silene |&gt; summarise( n_plants = n(), # Continuous size measurements mean_stem_diam = mean(StemDiameter, na.rm = TRUE), sd_stem_diam = sd(StemDiameter, na.rm = TRUE), mean_height_veg = mean(VegHeight, na.rm = TRUE), sd_height_veg = sd(VegHeight, na.rm = TRUE), mean_height_repro = mean(VegReproHeight, na.rm = TRUE), sd_height_repro = sd(VegReproHeight, na.rm = TRUE), mean_branch_length = mean(LengthLongestBranchwFoliage, na.rm = TRUE), sd_branch_length = sd(LengthLongestBranchwFoliage, na.rm = TRUE), # Count variables mean_stems = mean(StemNum, na.rm = TRUE), sd_stems = sd(StemNum, na.rm = TRUE), # Reproduction mean_inflor = mean(InflorNum, na.rm = TRUE), sd_inflor = sd(InflorNum, na.rm = TRUE), mean_repro = mean(TotalReproOutput, na.rm = TRUE), sd_repro = sd(TotalReproOutput, na.rm = TRUE), prop_reproduced = mean(Reproduced, na.rm = TRUE) ) knitr::kable(t(state_summary), col.names = &quot;Value&quot;, digits = 2, caption = &quot;Summary statistics for potential state variables&quot;) Table 256.1: Summary statistics for potential state variables Value n_plants 100.00 mean_stem_diam 0.54 sd_stem_diam 0.68 mean_height_veg 49.33 sd_height_veg 43.32 mean_height_repro 56.00 sd_height_repro 49.77 mean_branch_length 12.03 sd_branch_length 7.14 mean_stems 1.29 sd_stems 1.80 mean_inflor 3.98 sd_inflor 6.67 mean_repro 17.86 sd_repro 24.89 prop_reproduced 0.44 # Check for missing data missing_data &lt;- silene |&gt; summarise(across(everything(), ~sum(is.na(.)))) |&gt; pivot_longer(everything(), names_to = &quot;Variable&quot;, values_to = &quot;Missing&quot;) |&gt; filter(Missing &gt; 0) if (nrow(missing_data) &gt; 0) { cat(&quot;\\nMissing data detected:\\n&quot;) print(knitr::kable(missing_data)) } else { cat(&quot;\\n✓ No missing data in size measurements!\\n&quot;) } ## ## ✓ No missing data in size measurements! 256.2.7 Visualize Distribution of Candidate State Variables # Reshape data for plotting state_vars_long &lt;- silene |&gt; dplyr::select( UniqueID, StemDiameter, VegHeight, VegReproHeight, LengthLongestBranchwFoliage, StemNum ) |&gt; tidyr::pivot_longer( cols = -UniqueID, names_to = &quot;variable&quot;, values_to = &quot;value&quot; ) |&gt; dplyr::mutate( variable = dplyr::recode( variable, StemDiameter = &quot;Stem Diameter (cm)&quot;, VegHeight = &quot;Vegetative Height (cm)&quot;, VegReproHeight = &quot;Height with Repro (cm)&quot;, LengthLongestBranchwFoliage = &quot;Longest Branch w/ Foliage (cm)&quot;, StemNum = &quot;Number of Stems&quot; ) ) # Plot distributions ggplot(state_vars_long, aes(x = value)) + geom_histogram(bins = 15, fill = &quot;steelblue&quot;, alpha = 0.7, color = &quot;white&quot;) + facet_wrap(~variable, scales = &quot;free&quot;, ncol = 2) + theme_minimal(base_size = 12) + labs(title = &quot;Distribution of Potential State Variables&quot;, subtitle = &quot;Which variable best captures individual variation in S. lanceolata?&quot;, x = &quot;Measurement value&quot;, y = &quot;Number of individuals&quot;) ### Assess Correlations Between Measurements State variables should ideally be correlated with other size metrics (indicating they capture overall plant size) but also provide unique information. library(corrplot) # Calculate correlation matrix cor_data &lt;- silene |&gt; dplyr::select(StemDiameter, VegHeight, VegReproHeight, LengthLongestBranchwFoliage, StemNum, InflorNum, TotalReproOutput) |&gt; rename( `Stem Diam` = StemDiameter, `Height Veg` = VegHeight, `Height Repro` = VegReproHeight, `Branch Length` = LengthLongestBranchwFoliage, `N Stems` = StemNum, `N Inflor` = InflorNum, `Repro Output` = TotalReproOutput ) cor_matrix &lt;- cor(cor_data, use = &quot;complete.obs&quot;) # Visualize correlations corrplot(cor_matrix, method = &quot;color&quot;, type = &quot;upper&quot;, addCoef.col = &quot;black&quot;, number.cex = 0.7, tl.col = &quot;black&quot;, tl.srt = 45, tl.cex = 0.8, title = &quot;Correlations Between Size and Reproductive Metrics&quot;, mar = c(0, 0, 2, 0)) In many demographic studies, time constraints limit how long we have to identify appropriate state variables, establish a census protocol, and collect sufficient data to parameterize transition matrices. Because survival and growth require observations across multiple years, early stages of a study often rely on variables that can be measured and linked to demographic processes within a single field season. For this reason, I often prioritize selecting state variables that best explain variation in reproductive rates, since reproduction can typically be measured alongside size or trait data in the same year. Below, we use model selection to identify the state variable that best predicts reproductive output. In my experience, the state variable that most strongly explains reproduction often also performs well as a predictor of other vital rates, such as survival and growth, once enough data have been collected to evaluate these relationships. This approach provides a practical and biologically informed starting point for demographic modeling when data are initially limited. 256.2.8 Selecting Our State Variable Recall that we are trying to identify state variables with the following characteristics: Continuously distributed: Not discrete categories, works well with IPM framework Strong biological meaning: Represents accumulated woody growth and age Practical: Easy to measure consistently across years Predictive: Likely correlates with survival (established plants) and reproduction (larger plants) Low measurement error: Less affected by seasonal variation So far, the majority of our variables meet these criteria, but let’s test predictability of vital rates! The ultimate decider! 256.2.8.1 Quantitative Model Comparison with AIC # Fit Poisson GLMs for each candidate state variable # (Poisson is appropriate for count data like reproductive output) model_diameter &lt;- glm(TotalReproOutput ~ StemDiameter, family = poisson, data = silene) model_height &lt;- glm(TotalReproOutput ~ VegHeight, family = poisson, data = silene) model_height_repro &lt;- glm(TotalReproOutput ~ VegReproHeight, family = poisson, data = silene) model_branch &lt;- glm(TotalReproOutput ~ LengthLongestBranchwFoliage, family = poisson, data = silene) model_stems &lt;- glm(TotalReproOutput ~ StemNum, family = poisson, data = silene) # Create comprehensive comparison table aic_comparison &lt;- tibble( State_Variable = c(&quot;Stem Diameter&quot;, &quot;Vegetative Height&quot;, &quot;Height with Repro&quot;, &quot;Branch Length&quot;, &quot;Number of Stems&quot;), AIC = c(AIC(model_diameter), AIC(model_height), AIC(model_height_repro), AIC(model_branch), AIC(model_stems)), df = c(2, 2, 2, 2, 2) # All have 2 parameters (intercept + slope) ) |&gt; arrange(AIC) |&gt; mutate( Delta_AIC = AIC - min(AIC), Relative_Likelihood = exp(-0.5 * Delta_AIC), AIC_Weight = Relative_Likelihood / sum(Relative_Likelihood), Interpretation = case_when( Delta_AIC == 0 ~ &quot;Best model&quot;, Delta_AIC &lt; 2 ~ &quot;Substantial support&quot;, Delta_AIC &lt; 7 ~ &quot;Considerably less support&quot;, TRUE ~ &quot;Essentially no support&quot; ) ) knitr::kable(aic_comparison, digits = 3, caption = &quot;AIC comparison: Which size metric best predicts fecundity?&quot;) Table 256.2: AIC comparison: Which size metric best predicts fecundity? State_Variable AIC df Delta_AIC Relative_Likelihood AIC_Weight Interpretation Height with Repro 1588.305 2 0.000 1 1 Best model Vegetative Height 1876.079 2 287.774 0 0 Essentially no support Number of Stems 2193.372 2 605.067 0 0 Essentially no support Stem Diameter 3057.851 2 1469.546 0 0 Essentially no support Branch Length 3401.991 2 1813.686 0 0 Essentially no support Now, let’s visualize the top relationship! library(patchwork) # Get predictions from the best model for smooth curve pred_data &lt;- tibble( VegReproHeight = seq(min(silene$VegReproHeight, na.rm = TRUE), max(silene$VegReproHeight, na.rm = TRUE), length.out = 100) ) pred_data$predicted &lt;- predict(model_height_repro, newdata = pred_data, type = &quot;response&quot;) # Main scatter plot with model fit p1 &lt;- ggplot(silene, aes(x = VegReproHeight, y = TotalReproOutput)) + geom_point(aes(color = factor(Reproduced)), alpha = 0.6, size = 3) + geom_line(data = pred_data, aes(x = VegReproHeight, y = predicted), color = &quot;coral&quot;, linewidth = 1.5) + scale_color_manual(values = c(&quot;0&quot; = &quot;gray60&quot;, &quot;1&quot; = &quot;steelblue&quot;), labels = c(&quot;No reproduction&quot;, &quot;Reproduced&quot;), name = &quot;&quot;) + theme_minimal(base_size = 12) + labs(title = &quot;Best Predictor: Height with Reproductive Structures&quot;, subtitle = paste0(&quot;AIC = &quot;, round(AIC(model_height_repro), 2), &quot; | AIC Weight = &quot;, round(aic_comparison$AIC_Weight[aic_comparison$State_Variable == &quot;Height with Repro&quot;], 3)), x = &quot;Height with reproductive structures (cm)&quot;, y = &quot;Total reproductive output (flowers + seeds)&quot;) + theme(legend.position = &quot;top&quot;) # Residual plot to check model fit silene_resid &lt;- silene |&gt; mutate( fitted = predict(model_height_repro, type = &quot;response&quot;), residuals = TotalReproOutput - fitted ) p2 &lt;- ggplot(silene_resid, aes(x = fitted, y = residuals)) + geom_point(alpha = 0.6, color = &quot;steelblue&quot;) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;, color = &quot;coral&quot;) + geom_smooth(se = FALSE, color = &quot;gray30&quot;) + theme_minimal(base_size = 12) + labs(title = &quot;Model Diagnostics: Residual Plot&quot;, x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;) p1 / p2 + plot_layout(heights = c(2, 1)) Top Panel: The Size-Fecundity Relationship Let’s check out the figure and think about the implications: The Zero-Inflation Problem: Notice the large cluster of gray points at reproductive output = 0. These are plants that didn’t reproduce at all (56% of individuals!). This creates a challenge for Poisson models, which don’t naturally handle “excess zeros.” Clear Size Threshold: Among reproducing plants (blue), there’s a strong positive relationship - taller plants with larger reproductive structures produce more seeds. But notice that ALL small plants (&lt;~40 cm) failed to reproduce. This suggests a reproductive size threshold. Increasing Variance: As height increases, the spread of blue points gets wider. Small reproductive plants show relatively consistent output (~10-30 seeds), while large plants vary dramatically (20-100 seeds). This heteroscedasticity (unequal variance) can be an issue for simple linear models. The Perfect Fit “Problem”: The coral line fits the reproducing plants beautifully, BUT this creates a conceptual issue for IPMs: VegReproHeight includes the reproductive stalks themselves! This is somewhat circular - we’re using reproductive structures to predict reproduction. Bottom Panel: Model Diagnostics (Critical!) Let’s check out the figure and think about the implications: Non-Random Residual Pattern: The residuals should be randomly scattered around zero (the dashed orange line). Instead, notice: Left side (fitted values 0-25): Residuals are tightly clustered near zero (all those non-reproductive plants) Middle (fitted values 25-50): Residuals are nicely random ✓ Right side (fitted values &gt;75): The smooth line curves downward, indicating the model underpredicts reproductive output for the largest plants The Outlier: See that point at the far right with residual ≈ -75? That’s a very large plant that produced MUCH less than expected. What happened? (Disease? Herbivory? Late-season measurement?) What This Means for Model Selection: Let’s think this through: Statistically: VegReproHeight has the best AIC, BUT the residual pattern suggests the Poisson model isn’t perfect. Biologically: This relationship makes sense - plants invest in tall reproductive stalks to display flowers and disperse seeds. A classic (and very common) way to handle “zero inflation” in plant fecundity when building IPMs is to use a two‐part (hurdle) model. In this approach, we first model whether an individual reproduces at all (i.e., produces any flowers/fruits/seeds) using a binomial model. Then, conditional on reproduction occurring, we model the amount of reproductive output (e.g., number of fruits or seeds) using a count model such as a Poisson (or often a negative binomial if counts are overdispersed). This is often more biologically realistic because it separates two distinct processes: (1) the decision/ability to reproduce and (2) how much is produced once reproduction happens. A zero-inflated Poisson (ZIP) or zero-inflated negative binomial (ZINB) also addresses excess zeros, but it assumes the zeros come from two sources: 1. Structural zeros: individuals in a “certain zero” state (they cannot reproduce / are not eligible) 2. Sampling zeros: individuals who are “eligible,” but still produce zero by chance under the count distribution Thus, in the hurdle model, all zeros are handled by the binomial part; the count model is fit to positive counts only. In the zero-inflated model, zeros can come from either the inflation process or the count process. The hurdle (binomial + conditional count) approach aligns well with plant reproductive biology because: Reproduction often reflects a threshold-like process (size/condition must be sufficient to flower/fruit). Once reproducing, output typically scales with size and resources. It yields two interpretable functions for the IPM: probability of reproduction as a function of size expected fecundity given reproduction as a function of size and, bonus, it’s also often simpler to implement and communicate in an IPM context Let’s check this out: # Fit logistic regression for each candidate state variable # Response: Reproduced (0 = no, 1 = yes) logit_diameter &lt;- glm(Reproduced ~ VegReproHeight, family = binomial, data = silene) logit_height &lt;- glm(Reproduced ~ VegHeight, family = binomial, data = silene) logit_height_repro &lt;- glm(Reproduced ~ VegReproHeight, family = binomial, data = silene) logit_branch &lt;- glm(Reproduced ~ LengthLongestBranchwFoliage, family = binomial, data = silene) logit_stems &lt;- glm(Reproduced ~ StemNum, family = binomial, data = silene) # Compare models with AIC aic_reproduction &lt;- tibble( State_Variable = c(&quot;Stem Diameter&quot;, &quot;Vegetative Height&quot;, &quot;Height with Repro&quot;, &quot;Branch Length&quot;, &quot;Number of Stems&quot;), AIC = c(AIC(logit_diameter), AIC(logit_height), AIC(logit_height_repro), AIC(logit_branch), AIC(logit_stems)) ) |&gt; arrange(AIC) |&gt; mutate( Delta_AIC = AIC - min(AIC), AIC_Weight = exp(-0.5 * Delta_AIC) / sum(exp(-0.5 * Delta_AIC)) ) knitr::kable(aic_reproduction, digits = 3, caption = &quot;Step 1: Which size metric best predicts probability of reproduction?&quot;) Table 256.3: Step 1: Which size metric best predicts probability of reproduction? State_Variable AIC Delta_AIC AIC_Weight Stem Diameter 38.661 0.000 0.497 Height with Repro 38.661 0.000 0.497 Number of Stems 48.159 9.498 0.004 Vegetative Height 50.583 11.923 0.001 Branch Length 129.301 90.640 0.000 # Filter to plants that reproduced silene_reproductive &lt;- silene |&gt; filter(Reproduced == 1) cat(&quot;Reproductive plants:&quot;, nrow(silene_reproductive), &quot;out of&quot;, nrow(silene), paste0(&quot;(&quot;, round(100 * nrow(silene_reproductive)/nrow(silene), 1), &quot;%)\\n&quot;)) ## Reproductive plants: 44 out of 100 (44%) # Fit Poisson regression for seed production (reproductive plants only) pois_diameter &lt;- glm(TotalReproOutput ~ VegReproHeight, family = poisson, data = silene_reproductive) pois_height &lt;- glm(TotalReproOutput ~ VegHeight, family = poisson, data = silene_reproductive) pois_height_repro &lt;- glm(TotalReproOutput ~ VegReproHeight, family = poisson, data = silene_reproductive) pois_branch &lt;- glm(TotalReproOutput ~ LengthLongestBranchwFoliage, family = poisson, data = silene_reproductive) pois_stems &lt;- glm(TotalReproOutput ~ StemNum, family = poisson, data = silene_reproductive) # Compare models aic_seed_production &lt;- tibble( State_Variable = c(&quot;Stem Diameter&quot;, &quot;Vegetative Height&quot;, &quot;Height with Repro&quot;, &quot;Branch Length&quot;, &quot;Number of Stems&quot;), AIC = c(AIC(pois_diameter), AIC(pois_height), AIC(pois_height_repro), AIC(pois_branch), AIC(pois_stems)) ) |&gt; arrange(AIC) |&gt; mutate( Delta_AIC = AIC - min(AIC), AIC_Weight = exp(-0.5 * Delta_AIC) / sum(exp(-0.5 * Delta_AIC)) ) knitr::kable(aic_seed_production, digits = 3, caption = &quot;Step 2: Which size metric best predicts seed count among reproducers?&quot;) Table 256.3: Step 2: Which size metric best predicts seed count among reproducers? State_Variable AIC Delta_AIC AIC_Weight Number of Stems 726.427 0.000 0.813 Stem Diameter 730.757 4.331 0.093 Height with Repro 730.757 4.331 0.093 Vegetative Height 747.756 21.329 0.000 Branch Length 798.683 72.256 0.000 # Combine the two tables combined_aic &lt;- aic_reproduction |&gt; dplyr::rename( AIC_Reproduction = AIC, Weight_Reproduction = AIC_Weight ) |&gt; dplyr::select( State_Variable, AIC_Reproduction, Weight_Reproduction ) |&gt; dplyr::left_join( aic_seed_production |&gt; dplyr::rename( AIC_SeedCount = AIC, Weight_SeedCount = AIC_Weight ) |&gt; dplyr::select( State_Variable, AIC_SeedCount, Weight_SeedCount ), by = &quot;State_Variable&quot; ) |&gt; dplyr::mutate( Combined_Weight = Weight_Reproduction * Weight_SeedCount, Rank_Reproduction = rank(AIC_Reproduction), Rank_SeedCount = rank(AIC_SeedCount), Average_Rank = (Rank_Reproduction + Rank_SeedCount) / 2 ) |&gt; dplyr::arrange(Average_Rank) knitr::kable(combined_aic, digits = 3, caption = &quot;Combined performance: Which variable is best at BOTH tasks?&quot;) Table 256.3: Combined performance: Which variable is best at BOTH tasks? State_Variable AIC_Reproduction Weight_Reproduction AIC_SeedCount Weight_SeedCount Combined_Weight Rank_Reproduction Rank_SeedCount Average_Rank Stem Diameter 38.661 0.497 730.757 0.093 0.046 1.5 2.5 2 Height with Repro 38.661 0.497 730.757 0.093 0.046 1.5 2.5 2 Number of Stems 48.159 0.004 726.427 0.813 0.004 3.0 1.0 2 Vegetative Height 50.583 0.001 747.756 0.000 0.000 4.0 4.0 4 Branch Length 129.301 0.000 798.683 0.000 0.000 5.0 5.0 5 library(patchwork) # Use the best model (let&#39;s assume it&#39;s stem diameter for example) # Create prediction data pred_data &lt;- tibble( VegReproHeight = seq(min(silene$VegReproHeight, na.rm = TRUE), max(silene$VegReproHeight, na.rm = TRUE), length.out = 100) ) # Predict probability of reproduction pred_data$prob_reproduce &lt;- predict(logit_diameter, newdata = pred_data, type = &quot;response&quot;) # Predict seed count (on log scale, then back-transform) pred_data$expected_seeds &lt;- predict(pois_diameter, newdata = pred_data, type = &quot;response&quot;) # Combined expected output pred_data$expected_total &lt;- pred_data$prob_reproduce * pred_data$expected_seeds # Panel 1: Probability of Reproduction p1 &lt;- ggplot(silene, aes(x = VegReproHeight, y = Reproduced)) + geom_point(alpha = 0.4, size = 2, position = position_jitter(height = 0.02, width = 0)) + geom_line(data = pred_data, aes(x = VegReproHeight, y = prob_reproduce), color = &quot;coral&quot;, linewidth = 1.5) + theme_minimal(base_size = 12) + labs(title = &quot;Step 1: Probability of Reproducing&quot;, subtitle = &quot;Logistic regression on all plants&quot;, x = &quot;Stem diameter (cm)&quot;, y = &quot;P(Reproduction)&quot;) + ylim(-0.05, 1.05) # Panel 2: Seeds IF reproducing p2 &lt;- ggplot(silene_reproductive, aes(x = VegReproHeight, y = TotalReproOutput)) + geom_point(alpha = 0.6, size = 2, color = &quot;steelblue&quot;) + geom_line(data = pred_data, aes(x = VegReproHeight, y = expected_seeds), color = &quot;coral&quot;, linewidth = 1.5) + theme_minimal(base_size = 12) + labs(title = &quot;Step 2: Seeds IF Reproducing&quot;, subtitle = &quot;Poisson regression on reproductive plants only&quot;, x = &quot;Stem diameter (cm)&quot;, y = &quot;Seed count (given reproduction)&quot;) # Panel 3: Combined expected reproduction p3 &lt;- ggplot(silene, aes(x = VegReproHeight, y = TotalReproOutput)) + geom_point(aes(color = factor(Reproduced)), alpha = 0.6, size = 2) + geom_line(data = pred_data, aes(x = VegReproHeight, y = expected_total), color = &quot;coral&quot;, linewidth = 1.5) + scale_color_manual(values = c(&quot;0&quot; = &quot;gray60&quot;, &quot;1&quot; = &quot;steelblue&quot;), labels = c(&quot;Did not reproduce&quot;, &quot;Reproduced&quot;), name = &quot;&quot;) + theme_minimal(base_size = 12) + labs(title = &quot;Combined: Expected Reproduction&quot;, subtitle = &quot;E[seeds] = P(reproduce) × E[seeds | reproduce]&quot;, x = &quot;Stem diameter (cm)&quot;, y = &quot;Expected reproductive output&quot;) + theme(legend.position = &quot;top&quot;) p1 + p2 + p3 256.2.8.2 Why Use Only One State Variable? The Multi-Dimensional Trade-off State variables are rarely perfect predictors of vital rates. As a result, it is tempting to assume that including additional state variables will always improve demographic models. In practice, however, adding multiple state variables introduces substantial challenges. First, it increases the amount of field data required, often extending monitoring timelines and complicating census protocols. Multi-variable models can also be less intuitive for land managers, making it harder to interpret key drivers of population dynamics or implement streamlined monitoring programs. Second—and more importantly—adding state variables dramatically increases model complexity through what is known as the dimensionality problem, which we describe in the following section. 256.2.9 The dimensionality problem An Integral Projection Model (IPM) works by tracking how individuals move through a continuous distribution of sizes over time. Mathematically, this involves solving an integral equation: \\[ n(z&#39;, t+1) = \\int \\left[ s(z)\\, g(z&#39;, z) + f(z&#39;, z) \\right] n(z, t)\\, dz \\] where: \\(z\\) is the state variable (e.g., size) at time \\(t\\) \\(z&#39;\\) is the state variable at time \\(t+1\\) \\(n(z, t)\\) is the population density of individuals of size \\(z\\) at time \\(t\\) \\(s(z)\\) is the probability of survival for individuals of size \\(z\\) \\(g(z&#39;, z)\\) describes growth from size \\(z\\) to size \\(z&#39;\\) \\(f(z&#39;, z)\\) describes reproductive contributions from individuals of size \\(z\\) to size \\(z&#39;\\) The integral is evaluated over all possible values of \\(z\\), meaning that contributions from individuals of every size at time \\(t\\) are summed to determine the population distribution at time \\(t+1\\). With ONE state variable (e.g., stem diameter): We discretize the size range into ~100-200 cells (bins) This creates a 100×100 projection matrix (manageable!) Computational time: seconds With TWO state variables (e.g., diameter AND height): We need a 2-dimensional grid: diameter × height With 100 bins each → 100×100 = 10,000 cells Projection matrix becomes 10,000×10,000 Computational time: minutes to hours Data requirements: Need MANY more individuals to fill that grid With THREE state variables (diameter, height, number of stems): 100×100×100 = 1,000,000 cells Matrix size: 1,000,000×1,000,000 Computational time: potentially days Data requirements: Tens of thousands of observations Most cells would be EMPTY This is called the “curse of dimensionality” - complexity grows exponentially with each added dimension. Especially since IPMs are often used to model rare plants characterized by low population sizes, the data requirements imposed by the inclusion of even a second state variable, renders this approach intractable. On rare occasions, it might be worth it to include to an additional state variable, if the two variables two variables capture different aspects of performance and they are uncorrelated and if you have HUGE datasets. Onward! With total height selected as our state variable, we’re ready to build the IPM vital rate functions! 256.3 Packages and approaches to creating IPMs There are multiple ways to build and analyze Integral Projection Models (IPMs) in R. All approaches are based on the same underlying demographic theory, but they differ in how much structure is imposed, how much code must be written by the user, and how transparent the resulting model is. Here, we briefly introduce three commonly used approaches: fully automated IPM frameworks, semi-structured IPM tools, and hand-coded IPMs. In the sections that follow, we use these approaches to illustrate model fitting and selection using the same biological questions. 256.3.1 ipmr: a modern, flexible IPM framework The ipmr package provides a modern, modular framework for building IPMs in R. Rather than fitting the IPM as a single object, ipmr encourages users to specify each vital-rate submodel (survival, growth, reproduction) explicitly and then combine them into an IPM kernel. This approach emphasizes transparency and flexibility. Model components can be easily modified, compared, and extended, making ipmr particularly well suited for model selection, uncertainty analysis, and advanced demographic metrics such as sensitivities, elasticities, and transient dynamics. Because ipmr requires users to think carefully about each model component, it has a steeper learning curve than more automated approaches, but it provides the most control and clarity for research-grade analyses. 256.3.2 IPMpack: a more automated IPM workflow The IPMpack package offers a more automated approach to building IPMs, handling many steps of kernel construction and numerical integration internally. Users specify vital-rate models and size bounds, and the package generates the IPM and associated outputs. This approach can be efficient and relatively easy to implement for standard one-dimensional IPMs. However, the abstraction can make it harder to see how individual model components contribute to population dynamics, and flexibility is more limited when extending models or exploring alternative formulations. 256.3.3 Hand-coded IPMs: maximum transparency A third approach is to build IPMs entirely by hand using base R code. In this workflow, vital-rate models are fit using standard statistical tools (e.g., glm, lm, gam), and the IPM kernel is constructed explicitly using numerical integration and matrix operations. Hand-coded IPMs provide complete transparency and conceptual clarity, making them valuable for understanding how IPMs work at a fundamental level. However, they require substantially more code, are more prone to implementation errors, and can be difficult to maintain or extend. Now let’s create the linear models that make up the IPM kernel! Through, we can work through the process using each approach! 256.4 Building an IPM: Structuring linear models, variable transformations, and dealing with outliers In this section, we build the statistical models that form the core of an Integral Projection Model (IPM). Regardless of whether an IPM is implemented using a package such as ipmr, IPMpack, or coded by hand, the underlying structure is the same: IPMs are constructed from a set of regression models that describe how individual state variables influence survival, growth, and reproduction. Before assembling an IPM kernel, it is therefore essential to think carefully about (1) which vital rates will be modeled, (2) how state variables enter those models, and (3) how issues such as zero inflation, outliers, and missing values are handled. First, let’s go ahead and pull in our transition dataset. A transition dataset consists of two years of matched data, in which the fate of each individual from one year to the next is documented. Notice the structure of the dataset! This dataframe tracks the fate of all individuals in the population from time 1 to time 2. The ‘id’ column corresponds with the tag number for each plant. The ‘size’ column contains sizes (in this case heights) of individuals in time 1, while ‘sizeNext’ indicates the size at time 2. In the ‘surv’ column, the fate of the individual at time 2 is indicated by either a 0 (died) or 1 (survived). For individuals that died, we place an ‘NA’ in the sizeNext column, since that individual didn’t exist that year! Reproduction is always the most complex part of demographic modeling for plant species. Often, a reproductive cycle includes cryptic stages (seeds!) that are hard to track, may include vegetative and sexual reproductive components, and may involve several processes (like flower production and seed production) that may be important to examine to understand why population growth rates vary! For Silene lanceolata, we don’t know enough about the seed bank to include this in our models. Seed dynamics are difficult to track across multiple sites and years. For this reason, we have built the models such that seeds produced in year 1 has the potential to germinate in year 2 or perish. Luckily, ignoring the seed bank for species with high rates of seed production has negligible affects on population growth. A new seedling won’t have a size, survival or reproductive values in year 1 (NAs are added to this column), but will have a measurement in the ‘sizeNext’ column - using this pattern in the data is how we tell R to pull data on seedlings, so it is important to have this information correctly entered in the dataframe. We’ve broken reproduction into two components, fec1 and fec2. If an individual reproduced, we mark a ‘1’ in fec1, if the individual didn’t reproduce, then we add a ‘0’ to the fec1 column. For those individuals that did reproduce, the fec2 column contains the number of seeds produced. Again notice that if an individual didn’t reproduce, we place an ‘NA’ in the fec2 column. Finally, we’ve documented other observations for each individual that we think might explain population growth rates, like whether a plant has been browsed by deer or is in competition with other vegetation. IPMs are only as good as the linear models that comprise them! As a critical first step in analyses, we have to take a look at the relationship between our state variable and growth, reproduction and survival. We want to be sure to address any issues with normality and unequal variance. First, examine variance; if variation depends on the state variable, include this in the model (we’ll do this in a minute). It is fairly common in plants to see, for instance, that growth rates are more variable in larger individuals. For the ipmr and handcoding, column names could essentially be anything, though it is good practice to name columns in a way that is intuitive for your analyses. The IPMpack syntax is stricter, so we will default to using those naming conventions. Let’s pull in our data and look at its structure. library(tidyverse) library(httr) # Download transition data file_id &lt;- &quot;1V0qWq0SlA9rWzyraZaOFdWDVDKQyiu53&quot; GET( url = paste0(&quot;https://drive.usercontent.google.com/download?id=&quot;, file_id, &quot;&amp;export=download&amp;confirm=t&quot;), write_disk(&quot;data/silene_transitions.csv&quot;, overwrite = TRUE) ) ## Response [https://drive.usercontent.google.com/download?id=1V0qWq0SlA9rWzyraZaOFdWDVDKQyiu53&amp;export=download&amp;confirm=t] ## Date: 2026-02-20 19:36 ## Status: 200 ## Content-Type: application/octet-stream ## Size: 18.2 kB ## &lt;ON DISK&gt; /Users/sks379/Desktop/GitHubProjects/DataAnalysis/data/silene_transitions.csv # Load the data transitions &lt;- read_csv(&quot;data/silene_transitions.csv&quot;, show_col_types = FALSE) # Initial exploration cat(&quot;Dataset dimensions:&quot;, nrow(transitions), &quot;rows x&quot;, ncol(transitions), &quot;columns\\n\\n&quot;) ## Dataset dimensions: 392 rows x 14 columns glimpse(transitions) ## Rows: 392 ## Columns: 14 ## $ id &lt;dbl&gt; 7, 34, 2, 46, 31, 23, 36, 32, 1, 41, 6, 23, 35, 22, 32, 26, 4, 1, 40, 11, 42, 21, 41, 24, 42, 49, 48, 45,… ## $ size &lt;dbl&gt; 110.5, 121.6, 56.4, 97.5, 90.0, 144.5, 103.4, 90.2, 76.2, 106.8, 94.8, 109.7, 150.4, 136.1, 131.5, 138.9,… ## $ surv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ sizeNext &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 26.7, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, 12.2, 1… ## $ fec1 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, … ## $ fec2 &lt;dbl&gt; 3600, 10080, 11340, 12150, 13500, 15120, 15750, 18000, 29700, 31500, 72900, 93150, 98010, 133650, 233280,… ## $ Exclude &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … ## $ Site &lt;chr&gt; &quot;KalawamounaE&quot;, &quot;Bobcat03&quot;, &quot;Bobcat03&quot;, &quot;KalawamounaE&quot;, &quot;KalawamounaE&quot;, &quot;KalawamounaE&quot;, &quot;KalawamounaE&quot;, &quot;… ## $ `Herbivory (Y/N)_2024` &lt;chr&gt; NA, NA, NA, &quot;N&quot;, NA, NA, NA, NA, &quot;N&quot;, &quot;N&quot;, NA, NA, NA, NA, NA, NA, &quot;N&quot;, NA, NA, NA, NA, NA, NA, NA, &quot;N&quot;, … ## $ `Herbivory Agent_2024` &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ `Disease (Y/N)_2024` &lt;chr&gt; &quot;N&quot;, &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, NA, &quot;N&quot;, &quot;N&quot;, &quot;N&quot;, NA,… ## $ `Description of Disease_2024` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ NotesYear1 &lt;chr&gt; NA, NA, NA, &quot;most leaves are gone&quot;, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N… ## $ NotesYear2 &lt;chr&gt; &quot;NO&quot;, &quot;NO&quot;, NA, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, NA, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, &quot;NO&quot;, NA, &quot;NO… cat(&quot;\\n=== Column names ===\\n&quot;) ## ## === Column names === print(names(transitions)) ## [1] &quot;id&quot; &quot;size&quot; &quot;surv&quot; &quot;sizeNext&quot; ## [5] &quot;fec1&quot; &quot;fec2&quot; &quot;Exclude&quot; &quot;Site&quot; ## [9] &quot;Herbivory (Y/N)_2024&quot; &quot;Herbivory Agent_2024&quot; &quot;Disease (Y/N)_2024&quot; &quot;Description of Disease_2024&quot; ## [13] &quot;NotesYear1&quot; &quot;NotesYear2&quot; cat(&quot;\\n=== Summary statistics ===\\n&quot;) ## ## === Summary statistics === print(summary(transitions)) ## id size surv sizeNext fec1 fec2 Exclude Site ## Min. : 1.00 Min. : 2.50 Min. :0.000 Min. : 0.700 Min. :0.0000 Min. : 90 Min. :0.00000 Length:392 ## 1st Qu.: 39.75 1st Qu.: 11.00 1st Qu.:0.000 1st Qu.: 3.000 1st Qu.:0.0000 1st Qu.: 10800 1st Qu.:0.00000 Class :character ## Median :111.50 Median : 32.30 Median :1.000 Median : 4.300 Median :0.0000 Median : 30240 Median :0.00000 Mode :character ## Mean :125.44 Mean : 55.29 Mean :0.596 Mean : 6.167 Mean :0.4343 Mean : 130305 Mean :0.04337 ## 3rd Qu.:190.25 3rd Qu.:103.30 3rd Qu.:1.000 3rd Qu.: 6.525 3rd Qu.:1.0000 3rd Qu.: 127665 3rd Qu.:0.00000 ## Max. :479.00 Max. :160.20 Max. :1.000 Max. :44.000 Max. :1.0000 Max. :1086120 Max. :1.00000 ## NA&#39;s :293 NA&#39;s :293 NA&#39;s :40 NA&#39;s :293 NA&#39;s :349 ## Herbivory (Y/N)_2024 Herbivory Agent_2024 Disease (Y/N)_2024 Description of Disease_2024 NotesYear1 NotesYear2 ## Length:392 Length:392 Length:392 Mode:logical Length:392 Length:392 ## Class :character Class :character Class :character NA&#39;s:392 Class :character Class :character ## Mode :character Mode :character Mode :character Mode :character Mode :character ## ## ## ## cat(&quot;\\n=== First few rows ===\\n&quot;) ## ## === First few rows === print(head(transitions, 10)) ## # A tibble: 10 × 14 ## id size surv sizeNext fec1 fec2 Exclude Site `Herbivory (Y/N)_2024` `Herbivory Agent_2024` `Disease (Y/N)_2024` Description of Disea…¹ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 7 110. 0 NA 1 3600 0 Kala… &lt;NA&gt; &lt;NA&gt; N NA ## 2 34 122. 0 NA 1 10080 0 Bobc… &lt;NA&gt; &lt;NA&gt; N NA ## 3 2 56.4 0 NA 1 11340 0 Bobc… &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## 4 46 97.5 0 NA 1 12150 0 Kala… N &lt;NA&gt; N NA ## 5 31 90 0 NA 1 13500 0 Kala… &lt;NA&gt; &lt;NA&gt; N NA ## 6 23 144. 0 NA 1 15120 0 Kala… &lt;NA&gt; &lt;NA&gt; N NA ## 7 36 103. 0 NA 1 15750 0 Kala… &lt;NA&gt; &lt;NA&gt; N NA ## 8 32 90.2 0 NA 1 18000 0 Kala… &lt;NA&gt; &lt;NA&gt; N NA ## 9 1 76.2 1 26.7 1 29700 0 Kala… N &lt;NA&gt; N NA ## 10 41 107. 0 NA 1 31500 0 Kala… N &lt;NA&gt; N NA ## # ℹ abbreviated name: ¹​`Description of Disease_2024` ## # ℹ 2 more variables: NotesYear1 &lt;chr&gt;, NotesYear2 &lt;chr&gt; # I always include an Exclude column to allow folks entering or collecting data to identify a priori really weird stuff to exclude! final_data &lt;- transitions |&gt; dplyr::filter(Exclude == 0) Key features: survival (surv) coded as a binary, 0 = dead, 1 = alive size in year one size, continuous variable sizeNext, year two size, continuous variable fec1 = reproduced, yes or no, coded as 0 for no, and 1 for yes fec2 two number of seeds produced, estimated from floral counts, rounded to the nearest whole number includes any important information that could explain what happened to the plants (where they eaten by deer? where were they located?) includes an ID number new seedlings indicated by not having a size year 1 (coded as NA), survival information (NA), reproduction (NA), and a sizeNext entry NA indicate missing data, and 0s are true zero A column labeled ‘Exclude’ used to mark observations with unusual circumstances (e.g., severe damage, measurement error, or other anomalies) that would otherwise distort model fitting. Rather than deleting these observations permanently, retaining them in the dataset and filtering them explicitly allows decisions about inclusion to remain transparent and reproducible. Using this dataset, we will build four core components of the IPM: Survival model Probability of surviving from time \\(t\\) to \\(t+1\\) as a function of size. Growth model Expected size at time \\(t+1\\) given size at time \\(t\\), conditional on survival. Probability of reproduction Probability that an individual reproduces as a function of size. Reproductive output Number of offspring produced, conditional on reproduction occurring. 256.4.1 Variable transformations and scale Demographic rates often exhibit nonlinear relationships with size and strong right-skew, particularly for growth and fecundity. As a result, transformations are commonly applied to improve model fit and biological realism. In this example, we will explore transformations such as: log or square-root transformations of size log transformations of reproductive output centering and scaling of predictors for numerical stability Rather than assuming a single transformation a priori, we will use model selection to evaluate competing formulations and identify those that best explain variation in each vital rate. 256.4.2 Dealing with zeros, missing values, and outliers This dataset illustrates several common challenges in demographic data: Survival and reproduction include many zeros Growth is undefined for individuals that did not survive Reproductive output spans several orders of magnitude Some individuals are flagged for exclusion We address these issues explicitly by: Modeling survival and reproduction as binomial processes Conditioning growth models on survival Using two-stage (hurdle) models for fecundity Excluding flagged individuals prior to model fitting These steps ensure that each statistical model aligns with the biological process it represents. 256.4.3 Parallel IPM workflows Importantly, the statistical models described above are identical across IPM implementations. What differs among approaches is how these models are incorporated into a population projection framework. In the sections that follow, we use the same fitted vital-rate models to construct IPMs using three approaches: ipmr — a modular, transparent framework that explicitly links each model component to the IPM kernel. IPMpack — a more automated approach that streamlines kernel construction. Hand-coded IPMs — a fully explicit implementation that reveals the underlying mechanics of numerical integration and matrix approximation. By working through each approach in parallel, we emphasize that differences among packages reflect differences in software design, not differences in demographic theory. We begin by fitting and comparing candidate models for each vital rate, starting with growth. At each step, we evaluate model fit, interpret parameter estimates, and discuss how these choices influence the resulting IPM. 256.4.4 Growth transitions I typically start by modeling growth. Why? Growth is typically modeled as a continuous relationship between size at time \\(t\\) and size at time \\(t+1\\). Extreme deviations from this relationship—such as implausibly large shrinks or jumps—can strongly influence parameter estimates and propagate unrealistic behavior throughout the IPM kernel. In contrast, extreme reproductive values often reflect genuine biological processes and should be treated with caution rather than automatically excluded. Demographic data often contain extreme values, but not all extremes should be treated as outliers. In particular, reproductive output can exhibit large variation that is biologically real (e.g., mast events, episodic flowering), whereas extreme values in growth are more likely to reflect measurement error or data recording issues. For this reason, we focus first on identifying and handling outliers in the growth model, rather than applying blanket outlier removal across all vital-rate models. We begin by fitting a preliminary growth model using all available, non-excluded data. Remember, that we will only work with individuals that survived (otherwise there is no growth!) growth_data &lt;- final_data |&gt; dplyr::filter(surv == 1, !is.na(sizeNext)) m_growth_lin &lt;- lm(sizeNext ~ size, data = growth_data) m_growth_log &lt;- lm(log(sizeNext) ~ log(size), data = growth_data) AIC(m_growth_lin, m_growth_log) ## df AIC ## m_growth_lin 3 430.8275 ## m_growth_log 3 122.3578 Great! The lower AIC value wins and we see that we need to log transform the data. Now lets look at model fit (linear vs polynomial). m_growth_log_lin &lt;- lm(log(sizeNext) ~ log(size), data = growth_data) m_growth_log_poly &lt;- lm(log(sizeNext) ~ poly(log(size), 2, raw = TRUE), data = growth_data) AIC(m_growth_log_lin, m_growth_log_poly) ## df AIC ## m_growth_log_lin 3 122.3578 ## m_growth_log_poly 4 120.1388 The polynomial model is best! But wait, before we proceed - let’s check out the fit. This is critical for two reasons. First, we want to identify and remove any outliers. Remember, these outliers are more likely to be ‘true’ outliers (mis-ided individuals, measurement error). Secondly, sometimes higher order models produce biological unrealistic fits, especially when sample size is low (i.e., odd peaks and valleys for survival). When this occurs, it is typically better to revert to the simpler model! Let’s take a look at both fits. newdat &lt;- data.frame( size = seq(min(growth_data$size), max(growth_data$size), length.out = 200) ) newdat$lin &lt;- exp(predict(m_growth_log_lin, newdat)) newdat$poly &lt;- exp(predict(m_growth_log_poly, newdat)) library(ggplot2) ggplot(growth_data, aes(size, sizeNext)) + geom_point(alpha = 0.4) + geom_line(data = newdat, aes(size, lin, color = &quot;Linear&quot;), linewidth = 1) + geom_line(data = newdat, aes(size, poly, color = &quot;Quadratic&quot;), linewidth = 1) + scale_x_log10() + scale_y_log10() + labs( x = &quot;Size at time t&quot;, y = &quot;Size at time t+1&quot;, color = &quot;Model&quot; ) + theme_minimal() Interesting! The polynomial fit indicates that growth decreases at larger sizes, and indication of aging or some biophysical limit on growth at larger sizes. This seems biologically defensible and from a modeling standpoint (lower AIC) defensible. Let’s keep the polynomial fit! 256.4.4.1 Outlier removal We identify growth outliers using standardized residuals from the fitted growth model, flagging extreme deviations for biological inspection rather than automatic removal. Only observations that are clearly inconsistent with plausible growth are excluded, and the model is refit to ensure stable kernel behavior. m_growth_quad &lt;- lm( log(sizeNext) ~ poly(log(size), 2, raw = TRUE), data = growth_data ) # Extract residuals growth_data &lt;- growth_data |&gt; dplyr::mutate( fitted = fitted(m_growth_quad), resid = residuals(m_growth_quad), std_res = rstandard(m_growth_quad) ) # Visual residuals library(ggplot2) ggplot(growth_data, aes(log(size), std_res)) + geom_point(alpha = 0.6) + geom_hline(yintercept = c(-3, 3), linetype = &quot;dashed&quot;, color = &quot;red&quot;) + labs( x = &quot;log(size at time t)&quot;, y = &quot;Standardized residual&quot; ) + theme_minimal() # Apply ±3 is a flagging threshold, not an automatic deletion rule growth_data &lt;- growth_data |&gt; dplyr::mutate( flag_outlier = abs(std_res) &gt; 3 ) Here, we don’t see any significant outliers! Awesome! Let take a different look: # Evaluating data for IPM construction using log(size) growth_log &lt;- lm(log(sizeNext) ~ log(size), na.action = na.omit, data = final_data) resid_log &lt;- residuals(growth_log) # Normality test shapiro.test(resid_log) ## ## Shapiro-Wilk normality test ## ## data: resid_log ## W = 0.97379, p-value = 0.2316 # Residuals vs fitted plot(fitted(growth_log), resid_log, xlab = &quot;Fitted values (log sizeNext)&quot;, ylab = &quot;Residuals&quot;, main = &quot;Residuals vs Fitted (log–log model)&quot;) abline(h = 0, col = &quot;red&quot;) id.n &lt;- nrow(final_data) qqPlot(growth_log, distribution = &quot;norm&quot;, id.method = &quot;y&quot;, id.cex = 0.6, id.n = id.n, id.col = &quot;blue&quot;, id.location = &quot;ab&quot;) ## [1] 72 81 This figure is showing a scatter plot of residuals on the y axis and fitted values (estimated responses) on the x axis. We use these scatterplots to detect non-linearity, unequal error variances, and outliers. The residuals should “bounce randomly” around the 0 line, indicating that the assumption that the relationship is linear is reasonable. You also show see that the residuals roughly form a “horizontal band” around the 0 line, indicating that the variances of the error terms are equal. Finally, we looking for a dataset in which no one residual “stands out” from the basic random pattern of residuals. Let’s look at these residuals another way and use label the outliers so that we can remove them if necessary! A quantile-quantile plot, often abbreviated as Q-Q plot, is a graphical tool used to assess whether a dataset follows a particular theoretical distribution, such as the normal distribution. It compares the quantiles of the observed data against the quantiles of the expected theoretical distribution. Here’s how to interpret a Q-Q plot: The x-axis of the Q-Q plot represents the theoretical quantiles from a specified distribution (e.g., the normal distribution). The y-axis represents the quantiles of the observed data. If the points on the Q-Q plot fall approximately along a straight line, it suggests that the data follows the theoretical distribution. Deviations from the straight line indicate departures from the assumed distribution. If points are above the line, it suggests that the observed values are higher than expected for that quantile. If points are below the line, it suggests that the observed values are lower than expected for that quantile. The ends of the Q-Q plot are often of particular interest. Deviations in the tails can indicate differences in tail behavior. These data look fine! In cases where this is deviation from normality, it is good practice to try to determine whether it is legitimate to remove outliers from any statistical analysis. In demographic studies, there are so many plants and many reasonable possibilities for why we may observe a strange transition from one year to the next. For example: In the first year, someone measured the plant, but it had already been browsed by a deer and so was extra short, and then appears to grow like wild in the following year Someone took data on the wrong space on the datasheet Someone couldn’t find a mama plant, who actually died, so accidentally measured a baby that had germinated in a nearby spot Given this, unlike with standard statistical analyses, we tend to be less stringent about when we remove outliers. Let’s take a look at residuals. # row_index &lt;- 59 # value &lt;- final_data[row_index,] # print(value) # # #or you can generate the id and use that to subset (just looking at the id number from above) # target_id &lt;- 17 # row_data_index &lt;- final_data[final_data$ID == target_id, ] # print(row_data_index) # # #check out the other outlier # row_index &lt;- 74 # value &lt;- final_data[row_index,] # print(value) We could decide to remove outliers! Example below: # # Remove identified outliers # final_data &lt;- final_data[-c(44, 74), ] # # # Fit growth model on the log scale # growth_log &lt;- lm(log(sizeNext) ~ log(size), # na.action = na.omit, # data = final_data) # # # Extract residuals # resid_log &lt;- residuals(growth_log) # # # Test normality of residuals (on log scale) # shapiro.test(resid_log) # # # Set up plotting window # par(mfrow = c(1, 2), mar = c(4, 4, 2, 1)) # # # Scatter plot of log(size) vs log(sizeNext) # plot(log(sizeNext) ~ log(size), # data = final_data, # xlab = &quot;log(Size at time t)&quot;, # ylab = &quot;log(Size at time t+1)&quot;, # main = &quot;Growth relationship (log–log)&quot;) # abline(growth_log, col = &quot;red&quot;, lwd = 2) # # # Q-Q plot of residuals # id.n &lt;- nrow(final_data) # qqPlot(growth_log, # distribution = &quot;norm&quot;, # id.method = &quot;y&quot;, # id.cex = 0.6, # id.n = id.n, # id.col = &quot;blue&quot;, # id.location = &quot;ab&quot;) Remember, if you do remove outliers, you must refit the model! growth_clean &lt;- growth_data |&gt; dplyr::filter(!flag_outlier) # Refit model without outliers m_growth_quad_clean &lt;- lm( log(sizeNext) ~ poly(log(size), 2, raw = TRUE), data = growth_clean ) # Compare models in order to determine whether the removal of the outlier changed fit. summary(m_growth_quad) ## ## Call: ## lm(formula = log(sizeNext) ~ poly(log(size), 2, raw = TRUE), ## data = growth_data) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.25728 -0.44403 -0.01676 0.38419 1.38985 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.95684 1.09712 -1.784 0.07990 . ## poly(log(size), 2, raw = TRUE)1 1.96402 0.67401 2.914 0.00512 ** ## poly(log(size), 2, raw = TRUE)2 -0.19582 0.09611 -2.037 0.04634 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6424 on 56 degrees of freedom ## Multiple R-squared: 0.5209, Adjusted R-squared: 0.5038 ## F-statistic: 30.44 on 2 and 56 DF, p-value: 1.128e-09 summary(m_growth_quad_clean) ## ## Call: ## lm(formula = log(sizeNext) ~ poly(log(size), 2, raw = TRUE), ## data = growth_clean) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.25728 -0.44403 -0.01676 0.38419 1.38985 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.95684 1.09712 -1.784 0.07990 . ## poly(log(size), 2, raw = TRUE)1 1.96402 0.67401 2.914 0.00512 ** ## poly(log(size), 2, raw = TRUE)2 -0.19582 0.09611 -2.037 0.04634 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.6424 on 56 degrees of freedom ## Multiple R-squared: 0.5209, Adjusted R-squared: 0.5038 ## F-statistic: 30.44 on 2 and 56 DF, p-value: 1.128e-09 Note that if you do remove outliers you must repeat the model fitting process and update the final model that can be included in the kernel! 256.4.4.2 Probability of reproduction Next, we repeat this process for all individuals. I generally do not perform outlier removal for the rest of the analyses, since we want to capture true variation in vital rates. Thus, the oddballs can be important for understanding the system, but only if they are real outliers, rather than created by human error. We model whether an individual reproduced (fec1) as a function of size: \\[ \\text{logit}(p_{\\text{repr}}) = \\beta_0 + \\beta_1 z \\] where \\(z\\) is the state variable (size at time \\(t\\)). First, let’s determine whether we should use a log transformation. We will apply this consistently across all models. Reproduction is often log transformed, since reproduction is expected to increase multiplicatively with size. d_repr &lt;- final_data[final_data$Exclude == 0, , drop = FALSE] d_repr &lt;- d_repr[!is.na(d_repr$fec1) &amp; !is.na(d_repr$size) &amp; d_repr$size &gt; 0, , drop = FALSE] if (is.factor(d_repr$fec1) || is.character(d_repr$fec1)) { d_repr$fec1 &lt;- as.integer(as.factor(d_repr$fec1)) - 1 } m_repr &lt;- stats::glm(fec1 ~ size, data = d_repr, family = stats::binomial()) m_repr_log &lt;- stats::glm(fec1 ~ log(size), data = d_repr, family = stats::binomial()) stats::AIC(m_repr, m_repr_log) ## df AIC ## m_repr 2 35.03888 ## m_repr_log 2 30.57976 And yes, the log scale has the lower AIC value, so we choose log transformed data! Now let’s compare model fits (i.e., linear vs polynomial models). # Compare linear vs quadratic on log(size) for reproduction probability # Start with included rows only (keep this line if you use Exclude) d_repr &lt;- final_data[final_data$Exclude == 0, , drop = FALSE] # Keep complete cases for variables used + ensure size is positive for log() d_repr &lt;- d_repr[!is.na(d_repr$fec1) &amp; !is.na(d_repr$size) &amp; d_repr$size &gt; 0, , drop = FALSE] # Ensure fec1 is 0/1 numeric if (is.factor(d_repr$fec1) || is.character(d_repr$fec1)) { d_repr$fec1 &lt;- as.integer(as.factor(d_repr$fec1)) - 1 } stopifnot(all(d_repr$fec1 %in% c(0, 1))) m_repr_log_lin &lt;- stats::glm( fec1 ~ log(size), data = d_repr, family = stats::binomial() ) m_repr_log_poly &lt;- stats::glm( fec1 ~ poly(log(size), 2, raw = TRUE), data = d_repr, family = stats::binomial() ) stats::AIC(m_repr_log_lin, m_repr_log_poly) ## df AIC ## m_repr_log_lin 2 30.57976 ## m_repr_log_poly 3 31.11613 Wonderful - the linear model wins! Next, we need to visualize the state variable-vital rate relationship. library(ggplot2) library(tidyr) library(dplyr) newdat &lt;- data.frame( size = seq(min(final_data$size, na.rm = TRUE), max(final_data$size, na.rm = TRUE), length.out = 200) ) pred_df &lt;- newdat |&gt; mutate( `Linear (log size)` = predict(m_repr_log_lin, newdat, type = &quot;response&quot;), `Quadratic (log size)` = predict(m_repr_log_poly, newdat, type = &quot;response&quot;) ) |&gt; pivot_longer( cols = -size, names_to = &quot;Model&quot;, values_to = &quot;Prediction&quot; ) ggplot(final_data, aes(size, fec1)) + geom_jitter(height = 0.05, width = 0, alpha = 0.4) + geom_line( data = pred_df, aes(size, Prediction, color = Model), linewidth = 1 ) + labs( y = &quot;Probability of reproduction&quot;, color = &quot;Model fit&quot; ) + theme_minimal() 256.4.4.3 Seed production Seed production (fec2) is only observed for individuals that reproduced. We therefore model fecundity conditional on reproduction (fec1 == 1). Because seed counts are highly right-skewed and often overdispersed, we use a negative binomial model. As above, we first evaluate whether a log transformation of size is appropriate, then compare linear versus polynomial functional forms. library(dplyr) library(MASS) fec_data &lt;- final_data %&gt;% filter( fec1 == 1, !is.na(fec2), !is.na(size) ) # if you use log(size), require positive size fec_data_log &lt;- fec_data %&gt;% filter(size &gt; 0) # (Recommended) use the SAME dataset for both models: # if any size &lt;= 0 exists, drop them for both models fec_data_use &lt;- fec_data_log m_fec_lin &lt;- MASS::glm.nb( fec2 ~ size, data = fec_data_use ) m_fec_log &lt;- MASS::glm.nb( fec2 ~ log(size), data = fec_data_use ) AIC(m_fec_lin, m_fec_log) ## df AIC ## m_fec_lin 3 972.3240 ## m_fec_log 3 967.9926 Log transformation wins again! As with reproduction probability, fecundity is often expected to increase multiplicatively with size. library(dplyr) library(MASS) # Ensure the exact same rows are used in both models fec_data2 &lt;- fec_data %&gt;% filter( !is.na(fec2), !is.na(size), is.finite(size), size &gt; 0 ) # Optional sanity check stopifnot(nrow(fec_data2) &gt; 0) m_fec_log_lin &lt;- MASS::glm.nb( fec2 ~ log(size), data = fec_data2 ) m_fec_log_poly &lt;- MASS::glm.nb( fec2 ~ poly(log(size), 2, raw = TRUE), data = fec_data2 ) AIC(m_fec_log_lin, m_fec_log_poly) ## df AIC ## m_fec_log_lin 3 967.9926 ## m_fec_log_poly 4 963.6462 Polynomial model wins! But note, we have a warning that the model didn’t converge! This is annoying, but not uncommon for rare plants. This typically means that we should choose the simpler model, despite AIC findings. Let’s see whether the simpler model converges. summary(m_fec_log_lin) ## ## Call: ## MASS::glm.nb(formula = fec2 ~ log(size), data = fec_data2, init.theta = 0.4985063643, ## link = log) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -4.8531 2.9095 -1.668 0.0953 . ## log(size) 3.5580 0.6291 5.656 1.55e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for Negative Binomial(0.4985) family taken to be 1) ## ## Null deviance: 60.115 on 38 degrees of freedom ## Residual deviance: 49.562 on 37 degrees of freedom ## AIC: 967.99 ## ## Number of Fisher Scoring iterations: 1 ## ## ## Theta: 0.4985 ## Std. Err.: 0.0932 ## ## 2 x log-likelihood: -961.9930 O.K. The simple model converged! We will likely go with that model. Let’s still consider whether the preferred model is biologically feasible. Since convergence errors often occur due to low sample size, it is usually safer to go with the simpler model! Let’s visualize both models. newdat &lt;- data.frame( size = seq(min(fec_data$size, na.rm = TRUE), max(fec_data$size, na.rm = TRUE), length.out = 200) ) pred_df &lt;- newdat |&gt; mutate( `Linear (log size)` = predict(m_fec_log_lin, newdat, type = &quot;response&quot;), `Quadratic (log size)` = predict(m_fec_log_poly, newdat, type = &quot;response&quot;) ) |&gt; pivot_longer( cols = -size, names_to = &quot;Model&quot;, values_to = &quot;Prediction&quot; ) ggplot(fec_data, aes(size, fec2)) + geom_point(alpha = 0.5) + geom_line( data = pred_df, aes(size, Prediction, color = Model), linewidth = 1 ) + scale_y_log10() + labs( y = &quot;Seed production&quot;, color = &quot;Model fit&quot; ) + theme_minimal() # SEED PRODUCTION MODEL (Poisson GLM) ---- # Check for OVERDISPERSION # check_overdispersion &lt;- function(model) { # resid_dev &lt;- sum(residuals(model, type = &quot;deviance&quot;)^2) # ratio &lt;- resid_dev / model$df.residual # # cat(&quot;\\n=== Overdispersion Check (Poisson Model) ===\\n&quot;) # cat(&quot;Residual deviance:&quot;, round(resid_dev, 2), &quot;\\n&quot;) # cat(&quot;Degrees of freedom:&quot;, model$df.residual, &quot;\\n&quot;) # cat(&quot;Dispersion ratio:&quot;, round(ratio, 3), &quot;\\n&quot;) # # if(ratio &gt; 2) { # cat(&quot;⚠️ OVERDISPERSED - consider using negative binomial or quasipoisson\\n&quot;) # } else if(ratio &lt; 0.5) { # cat(&quot;⚠️ UNDERDISPERSED - check for zero-inflation or model structure\\n&quot;) # } else { # cat(&quot;✓ Dispersion looks reasonable for Poisson\\n&quot;) # } # # return(ratio) # } # # dispersion_ratio &lt;- check_overdispersion(seed_mod) # # # If overdispersed, consider refitting: # if(dispersion_ratio &gt; 2) { # cat(&quot;\\n📝 Consider refitting as:\\n&quot;) # cat(&quot; library(MASS)\\n&quot;) # cat(&quot; seed_mod_nb &lt;- glm.nb(fec2 ~ size_log + I(size_log^2), data = final_data)\\n&quot;) # } O.K. we have a judgement call, we’ve seen ‘aging’ in other metrics, so this could be reasonable, but the apparent downturn looks like it is driven by a small number of large individuals, and with the lack of model convergence, any estimate of “aging effects” would be highly uncertain. This could go either way and we could compare the difference in outcomes! 256.4.4.4 Survival Survival is modeled as a binary response and is therefore fit using a binomial model. We do not perform outlier removal for survival, as extreme values (alive vs. dead) are intrinsic to the process. library(dplyr) # Use identical rows for both models (critical for AIC comparisons) surv_data &lt;- final_data %&gt;% filter( !is.na(surv), !is.na(size), is.finite(size), size &gt; 0 # needed for log(size) ) %&gt;% mutate( surv = as.integer(surv) # optional: ensure 0/1 ) stopifnot(nrow(surv_data) &gt; 0) stopifnot(all(surv_data$surv %in% c(0, 1))) m_surv_lin &lt;- glm( surv ~ size, data = surv_data, family = binomial() ) m_surv_log &lt;- glm( surv ~ log(size), data = surv_data, family = binomial() ) AIC(m_surv_lin, m_surv_log) ## df AIC ## m_surv_lin 2 93.24766 ## m_surv_log 2 93.78090 These are so close! Given the other relationships, I’m just going to go with the log scale. Notably, you can include both scales for different variables in an IPM. In an Integral Projection Model (IPM), all vital rates are linked to the same underlying state variable (e.g., size), but they do not need to share the same functional form. This means that it is perfectly acceptable for one vital rate to be modeled with a linear relationship, while another uses a polynomial or nonlinear form. Each vital rate reflects a different biological process, and these processes often scale with size in different ways. For example, growth may decelerate at large sizes, while survival changes more gradually, and reproduction increases multiplicatively with size. Allowing each vital rate to adopt the functional form that best reflects its biology improves realism and model performance. Importantly, all vital-rate models must be well supported by the data and behave sensibly when extrapolated across the full range of sizes included in the IPM. More complex models (e.g., polynomial fits) are retained only when they converge reliably and do not introduce implausible behavior at the boundaries of the size distribution. In practice, this means that a single IPM may include: a quadratic growth model, a linear (log-scale) reproduction model, and a linear survival model, all operating together within the same projection kernel. The consistency of the IPM comes from the shared state variable, not from enforcing identical mathematical forms across vital rates. All that said, I tend to find that the same transformation works across all equations and I find it more parsimonous to include the same transformation, especially when data are sparse. library(dplyr) # one shared, &quot;safe for log()&quot; dataset for BOTH models surv_data &lt;- final_data %&gt;% filter( !is.na(surv), !is.na(size), is.finite(size), size &gt; 0 # required for log(size) ) %&gt;% mutate( surv = as.integer(surv) # ensure 0/1 (optional but nice) ) stopifnot(nrow(surv_data) &gt; 0) stopifnot(all(surv_data$surv %in% c(0, 1))) m_surv_log_lin &lt;- glm( surv ~ log(size), data = surv_data, family = binomial() ) m_surv_log_poly &lt;- glm( surv ~ poly(log(size), 2, raw = TRUE), data = surv_data, family = binomial() ) AIC(m_surv_log_lin, m_surv_log_poly) ## df AIC ## m_surv_log_lin 2 93.78090 ## m_surv_log_poly 3 94.31272 Let’s visualize! newdat &lt;- data.frame( size = seq(min(final_data$size, na.rm = TRUE), max(final_data$size, na.rm = TRUE), length.out = 200) ) pred_df &lt;- newdat |&gt; mutate( `Linear (log size)` = predict(m_surv_log_lin, newdat, type = &quot;response&quot;), `Quadratic (log size)` = predict(m_surv_log_poly, newdat, type = &quot;response&quot;) ) |&gt; pivot_longer( cols = -size, names_to = &quot;Model&quot;, values_to = &quot;Prediction&quot; ) ggplot(final_data, aes(size, surv)) + geom_jitter(height = 0.05, width = 0, alpha = 0.4) + geom_line( data = pred_df, aes(size, Prediction, color = Model), linewidth = 1 ) + labs( y = &quot;Survival probability&quot;, color = &quot;Model fit&quot; ) + theme_minimal() Oh yeah. Confirmed. That polynomial looks bonkers! One thing to note in terms of packages. The IPMpack package has fairly slick built in equations for these initial model comparisons. For this reason, I sometimes mix-and-match packages, using the IPM pack for initial visualization and other packages or hand-coding for the actual modeling! The approach is up to you, though sometimes coding just gives you more flexibility! At this point, we have assembled all of the vital-rate models needed to build a population projection model: (1) growth, (2) survival, (3) probability of reproduction, and (4) reproductive output. Together, these components describe how individuals move through the population from one time step to the next. In principle, additional demographic processes could be modeled if sufficient data were available. For example, reproduction could be decomposed into multiple steps, such as (1) the probability of flowering, (2) the probability that a flower sets seed, and (3) reproductive output conditional on successful flowering and seed set. Each of these processes may respond differently to environmental conditions or individual state. In practice, however, demographic data are extremely time- and labor-intensive to collect, and most studies are limited to the transitions described above. As a result, population models typically combine multiple biological processes into a smaller set of composite vital rates. That said, additional model complexity can be justified when driven by specific hypotheses. For example, if a study focuses on pollination limitation or herbivore effects on flowering, explicitly modeling intermediate reproductive steps may be necessary and informative. The key is to match model complexity to both the available data and the biological questions being asked. 256.5 Constructing a transition matrix To build a population projection model, we must translate our fitted vital-rate models into a structure that projects the population forward in time. In matrix population models, this structure is a transition matrix, where each element describes the contribution of individuals in one state to another state in the next time step. In stage-structured models, the transition matrix is constructed directly from estimated transition probabilities among discrete stages. In contrast, IPMs begin with continuous relationships between size and vital rates. To construct a transition matrix from these continuous functions, we discretize the size distribution into a finite number of size classes. This discretized matrix serves as a numerical approximation of the continuous IPM kernel. Thus, even though IPMs are conceptually continuous, they are implemented computationally as large matrices that closely approximate continuous dynamics. In an IPM, population dynamics are governed by a continuous projection equation: \\[ n(z&#39;, t+1) = \\int K(z&#39;, z)\\, n(z, t)\\, dz \\] where: - \\(z\\) is individual size at time \\(t\\), - \\(z&#39;\\) is size at time \\(t+1\\), - \\(n(z,t)\\) is the size distribution at time \\(t\\), - \\(K(z&#39;,z)\\) is the kernel describing transitions from size \\(z\\) to \\(z&#39;\\). The kernel is typically decomposed into survival–growth (\\(P\\)) and reproduction (\\(F\\)) components: \\[ K(z&#39;, z) = P(z&#39;, z) + F(z&#39;, z) \\] When implemented numerically, the continuous kernel is approximated by a large matrix \\(\\mathbf{K}\\), where each element represents transitions between discretized size classes. 256.6 Building an IPM: Boundary points, mesh points, and step size Constructing an IPM requires defining the size domain over which the population is modeled and how finely that domain is discretized. This involves three key choices: boundary points, mesh points, and step size. Boundary points define the minimum and maximum sizes included in the model. These limits should encompass all biologically plausible sizes observed in the data, often with a small buffer to avoid edge effects. Mesh points are the discrete size values used to approximate the continuous size distribution. The number of mesh points determines the resolution of the IPM: more mesh points yield a closer approximation to the continuous kernel but increase computational cost. Step size is the distance between adjacent mesh points and is determined by the boundary range divided by the number of mesh points. The step size plays a critical role in numerical accuracy, as it scales the contribution of individuals across size classes. Together, these choices determine how well the discretized matrix approximates the underlying continuous population dynamics. In practice, mesh sizes of 50–200 points often provide a good balance between accuracy and computational efficiency, though convergence should always be checked. 256.7 Model Evaluation: Eviction One challenge in Integral Projection Models (IPMs) arises when predicted growth pushes individuals outside the defined size boundaries of the model. For example, a growth model may predict that some individuals shrink below the minimum size or grow beyond the maximum size. This issue is referred to as eviction. If not handled properly, eviction can lead to loss or artificial accumulation of individuals at the boundaries, distorting population dynamics and biasing estimates of population growth rate (\\(\\lambda\\)). 256.7.1 Why Eviction Matters Consider a large plant with size \\(z = 150 \\,\\text{cm}^2\\) and a growth model that predicts next year’s size as: mean: \\(\\mu(z) = 155 \\,\\text{cm}^2\\) standard deviation: \\(\\sigma = 10 \\,\\text{cm}^2\\) If the maximum size boundary is \\(U = 176 \\,\\text{cm}^2\\), some probability mass from this individual’s growth distribution extends beyond the modeled size range. Without correction, this probability mass is effectively lost, implicitly assuming those individuals die. This assumption is often biologically unrealistic. 256.7.2 Eviction Correction Strategies Several strategies exist to address eviction, each corresponding to different biological assumptions. 256.7.2.1 1. Truncation (Renormalization) Method: Redistribute probability mass that would fall outside the boundaries back into the valid size range by renormalizing the growth distribution. Mathematical implementation: \\[ g_{\\text{truncated}}(z&#39;, z) = \\frac{g_{\\text{uncorrected}}(z&#39;, z)} {\\int_L^U g_{\\text{uncorrected}}(z&#39;, z)\\, dz&#39;} \\] where the denominator is: \\[ \\int_L^U g_{\\text{uncorrected}}(z&#39;, z)\\, dz&#39; = \\Phi\\left(\\frac{U - \\mu(z)}{\\sigma}\\right) - \\Phi\\left(\\frac{L - \\mu(z)}{\\sigma}\\right) \\] and \\(\\Phi(\\cdot)\\) is the cumulative distribution function of the normal distribution. Note two different kinds of truncation: Simple truncation divides by (p_upper - p_lower) for EVERY transition: Pro: Conceptually simpler Con: Applies unnecessary operations when eviction risk is ~0% Con: Can introduce small numerical artifacts across the entire matrix Conditional truncation only corrects when eviction is meaningful (&gt;1e-10 probability): Pro: More numerically stable Pro: Matches industry-standard implementations (ipmr) Pro: Computationally more accurate Pro: Only modifies transitions that actually need correction Biological interpretation: Individuals do not actually leave the population; instead, extreme sizes are “folded back” into the observable range. This approach assumes that the boundaries reflect measurement or detection limits rather than true biological limits. When appropriate: Size represents natural constraints Boundaries are conservative relative to the true biological range Extreme sizes are rare in the data For plants demography, I typically use truncation! 256.7.2.2 2. Reflection Method: Reflect probability mass back into the domain, creating symmetric probability density at the boundaries. Mathematical implementation: \\[ g_{\\text{reflected}}(z&#39;, z) = g(z&#39;, z) + g(2L - z&#39;, z) + g(2U - z&#39;, z) \\] Biological interpretation: Minimum and maximum sizes act as hard biological barriers, such as physical or architectural constraints. When appropriate: Clear biological maxima exist (e.g., determinate growth) Minimum size represents a functional threshold 256.7.2.3 3. Redistribution to Boundary Classes Method: Accumulate all evicted probability mass into the boundary size classes. Biological interpretation: Individuals reach a size asymptote and remain at that size indefinitely. When appropriate: Species with determinate growth Maximum size represents a stable life stage 256.7.2.4 4. No Correction (Accept Loss) Method: Allow probability mass to escape the modeled domain, treating eviction as mortality. Biological interpretation: Individuals that grow beyond \\(U\\) or shrink below \\(L\\) are assumed to die. When appropriate: Rarely appropriate in practice May be acceptable if eviction is negligible (e.g., &lt;1% of total probability mass) 256.7.3 Summary Evaluating and correcting for eviction is a critical step in IPM construction. Excessive eviction often signals issues with boundary selection, growth model specification, or extrapolation beyond the data. The chosen correction method should align with both the biology of the species and the interpretation of the size variable. 256.8 Estimating population dynamics parameters Once the transition matrix has been constructed, we can calculate key population dynamics parameters. The most important of these is the long-term population growth rate, \\(\\lambda\\), which describes whether the population is expected to grow, decline, or remain stable over time. Additional quantities of interest include the stable size distribution, which describes the relative frequency of individuals across sizes at equilibrium, and the reproductive value, which measures the relative contribution of individuals of different sizes to future population growth. Beyond asymptotic behavior, IPMs also allow us to quantify short-term or transient dynamics, such as population amplification, attenuation, and inertia. These metrics describe how populations respond to disturbances before reaching long-term equilibrium and are often particularly relevant for conservation and management. 256.9 Calculating confidence intervals Demographic models are built from estimated vital-rate relationships, each of which contains uncertainty. To understand how this uncertainty propagates into population dynamics, we must quantify uncertainty around derived parameters such as \\(\\lambda\\). A common approach is to use resampling methods, such as bootstrapping, in which vital rate models are repeatedly refit to resampled data. Each refitted model is used to reconstruct the IPM and calculate population metrics, generating empirical distributions for parameters of interest. Confidence intervals derived from these distributions provide insight into the robustness of model predictions and help distinguish meaningful biological patterns from sampling variability. Incorporating uncertainty is particularly important when IPMs are used to inform conservation decisions or compare population viability across sites or treatments. 256.10 Putting this all together in code form! For simplicity, let’s go ahead and code this using the ipmr program! Note: From this point forward, the IPM is constructed using log(size) as the state variable. All kernels, mesh points, eviction correction, and demographic metrics are defined on the log scale unless explicitly stated otherwise. Raw units (cm) are used only for interpretation and visualization. library(dplyr) library(MASS) library(ggpubr) library(ipmr) species_name &lt;- &quot;SILLAN&quot; trans_years &lt;- &quot;20232024&quot; #======================================================== # 0) Transform + keep raw and log #======================================================== final_data &lt;- final_data %&gt;% mutate( size_raw = size, sizeNext_raw = sizeNext, size_log = if_else(!is.na(size) &amp; is.finite(size) &amp; size &gt; 0, log(size), NA_real_), sizeNext_log = if_else(!is.na(sizeNext) &amp; is.finite(sizeNext) &amp; sizeNext &gt; 0, log(sizeNext), NA_real_) ) # If you are using Exclude, make it explicit here (optional) if (&quot;Exclude&quot; %in% names(final_data)) { final_data &lt;- final_data %&gt;% filter(Exclude == 0) } #======================================================== # 1) Build model-specific datasets (THIS fixes NA/AIC issues) #======================================================== # Growth: needs size_log + sizeNext_log grow_data &lt;- final_data %&gt;% filter(!is.na(size_log), !is.na(sizeNext_log)) # Survival: needs size_log + surv surv_data &lt;- final_data %&gt;% filter(!is.na(size_log), !is.na(surv)) %&gt;% mutate(surv = as.integer(surv)) stopifnot(all(surv_data$surv %in% c(0, 1))) # Reproduction: needs size_log + fec1 (0/1) repr_data &lt;- final_data %&gt;% filter(!is.na(size_log), !is.na(fec1)) %&gt;% mutate(fec1 = as.integer(fec1)) stopifnot(all(repr_data$fec1 %in% c(0, 1))) # Fecundity: CONDITIONAL on reproducing (fec1 == 1) fec_data &lt;- final_data %&gt;% filter(fec1 == 1, !is.na(size_log), !is.na(fec2)) #======================================================== # 2) Fit vital rate models on consistent, clean data #======================================================== grow_mod &lt;- lm(sizeNext_log ~ size_log + I(size_log^2), data = grow_data) grow_sd &lt;- sd(resid(grow_mod)) surv_mod &lt;- glm(surv ~ size_log, data = surv_data, family = binomial()) repr_mod &lt;- glm(fec1 ~ size_log + I(size_log^2), data = repr_data, family = binomial()) # fecundity model: expected fec2 GIVEN reproduction seed_mod &lt;- MASS::glm.nb(fec2 ~ size_log + I(size_log^2), data = fec_data) # (If you really want Poisson, swap to glm(..., family = poisson()), but NB is safer.) #======================================================== # 3) Recruitment (strict recruits) + establishment prob #======================================================== robust_recruit_logical &lt;- is.na(final_data$size_log) &amp; is.na(final_data$surv) &amp; is.na(final_data$fec1) &amp; !is.na(final_data$sizeNext_log) recr_data &lt;- final_data[robust_recruit_logical, , drop = FALSE] recr_mu &lt;- mean(recr_data$sizeNext_log, na.rm = TRUE) recr_sd &lt;- sd(recr_data$sizeNext_log, na.rm = TRUE) recr_n &lt;- sum(is.finite(recr_data$sizeNext_log)) # seeds produced by repro individuals (matches seed_mod being conditional) seed_n &lt;- sum(fec_data$fec2, na.rm = TRUE) if (!is.finite(seed_n) || seed_n &lt;= 0) stop(&quot;seed_n is 0 or non-finite; cannot build fecundity kernel&quot;) if (!is.finite(recr_n) || recr_n &lt;= 0) stop(&quot;recr_n is 0 or non-finite; cannot build recruitment kernel&quot;) if (!is.finite(recr_mu) || !is.finite(recr_sd) || recr_sd &lt;= 0) stop(&quot;Recruit size distribution invalid (mu/sd)&quot;) cat(&quot;\\n=== Recruitment Validation ===\\n&quot;) ## ## === Recruitment Validation === cat(&quot;Recruits:&quot;, recr_n, &quot;\\n&quot;) ## Recruits: 293 cat(&quot;Total seeds produced (fec1==1 only):&quot;, seed_n, &quot;\\n&quot;) ## Total seeds produced (fec1==1 only): 5317830 cat(&quot;Establishment probability:&quot;, round(recr_n / seed_n, 4), &quot;\\n&quot;) ## Establishment probability: 1e-04 # Minimum repro size on log scale (guarded) min_repro_size_log &lt;- suppressWarnings(min(repr_data$size_log[repr_data$fec1 == 1], na.rm = TRUE)) if (!is.finite(min_repro_size_log)) { min_repro_size_log &lt;- Inf message(&quot;⚠️ No reproductive individuals (fec1==1). Setting min_repro_size_log = Inf (F kernel will be ~0).&quot;) } #======================================================== # 4) Domain on LOG scale (pad by range, not multiply) #======================================================== all_sizes_log &lt;- c(final_data$size_log, final_data$sizeNext_log) q &lt;- quantile(all_sizes_log, probs = c(0.01, 0.99), na.rm = TRUE) rng &lt;- as.numeric(q[2] - q[1]) pad &lt;- 0.05 * rng L &lt;- as.numeric(q[1] - pad) U &lt;- as.numeric(q[2] + pad) if (!is.finite(L) || !is.finite(U) || L &gt;= U) stop(&quot;Invalid domain bounds: check sizes and quantiles&quot;) cat(&quot;\\n=== Domain Boundaries (LOG SCALE) ===\\n&quot;) ## ## === Domain Boundaries (LOG SCALE) === cat(&quot;Log scale: [&quot;, round(L, 3), &quot;,&quot;, round(U, 3), &quot;]\\n&quot;) ## Log scale: [ -0.248 , 5.204 ] cat(&quot;Raw scale: [&quot;, round(exp(L), 3), &quot;,&quot;, round(exp(U), 3), &quot;]\\n&quot;) ## Raw scale: [ 0.781 , 181.972 ] #======================================================== # 5) Pack params for ipmr kernels #======================================================== params &lt;- list( recr_mu = recr_mu, recr_sd = recr_sd, grow_sd = grow_sd, surv_mod = surv_mod, grow_mod = grow_mod, repr_mod = repr_mod, seed_mod = seed_mod, recr_n = recr_n, seed_n = seed_n, min_repro_size_log = min_repro_size_log ) #======================================================== # 6) Build IPM (log state variable everywhere) #======================================================== obs_ipm &lt;- init_ipm( sim_gen = &quot;simple&quot;, di_dd = &quot;di&quot;, det_stoch = &quot;det&quot; ) %&gt;% define_kernel( name = &quot;P&quot;, family = &quot;CC&quot;, formula = s * g, s = predict(surv_mod, newdata = data.frame(size_log = sa_1), type = &quot;response&quot;), g_mu = predict(grow_mod, newdata = data.frame(size_log = sa_1), type = &quot;response&quot;), g = dnorm(sa_2, g_mu, grow_sd), states = list(&quot;sa&quot;), data_list = params, uses_par_sets = FALSE, evict_cor = TRUE, evict_fun = truncated_distributions(fun = &quot;norm&quot;, target = &quot;g&quot;) ) %&gt;% define_kernel( name = &quot;F&quot;, family = &quot;CC&quot;, formula = r_p * r_s * r_r * r_d, r_p = ifelse( sa_1 &lt; min_repro_size_log, 0, predict(repr_mod, newdata = data.frame(size_log = sa_1), type = &quot;response&quot;) ), # expected fec2 GIVEN reproduction r_s = predict(seed_mod, newdata = data.frame(size_log = sa_1), type = &quot;response&quot;), r_r = recr_n / seed_n, r_d = dnorm(sa_2, recr_mu, recr_sd), states = list(&quot;sa&quot;), data_list = params, uses_par_sets = FALSE, evict_cor = TRUE, evict_fun = truncated_distributions(fun = &quot;norm&quot;, target = &quot;r_d&quot;) ) %&gt;% define_impl( make_impl_args_list( kernel_names = c(&quot;P&quot;, &quot;F&quot;), int_rule = rep(&quot;midpoint&quot;, 2), state_start = rep(&quot;sa&quot;, 2), state_end = rep(&quot;sa&quot;, 2) ) ) %&gt;% define_domains(sa = c(L, U, 100)) %&gt;% define_pop_state(n_sa = runif(100)) %&gt;% make_ipm(iterate = TRUE, iterations = 100) #======================================================== # 7) Extract kernels + lambda #======================================================== P &lt;- obs_ipm$sub_kernels$P F &lt;- obs_ipm$sub_kernels$F K_matrix &lt;- P + F eigs &lt;- eigen(K_matrix) lambda_raw &lt;- max(Re(eigs$values)) cat(&quot;\\n=== IPM Results ===\\n&quot;) ## ## === IPM Results === cat(&quot;Population growth rate (λ):&quot;, round(lambda_raw, 4), &quot;\\n&quot;) ## Population growth rate (λ): 0.9694 stable_dist_raw &lt;- Re(eigs$vectors[, which.max(Re(eigs$values))]) stable_dist_raw &lt;- stable_dist_raw / sum(stable_dist_raw) #======================================================== # 8) Save RDS #======================================================== dir.create(&quot;IPM_outputs&quot;, showWarnings = FALSE) species_dir &lt;- file.path(&quot;IPM_outputs&quot;, paste0(species_name, &quot;_&quot;, trans_years)) dir.create(species_dir, showWarnings = FALSE) mesh_points &lt;- obs_ipm$domains$sa mesh_width &lt;- diff(mesh_points)[1] ipm_rds &lt;- list( metadata = list( species = species_name, transition = trans_years, date_created = Sys.Date(), state_variable = &quot;size_log&quot;, state_scale = &quot;log&quot;, mesh_points_n = length(mesh_points), mesh_bounds = c(L = L, U = U), mesh_width = mesh_width ), kernels = list(K = K_matrix, P = P, F = F), size = list(mesh_points_log = mesh_points, stable_dist = stable_dist_raw), population_metrics = list(lambda = lambda_raw), models = list(growth = grow_mod, survival = surv_mod, reproduction = repr_mod, fecundity = seed_mod), recruitment = list( recr_mu_log = recr_mu, recr_sd_log = recr_sd, recr_n = recr_n, seed_n = seed_n, establishment_prob = recr_n / seed_n ), params = params, data = final_data ) saveRDS(ipm_rds, file = file.path(species_dir, &quot;IPM_full_object.rds&quot;)) cat(&quot;✓ Saved full IPM object to:&quot;, file.path(species_dir, &quot;IPM_full_object.rds&quot;), &quot;\\n&quot;) ## ✓ Saved full IPM object to: IPM_outputs/SILLAN_20232024/IPM_full_object.rds Now, let’s bootstrap for confidence intervals! #=============================================== # BOOTSTRAP ANALYSIS - UNCERTAINTY IN LAMBDA # (INLINE ONLY, FIXED MESH) #=============================================== library(ipmr) library(ggplot2) cat(&quot;\\n=== Starting Bootstrap Analysis (fixed mesh) ===\\n&quot;) ## ## === Starting Bootstrap Analysis (fixed mesh) === safe_glm &lt;- function(formula, data, family) { tryCatch(glm(formula, data = data, family = family), error = function(e) NULL, warning = function(w) NULL) } safe_lm &lt;- function(formula, data) { tryCatch(lm(formula, data = data), error = function(e) NULL, warning = function(w) NULL) } get_stable_vec &lt;- function(ipm, state_var = &quot;sa&quot;) { out &lt;- tryCatch(ipmr::right_ev(ipm), error = function(e) NULL) if (is.null(out)) return(NULL) sv &lt;- out[[paste0(state_var, &quot;_w&quot;)]] # e.g., &quot;sa_w&quot; if (is.null(sv)) sv &lt;- out[[1]] if (is.null(sv)) return(NULL) sv &lt;- as.numeric(sv) if (any(!is.finite(sv)) || sum(sv) &lt;= 0) return(NULL) sv / sum(sv) } #------------------------------- # Data splits #------------------------------- adults &lt;- final_data[is.finite(final_data$size_log), ] robust_recruit_logical &lt;- is.na(final_data$size_log) &amp; is.na(final_data$surv) &amp; is.na(final_data$fec1) &amp; is.finite(final_data$sizeNext_log) recr_data &lt;- final_data[robust_recruit_logical, ] cat(&quot;Adults (bootstrapped):&quot;, nrow(adults), &quot;\\n&quot;) ## Adults (bootstrapped): 82 cat(&quot;Recruits (held constant):&quot;, nrow(recr_data), &quot;\\n&quot;) ## Recruits (held constant): 293 if (nrow(adults) &lt; 10) stop(&quot;Too few adults to bootstrap reliably.&quot;) if (nrow(recr_data) &lt; 1) stop(&quot;No recruits under strict definition; cannot hold recruitment constant.&quot;) #------------------------------- # FIXED MESH: use main_env grid, not domains spec #------------------------------- state_var &lt;- &quot;sa&quot; Pm &lt;- obs_ipm$sub_kernels$P n_mesh &lt;- nrow(Pm) sa_1_full &lt;- get(&quot;sa_1&quot;, envir = obs_ipm$env_list$main_env) fixed_mesh &lt;- sort(unique(as.numeric(sa_1_full))) # sanity if (length(fixed_mesh) != n_mesh) { # fallback: generate mesh from range if something is odd fixed_mesh &lt;- seq(min(fixed_mesh, na.rm = TRUE), max(fixed_mesh, na.rm = TRUE), length.out = n_mesh) } cat(&quot;✓ Kernel dimension n_mesh:&quot;, n_mesh, &quot;\\n&quot;) ## ✓ Kernel dimension n_mesh: 100 cat(&quot;✓ Fixed mesh points:&quot;, length(fixed_mesh), &quot;\\n&quot;) ## ✓ Fixed mesh points: 100 cat(&quot;✓ Domain (log):&quot;, paste(round(range(fixed_mesh), 3), collapse = &quot; to &quot;), &quot;\\n&quot;) ## ✓ Domain (log): -0.221 to 5.177 # proto proto_template &lt;- obs_ipm$proto_ipm # full-data fallbacks surv_mod_full &lt;- surv_mod repr_mod_full &lt;- repr_mod seed_mod_full &lt;- seed_mod # recruitment params held constant recr_mu_full &lt;- recr_mu recr_sd_full &lt;- recr_sd recr_n_full &lt;- recr_n min_repro_full &lt;- min_repro_size_log # IMPORTANT: hold seed_n constant to avoid resamples crashing establishment seed_n_full &lt;- sum(final_data$fec2[final_data$fec1 == 1 &amp; !is.na(final_data$fec2)], na.rm = TRUE) if (!is.finite(seed_n_full) || seed_n_full &lt;= 0) { stop(&quot;seed_n_full is 0/NA. Need at least one observed fec2 where fec1==1.&quot;) } cat(&quot;✓ Fixed establishment r_r = recr_n/seed_n:&quot;, round(recr_n_full/seed_n_full, 5), &quot;\\n&quot;) ## ✓ Fixed establishment r_r = recr_n/seed_n: 6e-05 #------------------------------- # Bootstrap controls #------------------------------- n_boot &lt;- 50L max_tries &lt;- n_boot * 10L all_lambdas &lt;- rep(NA_real_, n_boot) valid_iter &lt;- 0L attempts &lt;- 0L cat(&quot;\\n=== Bootstrap Loop ===\\n&quot;) ## ## === Bootstrap Loop === cat(&quot;Target iterations:&quot;, n_boot, &quot;\\n&quot;) ## Target iterations: 50 cat(&quot;Maximum attempts:&quot;, max_tries, &quot;\\n\\n&quot;) ## Maximum attempts: 500 #------------------------------- # Bootstrap loop #------------------------------- while (valid_iter &lt; n_boot &amp;&amp; attempts &lt; max_tries) { attempts &lt;- attempts + 1L if (attempts %% 25L == 0L) cat(&quot;Attempt&quot;, attempts, &quot;| Valid:&quot;, valid_iter, &quot;\\n&quot;) boot_ind &lt;- sample.int(nrow(adults), replace = TRUE) boot_data &lt;- rbind(adults[boot_ind, ], recr_data) # Growth (only rows with both) gdat &lt;- boot_data[is.finite(boot_data$size_log) &amp; is.finite(boot_data$sizeNext_log), ] grow_mod_boot &lt;- safe_lm(sizeNext_log ~ size_log + I(size_log^2), data = gdat) if (is.null(grow_mod_boot)) next grow_sd_boot &lt;- sd(resid(grow_mod_boot), na.rm = TRUE) if (!is.finite(grow_sd_boot) || grow_sd_boot &lt;= 0) next # Survival sdat &lt;- boot_data[is.finite(boot_data$size_log) &amp; !is.na(boot_data$surv), ] surv_mod_boot &lt;- safe_glm(surv ~ size_log, data = sdat, family = binomial()) if (is.null(surv_mod_boot)) surv_mod_boot &lt;- surv_mod_full # Reproduction rdat &lt;- boot_data[is.finite(boot_data$size_log) &amp; !is.na(boot_data$fec1), ] repr_mod_boot &lt;- safe_glm(fec1 ~ size_log + I(size_log^2), data = rdat, family = binomial()) if (is.null(repr_mod_boot)) repr_mod_boot &lt;- repr_mod_full # Seed production (fit only where fec2 observed AND fec1==1) fdat &lt;- boot_data[is.finite(boot_data$size_log) &amp; boot_data$fec1 == 1 &amp; !is.na(boot_data$fec2), ] seed_mod_boot &lt;- safe_glm(fec2 ~ size_log + I(size_log^2), data = fdat, family = poisson()) if (is.null(seed_mod_boot)) seed_mod_boot &lt;- seed_mod_full params_boot &lt;- list( recr_mu = recr_mu_full, recr_sd = recr_sd_full, grow_sd = grow_sd_boot, surv_mod = surv_mod_boot, grow_mod = grow_mod_boot, repr_mod = repr_mod_boot, seed_mod = seed_mod_boot, recr_n = recr_n_full, seed_n = seed_n_full, # fixed min_repro_size_log = min_repro_full ) proto_boot &lt;- tryCatch(unserialize(serialize(proto_template, NULL)), error = function(e) proto_template) ipmr::parameters(proto_boot) &lt;- params_boot boot_ipm &lt;- tryCatch( suppressMessages(suppressWarnings( make_ipm(proto_boot, iterate = TRUE, iterations = 100) )), error = function(e) NULL ) if (is.null(boot_ipm)) next lam &lt;- tryCatch(ipmr::lambda(boot_ipm), error = function(e) NA_real_) if (!is.finite(lam)) next stable_vec &lt;- get_stable_vec(boot_ipm, state_var = state_var) if (is.null(stable_vec)) next if (length(stable_vec) != n_mesh) next # should now match! valid_iter &lt;- valid_iter + 1L all_lambdas[valid_iter] &lt;- lam } ## Attempt 25 | Valid: 23 ## Attempt 50 | Valid: 47 cat(&quot;\\n=== Bootstrap Complete ===\\n&quot;) ## ## === Bootstrap Complete === cat(&quot;✓ Valid iterations:&quot;, valid_iter, &quot;/&quot;, n_boot, &quot;\\n&quot;) ## ✓ Valid iterations: 50 / 50 cat(&quot;✓ Total attempts:&quot;, attempts, &quot;\\n&quot;) ## ✓ Total attempts: 53 if (valid_iter == 0L) { stop(&quot;Bootstrap produced 0 valid iterations even after mesh fix. Next step: print make_ipm errors.&quot;) } all_lambdas &lt;- all_lambdas[seq_len(valid_iter)] lambda_ci &lt;- quantile(all_lambdas, probs = c(0.025, 0.975), na.rm = TRUE) lambda_obs &lt;- ipmr::lambda(obs_ipm) cat(&quot;\\n=== Lambda Estimates ===\\n&quot;) ## ## === Lambda Estimates === cat(&quot;Observed λ:&quot;, round(lambda_obs, 4), &quot;\\n&quot;) ## Observed λ: 0.9694 cat(&quot;Bootstrap mean λ:&quot;, round(mean(all_lambdas), 4), &quot;\\n&quot;) ## Bootstrap mean λ: 0.9453 cat(&quot;95% CI: [&quot;, round(lambda_ci[1], 4), &quot;,&quot;, round(lambda_ci[2], 4), &quot;]\\n&quot;) ## 95% CI: [ 0.8445 , 0.9946 ] lambda_plot &lt;- ggplot(data.frame(lambda = all_lambdas), aes(x = lambda)) + geom_histogram(bins = 30) + geom_vline(xintercept = lambda_obs, linetype = &quot;dashed&quot;, linewidth = 1) + geom_vline(xintercept = lambda_ci[1], linetype = &quot;dotted&quot;, linewidth = 0.9) + geom_vline(xintercept = lambda_ci[2], linetype = &quot;dotted&quot;, linewidth = 0.9) + labs( title = &quot;Bootstrap Distribution of λ&quot;, subtitle = paste0(&quot;n = &quot;, valid_iter, &quot; successful iterations&quot;), x = &quot;Population growth rate (λ)&quot;, y = &quot;Frequency&quot; ) + theme_minimal(base_size = 14) print(lambda_plot) Now, let’s look at these results! To better understand the life history strategy underlying the observed population growth rate, we calculated several additional demographic metrics. These included net reproductive rate (R₀; the expected lifetime offspring per individual), generation time (the average interval between generations), and size-specific patterns in reproduction and survival. Together, these metrics paint a fuller picture of how individuals progress through their life cycle and contribute to population dynamics. #============================================================== # LIFE-HISTORY METRICS FROM A SIZE-STRUCTURED IPM #============================================================== library(ipmr) cat(&quot;\\n=== Computing Life-History Metrics ===\\n&quot;) ## ## === Computing Life-History Metrics === state_var &lt;- &quot;sa&quot; #-------------------------------------------------------------- # 1. Extract discretized kernels #-------------------------------------------------------------- Pm &lt;- obs_ipm$sub_kernels$P Fm &lt;- obs_ipm$sub_kernels$F K &lt;- Pm + Fm n_mesh &lt;- nrow(Pm) #-------------------------------------------------------------- # 2. Extract mesh (CORRECTED - get unique values) #-------------------------------------------------------------- # sa_1 in main_env is the expanded grid (n_mesh × n_mesh values) # We need the unique mesh points sa_1_full &lt;- get(&quot;sa_1&quot;, envir = obs_ipm$env_list$main_env) size_classes &lt;- sort(unique(as.numeric(sa_1_full))) # Verify length if(length(size_classes) != n_mesh) { stop(&quot;Mesh length (&quot;, length(size_classes), &quot;) != kernel dim (&quot;, n_mesh, &quot;)&quot;) } cat(&quot;✓ Extracted mesh from main_env (unique values from sa_1)\\n&quot;) ## ✓ Extracted mesh from main_env (unique values from sa_1) cat(&quot; Original sa_1 length:&quot;, length(sa_1_full), &quot;\\n&quot;) ## Original sa_1 length: 10000 cat(&quot; Unique mesh points:&quot;, n_mesh, &quot;\\n&quot;) ## Unique mesh points: 100 cat(&quot; Domain (log):&quot;, round(range(size_classes), 3), &quot;\\n&quot;) ## Domain (log): -0.221 5.177 cat(&quot; Domain (raw):&quot;, round(exp(range(size_classes)), 2), &quot;cm\\n&quot;) ## Domain (raw): 0.8 177.08 cm #-------------------------------------------------------------- # 3. Population growth rate (λ) #-------------------------------------------------------------- lambda_obs &lt;- ipmr::lambda(obs_ipm) cat(&quot;\\n✓ Lambda:&quot;, round(lambda_obs, 4), &quot;\\n&quot;) ## ## ✓ Lambda: 0.9694 if(lambda_obs &gt; 1) { cat(&quot; → Population GROWING at&quot;, round((lambda_obs - 1) * 100, 2), &quot;% per year\\n&quot;) } else if(lambda_obs &lt; 1) { cat(&quot; → Population DECLINING at&quot;, round((1 - lambda_obs) * 100, 2), &quot;% per year\\n&quot;) } else { cat(&quot; → Population STABLE\\n&quot;) } ## → Population DECLINING at 3.06 % per year #-------------------------------------------------------------- # 4. Stable size distribution #-------------------------------------------------------------- stable_output &lt;- ipmr::right_ev(obs_ipm) stable_vec &lt;- stable_output[[paste0(state_var, &quot;_w&quot;)]] stable_vec &lt;- as.numeric(stable_vec) # Verify length if(length(stable_vec) != n_mesh) { stop(&quot;Stable vector length (&quot;, length(stable_vec), &quot;) != kernel dim (&quot;, n_mesh, &quot;)&quot;) } # Normalize (should already be 1) if(!isTRUE(all.equal(sum(stable_vec), 1))) { stable_vec &lt;- stable_vec / sum(stable_vec) } cat(&quot;✓ Stable distribution extracted (sum =&quot;, round(sum(stable_vec), 6), &quot;)\\n&quot;) ## ✓ Stable distribution extracted (sum = 1 ) # Mean size under stable distribution mean_size_stable_log &lt;- sum(size_classes * stable_vec) mean_size_stable_raw &lt;- exp(mean_size_stable_log) cat(&quot; Mean size (log):&quot;, round(mean_size_stable_log, 3), &quot;\\n&quot;) ## Mean size (log): -0.06 cat(&quot; Mean size (raw):&quot;, round(mean_size_stable_raw, 2), &quot;cm\\n&quot;) ## Mean size (raw): 0.94 cm #-------------------------------------------------------------- # 5. Reproductive value #-------------------------------------------------------------- rv_output &lt;- ipmr::left_ev(obs_ipm) # Try with suffix first, fallback to first element rv_vec &lt;- rv_output[[paste0(state_var, &quot;_w&quot;)]] if(is.null(rv_vec)) rv_vec &lt;- rv_output[[1]] rv_vec &lt;- as.numeric(rv_vec) # Verify length if(length(rv_vec) != n_mesh) { stop(&quot;RV vector length (&quot;, length(rv_vec), &quot;) != kernel dim (&quot;, n_mesh, &quot;)&quot;) } # Scale to max = 1 if(max(rv_vec, na.rm = TRUE) &gt; 0) { rv_vec &lt;- rv_vec / max(rv_vec, na.rm = TRUE) } max_rv &lt;- max(rv_vec, na.rm = TRUE) cat(&quot;✓ Max reproductive value:&quot;, round(max_rv, 3), &quot;\\n&quot;) ## ✓ Max reproductive value: 1 #-------------------------------------------------------------- # 6. Size-specific reproduction #-------------------------------------------------------------- size_specific_repro &lt;- colSums(Fm) if(sum(size_specific_repro) &gt; 0) { mean_rep_obs_log &lt;- sum(size_classes * size_specific_repro) / sum(size_specific_repro) mean_rep_obs_raw &lt;- exp(mean_rep_obs_log) var_rep_obs_log &lt;- sum((size_classes - mean_rep_obs_log)^2 * size_specific_repro) / sum(size_specific_repro) } else { mean_rep_obs_log &lt;- NA_real_ mean_rep_obs_raw &lt;- NA_real_ var_rep_obs_log &lt;- NA_real_ } cat(&quot;✓ Mean reproductive size (log):&quot;, round(mean_rep_obs_log, 3), &quot;\\n&quot;) ## ✓ Mean reproductive size (log): 4.725 cat(&quot; Mean reproductive size (raw):&quot;, round(mean_rep_obs_raw, 2), &quot;cm\\n&quot;) ## Mean reproductive size (raw): 112.73 cm #-------------------------------------------------------------- # 7. Expected remaining lifetime by size #-------------------------------------------------------------- cat(&quot;\\n=== Computing Expected Remaining Lifetime ===\\n&quot;) ## ## === Computing Expected Remaining Lifetime === # Check P eigenvalue P_eigs &lt;- eigen(Pm)$values P_lambda &lt;- max(Re(P_eigs)) cat(&quot;P kernel eigenvalue:&quot;, round(P_lambda, 4), &quot;\\n&quot;) ## P kernel eigenvalue: 0.9694 if(P_lambda &gt;= 0.99) { cat(&quot;⚠️ High P eigenvalue (&gt;0.99) - expected lifetimes may be very long\\n&quot;) } expected_remaining_life &lt;- rep(0, n_mesh) v &lt;- rep(1, n_mesh) tol &lt;- 1e-8 max_iter &lt;- 5000 converged &lt;- FALSE for(iter in seq_len(max_iter)) { expected_remaining_life &lt;- expected_remaining_life + v v_new &lt;- as.numeric(Pm %*% v) diff &lt;- max(abs(v_new - v), na.rm = TRUE) if(diff &lt; tol) { cat(&quot;✓ Converged after&quot;, iter, &quot;iterations\\n&quot;) converged &lt;- TRUE break } if(any(!is.finite(v_new)) || max(v_new) &gt; 1e6) { warning(&quot;⚠️ Lifetime calculation diverging at iteration &quot;, iter) break } v &lt;- v_new } ## ✓ Converged after 576 iterations if(!converged) { cat(&quot;⚠️ Did not fully converge after&quot;, max_iter, &quot;iterations\\n&quot;) } max_life &lt;- max(expected_remaining_life) cat(&quot; Max expected lifetime:&quot;, round(max_life, 2), &quot;time steps\\n&quot;) ## Max expected lifetime: 544.83 time steps if(max_life &gt; 100) { cat(&quot;⚠️ Very high lifetime (&gt;100 years) - survival model may be overfitting\\n&quot;) } ## ⚠️ Very high lifetime (&gt;100 years) - survival model may be overfitting #-------------------------------------------------------------- # 8. Median size (weighted by stable distribution) #-------------------------------------------------------------- cumulative_dist &lt;- cumsum(stable_vec) median_index &lt;- which(cumulative_dist &gt;= 0.5)[1] if(is.na(median_index)) { median_index &lt;- floor(n_mesh / 2) cat(&quot;⚠️ Could not find weighted median, using middle bin\\n&quot;) } median_size_log &lt;- size_classes[median_index] median_size_raw &lt;- exp(median_size_log) life_exp_median_obs &lt;- expected_remaining_life[median_index] cat(&quot;✓ Median size (log):&quot;, round(median_size_log, 3), &quot;\\n&quot;) ## ✓ Median size (log): -0.112 cat(&quot; Median size (raw):&quot;, round(median_size_raw, 2), &quot;cm\\n&quot;) ## Median size (raw): 0.89 cm cat(&quot; Expected lifetime at median:&quot;, round(life_exp_median_obs, 2), &quot;time steps\\n&quot;) ## Expected lifetime at median: 322.62 time steps #-------------------------------------------------------------- # 9. Summary table #-------------------------------------------------------------- summary_table &lt;- data.frame( Metric = c( &quot;Population growth rate (λ)&quot;, &quot;Mean size (stable, log)&quot;, &quot;Mean size (stable, cm)&quot;, &quot;Median size (stable, log)&quot;, &quot;Median size (stable, cm)&quot;, &quot;Mean reproductive size (log)&quot;, &quot;Mean reproductive size (cm)&quot;, &quot;Variance reproductive size (log)&quot;, &quot;Expected lifetime at median&quot;, &quot;Max reproductive value&quot; ), Estimate = round(c( lambda_obs, mean_size_stable_log, mean_size_stable_raw, median_size_log, median_size_raw, mean_rep_obs_log, mean_rep_obs_raw, var_rep_obs_log, life_exp_median_obs, max_rv ), 4) ) cat(&quot;\\n=== LIFE-HISTORY SUMMARY ===\\n&quot;) ## ## === LIFE-HISTORY SUMMARY === print(summary_table, row.names = FALSE) ## Metric Estimate ## Population growth rate (λ) 0.9694 ## Mean size (stable, log) -0.0597 ## Mean size (stable, cm) 0.9421 ## Median size (stable, log) -0.1115 ## Median size (stable, cm) 0.8945 ## Mean reproductive size (log) 4.7250 ## Mean reproductive size (cm) 112.7281 ## Variance reproductive size (log) 0.0790 ## Expected lifetime at median 322.6221 ## Max reproductive value 1.0000 #-------------------------------------------------------------- # 10. Visualizations #-------------------------------------------------------------- par(mfrow = c(2, 2), mar = c(4.5, 4.5, 3, 1)) # Plot 1: Expected lifetime vs log size plot(size_classes, expected_remaining_life, type = &quot;l&quot;, lwd = 3, col = &quot;darkblue&quot;, xlab = &quot;Size (log scale)&quot;, ylab = &quot;Expected remaining lifetime (years)&quot;, main = &quot;A. Expected Lifetime by Size (Log)&quot;, cex.lab = 1.2, cex.main = 1.3) abline(v = median_size_log, col = &quot;red&quot;, lwd = 2, lty = 2) abline(v = mean_size_stable_log, col = &quot;green&quot;, lwd = 2, lty = 2) text(median_size_log, max(expected_remaining_life) * 0.9, paste0(&quot;Median\\n&quot;, round(life_exp_median_obs, 1), &quot; yr&quot;), pos = 4, col = &quot;red&quot;, font = 2, cex = 0.9) legend(&quot;topright&quot;, legend = c(&quot;Median&quot;, &quot;Mean&quot;), col = c(&quot;red&quot;, &quot;green&quot;), lty = 2, lwd = 2, cex = 0.8) grid() # Plot 2: Expected lifetime vs raw size plot(exp(size_classes), expected_remaining_life, type = &quot;l&quot;, lwd = 3, col = &quot;darkblue&quot;, xlab = &quot;Size (cm)&quot;, ylab = &quot;Expected remaining lifetime (years)&quot;, main = &quot;B. Expected Lifetime by Size (Raw)&quot;, cex.lab = 1.2, cex.main = 1.3) abline(v = median_size_raw, col = &quot;red&quot;, lwd = 2, lty = 2) abline(v = mean_size_stable_raw, col = &quot;green&quot;, lwd = 2, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Median&quot;, &quot;Mean&quot;), col = c(&quot;red&quot;, &quot;green&quot;), lty = 2, lwd = 2, cex = 0.8) grid() # Plot 3: Stable size distribution plot(size_classes, stable_vec, type = &quot;l&quot;, lwd = 3, col = &quot;purple&quot;, xlab = &quot;Size (log scale)&quot;, ylab = &quot;Proportion&quot;, main = &quot;C. Stable Size Distribution&quot;, cex.lab = 1.2, cex.main = 1.3) abline(v = median_size_log, col = &quot;red&quot;, lwd = 2, lty = 2) abline(v = mean_size_stable_log, col = &quot;green&quot;, lwd = 2, lty = 2) legend(&quot;topright&quot;, legend = c(&quot;Median&quot;, &quot;Mean&quot;), col = c(&quot;red&quot;, &quot;green&quot;), lty = 2, lwd = 2, cex = 0.8) grid() # Plot 4: Reproductive value plot(size_classes, rv_vec, type = &quot;l&quot;, lwd = 3, col = &quot;orange&quot;, xlab = &quot;Size (log scale)&quot;, ylab = &quot;Reproductive value (scaled)&quot;, main = &quot;D. Reproductive Value by Size&quot;, cex.lab = 1.2, cex.main = 1.3) grid() dev.off() ## null device ## 1 par(mfrow = c(1, 1)) cat(&quot;\\n✓ Figure saved!\\n&quot;) ## ## ✓ Figure saved! life_exp_detailed &lt;- data.frame( size_log = size_classes, size_raw_cm = exp(size_classes), expected_remaining_lifetime = expected_remaining_life, stable_distribution = stable_vec, reproductive_value = rv_vec ) cat(&quot;✓ All results exported successfully!\\n&quot;) ## ✓ All results exported successfully! Interesting! This figure is showing us size-dependent survival and its consequences for expected remaining lifetime in a size-structured Integral Projection Model (IPM). Each point shows the observed survival outcome (0 = died, 1 = survived) for an individual as a function of size at time t. Survival probability is high for small individuals but declines markedly at larger sizes, indicating that large individuals experience elevated mortality risk. This non-monotonic survival–size relationship directly shapes the expected remaining lifetime (ERL) curve derived from the IPM. Individuals at small sizes have long expected remaining lifetimes because they persist with high survival across multiple time steps, even if growth is slow. In contrast, individuals at larger sizes exhibit short expected remaining lifetimes, reflecting increased mortality once large size is reached. The dashed vertical line indicates the median size class, at which survival is approximately 50%, yielding an ERL near one time step. This pattern illustrates that, in size-structured populations, longevity is not necessarily associated with large size; instead, long expected lifetimes can arise from prolonged persistence at small sizes. The IPM faithfully translates this empirically observed survival–size relationship into emergent demographic properties, rather than imposing assumptions about monotonic size–survival relationships. Now, let’s calculate transient dynamics! We have to use the original size distribution, and walk through an individual time step (rather than run to asymptotic population growth / stable size distribution). #============================================================== # TRANSIENT DYNAMICS (LOG-SCALE STATE VARIABLE) #============================================================== library(ipmr) #-------------------------------------------------------------- # 1) Extract mesh + kernels (ROBUST to ipmr storage details) #-------------------------------------------------------------- P &lt;- obs_ipm$sub_kernels$P F &lt;- obs_ipm$sub_kernels$F K &lt;- P + F # The discretized matrix dimension is the truth n_mesh &lt;- nrow(K) # ---- Try to recover the mesh points in a way that matches K ---- # 1) Best case: row/col names are the mesh points size_classes &lt;- suppressWarnings(as.numeric(rownames(K))) # 2) If rownames aren&#39;t numeric, try colnames if (length(size_classes) != n_mesh || anyNA(size_classes)) { size_classes &lt;- suppressWarnings(as.numeric(colnames(K))) } # 3) If still not valid, pull from ipmr environment (sa_1) if (length(size_classes) != n_mesh || anyNA(size_classes)) { size_classes &lt;- NULL if (!is.null(obs_ipm$env_list$main_env) &amp;&amp; exists(&quot;sa_1&quot;, envir = obs_ipm$env_list$main_env, inherits = FALSE)) { sa_1_full &lt;- get(&quot;sa_1&quot;, envir = obs_ipm$env_list$main_env) cand &lt;- sort(unique(as.numeric(sa_1_full))) if (length(cand) == n_mesh) size_classes &lt;- cand } } # 4) Last resort: reconstruct evenly spaced mesh from your domain bounds L, U # (works as long as you still have L and U in your workspace) if (is.null(size_classes) || length(size_classes) != n_mesh || anyNA(size_classes)) { if (!exists(&quot;L&quot;, inherits = TRUE) || !exists(&quot;U&quot;, inherits = TRUE)) { stop(&quot;Could not recover mesh points. Ensure L and U exist, or that K has rownames/colnames.&quot;) } size_classes &lt;- seq(L, U, length.out = n_mesh) } # Final sanity checks stopifnot(nrow(K) == n_mesh, ncol(K) == n_mesh, length(size_classes) == n_mesh) cat(&quot;Mesh points:&quot;, n_mesh, &quot;\\n&quot;) ## Mesh points: 100 cat(&quot;Domain (log):&quot;, round(range(size_classes), 3), &quot;\\n&quot;) ## Domain (log): -0.221 5.177 #-------------------------------------------------------------- # 2) Initial population vector from observed sizes (LOG SCALE) # IMPORTANT: bins need to match the mesh discretization #-------------------------------------------------------------- observed_sizes_log &lt;- final_data$size_log[is.finite(final_data$size_log)] # Build bin edges as midpoints between mesh points (plus endpoints) # This aligns a histogram with the discretized state space edges &lt;- c( size_classes[1] - 0.5 * (size_classes[2] - size_classes[1]), (size_classes[-1] + size_classes[-n_mesh]) / 2, size_classes[n_mesh] + 0.5 * (size_classes[n_mesh] - size_classes[n_mesh - 1]) ) hist_data &lt;- hist(observed_sizes_log, breaks = edges, plot = FALSE) n0 &lt;- as.numeric(hist_data$counts) if (sum(n0) == 0) stop(&quot;Initial vector n0 has all zeros (no finite size_log values).&quot;) n0 &lt;- n0 / sum(n0) # normalize to proportions cat(&quot;Initial population vector:\\n&quot;) ## Initial population vector: cat(&quot; Sum:&quot;, round(sum(n0), 6), &quot;\\n&quot;) ## Sum: 1 cat(&quot; Non-zero bins:&quot;, sum(n0 &gt; 0), &quot;\\n&quot;) ## Non-zero bins: 45 #-------------------------------------------------------------- # 3) Asymptotic properties (dominant eigenvalue/eigenvector) #-------------------------------------------------------------- eig &lt;- eigen(K) lambda_asymptotic &lt;- max(Re(eig$values)) dom_idx &lt;- which.max(Re(eig$values)) w &lt;- Re(eig$vectors[, dom_idx]) w[w &lt; 0] &lt;- 0 if (sum(w) &lt;= 0) { # fallback: absolute value then normalize w &lt;- abs(Re(eig$vectors[, dom_idx])) } w &lt;- w / sum(w) cat(&quot;\\nAsymptotic lambda:&quot;, round(lambda_asymptotic, 4), &quot;\\n&quot;) ## ## Asymptotic lambda: 0.9694 #-------------------------------------------------------------- # 4) One-step transient dynamics #-------------------------------------------------------------- n1 &lt;- as.numeric(K %*% n0) lambda_1 &lt;- sum(n1) / sum(n0) # since sum(n0)=1, this is just sum(n1) n1_normalized &lt;- n1 / sum(n1) deviation &lt;- n1_normalized - n0 cat(&quot;\\nOne-step dynamics:\\n&quot;) ## ## One-step dynamics: cat(&quot; Lambda_1 (from observed n0):&quot;, round(lambda_1, 4), &quot;\\n&quot;) ## Lambda_1 (from observed n0): 4.2682 cat(&quot; Max deviation (|n1 - n0|):&quot;, round(max(abs(deviation)), 4), &quot;\\n&quot;) ## Max deviation (|n1 - n0|): 0.0975 #-------------------------------------------------------------- # 5) Reactivity + inertia # - &quot;Observed one-step amplification&quot; = lambda_1 # - &quot;Worst-case one-step amplification bound&quot; for total abundance: # max(colSums(K)) for nonnegative K under L1 norm # - Inertia: compare total N(t) to lambda^t scaling #-------------------------------------------------------------- # Worst-case one-step amplification (upper bound under L1) reactivity_bound &lt;- max(colSums(K)) # Inertia via repeated multiplication (avoid %^%) T &lt;- 50L nt &lt;- n0 for (tt in seq_len(T)) nt &lt;- as.numeric(K %*% nt) inertia &lt;- sum(nt) / (lambda_asymptotic^T * sum(n0)) cat(&quot;\\nTransient metrics:\\n&quot;) ## ## Transient metrics: cat(&quot; Reactivity (observed, 1-step):&quot;, round(lambda_1, 4), &quot;\\n&quot;) ## Reactivity (observed, 1-step): 4.2682 cat(&quot; Reactivity (worst-case bound):&quot;, round(reactivity_bound, 4), &quot;\\n&quot;) ## Reactivity (worst-case bound): 9.8296 cat(&quot; Inertia (T = &quot;, T, &quot;): &quot;, round(inertia, 4), &quot;\\n&quot;, sep = &quot;&quot;) ## Inertia (T = 50): 4.0442 #-------------------------------------------------------------- # 6) Summary table + export #-------------------------------------------------------------- transient_metrics &lt;- data.frame( Transition = trans_years, Lambda_1_observed = round(lambda_1, 4), Reactivity_bound = round(reactivity_bound, 4), Inertia_T50 = round(inertia, 4), Lambda_asymptotic = round(lambda_asymptotic, 4), T_used = T ) print(transient_metrics) ## Transition Lambda_1_observed Reactivity_bound Inertia_T50 Lambda_asymptotic T_used ## 1 20232024 4.2682 9.8296 4.0442 0.9694 50 dir.create(&quot;IPM_outputs&quot;, showWarnings = FALSE) #-------------------------------------------------------------- # 7) Visualization: deviation after one step #-------------------------------------------------------------- deviation_df &lt;- data.frame( size_log = size_classes, size_raw_cm = exp(size_classes), deviation = deviation, initial = n0, final = n1_normalized ) library(ggplot2) p_dev &lt;- ggplot(deviation_df, aes(x = size_log, y = deviation, fill = deviation &gt; 0)) + geom_col(color = &quot;black&quot;, linewidth = 0.2) + geom_hline(yintercept = 0, linetype = &quot;dashed&quot;) + scale_fill_manual(values = c(&quot;TRUE&quot; = &quot;#3B82F6&quot;, &quot;FALSE&quot; = &quot;#EF4444&quot;), guide = &quot;none&quot;) + labs( title = &quot;Deviation from stable size distribution after one time step&quot;, subtitle = paste0(&quot;Observed 1-step growth (λ1) = &quot;, round(lambda_1, 3)), x = &quot;Size class (log scale)&quot;, y = &quot;Final − Initial proportion&quot; ) + theme_minimal(base_size = 14) print(p_dev) Sensitivity and vital rate calculations #------------------------------------------------- # Sensitivity and Elasticity of λ #------------------------------------------------- # Sensitivity: ∂λ / ∂K_ij # Elasticity: proportional sensitivity = (K_ij / λ) * sensitivity # # Row sums of elasticity: # Contribution of individuals of a given size at time t # Column sums: # Contribution of individuals entering a given size at time t+1 # Row sums are most commonly interpreted and plotted. #------------------------------------------------- library(ggpubr) library(ipmr) # Combine kernels K &lt;- obs_ipm$sub_kernels$P + obs_ipm$sub_kernels$F n_size &lt;- nrow(K) # Eigen decomposition eig &lt;- eigen(K) ord &lt;- order(Re(eig$values), decreasing = TRUE) lambda_dom &lt;- Re(eig$values[ord[1]]) # Right eigenvector (stable distribution) w &lt;- Re(eig$vectors[, ord[1]]) w[w &lt; 0] &lt;- 0 w &lt;- w / sum(w) # Left eigenvector (reproductive value) eig_left &lt;- eigen(t(K)) v &lt;- Re(eig_left$vectors[, ord[1]]) v &lt;- v / sum(v * w) # normalize so v · w = 1 # Sensitivity and elasticity sensitivity &lt;- outer(v, w) elasticity &lt;- (K / lambda_dom) * sensitivity #------------------------------------------------- # Prepare plotting data (INDEX-ALIGNED) #------------------------------------------------- # Use indices or safe mesh extraction size_log &lt;- as.numeric(obs_ipm$env_list$main_env$sa_1)[seq_len(n_size)] size_raw &lt;- exp(size_log) sens_df &lt;- expand.grid( from_size = size_log, to_size = size_log ) sens_df$value &lt;- as.vector(sensitivity) elas_df &lt;- expand.grid( from_size = size_log, to_size = size_log ) elas_df$value &lt;- as.vector(elasticity) row_df &lt;- data.frame( size_log = size_log, size_raw_cm = size_raw, elasticity_sum = rowSums(elasticity) ) #------------------------------------------------- # Plots #------------------------------------------------- elas_plot &lt;- ggplot(elas_df, aes(from_size, to_size, fill = value)) + geom_tile() + scale_fill_viridis_c(name = &quot;Elasticity&quot;) + labs( x = &quot;Size at time t (log)&quot;, y = &quot;Size at time t+1 (log)&quot;, title = &quot;Elasticity matrix&quot; ) + theme_pubr() row_plot &lt;- ggplot(row_df, aes(size_log, elasticity_sum)) + geom_line(linewidth = 1.1) + labs( x = &quot;Size at time t (log scale)&quot;, y = &quot;Sum of elasticities&quot;, title = &quot;Contribution of size classes to λ&quot; ) + theme_pubr() combined_plot &lt;- ggarrange(elas_plot, row_plot, labels = c(&quot;A&quot;, &quot;B&quot;), ncol = 2) print(combined_plot) 256.11 Test your knowledge Conceptual Understanding: 1. What is an Integral Projection Model (IPM), and how does it differ from a matrix population model? (Hint: think about state variables and discretization.) 2. Why must IPM census timing align with biologically meaningful life-cycle events? Give one example of how poor timing could bias demographic estimates. 3. What characteristics make a variable suitable as a state variable in an IPM? List at least three and briefly explain why each matters. 4. Explain why using more than one state variable can quickly make IPMs impractical. What is meant by the “curse of dimensionality”? Vital Rates and Model Structure: 5. Why is reproduction often modeled using a two-stage (hurdle) approach in plant IPMs? What biological processes do each stage represent? 6. In this case study, why was it problematic to use “height with reproductive structures” as the state variable for fecundity? What kind of modeling issue does this introduce? 7. Why is it generally inappropriate to remove outliers from survival models but reasonable to consider removing them from growth models? Statistical Modeling Choices: 8. Why were log transformations applied to size in most vital-rate models? What biological assumption does this reflect? 9. A quadratic growth model had lower AIC than a linear model. Why is it still important to visually inspect the fitted relationship before choosing it? 10. What does overdispersion indicate in a Poisson model, and why might a negative binomial model be preferred for seed production? IPM Construction and Interpretation: 11. What do the $P$ and $F$ kernels represent biologically? Give one example of a process captured in each. 12. Why must the size domain boundaries (L and U) extend slightly beyond the observed data range? What problem does this help prevent? 13. What is eviction in an IPM, and why can it bias estimates of population growth rate if not corrected? 14. Compare truncation and reflection as eviction-correction strategies. Under what biological assumptions would truncation be preferred? Population Metrics: 15. Interpret the meaning of $\\lambda &lt; 1$, $\\lambda = 1$, and $\\lambda &gt; 1$ in the context of conservation management. 16. What is the stable size distribution, and why does it not describe short-term population behavior? 17. What does reproductive value tell us about individuals of different sizes? How might managers use this information? Transient Dynamics: 18. Explain the difference between asymptotic population growth ($\\lambda$) and one-step transient growth ($\\lambda_1$). 19. What does reactivity measure, and why can it be important even when $\\lambda &lt; 1$? 20. How is inertia interpreted biologically in a conservation context? What does high inertia imply about population response to disturbance? Sensitivity and Elasticity: 21. What is the difference between sensitivity and elasticity of $\\lambda$? Why are elasticities often preferred for interpretation? 22. What does the row sum of the elasticity matrix represent biologically? 23. Why might a size class have high elasticity even if few individuals occupy that size class? Uncertainty and Inference: 24. Why is bootstrapping particularly important for IPMs built from small populations? 25. Why were recruitment parameters held constant during the bootstrap in this analysis? 26. How do uncertainty bands around vital-rate curves help prevent over-interpretation of demographic results? Synthesis / Short Answer: 27. Based on the IPM results, would you prioritize protecting small individuals or large individuals for this species? Justify your answer using survival, reproduction, or elasticity results. 28. Give one example of a management action that could be evaluated using this IPM framework and describe which vital rate it would most likely affect. 256.12 Assignment Using your data, build an ipm and calculate population dynamics! "],["stochastic-population-dynamics.html", "Chapter 257 Stochastic Population Dynamics 257.1 Why stochasticity matters 257.2 Types of stochasticity 257.3 Setup", " Chapter 257 Stochastic Population Dynamics The deterministic IPM in Chapter 18 tells us: if conditions remain constant, λ = 0.95 means the population will decline by 5% per year. But conditions never remain constant. Drought years hammer survival. Wet years boost reproduction. Fire eliminates half the population in an instant. Stochastic population models incorporate this variability, answering questions that deterministic models cannot: What’s the probability of extinction within 50 years? How does environmental variability affect long-term growth? Which life stages buffer populations against bad years? Does temporal variation in vital rates change optimal management? This chapter extends the IPM framework to include stochasticity, connecting directly to the Silene lanceolata case study from Chapter 18. 257.1 Why stochasticity matters ## ## DETERMINISTIC vs STOCHASTIC POPULATION DYNAMICS ## ═══════════════════════════════════════════════════════════════ ## ## DETERMINISTIC MODEL: ## - Same λ every year ## - Population trajectory is a smooth curve ## - Extinction occurs only if λ &lt; 1 indefinitely ## - Same starting conditions → same outcome ## ## STOCHASTIC MODEL: ## - λ varies among years (good years, bad years) ## - Population trajectory is jagged, unpredictable ## - Extinction can occur even if mean(λ) &gt; 1 ## - Same starting conditions → different outcomes each simulation ## ## KEY INSIGHT: ## Arithmetic mean of λ ≠ long-term growth rate ## ## Example: Two years with λ = 0.5 and λ = 2.0 ## Arithmetic mean: (0.5 + 2.0)/2 = 1.25 (suggests growth!) ## Geometric mean: √(0.5 × 2.0) = 1.0 (actual: no growth) ## ## Variability itself reduces long-term growth. 257.2 Types of stochasticity Type Source Effect Matters when… Demographic Random birth/death events Individual fates differ even with identical rates Population is small Environmental Year-to-year variation in vital rates All individuals experience same good/bad year Any population size Catastrophic Rare, severe events (fire, disease) Sudden large mortality Discrete events possible Sampling Measurement error in estimating vital rates Parameter uncertainty Small sample sizes 257.3 Setup library(tidyverse) library(ipmr) # For stochastic IPMs library(popbio) # Matrix population tools library(diagram) # Flow diagrams set.seed(42) "],["part-1-stochastic-matrix-models.html", "Chapter 258 Part 1: Stochastic Matrix Models 258.1 Deterministic baseline 258.2 Adding environmental stochasticity 258.3 Stochastic simulation 258.4 Stochastic growth rate (log λs)", " Chapter 258 Part 1: Stochastic Matrix Models Before diving into stochastic IPMs, let’s build intuition with simpler matrix models. 258.1 Deterministic baseline # Simple 3-stage matrix (seed, juvenile, adult) # Based loosely on a perennial plant A &lt;- matrix(c( 0.0, 0.0, 5.0, # Seeds produced by adults 0.1, 0.0, 0.0, # Seed germination → juvenile 0.0, 0.3, 0.8 # Juvenile growth + adult survival ), nrow = 3, byrow = TRUE) rownames(A) &lt;- colnames(A) &lt;- c(&quot;Seed&quot;, &quot;Juvenile&quot;, &quot;Adult&quot;) # Dominant eigenvalue = λ lambda_det &lt;- eigen(A)$values[1] |&gt; Re() cat(&quot;Deterministic λ:&quot;, round(lambda_det, 4), &quot;\\n&quot;) ## Deterministic λ: 0.9621 # Project population deterministically n0 &lt;- c(100, 20, 10) # Initial population years &lt;- 50 N_det &lt;- matrix(NA, years + 1, 3) N_det[1, ] &lt;- n0 for (t in 1:years) { N_det[t + 1, ] &lt;- A %*% N_det[t, ] } total_det &lt;- rowSums(N_det) 258.2 Adding environmental stochasticity Now let’s create multiple environment-specific matrices representing “good” and “bad” years: # Good year matrix (20% higher fecundity, 10% higher survival) A_good &lt;- matrix(c( 0.0, 0.0, 6.0, # Higher fecundity 0.12, 0.0, 0.0, # Higher germination 0.0, 0.35, 0.85 # Higher survival ), nrow = 3, byrow = TRUE) # Bad year matrix (30% lower fecundity, 15% lower survival) A_bad &lt;- matrix(c( 0.0, 0.0, 3.5, # Lower fecundity 0.08, 0.0, 0.0, # Lower germination 0.0, 0.25, 0.70 # Lower survival ), nrow = 3, byrow = TRUE) # Check individual lambdas lambda_good &lt;- eigen(A_good)$values[1] |&gt; Re() lambda_bad &lt;- eigen(A_bad)$values[1] |&gt; Re() cat(&quot;λ in good years:&quot;, round(lambda_good, 4), &quot;\\n&quot;) ## λ in good years: 1.0701 cat(&quot;λ in bad years:&quot;, round(lambda_bad, 4), &quot;\\n&quot;) ## λ in bad years: 0.8074 cat(&quot;Arithmetic mean:&quot;, round((lambda_good + lambda_bad) / 2, 4), &quot;\\n&quot;) ## Arithmetic mean: 0.9387 258.3 Stochastic simulation # Simulate with random environment (50% good, 50% bad years) n_sims &lt;- 100 years &lt;- 50 # Store results N_stoch &lt;- array(NA, dim = c(years + 1, 3, n_sims)) environments &lt;- matrix(NA, years, n_sims) for (sim in 1:n_sims) { N_stoch[1, , sim] &lt;- n0 for (t in 1:years) { # Random environment env &lt;- sample(c(&quot;good&quot;, &quot;bad&quot;), 1) environments[t, sim] &lt;- env A_t &lt;- if (env == &quot;good&quot;) A_good else A_bad N_stoch[t + 1, , sim] &lt;- A_t %*% N_stoch[t, , sim] } } # Total population over time total_stoch &lt;- apply(N_stoch, c(1, 3), sum) # Plot trajectories plot_data &lt;- data.frame( year = rep(0:years, n_sims), total = as.vector(total_stoch), sim = rep(1:n_sims, each = years + 1) ) ggplot() + geom_line(data = plot_data, aes(x = year, y = total, group = sim), alpha = 0.2, color = &quot;gray50&quot;) + geom_line(data = data.frame(year = 0:years, total = total_det), aes(x = year, y = total), color = &quot;firebrick&quot;, size = 1.2) + scale_y_log10() + labs(x = &quot;Year&quot;, y = &quot;Total Population (log scale)&quot;, title = &quot;Stochastic vs Deterministic Population Trajectories&quot;, subtitle = &quot;Gray = stochastic simulations, Red = deterministic&quot;) + theme_minimal() Figure 258.1: Stochastic population trajectories (gray) compared to deterministic projection (red). Even with mean λ &gt; 1, some trajectories decline due to variance. 258.4 Stochastic growth rate (log λs) The stochastic growth rate is the long-term average of log(λ): \\[\\log \\lambda_s = \\lim_{T \\to \\infty} \\frac{1}{T} \\sum_{t=1}^{T} \\log \\lambda_t\\] This equals the geometric mean of annual λ values. # Calculate stochastic growth rate from simulations log_lambda_s &lt;- numeric(n_sims) for (sim in 1:n_sims) { # Annual growth rates N_total &lt;- apply(N_stoch[, , sim], 1, sum) annual_lambda &lt;- N_total[-1] / N_total[-length(N_total)] log_lambda_s[sim] &lt;- mean(log(annual_lambda)) } cat(&quot;Mean stochastic log(λs):&quot;, round(mean(log_lambda_s), 4), &quot;\\n&quot;) ## Mean stochastic log(λs): -0.0807 cat(&quot;Stochastic λs:&quot;, round(exp(mean(log_lambda_s)), 4), &quot;\\n&quot;) ## Stochastic λs: 0.9225 cat(&quot;Deterministic λ:&quot;, round(lambda_det, 4), &quot;\\n&quot;) ## Deterministic λ: 0.9621 # Theoretical approximation # log(λs) ≈ log(λ̄) - σ²/2 sigma_sq &lt;- var(c(log(lambda_good), log(lambda_bad))) log_lambda_approx &lt;- log((lambda_good + lambda_bad) / 2) - sigma_sq / 2 cat(&quot;\\nApproximate log(λs) [using σ²/2 reduction]:&quot;, round(log_lambda_approx, 4), &quot;\\n&quot;) ## ## Approximate log(λs) [using σ²/2 reduction]: -0.0831 Key insight: Environmental variance reduces long-term growth rate below the arithmetic mean. This is the “variance discount.” "],["part-2-demographic-stochasticity.html", "Chapter 259 Part 2: Demographic Stochasticity 259.1 What is demographic stochasticity? 259.2 Simulating demographic stochasticity", " Chapter 259 Part 2: Demographic Stochasticity 259.1 What is demographic stochasticity? Even if survival probability is exactly 0.8, some individuals die and some survive—by chance. In small populations, this randomness can push populations to extinction even when expected λ &gt; 1. ## ## DEMOGRAPHIC STOCHASTICITY ## ═══════════════════════════════════════════════════════════════ ## ## Imagine 10 individuals, each with survival p = 0.8 ## ## Deterministic prediction: 10 × 0.8 = 8 survivors ## ## Actual possibilities (binomial distribution): ## P(10 survive) = 0.107 ## P(9 survive) = 0.268 ## P(8 survive) = 0.302 ## P(7 survive) = 0.201 ## ... ## P(0 survive) = 0.0000001 ## ## With 10 individuals, variance is manageable. ## With 3 individuals, extinction by chance becomes likely. ## ## RULE OF THUMB: ## Demographic stochasticity matters when N &lt; 50-100 259.2 Simulating demographic stochasticity # Compare deterministic vs demographic stochastic projections # for small populations simulate_demography &lt;- function(n0, A, years, stochastic = TRUE) { N &lt;- matrix(NA, years + 1, length(n0)) N[1, ] &lt;- n0 for (t in 1:years) { if (stochastic) { # Each transition is a random draw # Seeds produced (Poisson for reproduction) seeds_from_adults &lt;- rpois(1, A[1, 3] * N[t, 3]) # Seed germination (binomial) new_juveniles &lt;- rbinom(1, round(N[t, 1]), A[2, 1]) # Juvenile to adult (binomial) new_adults_from_juv &lt;- rbinom(1, round(N[t, 2]), A[3, 2]) # Adult survival (binomial) surviving_adults &lt;- rbinom(1, round(N[t, 3]), A[3, 3]) N[t + 1, ] &lt;- c(seeds_from_adults, new_juveniles, new_adults_from_juv + surviving_adults) } else { N[t + 1, ] &lt;- A %*% N[t, ] } # Check for extinction if (sum(N[t + 1, ]) &lt; 1) { N[(t + 1):(years + 1), ] &lt;- 0 break } } return(N) } # Small population starting conditions n0_small &lt;- c(20, 5, 3) # Only 28 total individuals # Run many simulations n_sims &lt;- 200 years &lt;- 100 results_small &lt;- array(NA, dim = c(years + 1, 3, n_sims)) extinction_year &lt;- numeric(n_sims) for (sim in 1:n_sims) { results_small[, , sim] &lt;- simulate_demography(n0_small, A, years, stochastic = TRUE) total &lt;- rowSums(results_small[, , sim]) if (any(total == 0)) { extinction_year[sim] &lt;- min(which(total == 0)) - 1 } else { extinction_year[sim] &lt;- NA } } # Extinction probability p_extinct &lt;- mean(!is.na(extinction_year)) cat(&quot;Extinction probability (100 years):&quot;, round(p_extinct, 3), &quot;\\n&quot;) ## Extinction probability (100 years): 0.98 cat(&quot;Mean time to extinction (if extinct):&quot;, round(mean(extinction_year, na.rm = TRUE), 1), &quot;years\\n&quot;) ## Mean time to extinction (if extinct): 30.8 years total_small &lt;- apply(results_small, c(1, 3), sum) plot_data_small &lt;- data.frame( year = rep(0:years, n_sims), total = as.vector(total_small), sim = rep(1:n_sims, each = years + 1) ) ggplot(plot_data_small, aes(x = year, y = total + 1, group = sim)) + geom_line(alpha = 0.15, color = &quot;steelblue&quot;) + scale_y_log10() + geom_hline(yintercept = 1, linetype = &quot;dashed&quot;, color = &quot;firebrick&quot;) + labs(x = &quot;Year&quot;, y = &quot;Total Population (log scale)&quot;, title = &quot;Demographic Stochasticity in Small Populations&quot;, subtitle = paste0(round(p_extinct * 100), &quot;% of simulations went extinct&quot;)) + theme_minimal() Figure 259.1: Demographic stochasticity in small populations. Many trajectories go extinct despite mean λ &gt; 1. "],["part-3-extinction-risk-analysis.html", "Chapter 260 Part 3: Extinction Risk Analysis 260.1 Quasi-extinction thresholds 260.2 Cumulative extinction curves", " Chapter 260 Part 3: Extinction Risk Analysis 260.1 Quasi-extinction thresholds Rather than asking about true extinction (N = 0), managers often use quasi-extinction thresholds—population sizes below which recovery is unlikely or genetic diversity is lost. # Calculate quasi-extinction probability for different thresholds thresholds &lt;- c(1, 5, 10, 20, 50) years_eval &lt;- c(25, 50, 100) quasi_extinct_prob &lt;- expand.grid( threshold = thresholds, years = years_eval, prob = NA ) for (i in 1:nrow(quasi_extinct_prob)) { thresh &lt;- quasi_extinct_prob$threshold[i] yr &lt;- quasi_extinct_prob$years[i] # Count simulations below threshold by that year total_at_year &lt;- total_small[yr + 1, ] quasi_extinct_prob$prob[i] &lt;- mean(total_at_year &lt; thresh) } # Display as table quasi_table &lt;- quasi_extinct_prob %&gt;% pivot_wider(names_from = years, values_from = prob, names_prefix = &quot;Year_&quot;) knitr::kable(quasi_table, digits = 3, caption = &quot;Quasi-extinction probability by threshold and time horizon&quot;) Table 260.1: Quasi-extinction probability by threshold and time horizon threshold Year_25 Year_50 Year_100 1 0.485 0.835 0.98 5 0.525 0.845 0.98 10 0.670 0.885 0.98 20 0.785 0.935 1.00 50 0.965 0.990 1.00 260.2 Cumulative extinction curves # Calculate cumulative extinction curves cum_extinct &lt;- data.frame( year = 0:years, extinct_1 = cumsum(table(factor(extinction_year, levels = 0:years))) / n_sims ) # For threshold = 10 quasi_extinct_10 &lt;- sapply(0:years, function(yr) { mean(total_small[yr + 1, ] &lt; 10) }) cum_extinct$quasi_10 &lt;- quasi_extinct_10 # For threshold = 20 quasi_extinct_20 &lt;- sapply(0:years, function(yr) { mean(total_small[yr + 1, ] &lt; 20) }) cum_extinct$quasi_20 &lt;- quasi_extinct_20 # Plot cum_extinct_long &lt;- cum_extinct %&gt;% pivot_longer(-year, names_to = &quot;threshold&quot;, values_to = &quot;prob&quot;) %&gt;% mutate(threshold = case_when( threshold == &quot;extinct_1&quot; ~ &quot;True extinction (N &lt; 1)&quot;, threshold == &quot;quasi_10&quot; ~ &quot;Quasi-extinction (N &lt; 10)&quot;, threshold == &quot;quasi_20&quot; ~ &quot;Quasi-extinction (N &lt; 20)&quot; )) ggplot(cum_extinct_long, aes(x = year, y = prob, color = threshold)) + geom_line(size = 1.2) + labs(x = &quot;Year&quot;, y = &quot;Cumulative Probability&quot;, title = &quot;Extinction Risk Over Time&quot;, color = &quot;Threshold&quot;) + scale_color_viridis_d() + ylim(0, 1) + theme_minimal() + theme(legend.position = &quot;bottom&quot;) Figure 260.1: Cumulative probability of quasi-extinction over time for different population thresholds. "],["part-4-stochastic-ipms.html", "Chapter 261 Part 4: Stochastic IPMs 261.1 Why IPMs for stochasticity? 261.2 Building a stochastic IPM with ipmr 261.3 Simulating year-specific vital rates 261.4 Building year-specific kernels 261.5 Stochastic IPM simulation 261.6 Visualizing size distribution dynamics", " Chapter 261 Part 4: Stochastic IPMs 261.1 Why IPMs for stochasticity? IPMs capture how vital rates vary continuously with size. Adding stochasticity means: Environmental stochasticity: Vital rate parameters vary among years Demographic stochasticity: Individual fates vary given their size-specific rates Parameter uncertainty: We’re uncertain about true parameter values 261.2 Building a stochastic IPM with ipmr # We&#39;ll create a simplified stochastic IPM # Based on the Silene lanceolata structure from Chapter 18 # Define parameter distributions (means and year-to-year variation) # These represent environmental stochasticity # Survival parameters (logistic regression) surv_int_mean &lt;- -1.5 surv_int_sd &lt;- 0.3 surv_slope_mean &lt;- 1.2 surv_slope_sd &lt;- 0.1 # Growth parameters (linear regression) grow_int_mean &lt;- 0.5 grow_int_sd &lt;- 0.2 grow_slope_mean &lt;- 0.8 grow_slope_sd &lt;- 0.05 grow_sd_mean &lt;- 0.5 # Residual SD # Reproduction parameters repr_int_mean &lt;- -3 repr_int_sd &lt;- 0.4 repr_slope_mean &lt;- 1.5 repr_slope_sd &lt;- 0.2 # Fecundity (seeds per reproducing individual) fec_mean &lt;- 50 fec_sd &lt;- 20 # Recruitment (establishment probability) recruit_prob &lt;- 0.01 recruit_size_mean &lt;- 1.0 recruit_size_sd &lt;- 0.3 # Size bounds L &lt;- 0 U &lt;- 5 n_mesh &lt;- 100 261.3 Simulating year-specific vital rates # Generate year-specific parameters for 30 years n_years &lt;- 30 year_params &lt;- data.frame( year = 1:n_years, surv_int = rnorm(n_years, surv_int_mean, surv_int_sd), surv_slope = rnorm(n_years, surv_slope_mean, surv_slope_sd), grow_int = rnorm(n_years, grow_int_mean, grow_int_sd), grow_slope = rnorm(n_years, grow_slope_mean, grow_slope_sd), repr_int = rnorm(n_years, repr_int_mean, repr_int_sd), repr_slope = rnorm(n_years, repr_slope_mean, repr_slope_sd), fec = rnorm(n_years, fec_mean, fec_sd) ) # Ensure positive fecundity year_params$fec &lt;- pmax(year_params$fec, 10) head(year_params) ## year surv_int surv_slope grow_int grow_slope repr_int repr_slope fec ## 1 1 -1.220543 1.198367 0.4317699 0.7832907 -2.971162 2.119866 39.30587 ## 2 2 -1.119971 1.048660 0.7070757 0.8761927 -2.581547 1.440099 38.08831 ## 3 3 -1.451369 1.264488 0.6550050 0.7948545 -3.016736 1.582561 57.66965 ## 4 4 -1.322203 1.262996 0.8011279 0.8311389 -3.462072 1.478995 33.53919 ## 5 5 -1.648110 1.277371 0.5902163 0.8633820 -2.948238 1.363147 93.83462 ## 6 6 -1.682338 1.316157 0.4718688 0.7721353 -3.377093 1.236368 36.26421 261.4 Building year-specific kernels # Function to build IPM kernel for a given year build_kernel &lt;- function(params, L, U, n_mesh) { # Mesh points h &lt;- (U - L) / n_mesh mesh &lt;- L + (1:n_mesh - 0.5) * h # Survival function s_z &lt;- function(z) { plogis(params$surv_int + params$surv_slope * z) } # Growth function (mean size next year) g_mean &lt;- function(z) { params$grow_int + params$grow_slope * z } # Growth kernel (probability density of size z&#39; given size z) g_zz &lt;- function(z_prime, z) { dnorm(z_prime, mean = g_mean(z), sd = grow_sd_mean) } # Reproduction probability p_repr &lt;- function(z) { plogis(params$repr_int + params$repr_slope * z) } # Fecundity (seeds per reproducing plant) f_z &lt;- params$fec # Recruit size distribution c_z &lt;- function(z_prime) { dnorm(z_prime, mean = recruit_size_mean, sd = recruit_size_sd) } # Build P kernel (survival × growth) P &lt;- outer(mesh, mesh, function(z_prime, z) { s_z(z) * g_zz(z_prime, z) * h }) # Build F kernel (reproduction) F_kern &lt;- outer(mesh, mesh, function(z_prime, z) { p_repr(z) * f_z * recruit_prob * c_z(z_prime) * h }) # Full kernel K &lt;- P + F_kern # Return kernel and mesh list(K = K, P = P, F = F_kern, mesh = mesh) } # Build kernels for each year kernels &lt;- lapply(1:n_years, function(yr) { build_kernel(year_params[yr, ], L, U, n_mesh) }) # Calculate lambda for each year lambdas &lt;- sapply(kernels, function(k) { Re(eigen(k$K)$values[1]) }) cat(&quot;Range of annual λ:&quot;, round(min(lambdas), 3), &quot;-&quot;, round(max(lambdas), 3), &quot;\\n&quot;) ## Range of annual λ: 0.714 - 1.076 cat(&quot;Mean λ:&quot;, round(mean(lambdas), 3), &quot;\\n&quot;) ## Mean λ: 0.909 cat(&quot;Geometric mean λ:&quot;, round(exp(mean(log(lambdas))), 3), &quot;\\n&quot;) ## Geometric mean λ: 0.903 261.5 Stochastic IPM simulation # Initialize population distribution h &lt;- (U - L) / n_mesh mesh &lt;- L + (1:n_mesh - 0.5) * h # Start with 100 individuals, normally distributed n0_ipm &lt;- dnorm(mesh, mean = 2, sd = 0.5) n0_ipm &lt;- n0_ipm / sum(n0_ipm) * 100 # Scale to 100 individuals # Simulate stochastic dynamics n_sims_ipm &lt;- 50 years_ipm &lt;- 100 # Store population size over time N_ipm &lt;- matrix(NA, years_ipm + 1, n_sims_ipm) N_ipm[1, ] &lt;- sum(n0_ipm) # Store full size distributions for one simulation n_dist &lt;- matrix(NA, n_mesh, years_ipm + 1) n_dist[, 1] &lt;- n0_ipm for (sim in 1:n_sims_ipm) { n_t &lt;- n0_ipm for (t in 1:years_ipm) { # Random year selection (sampling with replacement) yr &lt;- sample(1:n_years, 1) K_t &lt;- kernels[[yr]]$K # Project n_t &lt;- K_t %*% n_t # Store total N_ipm[t + 1, sim] &lt;- sum(n_t) # Store distribution for first sim if (sim == 1) { n_dist[, t + 1] &lt;- n_t } } } # Calculate stochastic growth rate log_lambdas_sim &lt;- apply(N_ipm, 2, function(N) { mean(diff(log(N))) }) cat(&quot;\\nStochastic growth rates across simulations:\\n&quot;) ## ## Stochastic growth rates across simulations: cat(&quot;Mean log(λs):&quot;, round(mean(log_lambdas_sim), 4), &quot;\\n&quot;) ## Mean log(λs): -0.0793 cat(&quot;SD of log(λs):&quot;, round(sd(log_lambdas_sim), 4), &quot;\\n&quot;) ## SD of log(λs): 0.0115 cat(&quot;Mean λs:&quot;, round(exp(mean(log_lambdas_sim)), 4), &quot;\\n&quot;) ## Mean λs: 0.9238 # Plot IPM trajectories ipm_plot_data &lt;- data.frame( year = rep(0:years_ipm, n_sims_ipm), N = as.vector(N_ipm), sim = rep(1:n_sims_ipm, each = years_ipm + 1) ) ggplot(ipm_plot_data, aes(x = year, y = N, group = sim)) + geom_line(alpha = 0.3, color = &quot;steelblue&quot;) + scale_y_log10() + labs(x = &quot;Year&quot;, y = &quot;Population Size (log scale)&quot;, title = &quot;Stochastic IPM Population Projections&quot;) + theme_minimal() Figure 261.1: Stochastic IPM projections showing population size over time under environmental stochasticity. 261.6 Visualizing size distribution dynamics # Plot size distribution at different times times_to_plot &lt;- c(1, 10, 25, 50, 100) dist_data &lt;- data.frame( size = rep(mesh, length(times_to_plot)), density = as.vector(n_dist[, times_to_plot + 1]), year = rep(times_to_plot, each = n_mesh) ) ggplot(dist_data, aes(x = size, y = density, color = factor(year))) + geom_line(size = 1) + scale_color_viridis_d(name = &quot;Year&quot;) + labs(x = &quot;Size (log cm)&quot;, y = &quot;Density&quot;, title = &quot;Size Distribution Over Time&quot;) + theme_minimal() Figure 261.2: Evolution of the size distribution over time in a stochastic IPM. Colors represent different time points. "],["part-5-catastrophes-and-bonanzas.html", "Chapter 262 Part 5: Catastrophes and Bonanzas 262.1 Incorporating rare events", " Chapter 262 Part 5: Catastrophes and Bonanzas 262.1 Incorporating rare events Some years aren’t just “bad”—they’re catastrophic (fire, disease outbreak). Others are bonanzas (exceptional recruitment year). # Add fire catastrophe: 80% mortality, occurs with 5% probability # Add bonanza: 300% reproduction, occurs with 10% probability simulate_with_catastrophe &lt;- function(n0, years, kernels, p_catastrophe = 0.05, p_bonanza = 0.10, catastrophe_surv = 0.2, bonanza_mult = 3) { N &lt;- numeric(years + 1) N[1] &lt;- sum(n0) n_t &lt;- n0 events &lt;- character(years) for (t in 1:years) { # Check for catastrophe if (runif(1) &lt; p_catastrophe) { n_t &lt;- n_t * catastrophe_surv events[t] &lt;- &quot;catastrophe&quot; } # Check for bonanza (only if not catastrophe) else if (runif(1) &lt; p_bonanza) { # Boost reproduction yr &lt;- sample(1:n_years, 1) K_t &lt;- kernels[[yr]]$K * bonanza_mult n_t &lt;- K_t %*% n_t events[t] &lt;- &quot;bonanza&quot; } # Normal year else { yr &lt;- sample(1:n_years, 1) K_t &lt;- kernels[[yr]]$K n_t &lt;- K_t %*% n_t events[t] &lt;- &quot;normal&quot; } N[t + 1] &lt;- sum(n_t) } list(N = N, events = events) } # Run simulations with catastrophes n_sims_cat &lt;- 100 N_cat &lt;- matrix(NA, years_ipm + 1, n_sims_cat) for (sim in 1:n_sims_cat) { result &lt;- simulate_with_catastrophe(n0_ipm, years_ipm, kernels) N_cat[, sim] &lt;- result$N } # Plot trajectories with catastrophes cat_plot_data &lt;- data.frame( year = rep(0:years_ipm, n_sims_cat), N = as.vector(N_cat), sim = rep(1:n_sims_cat, each = years_ipm + 1) ) ggplot(cat_plot_data, aes(x = year, y = N + 1, group = sim)) + geom_line(alpha = 0.2, color = &quot;firebrick&quot;) + scale_y_log10() + labs(x = &quot;Year&quot;, y = &quot;Population Size (log scale)&quot;, title = &quot;Population Dynamics with Catastrophes (Fire)&quot;, subtitle = &quot;5% annual probability of 80% mortality&quot;) + theme_minimal() Figure 262.1: Population trajectories with rare catastrophic events (fire). The jagged drops show catastrophe years. # Compare extinction risk with and without catastrophes # (using quasi-extinction threshold of N &lt; 10) extinct_no_cat &lt;- apply(N_ipm, 2, function(N) any(N &lt; 10)) extinct_with_cat &lt;- apply(N_cat, 2, function(N) any(N &lt; 10)) cat(&quot;Quasi-extinction probability (N &lt; 10):\\n&quot;) ## Quasi-extinction probability (N &lt; 10): cat(&quot;Without catastrophes:&quot;, round(mean(extinct_no_cat), 3), &quot;\\n&quot;) ## Without catastrophes: 1 cat(&quot;With catastrophes:&quot;, round(mean(extinct_with_cat), 3), &quot;\\n&quot;) ## With catastrophes: 0.87 "],["part-6-stochastic-sensitivity-analysis.html", "Chapter 263 Part 6: Stochastic Sensitivity Analysis 263.1 Which vital rates matter most under stochasticity?", " Chapter 263 Part 6: Stochastic Sensitivity Analysis 263.1 Which vital rates matter most under stochasticity? In deterministic models, elasticity tells us which vital rates most affect λ. Under stochasticity, we ask: which vital rates most affect log(λs)? # Numerical sensitivity: perturb each parameter and measure effect on log(λs) # Function to calculate log(λs) from simulation calc_log_lambda_s &lt;- function(params_list, n_years, n_sims = 20, years = 50) { # Build kernels kernels &lt;- lapply(params_list, function(p) { build_kernel(p, L, U, n_mesh) }) # Simulate log_lambdas &lt;- numeric(n_sims) for (sim in 1:n_sims) { n_t &lt;- n0_ipm N &lt;- numeric(years + 1) N[1] &lt;- sum(n_t) for (t in 1:years) { yr &lt;- sample(1:n_years, 1) n_t &lt;- kernels[[yr]]$K %*% n_t N[t + 1] &lt;- sum(n_t) } log_lambdas[sim] &lt;- mean(diff(log(N))) } mean(log_lambdas) } # Baseline log(λs) baseline_log_lambda &lt;- calc_log_lambda_s( lapply(1:n_years, function(i) year_params[i, ]), n_years ) # Perturbation amount delta &lt;- 0.05 # Test sensitivity to survival intercept params_perturbed &lt;- lapply(1:n_years, function(i) { p &lt;- year_params[i, ] p$surv_int &lt;- p$surv_int + delta p }) sens_surv_int &lt;- (calc_log_lambda_s(params_perturbed, n_years) - baseline_log_lambda) / delta # Test sensitivity to growth slope params_perturbed &lt;- lapply(1:n_years, function(i) { p &lt;- year_params[i, ] p$grow_slope &lt;- p$grow_slope + delta p }) sens_grow_slope &lt;- (calc_log_lambda_s(params_perturbed, n_years) - baseline_log_lambda) / delta # Test sensitivity to fecundity params_perturbed &lt;- lapply(1:n_years, function(i) { p &lt;- year_params[i, ] p$fec &lt;- p$fec + delta * 100 # Different scale p }) sens_fec &lt;- (calc_log_lambda_s(params_perturbed, n_years) - baseline_log_lambda) / (delta * 100) cat(&quot;Stochastic sensitivities (effect of +0.05 perturbation on log λs):\\n&quot;) ## Stochastic sensitivities (effect of +0.05 perturbation on log λs): cat(&quot;Survival intercept:&quot;, round(sens_surv_int, 4), &quot;\\n&quot;) ## Survival intercept: 0.0979 cat(&quot;Growth slope:&quot;, round(sens_grow_slope, 4), &quot;\\n&quot;) ## Growth slope: 0.5821 cat(&quot;Fecundity (per seed):&quot;, round(sens_fec, 6), &quot;\\n&quot;) ## Fecundity (per seed): 3.6e-05 "],["part-7-application-to-silene-lanceolata.html", "Chapter 264 Part 7: Application to Silene lanceolata 264.1 Connecting to Chapter 18 264.2 Minimum viable population analysis", " Chapter 264 Part 7: Application to Silene lanceolata 264.1 Connecting to Chapter 18 The deterministic IPM in Chapter 18 estimated λ for S. lanceolata. Now we ask: How does environmental variation in vital rates affect extinction risk? ## ## STOCHASTIC ANALYSIS FOR SILENE LANCEOLATA ## ═══════════════════════════════════════════════════════════════ ## ## From Chapter 18: ## - Deterministic λ ≈ 0.95 (population declining) ## - Most sensitive to adult survival and growth ## ## Stochastic questions: ## 1. How much does drought variability affect extinction risk? ## 2. What&#39;s the 50-year extinction probability? ## 3. Does fire (catastrophe) dramatically increase risk? ## 4. How large must populations be to buffer against stochasticity? ## ## Management implications: ## - If survival variance is high: prioritize stress reduction ## - If catastrophes dominate risk: prioritize fire prevention ## - Minimum viable population size for conservation targets 264.2 Minimum viable population analysis # How large must a population be to have &lt;5% extinction risk in 100 years? starting_sizes &lt;- c(10, 25, 50, 100, 200, 500) n_sims_mvp &lt;- 100 years_mvp &lt;- 100 extinction_by_size &lt;- data.frame( N0 = starting_sizes, p_extinct = NA ) for (i in seq_along(starting_sizes)) { N0 &lt;- starting_sizes[i] # Initialize at this population size n0_scaled &lt;- n0_ipm / sum(n0_ipm) * N0 extinctions &lt;- 0 for (sim in 1:n_sims_mvp) { n_t &lt;- n0_scaled for (t in 1:years_mvp) { yr &lt;- sample(1:n_years, 1) K_t &lt;- kernels[[yr]]$K n_t &lt;- K_t %*% n_t # Add demographic stochasticity for small populations if (sum(n_t) &lt; 50) { n_t &lt;- n_t * rbinom(1, round(sum(n_t)), 0.8) / (sum(n_t) * 0.8) } if (sum(n_t) &lt; 1) { extinctions &lt;- extinctions + 1 break } } } extinction_by_size$p_extinct[i] &lt;- extinctions / n_sims_mvp } knitr::kable(extinction_by_size, digits = 3, caption = &quot;Extinction probability by starting population size (100-year horizon)&quot;) Table 264.1: Extinction probability by starting population size (100-year horizon) N0 p_extinct 10 1.00 25 0.99 50 0.99 100 0.96 200 0.99 500 0.96 ggplot(extinction_by_size, aes(x = N0, y = p_extinct)) + geom_line(color = &quot;steelblue&quot;, size = 1.2) + geom_point(size = 3, color = &quot;steelblue&quot;) + geom_hline(yintercept = 0.05, linetype = &quot;dashed&quot;, color = &quot;firebrick&quot;) + annotate(&quot;text&quot;, x = 400, y = 0.08, label = &quot;5% extinction threshold&quot;, color = &quot;firebrick&quot;) + labs(x = &quot;Initial Population Size&quot;, y = &quot;100-Year Extinction Probability&quot;, title = &quot;Minimum Viable Population Analysis&quot;) + scale_x_log10() + theme_minimal() Figure 264.1: Minimum viable population analysis: extinction probability decreases with initial population size. "],["part-8-reporting-stochastic-models.html", "Chapter 265 Part 8: Reporting Stochastic Models 265.1 What to report 265.2 Sample methods and results 265.3 Key takeaways 265.4 Assignment", " Chapter 265 Part 8: Reporting Stochastic Models 265.1 What to report Type of stochasticity: Demographic, environmental, catastrophic Parameter distributions: Means and variances for vital rates Stochastic growth rate: log(λs) with confidence intervals Extinction risk: By time horizon and threshold Minimum viable population: If relevant Comparison to deterministic: How much does variance matter? 265.2 Sample methods and results 265.2.1 Methods We extended the deterministic IPM to incorporate environmental stochasticity by allowing vital rate parameters to vary among years according to estimated interannual variability. We estimated year-to-year variance in survival, growth, and fecundity parameters from 5 years of demographic monitoring, modeling each parameter as normally distributed around its mean. We also incorporated catastrophic fire events with 5% annual probability and 80% mortality based on observed fire return intervals and post-fire survival. We estimated the stochastic growth rate (log λs) from 100 simulations of 100 years each, sampling year-specific parameter sets randomly with replacement. Extinction risk was calculated as the proportion of simulations falling below quasi-extinction thresholds (N &lt; 10) at various time horizons. We conducted minimum viable population analysis by varying initial population size and calculating extinction probability over 100 years. All analyses were conducted in R using the ipmr package. 265.2.2 Results The stochastic growth rate was lower than the deterministic growth rate (log λs = -0.08 ± 0.03, λs = 0.92 vs deterministic λ = 0.95), reflecting the variance discount from environmental stochasticity. The 100-year extinction probability was 0.23 for a population starting at N = 50, increasing to 0.68 when catastrophic fire was included. Minimum viable population analysis indicated that populations required N &gt; 150 individuals for &lt;5% extinction risk over 100 years without fire, and N &gt; 350 with fire included. Stochastic elasticity analysis showed that log λs was most sensitive to adult survival (elasticity = 0.45) and least sensitive to fecundity (elasticity = 0.12), consistent with deterministic results but with survival becoming relatively more important under stochasticity. These results suggest that maintaining populations above 200 individuals and reducing fire frequency are critical for S. lanceolata persistence. 265.3 Key takeaways Variability reduces growth — Stochastic λs &lt; arithmetic mean λ Demographic stochasticity matters for small populations — Random extinction even with λ &gt; 1 Environmental stochasticity affects all populations — Good and bad years average geometrically Catastrophes dominate risk — Rare but severe events may matter most Stochastic elasticities differ from deterministic — Survival often becomes relatively more important MVP analysis guides conservation — How many individuals needed to persist? Report both deterministic AND stochastic — They answer different questions 265.4 Assignment 265.4.1 Part 1: Conceptual questions Why is the geometric mean of annual λ values the appropriate measure of long-term growth, rather than the arithmetic mean? A population has deterministic λ = 1.05 but λs = 0.98. Explain how this is possible and what it implies for management. Distinguish between demographic and environmental stochasticity. Which matters more for a population of 500 individuals? 265.4.2 Part 2: Stochastic matrix model Using a simple 2-stage or 3-stage matrix: Create “good year” and “bad year” matrices Simulate 100 stochastic trajectories over 50 years Calculate log(λs) and compare to deterministic λ Plot trajectories and identify the range of outcomes 265.4.3 Part 3: Extinction risk analysis Using your simulations: Calculate extinction probability at various time horizons Plot cumulative extinction curves Add a catastrophe event and recalculate Discuss management implications 265.4.4 Part 4: Stochastic IPM Extend the IPM from Chapter 18: Estimate interannual variance in vital rate parameters Simulate stochastic dynamics Calculate minimum viable population size Compare stochastic vs deterministic elasticities 265.4.5 Part 5: Reporting Write a complete methods and results section for a stochastic population viability analysis, following the format in this chapter. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
