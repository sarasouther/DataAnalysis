n_mesh <- length(size_classes)
P <- obs_ipm$sub_kernels$P
F <- obs_ipm$sub_kernels$F
K <- P + F
stopifnot(nrow(K) == n_mesh, ncol(K) == n_mesh)
#==============================================================
# TRANSIENT DYNAMICS (LOG-SCALE STATE VARIABLE)
#==============================================================
#--------------------------------------------------------------
> # 1) Extract mesh + kernels (use ipmr domain directly)
#==============================================================
# TRANSIENT DYNAMICS (LOG-SCALE STATE VARIABLE)
#==============================================================
#--------------------------------------------------------------
# 1) Extract mesh + kernels (ROBUST to ipmr storage details)
#--------------------------------------------------------------
P <- obs_ipm$sub_kernels$P
F <- obs_ipm$sub_kernels$F
K <- P + F
# The discretized matrix dimension is the truth
n_mesh <- nrow(K)
# ---- Try to recover the mesh points in a way that matches K ----
# 1) Best case: row/col names are the mesh points
size_classes <- suppressWarnings(as.numeric(rownames(K)))
# 2) If rownames aren't numeric, try colnames
if (length(size_classes) != n_mesh || anyNA(size_classes)) {
size_classes <- suppressWarnings(as.numeric(colnames(K)))
}
# 3) If still not valid, pull from ipmr environment (sa_1)
if (length(size_classes) != n_mesh || anyNA(size_classes)) {
size_classes <- NULL
if (!is.null(obs_ipm$env_list$main_env) &&
exists("sa_1", envir = obs_ipm$env_list$main_env, inherits = FALSE)) {
sa_1_full <- get("sa_1", envir = obs_ipm$env_list$main_env)
cand <- sort(unique(as.numeric(sa_1_full)))
if (length(cand) == n_mesh) size_classes <- cand
}
}
# 4) Last resort: reconstruct evenly spaced mesh from your domain bounds L, U
#    (works as long as you still have L and U in your workspace)
if (is.null(size_classes) || length(size_classes) != n_mesh || anyNA(size_classes)) {
if (!exists("L", inherits = TRUE) || !exists("U", inherits = TRUE)) {
stop("Could not recover mesh points. Ensure L and U exist, or that K has rownames/colnames.")
}
size_classes <- seq(L, U, length.out = n_mesh)
}
# Final sanity checks
stopifnot(nrow(K) == n_mesh, ncol(K) == n_mesh, length(size_classes) == n_mesh)
cat("Mesh points:", n_mesh, "\n")
cat("Domain (log):", round(range(size_classes), 3), "\n")
#--------------------------------------------------------------
# 2) Initial population vector from observed sizes (LOG SCALE)
#    IMPORTANT: bins need to match the mesh discretization
#--------------------------------------------------------------
observed_sizes_log <- final_data$size_log[is.finite(final_data$size_log)]
# Build bin edges as midpoints between mesh points (plus endpoints)
# This aligns a histogram with the discretized state space
edges <- c(
size_classes[1] - 0.5 * (size_classes[2] - size_classes[1]),
(size_classes[-1] + size_classes[-n_mesh]) / 2,
size_classes[n_mesh] + 0.5 * (size_classes[n_mesh] - size_classes[n_mesh - 1])
)
hist_data <- hist(observed_sizes_log, breaks = edges, plot = FALSE)
n0 <- as.numeric(hist_data$counts)
if (sum(n0) == 0) stop("Initial vector n0 has all zeros (no finite size_log values).")
n0 <- n0 / sum(n0)  # normalize to proportions
cat("Initial population vector:\n")
cat("  Sum:", round(sum(n0), 6), "\n")
cat("  Non-zero bins:", sum(n0 > 0), "\n")
#--------------------------------------------------------------
# 3) Asymptotic properties (dominant eigenvalue/eigenvector)
#--------------------------------------------------------------
eig <- eigen(K)
lambda_asymptotic <- max(Re(eig$values))
dom_idx <- which.max(Re(eig$values))
w <- Re(eig$vectors[, dom_idx])
w[w < 0] <- 0
if (sum(w) <= 0) {
# fallback: absolute value then normalize
w <- abs(Re(eig$vectors[, dom_idx]))
}
w <- w / sum(w)
cat("\nAsymptotic lambda:", round(lambda_asymptotic, 4), "\n")
#--------------------------------------------------------------
# 4) One-step transient dynamics
#--------------------------------------------------------------
n1 <- as.numeric(K %*% n0)
lambda_1 <- sum(n1) / sum(n0)     # since sum(n0)=1, this is just sum(n1)
n1_normalized <- n1 / sum(n1)
deviation <- n1_normalized - n0
cat("\nOne-step dynamics:\n")
cat("  Lambda_1 (from observed n0):", round(lambda_1, 4), "\n")
cat("  Max deviation (|n1 - n0|):", round(max(abs(deviation)), 4), "\n")
#--------------------------------------------------------------
# 5) Reactivity + inertia
#    - "Observed one-step amplification" = lambda_1
#    - "Worst-case one-step amplification bound" for total abundance:
#         max(colSums(K)) for nonnegative K under L1 norm
#    - Inertia: compare total N(t) to lambda^t scaling
#--------------------------------------------------------------
# Worst-case one-step amplification (upper bound under L1)
reactivity_bound <- max(colSums(K))
# Inertia via repeated multiplication (avoid %^%)
T <- 50L
nt <- n0
for (tt in seq_len(T)) nt <- as.numeric(K %*% nt)
inertia <- sum(nt) / (lambda_asymptotic^T * sum(n0))
cat("\nTransient metrics:\n")
cat("  Reactivity (observed, 1-step):", round(lambda_1, 4), "\n")
cat("  Reactivity (worst-case bound):", round(reactivity_bound, 4), "\n")
cat("  Inertia (T = ", T, "): ", round(inertia, 4), "\n", sep = "")
#--------------------------------------------------------------
# 6) Summary table + export
#--------------------------------------------------------------
transient_metrics <- data.frame(
Transition          = trans_years,
Lambda_1_observed   = round(lambda_1, 4),
Reactivity_bound    = round(reactivity_bound, 4),
Inertia_T50         = round(inertia, 4),
Lambda_asymptotic   = round(lambda_asymptotic, 4),
T_used              = T
)
print(transient_metrics)
dir.create("IPM_outputs", showWarnings = FALSE)
write.csv(
transient_metrics,
file = paste0("IPM_outputs/", species_name, "_", trans_years, "_transient_metrics.csv"),
row.names = FALSE
)
#--------------------------------------------------------------
# 7) Visualization: deviation after one step
#--------------------------------------------------------------
deviation_df <- data.frame(
size_log     = size_classes,
size_raw_cm  = exp(size_classes),
deviation    = deviation,
initial      = n0,
final        = n1_normalized
)
library(ggplot2)
p_dev <- ggplot(deviation_df, aes(x = size_log, y = deviation, fill = deviation > 0)) +
geom_col(color = "black", linewidth = 0.2) +
geom_hline(yintercept = 0, linetype = "dashed") +
scale_fill_manual(values = c("TRUE" = "#3B82F6", "FALSE" = "#EF4444"), guide = "none") +
labs(
title = "Deviation from stable size distribution after one time step",
subtitle = paste0("Observed 1-step growth (λ1) = ", round(lambda_1, 3)),
x = "Size class (log scale)",
y = "Final − Initial proportion"
) +
theme_minimal(base_size = 14)
print(p_dev)
ggsave(
filename = paste0("IPM_outputs/", species_name, "_", trans_years, "_transient_deviation.png"),
plot = p_dev, width = 8, height = 5, dpi = 300
)
transient_metrics
#-------------------------------------------------
# Sensitivity and Elasticity of λ (ALIGNED WITH IPM)
#-------------------------------------------------
# Combine kernels
K <- obs_ipm$sub_kernels$P + obs_ipm$sub_kernels$F
# Eigen decomposition
eig <- eigen(K)
ord <- order(Re(eig$values), decreasing = TRUE)
lambda_dom <- Re(eig$values[ord[1]])
# Stable size distribution (right eigenvector)
w <- Re(eig$vectors[, ord[1]])
w <- w / sum(w)
# Reproductive value (left eigenvector)
eig_left <- eigen(t(K))
v <- Re(eig_left$vectors[, ord[1]])
v <- v / sum(v * w)  # normalize so v·w = 1
# Sensitivity and elasticity matrices
sensitivity <- outer(v, w)
elasticity  <- (K / lambda_dom) * sensitivity
#-------------------------------------------------
# Prepare data for plotting (USE IPM DOMAIN)
#-------------------------------------------------
# Use exact IPM mesh (LOG SCALE)
size_midpoints <- as.numeric(obs_ipm$domains$sa)
n_size <- length(size_midpoints)
sens_df <- expand.grid(
from_size = size_midpoints,
to_size   = size_midpoints
)
sens_df$value <- as.vector(sensitivity)
#-------------------------------------------------
# Sensitivity and Elasticity of λ (ROBUST VERSION)
#-------------------------------------------------
# Combine kernels
K <- obs_ipm$sub_kernels$P + obs_ipm$sub_kernels$F
n_size <- nrow(K)
# Eigen decomposition
eig <- eigen(K)
ord <- order(Re(eig$values), decreasing = TRUE)
lambda_dom <- Re(eig$values[ord[1]])
# Right eigenvector (stable distribution)
w <- Re(eig$vectors[, ord[1]])
w[w < 0] <- 0
w <- w / sum(w)
# Left eigenvector (reproductive value)
eig_left <- eigen(t(K))
v <- Re(eig_left$vectors[, ord[1]])
v <- v / sum(v * w)   # normalize so v · w = 1
# Sensitivity and elasticity
sensitivity <- outer(v, w)
elasticity  <- (K / lambda_dom) * sensitivity
#-------------------------------------------------
# Prepare plotting data (INDEX-ALIGNED)
#-------------------------------------------------
# Use indices or safe mesh extraction
size_log <- as.numeric(obs_ipm$env_list$main_env$sa_1)[seq_len(n_size)]
size_raw <- exp(size_log)
sens_df <- expand.grid(
from_size = size_log,
to_size   = size_log
)
sens_df$value <- as.vector(sensitivity)
elas_df <- expand.grid(
from_size = size_log,
to_size   = size_log
)
elas_df$value <- as.vector(elasticity)
row_df <- data.frame(
size_log = size_log,
size_raw_cm = size_raw,
elasticity_sum = rowSums(elasticity)
)
#-------------------------------------------------
# Plots
#-------------------------------------------------
elas_plot <- ggplot(elas_df, aes(from_size, to_size, fill = value)) +
geom_tile() +
scale_fill_viridis_c(name = "Elasticity") +
labs(
x = "Size at time t (log)",
y = "Size at time t+1 (log)",
title = "Elasticity matrix"
) +
theme_pubr()
row_plot <- ggplot(row_df, aes(size_log, elasticity_sum)) +
geom_line(linewidth = 1.1) +
labs(
x = "Size at time t (log scale)",
y = "Sum of elasticities",
title = "Contribution of size classes to λ"
) +
theme_pubr()
ggarrange(elas_plot, row_plot, labels = c("A", "B"), ncol = 2)
#-------------------------------------------------
# Sensitivity and Elasticity of λ
#-------------------------------------------------
# Sensitivity: ∂λ / ∂K_ij
# Elasticity: proportional sensitivity = (K_ij / λ) * sensitivity
#
# Row sums of elasticity:
#   Contribution of individuals of a given size at time t
# Column sums:
#   Contribution of individuals entering a given size at time t+1
# Row sums are most commonly interpreted and plotted.
#-------------------------------------------------
# Combine kernels
K <- obs_ipm$sub_kernels$P + obs_ipm$sub_kernels$F
n_size <- nrow(K)
# Eigen decomposition
eig <- eigen(K)
ord <- order(Re(eig$values), decreasing = TRUE)
lambda_dom <- Re(eig$values[ord[1]])
# Right eigenvector (stable distribution)
w <- Re(eig$vectors[, ord[1]])
w[w < 0] <- 0
w <- w / sum(w)
# Left eigenvector (reproductive value)
eig_left <- eigen(t(K))
v <- Re(eig_left$vectors[, ord[1]])
v <- v / sum(v * w)   # normalize so v · w = 1
# Sensitivity and elasticity
sensitivity <- outer(v, w)
elasticity  <- (K / lambda_dom) * sensitivity
#-------------------------------------------------
# Prepare plotting data (INDEX-ALIGNED)
#-------------------------------------------------
# Use indices or safe mesh extraction
size_log <- as.numeric(obs_ipm$env_list$main_env$sa_1)[seq_len(n_size)]
size_raw <- exp(size_log)
sens_df <- expand.grid(
from_size = size_log,
to_size   = size_log
)
sens_df$value <- as.vector(sensitivity)
elas_df <- expand.grid(
from_size = size_log,
to_size   = size_log
)
elas_df$value <- as.vector(elasticity)
row_df <- data.frame(
size_log = size_log,
size_raw_cm = size_raw,
elasticity_sum = rowSums(elasticity)
)
#-------------------------------------------------
# Plots
#-------------------------------------------------
elas_plot <- ggplot(elas_df, aes(from_size, to_size, fill = value)) +
geom_tile() +
scale_fill_viridis_c(name = "Elasticity") +
labs(
x = "Size at time t (log)",
y = "Size at time t+1 (log)",
title = "Elasticity matrix"
) +
theme_pubr()
row_plot <- ggplot(row_df, aes(size_log, elasticity_sum)) +
geom_line(linewidth = 1.1) +
labs(
x = "Size at time t (log scale)",
y = "Sum of elasticities",
title = "Contribution of size classes to λ"
) +
theme_pubr()
ggarrange(elas_plot, row_plot, labels = c("A", "B"), ncol = 2)
combined_plot <- ggarrange(elas_plot, row_plot, labels = c("A", "B"), ncol = 2)
print(combined_plot)
#==============================================================
# VITAL RATE UNCERTAINTY (LOG-SCALE STATE VARIABLE)
#==============================================================
library(tidyverse)
library(ggpubr)
library(viridis)
#--------------------------------------------------------------
# Helper
#--------------------------------------------------------------
safe_boot_glm <- function(formula, data, family) {
tryCatch(
glm(formula, data = data, family = family),
error = function(e) NULL
)
}
#--------------------------------------------------------------
# Prediction grid (LOG SCALE)
#--------------------------------------------------------------
size_seq_log <- seq(
min(final_data$size_log, na.rm = TRUE),
max(final_data$size_log, na.rm = TRUE),
length.out = 100
)
n_boot <- 100
set.seed(42)
#==============================================================
# SURVIVAL
#==============================================================
boot_preds <- matrix(NA, length(size_seq_log), n_boot)
for (i in seq_len(n_boot)) {
boot_data <- final_data[sample(nrow(final_data), replace = TRUE), ]
mod <- safe_boot_glm(surv ~ size_log, boot_data, binomial())
if (!is.null(mod)) {
boot_preds[, i] <- predict(
mod,
newdata = data.frame(size_log = size_seq_log),
type = "response"
)
}
}
boot_df_surv <- tibble(
size_log = size_seq_log,
size_cm  = exp(size_seq_log),
median = apply(boot_preds, 1, median, na.rm = TRUE),
lower  = apply(boot_preds, 1, quantile, 0.025, na.rm = TRUE),
upper  = apply(boot_preds, 1, quantile, 0.975, na.rm = TRUE)
)
surv_uncertainty_plot <- ggplot() +
geom_point(data = final_data,
aes(x = size_log, y = surv),
alpha = 0.3) +
geom_line(data = boot_df_surv,
aes(x = size_log, y = median),
linewidth = 1) +
geom_ribbon(data = boot_df_surv,
aes(x = size_log, ymin = lower, ymax = upper),
alpha = 0.35, fill = "skyblue") +
labs(x = "Size (log cm)",
y = "Survival probability") +
theme_pubr(base_size = 16)
#==============================================================
# FLOWERING PROBABILITY
#==============================================================
boot_preds <- matrix(NA, length(size_seq_log), n_boot)
for (i in seq_len(n_boot)) {
boot_data <- final_data[sample(nrow(final_data), replace = TRUE), ]
mod <- safe_boot_glm(
fec1 ~ size_log + I(size_log^2),
boot_data,
binomial()
)
if (!is.null(mod)) {
boot_preds[, i] <- predict(
mod,
newdata = data.frame(size_log = size_seq_log),
type = "response"
)
}
}
boot_df_fec1 <- tibble(
size_log = size_seq_log,
size_cm  = exp(size_seq_log),
median = apply(boot_preds, 1, median, na.rm = TRUE),
lower  = apply(boot_preds, 1, quantile, 0.025, na.rm = TRUE),
upper  = apply(boot_preds, 1, quantile, 0.975, na.rm = TRUE)
)
fec1_uncertainty_plot <- ggplot() +
geom_point(data = final_data,
aes(x = size_log, y = fec1),
alpha = 0.3) +
geom_line(data = boot_df_fec1,
aes(x = size_log, y = median),
linewidth = 1) +
geom_ribbon(data = boot_df_fec1,
aes(x = size_log, ymin = lower, ymax = upper),
alpha = 0.35, fill = "plum") +
labs(x = "Size (log cm)",
y = "Flowering probability") +
theme_pubr(base_size = 16)
#==============================================================
# SEED PRODUCTION
#==============================================================
boot_preds <- matrix(NA, length(size_seq_log), n_boot)
for (i in seq_len(n_boot)) {
boot_data <- final_data[sample(nrow(final_data), replace = TRUE), ]
mod <- safe_boot_glm(
fec2 ~ size_log + I(size_log^2),
boot_data,
poisson()
)
if (!is.null(mod)) {
boot_preds[, i] <- predict(
mod,
newdata = data.frame(size_log = size_seq_log),
type = "response"
)
}
}
boot_df_fec2 <- tibble(
size_log = size_seq_log,
size_cm  = exp(size_seq_log),
median = apply(boot_preds, 1, median, na.rm = TRUE),
lower  = apply(boot_preds, 1, quantile, 0.025, na.rm = TRUE),
upper  = apply(boot_preds, 1, quantile, 0.975, na.rm = TRUE)
)
fec2_uncertainty_plot <- ggplot() +
geom_point(data = final_data,
aes(x = size_log, y = fec2),
alpha = 0.3) +
geom_line(data = boot_df_fec2,
aes(x = size_log, y = median),
linewidth = 1) +
geom_ribbon(data = boot_df_fec2,
aes(x = size_log, ymin = lower, ymax = upper),
alpha = 0.35, fill = "goldenrod") +
labs(x = "Size (log cm)",
y = "Seed production") +
theme_pubr(base_size = 16)
#==============================================================
# GROWTH
#==============================================================
boot_preds <- matrix(NA, length(size_seq_log), n_boot)
for (i in seq_len(n_boot)) {
boot_data <- final_data[sample(nrow(final_data), replace = TRUE), ]
mod <- lm(
sizeNext_log ~ size_log + I(size_log^2),
data = boot_data
)
boot_preds[, i] <- predict(
mod,
newdata = data.frame(size_log = size_seq_log)
)
}
boot_df_growth <- tibble(
size_log = size_seq_log,
size_cm  = exp(size_seq_log),
median = apply(boot_preds, 1, median),
lower  = apply(boot_preds, 1, quantile, 0.025),
upper  = apply(boot_preds, 1, quantile, 0.975)
)
growth_uncertainty_plot <- ggplot() +
geom_point(data = final_data,
aes(x = size_log, y = sizeNext_log),
alpha = 0.3) +
geom_line(data = boot_df_growth,
aes(x = size_log, y = median),
linewidth = 1) +
geom_ribbon(data = boot_df_growth,
aes(x = size_log, ymin = lower, ymax = upper),
alpha = 0.35, fill = "darkseagreen3") +
labs(x = "Size (log cm)",
y = "Size at t+1 (log cm)") +
theme_pubr(base_size = 16)
#==============================================================
# COMBINE
#==============================================================
uncertainty_panel <- ggarrange(
surv_uncertainty_plot,
fec1_uncertainty_plot,
fec2_uncertainty_plot,
growth_uncertainty_plot,
labels = c("A", "B", "C", "D"),
ncol = 2, nrow = 2
)
print(uncertainty_panel)
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
bookdown::render_book("index.Rmd")
