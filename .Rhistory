packageVersion("spsurvey")
ls("package:spsurvey")
library(sf)
library(tidyverse)
library(spsurvey)
library(ggplot2)
#------------------------------------------------------------
# 1. Filter sampling frame to Madrean Encinal Woodland
#------------------------------------------------------------
encinal <- sampling_frame %>%
filter(R3ERU == "Madrean Encinal Woodland") %>%
st_transform(5070)
#------------------------------------------------------------
# 2. Define GRTS sampling design
#------------------------------------------------------------
design <- list(
Base = list(
panel = c(Base = 40),
seltype = "Equal",
over = 10
)
)
#------------------------------------------------------------
# 3. Run GRTS sample (correct syntax for your version)
#------------------------------------------------------------
grts_out <- grts(
design      = design,
DesignID    = "GRTS_Encinal",
type.frame  = "area",
src.frame   = "sp.object",
sp.obj      = encinal
)
getAnywhere(grts)
# Set your working directory to the Sampling folder
setwd("/Users/sks379/Desktop/GitHubProjects/DataAnalysis/data/sampling/grts/")
# Set your working directory to the Sampling folder
setwd("/Users/sks379/Desktop/GitHubProjects/DataAnalysis/data/sampling/grts/")
# Zip each folder
zip("ExistingPlots.zip", "legacy_points/")
zip("SamplingArea.zip", "sampling_frame/")
zip("Strata.zip", "strata/")
library(sf)
library(tidyverse)
library(spsurvey)
library(ggplot2)
#============================================================
# PART 1: Load Spatial Data from Google Drive
#============================================================
# Temporary directory to store downloaded data
temp_dir <- tempdir()
# Google Drive direct download URLs
urls <- list(
sampling_area  = "https://drive.google.com/uc?export=download&id=1TOr1BKJpqKYWsGOAPaGrhkql5ovNglqa",
strata         = "https://drive.google.com/uc?export=download&id=1ms4ErDYOFkkB4UFFeGGXmWITa3WE97KS",
existing_plots = "https://drive.google.com/uc?export=download&id=1EfI6AorSrgOS1xmr4LioJeSCp6F1yFfZ"
)
# Download and unzip helper function
download_and_unzip <- function(url, dest, name) {
zip_path <- file.path(dest, paste0(name, ".zip"))
# Download with progress
cat("Downloading", name, "...\n")
download.file(url, zip_path, mode = "wb", quiet = FALSE)
# Unzip
out_dir <- file.path(dest, name)
unzip(zip_path, exdir = out_dir)
cat("✓ Extracted", name, "\n\n")
out_dir
}
# Download and extract all files
sampling_dir <- download_and_unzip(urls$sampling_area, temp_dir, "SamplingArea")
strata_dir   <- download_and_unzip(urls$strata, temp_dir, "Strata")
existing_dir <- download_and_unzip(urls$existing_plots, temp_dir, "ExistingPlots")
# Helper to read shapefile from extracted folder
read_shp <- function(dir) {
shp <- list.files(dir, pattern = "\\.shp$", recursive = TRUE, full.names = TRUE)
if(length(shp) == 0) stop("No .shp file found in ", dir)
st_read(shp[1], quiet = TRUE)
}
# Read spatial data
sampling_frame <- read_shp(sampling_dir)
list.files(sampling_dir, recursive = TRUE)
# Step 1: Check if the zip file actually downloaded
zip_path <- file.path(temp_dir, "SamplingArea.zip")
file.exists(zip_path)  # Should return TRUE
# Step 2: Check the size of the downloaded file
file.info(zip_path)$size  # Should be > 1000 bytes (if it's tiny, it's not the actual file)
# Step 3: Try to see what's in the zip before extracting
zip_list <- unzip(zip_path, list = TRUE)
# Read what's actually in the file
readLines(zip_path, n = 10)
library(sf)
library(tidyverse)
library(httr)
#============================================================
# PART 1: Load Spatial Data from Google Drive
#============================================================
temp_dir <- tempdir()
# Google Drive file IDs
file_ids <- list(
sampling_area  = "1TOr1BKJpqKYWsGOAPaGrhkql5ovNglqa",
strata         = "1ms4ErDYOFkkB4UFFeGGXmWITa3WE97KS",
existing_plots = "1EfI6AorSrgOS1xmr4LioJeSCp6F1yFfZ"
)
# Download function that bypasses Google Drive virus scan
download_from_gdrive <- function(file_id, dest, name) {
zip_path <- file.path(dest, paste0(name, ".zip"))
cat("Downloading", name, "...\n")
# Use the direct download URL that bypasses virus scan
url <- paste0(
"https://drive.usercontent.google.com/download?id=",
file_id,
"&export=download&confirm=t"
)
# Download with httr
GET(
url,
write_disk(zip_path, overwrite = TRUE),
progress()  # Shows download progress
)
# Check file size
size_mb <- round(file.info(zip_path)$size / 1024^2, 2)
cat("✓ Downloaded", name, "(", size_mb, "MB )\n")
# Unzip
out_dir <- file.path(dest, name)
unzip(zip_path, exdir = out_dir)
cat("✓ Extracted", name, "\n\n")
out_dir
}
# Download and extract all files
sampling_dir <- download_from_gdrive(file_ids$sampling_area, temp_dir, "SamplingArea")
strata_dir   <- download_from_gdrive(file_ids$strata, temp_dir, "Strata")
existing_dir <- download_from_gdrive(file_ids$existing_plots, temp_dir, "ExistingPlots")
# Helper to read shapefile from extracted folder
read_shp <- function(dir) {
shp <- list.files(dir, pattern = "\\.shp$", recursive = TRUE, full.names = TRUE)
if(length(shp) == 0) {
cat("\nDirectory contents:\n")
print(list.files(dir, recursive = TRUE))
stop("No .shp file found in ", dir)
}
cat("Reading:", basename(shp[1]), "\n")
st_read(shp[1], quiet = TRUE)
}
# Read spatial data
cat("\n=== Loading Spatial Data ===\n\n")
sampling_frame <- read_shp(sampling_dir)
strata         <- read_shp(strata_dir)
legacy_points  <- read_shp(existing_dir)
# Quick preview
cat("\n=== Data Summary ===\n\n")
cat("Sampling Frame:\n")
print(sampling_frame)
cat("\nStrata:\n")
print(strata)
cat("\nExisting Plots:\n")
print(legacy_points)
#============================================================
# PART 2: GRTS Sampling Design
#============================================================
# Filter sampling frame to Madrean Encinal Woodland
# (Adjust the filter based on your actual column names and values)
encinal <- sampling_frame %>%
filter(R3ERU == "Madrean Encinal Woodland") %>%
st_transform(5070)  # Transform to Albers Equal Area
# Run GRTS sample
# Note: Syntax depends on your spsurvey version
# For spsurvey >= 5.0:
grts_sites <- grts(
sframe = encinal,
n_base = 40,
n_over = 10
)
# For older spsurvey (< 5.0), use:
# design <- list(
#   Base = list(
#     panel = c(Base = 40),
#     seltype = "Equal",
#     over = 10
#   )
# )
# grts_out <- grts(
#   design = design,
#   DesignID = "GRTS_Encinal",
#   type.frame = "area",
#   src.frame = "sp.object",
#   sp.obj = as(encinal, "Spatial")
# )
# grts_sites <- st_as_sf(grts_out)
#============================================================
# PART 3: Visualization
#============================================================
ggplot() +
geom_sf(data = encinal, fill = "forestgreen", alpha = 0.3, color = "darkgreen") +
geom_sf(data = grts_sites %>% filter(siteuse == "Base"),
aes(color = "Base Sites"), size = 3, shape = 19) +
geom_sf(data = grts_sites %>% filter(siteuse == "Over"),
aes(color = "Oversample"), size = 2, shape = 1) +
geom_sf(data = legacy_points, aes(color = "Legacy Plots"),
size = 2, shape = 4, stroke = 1.5) +
scale_color_manual(
values = c("Base Sites" = "gold",
"Oversample" = "orange",
"Legacy Plots" = "blue"),
name = "Site Type"
) +
theme_minimal(base_size = 14) +
labs(
title = "GRTS Probability-Based Sampling in Madrean Encinal Woodland",
subtitle = "40 base samples + 10 oversample sites + existing monitoring plots"
)
packageVersion("spsurvey")
library(spsurvey)
library(ggplot2)
# First check your column names
names(sampling_frame)
# Filter to your study area (adjust column/value as needed)
encinal <- sampling_frame %>%
filter(R3ERU == "Madrean Encinal Woodland") %>%  # Update this to match your data
st_transform(5070)  # Albers Equal Area
# Simple, clean syntax for modern spsurvey
grts_sites <- grts(
sframe = encinal,      # Your sampling frame (sf object)
n_base = 40,          # Number of base sites
n_over = 10           # Number of oversample sites
)
# Check what we got
class(grts_sites)      # Should be "sf"
names(grts_sites)      # See available columns
head(grts_sites)       # Preview the data
ggplot() +
geom_sf(data = encinal, fill = "forestgreen", alpha = 0.3, color = "darkgreen") +
geom_sf(data = grts_sites %>% filter(siteuse == "Base"),
aes(color = "Base Sites"), size = 3, shape = 19) +
geom_sf(data = grts_sites %>% filter(siteuse == "Over"),
aes(color = "Oversample"), size = 2, shape = 1) +
geom_sf(data = legacy_points, aes(color = "Legacy Plots"),
size = 2, shape = 4, stroke = 1.5) +
scale_color_manual(
values = c("Base Sites" = "gold",
"Oversample" = "orange",
"Legacy Plots" = "blue"),
name = "Site Type"
) +
theme_minimal(base_size = 14) +
labs(
title = "GRTS Probability-Based Sampling in Madrean Encinal Woodland",
subtitle = "40 base samples + 10 oversample sites + existing monitoring plots"
)
# Extract the base and oversample sites from the list
base_sites <- grts_sites$sites_base    # 40 base sites
over_sites <- grts_sites$sites_over    # 10 oversample sites
# Check what we extracted
class(base_sites)  # Should be "sf"
class(over_sites)  # Should be "sf"
# Now plot!
ggplot() +
geom_sf(data = encinal, fill = "forestgreen", alpha = 0.3, color = "darkgreen") +
geom_sf(data = base_sites,
aes(color = "Base Sites"), size = 3, shape = 19) +
geom_sf(data = over_sites,
aes(color = "Oversample"), size = 2, shape = 1) +
geom_sf(data = legacy_points, aes(color = "Legacy Plots"),
size = 2, shape = 4, stroke = 1.5) +
scale_color_manual(
values = c("Base Sites" = "gold",
"Oversample" = "orange",
"Legacy Plots" = "blue"),
name = "Site Type"
) +
theme_minimal(base_size = 14) +
labs(
title = "GRTS Probability-Based Sampling in Madrean Encinal Woodland",
subtitle = "40 base samples + 10 oversample sites + existing monitoring plots"
)
# In your R console, run:
unlink(".Rproj.user", recursive = TRUE)
knitr::opts_chunk$set(
echo = TRUE,
cache = FALSE,
warning = FALSE,
message = FALSE,
fig.width = 10,
fig.height = 8,
fig.align = "center"
)
library(sf)
library(tidyverse)
library(spsurvey)
library(ggplot2)
library(httr)
# Temporary directory for downloads
temp_dir <- tempdir()
# Google Drive file IDs
file_ids <- list(
sampling_area  = "1TOr1BKJpqKYWsGOAPaGrhkql5ovNglqa",
strata         = "1ms4ErDYOFkkB4UFFeGGXmWITa3WE97KS",
existing_plots = "1EfI6AorSrgOS1xmr4LioJeSCp6F1yFfZ"
)
# Download function (handles Google Drive's virus scan page)
download_from_gdrive <- function(file_id, dest, name) {
zip_path <- file.path(dest, paste0(name, ".zip"))
url <- paste0(
"https://drive.usercontent.google.com/download?id=",
file_id,
"&export=download&confirm=t"
)
GET(url, write_disk(zip_path, overwrite = TRUE), progress())
out_dir <- file.path(dest, name)
unzip(zip_path, exdir = out_dir, quiet = TRUE)
out_dir
}
# Download all datasets
sampling_dir <- download_from_gdrive(file_ids$sampling_area, temp_dir, "SamplingArea")
# Temporary directory for downloads
temp_dir <- tempdir()
# Google Drive file IDs
file_ids <- list(
sampling_area  = "1TOr1BKJpqKYWsGOAPaGrhkql5ovNglqa",
strata         = "1ms4ErDYOFkkB4UFFeGGXmWITa3WE97KS",
existing_plots = "1EfI6AorSrgOS1xmr4LioJeSCp6F1yFfZ"
)
# Download function (handles Google Drive's virus scan page)
download_from_gdrive <- function(file_id, dest, name) {
zip_path <- file.path(dest, paste0(name, ".zip"))
url <- paste0(
"https://drive.usercontent.google.com/download?id=",
file_id,
"&export=download&confirm=t"
)
GET(url, write_disk(zip_path, overwrite = TRUE), progress())
out_dir <- file.path(dest, name)
unzip(zip_path, exdir = out_dir)  # Removed quiet = TRUE
out_dir
}
# Download all datasets
sampling_dir <- download_from_gdrive(file_ids$sampling_area, temp_dir, "SamplingArea")
strata_dir   <- download_from_gdrive(file_ids$strata, temp_dir, "Strata")
existing_dir <- download_from_gdrive(file_ids$existing_plots, temp_dir, "ExistingPlots")
# Helper function to read shapefiles
read_shp <- function(dir) {
shp <- list.files(dir, pattern = "\\.shp$", recursive = TRUE, full.names = TRUE)
st_read(shp[1], quiet = TRUE)
}
# Read the spatial data
sampling_frame <- read_shp(sampling_dir)
strata         <- read_shp(strata_dir)
legacy_points  <- read_shp(existing_dir)
# Check the sampling frame
cat("Sampling Frame contains", nrow(sampling_frame), "features\n")
cat("Available habitat types:", unique(sampling_frame$R3ERU), "\n\n")
# Check existing plots
cat("Number of existing monitoring plots:", nrow(legacy_points), "\n")
encinal <- sampling_frame %>%
filter(R3ERU == "Madrean Encinal Woodland") %>%
st_transform(5070)  # NAD83 / Conus Albers
# Calculate study area
study_area_km2 <- round(as.numeric(st_area(encinal)) / 1e6, 2)
cat("Study area:", study_area_km2, "km²\n")
set.seed(123)  # For reproducibility
grts_sites <- grts(
sframe = encinal,
n_base = 40,
n_over = 10
)
# Extract base and oversample sites
base_sites <- grts_sites$sites_base
over_sites <- grts_sites$sites_over
cat("Generated", nrow(base_sites), "base sites\n")
cat("Generated", nrow(over_sites), "oversample sites\n")
ggplot() +
geom_sf(data = encinal, fill = "forestgreen", alpha = 0.3,
color = "darkgreen", linewidth = 0.5) +
geom_sf(data = base_sites,
aes(color = "Base Sites"), size = 3, shape = 19) +
geom_sf(data = over_sites,
aes(color = "Oversample"), size = 2.5, shape = 1, stroke = 1) +
geom_sf(data = legacy_points,
aes(color = "Legacy Plots"), size = 2.5, shape = 4, stroke = 1.5) +
scale_color_manual(
values = c("Base Sites" = "gold",
"Oversample" = "orange",
"Legacy Plots" = "dodgerblue"),
name = "Site Type"
) +
theme_minimal(base_size = 14) +
theme(legend.position = "bottom") +
labs(
title = "GRTS Sampling Design: Madrean Encinal Woodland",
subtitle = paste0("Study area: ", study_area_km2, " km²")
)
# Create output directory
dir.create("grts_outputs", showWarnings = FALSE)
# Combine all new sample sites
all_sites <- rbind(
base_sites %>% mutate(site_type = "Base"),
over_sites %>% mutate(site_type = "Oversample")
)
# Export for GPS
st_write(all_sites, "grts_outputs/grts_sampling_points.gpx",
driver = "GPX", delete_dsn = TRUE)
#------------------------------------------------------------
# Step 5: Export Data for Field Work
#------------------------------------------------------------
# Create output directory
dir.create("grts_outputs", showWarnings = FALSE)
# Combine all new sample sites
all_sites <- rbind(
base_sites %>% mutate(site_type = "Base"),
over_sites %>% mutate(site_type = "Oversample")
)
#------------------------------------------------------------
# 1. FOR GPS DEVICES - Simplified GPX (just coordinates & names)
#------------------------------------------------------------
# GPX format is strict - only include essential fields
gpx_sites <- all_sites %>%
select(siteID, geometry) %>%
mutate(name = siteID)  # GPX uses "name" field
st_write(gpx_sites, "grts_outputs/grts_sampling_points.gpx",
driver = "GPX", delete_dsn = TRUE)
#------------------------------------------------------------
# Step 5: Export Data for Field Work
#------------------------------------------------------------
# Create output directory
dir.create("grts_outputs", showWarnings = FALSE)
# Combine all new sample sites
all_sites <- rbind(
base_sites %>% mutate(site_type = "Base"),
over_sites %>% mutate(site_type = "Oversample")
)
#------------------------------------------------------------
# 1. FOR GPS DEVICES - Simplified GPX (just coordinates & names)
#------------------------------------------------------------
# GPX format is VERY strict - only name and geometry allowed
gpx_sites <- all_sites %>%
mutate(name = paste0(siteID, " (", site_type, ")")) %>%
select(name, geometry)
st_write(gpx_sites, "grts_outputs/grts_sampling_points.gpx",
driver = "GPX", delete_dsn = TRUE)
#------------------------------------------------------------
# 2. FOR GIS - Full data in Shapefile & GeoJSON
#------------------------------------------------------------
# Shapefile (most universal for GIS)
st_write(all_sites, "grts_outputs/grts_sampling_points.shp",
delete_dsn = TRUE)
# GeoJSON (modern, includes all fields)
st_write(all_sites, "grts_outputs/grts_sampling_points.geojson",
delete_dsn = TRUE)
# KML (alternative to GPX - works in Google Earth & many GPS apps)
kml_sites <- all_sites %>%
mutate(Name = paste0(siteID, " (", site_type, ")"))
st_write(kml_sites %>% select(Name, geometry),
"grts_outputs/grts_sampling_points.kml",
driver = "KML", delete_dsn = TRUE)
#------------------------------------------------------------
# 3. FOR ANALYSIS - Full data in CSV
#------------------------------------------------------------
# Get coordinates
coords_albers <- st_coordinates(all_sites)
# Create comprehensive CSV with all information
sites_csv <- all_sites %>%
st_drop_geometry() %>%
mutate(
X_albers = coords_albers[,1],
Y_albers = coords_albers[,2]
) %>%
select(siteID, site_type, lon_WGS84, lat_WGS84,
X_albers, Y_albers, wgt, ip, R3ERU)
write.csv(sites_csv, "grts_outputs/grts_sampling_points.csv",
row.names = FALSE)
#------------------------------------------------------------
# 4. FIELD DATA SHEET - Blank template for data collection
#------------------------------------------------------------
field_sheet <- sites_csv %>%
select(siteID, site_type, lon_WGS84, lat_WGS84) %>%
mutate(
date_visited = "",
observers = "",
canopy_cover_pct = "",
tree_density = "",
notes = ""
)
write.csv(field_sheet, "grts_outputs/field_data_sheet.csv",
row.names = FALSE)
#------------------------------------------------------------
# 5. SAMPLING SUMMARY
#------------------------------------------------------------
summary_table <- data.frame(
Category = c("Total Sites", "Base Sites", "Oversample Sites",
"Study Area (km²)", "Average Site Weight (km²)"),
Value = c(
nrow(all_sites),
nrow(base_sites),
nrow(over_sites),
round(as.numeric(st_area(encinal)) / 1e6, 2),
round(mean(as.numeric(all_sites$wgt)) / 1e6, 2)
)
)
write.csv(summary_table, "grts_outputs/sampling_summary.csv",
row.names = FALSE)
cat("\n✓ Export complete! Files created:\n")
cat("  - grts_sampling_points.gpx (simplified for GPS)\n")
cat("  - grts_sampling_points.kml (alternative GPS format)\n")
cat("  - grts_sampling_points.shp (for GIS with full data)\n")
cat("  - grts_sampling_points.geojson (modern GIS format)\n")
cat("  - grts_sampling_points.csv (full data for analysis)\n")
cat("  - field_data_sheet.csv (blank template)\n")
cat("  - sampling_summary.csv (design summary)\n")
# Example: First 5 sites and their weights
sites_csv %>%
select(siteID, site_type, wgt) %>%
head(5) %>%
knitr::kable(caption = "Sample sites with design weights")
