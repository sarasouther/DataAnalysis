# Piecewise SEMs

## Overview

This chapter introduces local (piece wise) SEM — an approach that decomposes a full SEM into a series of local linear models. It contrasts with global covariance-based SEM in both assumptions and evaluation. We will compare and contrast the two approaches, then perform an SEM.

## Why Use Piecewise SEM, instead of a covariance SEM?

Structural Equation Modeling (SEM) is a powerful framework for testing hypotheses about complex systems with multiple interrelated variables. In ecology and related fields, researchers often choose between **covariance-based SEM** (e.g., `lavaan`) and **piecewise SEM** (e.g., `piecewiseSEM` package in R). The choice depends on data structure, sample size, distributional assumptions, and model complexity.

### Comparison of SEM Approaches

| Feature                            | Covariance-Based SEM (`lavaan`)                         | Piecewise SEM (`piecewiseSEM`)                            |
|------------------------------------|---------------------------------------------------------|-----------------------------------------------------------|
| **Data assumptions**               | Assumes multivariate normality                          | Robust to non-normal and non-independent data             |
| **Model structure**                | All relationships modeled simultaneously                | Model split into a series of (G)LMs or mixed models       |
| **Fit assessment**                | Global fit indices: χ², RMSEA, CFI, SRMR                | Local fit via D-separation and AIC                        |
| **Handling of missing data**       | Can use FIML or imputation                              | Depends on method used in each submodel                   |
| **Sample size requirements**       | High; often N > 200 recommended                         | More flexible with small N                                |
| **Handling of hierarchical data**  | Difficult to include random effects                     | Allows mixed models with random effects                   |
| **Ease of interpreting indirect effects** | Straightforward via standardized estimates      | Requires manual calculation of indirect effects           |
| **Measurement error modeling**     | Supports latent variables                               | Does not support latent variables                         |
| **Modularity and flexibility**     | Limited to standard SEM frameworks                      | Very flexible, especially for ecological or field data     |

### Benefits of Covariance-Based SEM

- Ideal for testing complex, **theory-driven** models with latent variables.
- Provides **global fit indices** to assess how well the model captures the covariance structure.
- Useful for **confirmatory modeling**, especially in psychology, sociology, and tightly controlled experimental settings.

### Benefits of Piecewise SEM

- **More robust to violations** of normality, independence, and small sample size.
- Supports **hierarchical, nested, or repeated-measures data** using mixed models.
- Offers **modular modeling**, which makes it easier to fit ecologically realistic models (e.g., with random effects, Poisson/binomial responses).
- Better suited to **observational field data** with complex structure.

### When to Use Each Approach

- Use **Covariance SEM (`lavaan`)** when:

  - You have a large sample size and want to model latent variables or measurement error.
  - Your data meet assumptions of multivariate normality and independence.
  - You're interested in global fit metrics and standardized path coefficients.

- Use **Piecewise SEM** when:

  - Your data are **non-normal**, **hierarchical**, or have **small sample sizes**.
  - You need to model **random effects**, **non-Gaussian outcomes**, or **temporal/spatial structure**.
  - You’re focusing on **causal structure**, not measurement error.

### Models differ in approach

Covariance-based SEM estimates all paths simultaneously by fitting a single model to the entire covariance matrix, providing global fit indices like chi-square (χ²) and RMSEA to assess how well the model reproduces the observed data. In contrast, piecewise SEM fits each path as a separate (G)LM or mixed model, and assesses overall model fit using **D-separation tests**, which evaluate whether any unmodeled relationships (i.e., conditional independencies) remain between variables—offering a more flexible, modular approach to complex or non-normal ecological data.

## SEM Example: Resilience to Drought in Arizona 

This dataset combines climate, topographic, and biodiversity indicators to evaluate ecological resilience across sites in Arizona following a period of drought. The central research question is: What environmental and community-level factors predict ecological recovery or resilience in post-drought southwestern ecosystems?

Data were compiled from NOAA climate records and field observations, and are structured at the plot level. Key variables include:

- Climate trends: Change in growing season temperature (gs_temp_slope) and precipitation (ppt_slope)
- Post-recovery climate: Including growing season mean maximum temperature (gs_tmax_mean), total monsoonal precipitation averaged over 2022-2023 (monsoon_ppt_mean), total winter precipitation averageed over 2022-2023 post-recovery time period (winter_ppt_mean)
- Drought indicators: Standardized indices including PET (potential evapotranspiration), DDD (drought duration), and DSI (drought severity index), and SWA (Soil Water Availability)
- Site disturbance history: Time since last fire (TSLF_imputed)	
- Topographic factors: Slope and Aspect
- Community attributes: Taxonomic diversity (Diversity_q1), connectivity (dist_km)
- Recovery outcomes: Compositional dissimilarity (bray_curtis) and a remotely sensed recovery index (Recovery_Index)

As always, a first step is to remove colinear variables from your dataset. When two or more predictors in your model are highly correlated, they share overlapping information about the response variable. This makes it harder for the model to determine which variable is actually responsible for changes in the response. We removed colinear variables in Chapter 11 - Model selection. After finalizing a dataset, we can build a path diagram and and SEM. 

We hypothesize that: 

1) Recovery (here modeled as bray_curtis) will depend on recent climatic trends (temp_slope, ppt_slope), the strength of the drought (PET, DDD, DSI, SWA), post-drought climatic recovery conditions (gs_tmax_mean, monsoon_ppt_mean, winter_ppt_mean), which could be mediated by the location of the community (Slope, Aspect) and affected by disturbance history (TSLF_imputed). Finally, we expect communities that are more diverse (Diversity_q1) and connected (dist_km) to demonstrate higher levels of recovery. 
2) Similarly, overall site diversity (Diversity_q1) will be affected by connectivity (dist_km), as well as recent climatic trends (temp_slope, ppt_slope), the strength of the drought (PET, DDD, DSI, SWA), post-drought climatic recovery conditions (gs_tmax_mean, monsoon_ppt_mean, winter_ppt_mean), which could be mediated by the location of the community (Slope, Aspect) and affected by disturbance history (TSLF_imputed).
3) We expect that the strength of the drought (PET, DDD, DSI, SWA) will depend on its landscape position (Slope, Aspect), as well as recent climate change (temp_slope, ppt_slope).

Go over causal chains, mediators, etc.

Directed Separation (D-sep)

D-sep tests whether missing paths in your model are truly unnecessary. It evaluates conditional independence assumptions implied by the model.

Understanding D-separation
	•	Two variables are D-separated if they are conditionally independent, given a set of other variables.
	•	D-sep claims are testable and form the basis set of the model.

```{r}
# Define DAG
g <- dagitty("dag {
  x -> y2 -> y1
  x -> y1
}")

# View conditional independencies implied by the DAG
impliedConditionalIndependencies(g)
```

Model Fit via Fisher’s C

```{r}
library(piecewiseSEM)

# Simulate example data
set.seed(123)
n <- 100
x <- rnorm(n)
y2 <- 0.5 * x + rnorm(n)
y1 <- 0.6 * x + 0.4 * y2 + rnorm(n)
example_data <- data.frame(x, y1, y2)

# Fit piecewise SEM
mod_list <- psem(
  lm(y2 ~ x, data = example_data),
  lm(y1 ~ x + y2, data = example_data)
)

# ✅ Get model fit statistics (replaces sem.fit)
fisherC(mod_list)

# ✅ Get standardized path coefficients (replaces sem.coefs)
stdCoefs(mod_list)

# ✅ Optional: View DAG
plot(getDAG(mod_list))
```

Interpretation:

	•	p > 0.05 → model is not rejected, D-sep claims hold.
	•	p < 0.05 → model is rejected, missing paths may be important.

Summary:

	•	Piecewise SEM is ideal when:
	•	Data violate global SEM assumptions.
	•	Sample size is too small for global SEM.
	•	You want to understand model fit using D-sep logic.
	•	Use dagitty to derive implied independencies.
	•	Use sem.fit() and sem.coefs() for piecewise evaluation.

### Chapter 12: Introduction to Piecewise SEM in R

Learning Objectives

By the end of this tutorial, you will be able to:

	•	Understand how piecewise SEM differs from traditional covariance-based SEM.
	•	Fit a multi-equation SEM using the psem() function.
	•	Evaluate model fit using d-separation tests and Fisher’s C statistic.
	•	Extract path coefficients and R² values.
	•	Visualize model structure and relationships using visreg, DiagrammeR, or plot().

## Part 1: What is Piecewise SEM?

Piecewise SEM uses a local estimation approach: each equation is estimated separately using standard regression (e.g., lm, lmer). This provides greater flexibility, such as:

	•	Including non-normal or mixed-effect models.
	•	Dealing with small sample sizes or nested structures.
	•	Assessing model fit through d-sep tests (Shipley’s test of directed separation).

Limitations:

	•	Can’t handle latent variables.
	•	Not suitable for cyclical feedback loops.
	•	Difficult to interpret in overidentified models with correlated errors.

Part 2: Load Packages and Data

```{r}
# Load libraries
library(piecewiseSEM)
library(visreg)
library(DiagrammeR)

data(keeley)  # from piecewiseSEM and work through this information 
```

Part 3: Specify and Fit Your Model

```{r}
# Fit individual models
mod1 <- lm(abiotic ~ distance, data = keeley)
mod2 <- lm(hetero ~ distance, data = keeley)
mod3 <- lm(rich ~ abiotic + hetero, data = keeley)

# Combine into a psem object
keeley_sem <- psem(mod1, mod2, mod3)
```

Evaluate Model Fit

```{r}
# Directed separation test
#dSep(keeley_sem)

# Fisher's C statistic
#fisherC(keeley_sem)
```

Interpret:

	•	A non-significant p-value (> 0.05) = model fits.
	•	Significant missing paths → consider adding them and reassessing fit.

Part 5: Summarize Coefficients and R²

```{r}
# Coefficients
coefs(keeley_sem)

# R-squared values
rsquared(keeley_sem)
```

Part 6: Plot Your Model

Option A: Quick Base R Plot

```{r}

keeley_sem <- psem(
  lm(firesev ~ age + cover, data = keeley),
  lm(cover ~ age + elev + firesev, data = keeley),
  data = keeley
)

plot(keeley_sem)

#Option B: Refined Graph with DiagrammeR

# Optional customization
plot(keeley_sem,
     node_attrs = list(
       x = c(2.5, 2.5, 4, 1),
       y = c(3, 1, 2, 2),
       shape = "rectangle",
       fillcolor = "white"
     ))
```

Part 7: Mediation Example

```{r}
mod_firesev  <- lm(firesev ~ age, data = keeley)
mod_cover    <- lm(cover ~ firesev, data = keeley)

firesev_model <- psem(mod_firesev, mod_cover)

summary(firesev_model)
dSep(firesev_model)
rsquared(firesev_model)
```

Bonus: Visualize with Covariates Held Constant

```{r}
# Visualize fire severity's effect on cover
# visreg(firesev_model[[2]], xvar = "firesev")
```

Wrap-Up

Piecewise SEM offers flexibility and clarity for causal inference in ecology. By evaluating each path independently while testing the full model’s coherence through d-sep and Fisher’s C, you gain both transparency and statistical rigor.

### Chapter 13: Nonlinearity and Interaction in SEM

Overview

In this tutorial, we’ll cover:

	•	Nonlinearities (e.g., polynomial terms)
	•	Centering variables to reduce multicollinearity
	•	Interaction terms
	•	Implementing these in SEM frameworks

Example 1: Nonlinear Effects (Cardinale et al. 2009)

Load and Prepare Data

```{r}
# Download data
url <- "https://drive.google.com/uc?export=download&id=1oHBul4_JcqlPFZgYsH3WOIZJRQRw1O4F"
cardinale <- read.csv(url)

# Check it loaded
head(cardinale)

# Log-transform variables
cardinale$logN <- log10(cardinale$N + 1e-6)
cardinale$logN2 <- cardinale$logN^2
cardinale$logChl <- log10(cardinale$Chl)

#Fit SEM with piecewiseSEM

model1 <- psem(
  lm(SA ~ logN + logN2 + SR, data = cardinale),
  lm(logChl ~ SA + logN + logN2, data = cardinale),
  logN %~~% logN2,
  data = cardinale
)

summary(model1)
```

Reducing Collinearity via Centering

```{r}
# Center predictors
cardinale$logN.cen <- scale(cardinale$logN, scale = FALSE)
cardinale$logN2.cen <- cardinale$logN.cen^2

# Check correlation
cor(cardinale$logN.cen, cardinale$logN2.cen)
```

Refit Model with Centered Predictors

```{r}
model2 <- psem(
  lm(SA ~ logN.cen + logN2.cen + SR, data = cardinale),
  lm(logChl ~ SA + logN.cen + logN2.cen, data = cardinale),
  logN.cen %~~% logN2.cen,
  data = cardinale
)

summary(model2)
```

Try lavaan for the Same Model

Example 2: Interaction Effects (Keeley et al.)

Center and Create Interaction

```{r}

url2 <- "https://drive.google.com/uc?export=download&id=1YTsFP1T__Hn13hTvj9TVOK-wbGDxLd01"

# Try to read the CSV directly
keeley <- read.csv(url2)

keeley$age_cent <- scale(keeley$age, scale = FALSE)
keeley$fire_cent <- scale(keeley$firesev, scale = FALSE)
keeley$int_term <- keeley$age_cent * keeley$fire_cent
```

Fit SEM with Interaction in piecewiseSEM

```{r}
keeley_int <- psem(
  lm(cover ~ age_cent * fire_cent, data = keeley),
  lm(fire_cent ~ age_cent, data = keeley),
  data = keeley
)

summary(keeley_int)
```

Optional: Fit Interaction SEM with lavaan

Final Notes:

	•	Polynomial terms allow us to model curvature.
	•	Centering reduces collinearity and changes interpretation.
	•	Interaction terms help model conditional effects.
	•	Both lavaan and piecewiseSEM support these techniques.

### Chapter 14: GLMs with SEM using PiecewiseSEM

This section integrates Generalized Linear Models (GLMs) into Structural Equation Modeling, especially using piecewiseSEM. It also introduces important adjustments for working with non-normal data, and compares latent theoretic (LT) and observed error (OE) approaches for standardizing coefficients.

## Overview

In this tutorial, we’ll:

	•	Learn how to incorporate GLMs into SEM using piecewiseSEM
	•	Understand how to deal with non-normality and directed separation warnings
	•	Compute standardized coefficients using both Latent Theoretic (LT) and Observed Error (OE) approaches

Data from Anderson et al. 2010 Example

```{r}
# Simulated structure to mimic Anderson et al.
set.seed(42)
anderson <- data.frame(
  biomass.kg = rnorm(100, mean = 10, sd = 2),
  leafN = rnorm(100, mean = 3, sd = 0.5),
  landscape = sample(0:1, 100, replace = TRUE),
  hotspotYN = rbinom(100, 1, 0.4)
)
```

Model: Using GLM in psem

```{r}
library(piecewiseSEM)

anderson.sem <- psem(
  lm(leafN ~ biomass.kg, data = anderson),
  glm(hotspotYN ~ leafN + biomass.kg + landscape, family = "binomial", data = anderson)
)

summary(anderson.sem)
```

Directed Separation & Non-Normality

```{r}
# Add the 'conserve = TRUE' argument to be conservative in tests
summary(anderson.sem, conserve = TRUE)
```

If the model includes non-normal endogenous variables (e.g., binary hotspotYN), the direction of independence tests matters. Use:

```{r}
dSep(anderson.sem, direction = c("hotspotYN <- leafN"))
```

Or specify a correlated error structure:

```{r}
anderson.sem2 <- update(anderson.sem, hotspotYN %~~% leafN)
dSep(anderson.sem2)
```

Standardizing Coefficients

Latent Theoretic (LT) Approach

```{r}
anderson.glm <- anderson.sem[[2]]
Betas <- coefs(anderson.sem)[2:4, 3]  # GLM coefficients
preds <- predict(anderson.glm, type = "link")

sd.y.LT <- sqrt(var(preds) + pi^2/3)
sd.x <- sapply(anderson[, c("leafN", "biomass.kg", "landscape")], sd)

Betas.LT <- Betas * sd.x / sd.y.LT
Betas.LT
```

Observed Error (OE) Approach

```{r}
preds_response <- predict(anderson.glm, type = "response")
R <- cor(anderson$hotspotYN, preds_response)

sd.y.OE <- sqrt(var(preds_response)) / R
Betas.OE <- Betas * sd.x / sd.y.OE
Betas.OE
```

Compare LT vs OE Approaches

```{r}
# Indirect effect: leafN → hotspotYN (through biomass.kg)
Beta.leafN <- coefs(anderson.sem)$Std.Estimate[1]
indirect_LT <- Beta.leafN * Betas.LT[1]
indirect_OE <- Beta.leafN * Betas.OE[1]

c(LT = indirect_LT, OE = indirect_OE)
```

Summary:

	•	Use glm() in psem() for binary or count outcomes.
	•	Add conserve = TRUE for directed separation when variables are non-normal.
	•	Use both LT and OE standardization to interpret effect sizes.

### Chapter 15: Categorical Predictors & Multigroup SEM

```{r}
# library(lme4)
# library(piecewiseSEM)
# library(emmeans)
# library(lavaan)
```

Overview

In this tutorial, we explore:

	•	Using categorical predictors in SEM.
	•	Accounting for random effects (e.g., genotype).
	•	Conducting multigroup SEM across different contexts or study sites.

1. Categorical Predictors and Random Effects

Example: Bowen et al. (2017) tested whether Phragmites genotype affects soil microbes and productivity.

```{r}

library(multcompView)

# Simulated structure: Genotype nested within Phragmites status (e.g., native, invasive)
set.seed(1)
n <- 90
bowen <- data.frame(
  status = factor(rep(c("native", "invasive", "introduced"), each = 30)),
  Genotype = rep(paste0("G", 1:9), each = 10),
  observed_otus = rnorm(n, mean = 2500, sd = 100),
  RNA.DNA = rnorm(n, 0.7, 0.05),
  below.C = rnorm(n, 43, 1),
  abovebiomass_g = rnorm(n, 2, 0.5)
)

# Mixed models for each component
div_mod <- lmer(observed_otus ~ status + (1 | Genotype), data = bowen)
activity_mod <- lmer(RNA.DNA ~ status + observed_otus + (1 | Genotype), data = bowen)
carbon_mod <- lmer(below.C ~ observed_otus + status + (1 | Genotype), data = bowen)
biomass_mod <- lmer(abovebiomass_g ~ RNA.DNA + observed_otus + below.C + status + (1 | Genotype), data = bowen)

# Build piecewise SEM
bowen_mod <- psem(div_mod, activity_mod, carbon_mod, biomass_mod, data = bowen)
summary(bowen_mod)
```

2. Visualizing Categorical Effects

Estimated Marginal Means by status:

```{r}
lapply(bowen_mod[-length(bowen_mod)], emmeans, specs = ~status)

# Post-hoc Tests (Tukey)

generic_tukey <- function(x) emmeans(x, list(pairwise ~ status))
lapply(bowen_mod[-length(bowen_mod)], generic_tukey)
```

3. Multigroup SEM in lavaan

Fit the same SEM model to different groups and test whether path coefficients differ.

```{r}
# Create simulated dataset
group_df <- data.frame(
  site = rep(c("A", "B"), each = 50),
  x = rnorm(100),
  m = rnorm(100),
  y = rnorm(100)
)

# Define a simple SEM model
sem_model <- '
  m ~ a*x
  y ~ b*m + c*x
'

# Fit multi-group SEM
fit_multi <- lavaan::sem(sem_model, data = group_df, group = "site")

# View summary
summary(fit_multi, fit.measures = TRUE, standardized = TRUE)
```

4. Constraining Parameters Across Groups

You can test whether coefficients differ across groups:

A significant p-value means the unconstrained model fits better — i.e., group differences do matter.

Summary:

	•	Use lme4 and piecewiseSEM for models with random effects.
	•	lavaan enables multigroup comparisons to test generality across systems.
	•	Post-hoc tools like emmeans help interpret categorical predictors.

### Chapter 16: Multigroup SEM in R

```{r}
library(lavaan)
```

What is Multigroup SEM?

Multigroup SEM allows you to:
	•	Test whether path coefficients differ between groups.
	•	Assess whether a model holds equally well across groups.
	•	Investigate measurement invariance (i.e., whether constructs are perceived similarly).

Example Model Setup

We’ll build a simple mediation model where group is a binary factor ("A" vs "B").

```{r}
# Simulate data
set.seed(123)
n <- 100
group <- rep(c("A", "B"), each = n)
x <- rnorm(2 * n)
m <- 0.5 * x + rnorm(2 * n)
y <- 0.6 * m + 0.3 * x + rnorm(2 * n)
data <- data.frame(group, x, m, y)
```

Define the SEM

```{r}
model <- '
  m ~ a*x
  y ~ b*m + c*x
'
```

Fit Multigroup SEM

```{r}
fit_multi <- sem(model, data = data, group = "group")
summary(fit_multi, fit.measures = TRUE, standardized = TRUE)
```

Test for Invariance

To test for invariance, you can constrain parameters to be equal across groups.

```{r}
# Constrain 'a' and 'b' to be equal across groups
model_constrained <- '
  m ~ c(a, a)*x
  y ~ c(b, b)*m + c*x
'

fit_constrained <- sem(model_constrained, data = data, group = "group")
anova(fit_multi, fit_constrained)  # Chi-square test for invariance
```

Interpretation:

	•	If the constrained model does not significantly worsen fit, the paths a and b are likely invariant.
	•	If the fit gets significantly worse, the relationship differs between groups and should be modeled separately.

Visualizing Standardized Results

```{r}
# library(semPlot)
# semPaths(fit_multi, "std", layout = "tree", whatLabels = "std", edge.label.cex = 1.2)
```

Summary

Multigroup SEM lets you:

	•	Evaluate moderation by group.
	•	Test for measurement invariance.
	•	Gain deeper insight into context-dependent pathways.

### Chapter 17: 

Mixed models in piecewise SEM, covering:

	•	Fixed vs. random effects
	•	Adding group-level predictors
	•	Understanding R² and model comparison
	•	Dealing with hierarchical structure and sample size

Introduction

This tutorial introduces how to incorporate mixed effects into Structural Equation Modeling using the piecewiseSEM package. Mixed models are essential when your data are hierarchically structured (e.g., plots within sites, streams within watersheds).

Step 1: Load Data and Create Variables

```{r}
# Log-transform predictors
cardinale$logN <- log10(cardinale$N + 1e-6)
cardinale$logN2 <- cardinale$logN^2
cardinale$logChl <- log10(cardinale$Chl)

# Centering predictors to reduce multicollinearity
cardinale$logN.cen <- scale(cardinale$logN, scale = FALSE)
cardinale$logN2.cen <- scale(cardinale$logN^2, scale = FALSE)
```

Step 2: Fit Fixed Effects SEM

```{r}
cardinale.sem <- psem(
  lm(SA ~ logN.cen + logN2.cen + SR, data = cardinale),
  lm(logChl ~ SA + logN.cen + logN2.cen, data = cardinale),
  logN.cen %~~% logN2.cen,
  data = cardinale
)

summary(cardinale.sem)
```
Step 3: Add Random Effects with lme()

```{r}
#cardinale.mixed <- psem(
#  lme(SA ~ logN.cen + logN2.cen + SR, random = ~1 | Stream, data = cardinale),
#  lme(logChl ~ SA + logN.cen + logN2.cen, random = ~1 | Stream, data = cardinale),
#  logN.cen %~~% logN2.cen,
#  data = cardinale
#)

#summary(cardinale.mixed)
```

Step 4: Compare Models

```{r}
# Compare R-squared
#rsquared(cardinale.sem)
#rsquared(cardinale.mixed)
```

	•	Marginal R²: Variance explained by fixed effects.
	•	Conditional R²: Variance explained by both fixed and random effects.

Step 5: Optional — Add Group-Level Predictors

To address possible Simpson’s paradox or correlated random effects:

```{r}
# Example if "site_mean" were available
# cardinale$stream_mean_logN <- ave(cardinale$logN, cardinale$Stream)
# cardinale$deviation_logN <- cardinale$logN - cardinale$stream_mean_logN
```

This lets you include:

	•	stream_mean_logN (group-level predictor)
	•	deviation_logN (individual-level deviation)

Step 6: Basis Set and Fisher’s C

```{r}
#fisherC(cardinale.mixed)
```
Summary:

	•	Mixed models let you account for non-independence among groups.
	•	Use piecewiseSEM with lme() to include random effects.
	•	Be cautious of group-level confounding — use centering and group-level predictors.
	•	Use rsquared() and fisherC() for model comparison and goodness-of-fit.
	
## Example with Powerline Data
	
```{r}
library(readr)

# Read CSV directly into R
powerline_plants <- read.csv("https://drive.google.com/uc?export=download&id=1MVlaEshEn7M2g8rOiJ8fWPPozn3WgNck")

```

We are starting with our original DAG (below):

```{r}
library(dagitty)
library(ggdag)
library(tidyverse)


# Define the DAG
ivm_dag <- dagitty("
dag {
  Treatment
  Soil_Substrate
  Cattle
  Plant_Richness
  Plant_Cover
  Plant_Height
  Ceanothus
  Woody_Debris
  Pollinator_Richness
  Pollinator_Abundance

  Treatment -> Plant_Richness
  Treatment -> Plant_Cover
  Treatment -> Plant_Height
  Treatment -> Ceanothus
  Treatment -> Woody_Debris

  Soil_Substrate -> Treatment
  Soil_Substrate -> Plant_Richness
  Soil_Substrate -> Plant_Cover
  Soil_Substrate -> Woody_Debris

  Cattle -> Plant_Richness
  Cattle -> Plant_Cover
  Cattle -> Pollinator_Abundance
  Cattle -> Pollinator_Richness

  Plant_Richness -> Pollinator_Richness
  Plant_Richness -> Pollinator_Abundance
  Plant_Cover -> Pollinator_Richness
  Plant_Cover -> Pollinator_Abundance
  Plant_Height -> Pollinator_Richness
  Plant_Height -> Pollinator_Abundance
  Ceanothus -> Pollinator_Richness
  Ceanothus -> Pollinator_Abundance
  Woody_Debris -> Pollinator_Richness
  Woody_Debris -> Pollinator_Abundance
}
")

node_roles <- tibble(
  name = c(
    "Treatment",
    "Soil_Substrate",
    "Cattle",
    "Plant_Richness",
    "Plant_Cover",
    "Plant_Height",
    "Ceanothus",
    "Woody_Debris",
    "Pollinator_Richness",
    "Pollinator_Abundance"
  ),
  role = c(
    "Treatment",
    "Context",
    "Context",
    "Plant",
    "Plant",
    "Plant",
    "Plant",
    "Plant",
    "Pollinator",
    "Pollinator"
  )
)

# Get layout and merge roles
dag_df <- tidy_dagitty(ivm_dag, layout = "nicely") %>%
  left_join(node_roles, by = "name")

# Plot using ggplot2 with corrected edge structure
ggplot() +
  geom_segment(
    data = dag_df %>% filter(!is.na(xend)),
    aes(x = x, y = y, xend = xend, yend = yend),
    arrow = arrow(length = unit(0.02, "npc")),
    color = "grey40"
  ) +
  geom_point(
    data = dag_df %>% filter(!is.na(x)), 
    aes(x = x, y = y, color = role), 
    size = 8, alpha = 0.85
  ) +
  geom_text(
    data = dag_df %>% filter(!is.na(x)), 
    aes(x = x, y = y, label = name), 
    color = "black", size = 4
  ) +
  scale_color_manual(values = c(
    "Treatment" = "#FB7E21FF",
    "Context" = "#A91601FF",
    "Plant" = "#18DDC2FF",
    "Pollinator" = "#00468BFF"
  )) +
  labs(
    title = "Hypothesized DAG for Pollinator Response to IVM Treatment",
    color = "Variable Type"
  ) +
  theme_void()
```

Then, we need to create linear models, treating each “node” as its own regression model. First, we have to summarize our data in a way that can be input into the linear models.

```{r}
library(piecewiseSEM)

# List all variables used across all models
sem_vars <- c(
  "Species_Richness", "Treatment", "Q_Substrate_BareSoil",
  "Plot_DungCount", "Q_Substrate_PerennialVeg", "Height_cm",
  "Q_Substrate_WoodyDebris"
)

# Remove all rows with NA in any of the SEM variables
sem_data <- powerline_plants %>%
  dplyr::select(all_of(sem_vars)) %>%
  na.omit()

# Ensure Treatment is a factor
sem_data$Treatment <- as.factor(sem_data$Treatment)

# Check structure to confirm
str(sem_data)

sem_data$Treatment <- droplevels(as.factor(sem_data$Treatment))

# Fit models using this cleaned dataset
mod1 <- lm(Species_Richness ~ Treatment + Q_Substrate_BareSoil + Plot_DungCount, data = sem_data)
mod2 <- lm(Q_Substrate_PerennialVeg ~ Species_Richness + Treatment + Q_Substrate_BareSoil + Plot_DungCount, data = sem_data)
mod3 <- lm(Height_cm ~ Q_Substrate_PerennialVeg + Treatment, data = sem_data)
mod4 <- lm(Q_Substrate_WoodyDebris ~ Height_cm + Treatment + Q_Substrate_BareSoil, data = sem_data)
# You can now also safely fit mod6 and mod7 using the same sem_data:
# mod6 <- lm(Pollinator_Richness ~ Species_Richness + Q_Substrate_PerennialVeg + Height_cm + Ceanothus + Q_Substrate_WoodyDebris + Plot_DungCount, data = sem_data)
# mod7 <- lm(Pollinator_Abundance ~ Species_Richness + Q_Substrate_PerennialVeg + Height_cm + Ceanothus + Q_Substrate_WoodyDebris + Plot_DungCount, data = sem_data)

# Assemble SEM
mods <- list(mod1, mod2, mod3, mod4)
lapply(mods, function(m) summary(m)$coefficients)

# Rebuild the SEM with standardized estimates
sem_model <- psem(mod1, mod2, mod3, mod4)

# Summary with standardization explicitly requested
summary(sem_model, standardize = "scale")

```

